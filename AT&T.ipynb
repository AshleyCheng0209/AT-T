{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 作業三: 分類器的原理與評比實驗\n",
    "## 資料來源: \n",
    "## 來自 AT&T 10 個人的人臉影像共 400 張，每張大小 64×64\n",
    "## 目標: \n",
    "## 計畫執行這篇講義描述的分類器比較，即採用三種分類器分別對三組資料進行分類學習與測試。其中分類器包括： \n",
    "## 1.多元羅吉斯回歸 2.支援向量機 3.神經網路\n",
    "## 影像資料處理: \n",
    "## 1.原始資料 2.進行PCA主成分分析\n",
    "## 分類方法: \n",
    "- ## Logistic Regression\n",
    "- ## SVM\n",
    "- ## Neural Network\n",
    "### 姓名: 鄭欣莉\n",
    "### 學號: 410877039"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 標準化後原始資料 + 羅吉斯迴歸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv('face_data.csv')\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "face_data = np.array(X) #400X4096\n",
    "test_size = 0.30 \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size)\n",
    "#標準化\n",
    "scalar = StandardScaler()\n",
    "X_train_ = scalar.fit_transform(X_train)\n",
    "X_test_ = scalar.fit_transform(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- solver = 'lbfgs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clf_LR(solver):\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.metrics import classification_report\n",
    "    opts = dict(tol = 1e-6, max_iter = int(1e6), verbose = 1)\n",
    "    clf_LR = LogisticRegression(solver = solver, **opts)\n",
    "    clf_LR.fit(X_train_,y_train)\n",
    "    y_pred = clf_LR.predict(X_test_)\n",
    "    print(f\"{clf_LR.score(X_test_, y_test):.2%}\\n\")\n",
    "    print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95.83%\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         3\n",
      "           1       1.00      1.00      1.00         2\n",
      "           2       1.00      1.00      1.00         3\n",
      "           3       1.00      1.00      1.00         4\n",
      "           4       1.00      1.00      1.00         2\n",
      "           5       1.00      1.00      1.00         4\n",
      "           6       1.00      1.00      1.00         3\n",
      "           7       1.00      0.60      0.75         5\n",
      "           8       1.00      1.00      1.00         2\n",
      "           9       0.50      1.00      0.67         2\n",
      "          10       1.00      1.00      1.00         4\n",
      "          11       1.00      1.00      1.00         3\n",
      "          12       1.00      1.00      1.00         1\n",
      "          13       1.00      1.00      1.00         2\n",
      "          14       1.00      1.00      1.00         3\n",
      "          15       1.00      1.00      1.00         3\n",
      "          16       1.00      0.60      0.75         5\n",
      "          17       1.00      1.00      1.00         5\n",
      "          18       1.00      1.00      1.00         1\n",
      "          19       1.00      1.00      1.00         4\n",
      "          20       1.00      1.00      1.00         3\n",
      "          21       1.00      1.00      1.00         3\n",
      "          22       1.00      1.00      1.00         4\n",
      "          23       1.00      1.00      1.00         5\n",
      "          24       0.00      0.00      0.00         0\n",
      "          25       0.50      1.00      0.67         2\n",
      "          26       1.00      1.00      1.00         3\n",
      "          27       1.00      1.00      1.00         2\n",
      "          28       1.00      1.00      1.00         5\n",
      "          29       1.00      1.00      1.00         3\n",
      "          30       1.00      1.00      1.00         1\n",
      "          31       1.00      1.00      1.00         2\n",
      "          32       1.00      1.00      1.00         3\n",
      "          33       1.00      1.00      1.00         2\n",
      "          34       1.00      1.00      1.00         3\n",
      "          35       1.00      1.00      1.00         2\n",
      "          36       1.00      1.00      1.00         5\n",
      "          37       1.00      1.00      1.00         3\n",
      "          38       1.00      1.00      1.00         4\n",
      "          39       1.00      0.75      0.86         4\n",
      "\n",
      "    accuracy                           0.96       120\n",
      "   macro avg       0.95      0.95      0.94       120\n",
      "weighted avg       0.98      0.96      0.96       120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   33.5s finished\n",
      "c:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "clf_LR(solver = 'lbfgs')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- solver = 'liblinear'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]95.83%\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         3\n",
      "           1       1.00      1.00      1.00         2\n",
      "           2       1.00      1.00      1.00         3\n",
      "           3       1.00      1.00      1.00         4\n",
      "           4       1.00      1.00      1.00         2\n",
      "           5       1.00      1.00      1.00         4\n",
      "           6       1.00      1.00      1.00         3\n",
      "           7       1.00      1.00      1.00         5\n",
      "           8       0.50      1.00      0.67         2\n",
      "           9       1.00      1.00      1.00         2\n",
      "          10       1.00      1.00      1.00         4\n",
      "          11       1.00      1.00      1.00         3\n",
      "          12       0.00      0.00      0.00         1\n",
      "          13       1.00      1.00      1.00         2\n",
      "          14       1.00      1.00      1.00         3\n",
      "          15       1.00      1.00      1.00         3\n",
      "          16       1.00      0.80      0.89         5\n",
      "          17       1.00      1.00      1.00         5\n",
      "          18       1.00      1.00      1.00         1\n",
      "          19       1.00      1.00      1.00         4\n",
      "          20       1.00      1.00      1.00         3\n",
      "          21       1.00      1.00      1.00         3\n",
      "          22       1.00      1.00      1.00         4\n",
      "          23       1.00      0.80      0.89         5\n",
      "          25       1.00      1.00      1.00         2\n",
      "          26       1.00      1.00      1.00         3\n",
      "          27       1.00      1.00      1.00         2\n",
      "          28       1.00      1.00      1.00         5\n",
      "          29       1.00      1.00      1.00         3\n",
      "          30       0.50      1.00      0.67         1\n",
      "          31       1.00      1.00      1.00         2\n",
      "          32       1.00      1.00      1.00         3\n",
      "          33       1.00      1.00      1.00         2\n",
      "          34       0.75      1.00      0.86         3\n",
      "          35       1.00      1.00      1.00         2\n",
      "          36       1.00      1.00      1.00         5\n",
      "          37       1.00      1.00      1.00         3\n",
      "          38       0.80      1.00      0.89         4\n",
      "          39       1.00      0.50      0.67         4\n",
      "\n",
      "    accuracy                           0.96       120\n",
      "   macro avg       0.94      0.95      0.94       120\n",
      "weighted avg       0.97      0.96      0.96       120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "clf_LR(solver = 'liblinear')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- solver = 'newton-cg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95.83%\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         3\n",
      "           1       1.00      1.00      1.00         2\n",
      "           2       1.00      1.00      1.00         3\n",
      "           3       1.00      1.00      1.00         4\n",
      "           4       1.00      1.00      1.00         2\n",
      "           5       1.00      1.00      1.00         4\n",
      "           6       1.00      1.00      1.00         3\n",
      "           7       1.00      0.60      0.75         5\n",
      "           8       1.00      1.00      1.00         2\n",
      "           9       0.50      1.00      0.67         2\n",
      "          10       1.00      1.00      1.00         4\n",
      "          11       1.00      1.00      1.00         3\n",
      "          12       1.00      1.00      1.00         1\n",
      "          13       1.00      1.00      1.00         2\n",
      "          14       1.00      1.00      1.00         3\n",
      "          15       1.00      1.00      1.00         3\n",
      "          16       1.00      0.60      0.75         5\n",
      "          17       1.00      1.00      1.00         5\n",
      "          18       1.00      1.00      1.00         1\n",
      "          19       1.00      1.00      1.00         4\n",
      "          20       1.00      1.00      1.00         3\n",
      "          21       1.00      1.00      1.00         3\n",
      "          22       1.00      1.00      1.00         4\n",
      "          23       1.00      1.00      1.00         5\n",
      "          24       0.00      0.00      0.00         0\n",
      "          25       0.50      1.00      0.67         2\n",
      "          26       1.00      1.00      1.00         3\n",
      "          27       1.00      1.00      1.00         2\n",
      "          28       1.00      1.00      1.00         5\n",
      "          29       1.00      1.00      1.00         3\n",
      "          30       1.00      1.00      1.00         1\n",
      "          31       1.00      1.00      1.00         2\n",
      "          32       1.00      1.00      1.00         3\n",
      "          33       1.00      1.00      1.00         2\n",
      "          34       1.00      1.00      1.00         3\n",
      "          35       1.00      1.00      1.00         2\n",
      "          36       1.00      1.00      1.00         5\n",
      "          37       1.00      1.00      1.00         3\n",
      "          38       1.00      1.00      1.00         4\n",
      "          39       1.00      0.75      0.86         4\n",
      "\n",
      "    accuracy                           0.96       120\n",
      "   macro avg       0.95      0.95      0.94       120\n",
      "weighted avg       0.98      0.96      0.96       120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   29.7s finished\n",
      "c:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "clf_LR(solver = 'newton-cg')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 結論:\n",
    "### 1. Classification Report 的衡量指標個別代表的意思:\n",
    "- ### 精確率(Precision) 為預測為真的樣本有幾個預測正確\n",
    "- ### 召回率(Recall) 為事實為真的樣本中有幾個是預測正確的\n",
    "- ### F1 score 為精確率和召回率的調和平均數\n",
    "### 2. 以不同solver的第一筆資料為例:\n",
    "- ### 每個 solver 的精確率、召回率、F1 score 為百分之百\n",
    "### 3. 標準化過後的AT&T資料，各個solver = 'lbfgs' 或 'liblinear' 或 'newton-cg' 表現差不多，準確率均為96%，對於40筆資料來說表現絕對不差。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA主成分分析(成分比例採0.8) + 羅吉斯迴歸"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- solver = 'lbfgs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def PCA_LR(n_components, solver):\n",
    "    pca = PCA(n_components=n_components).fit(X_train_)\n",
    "    Z_train = pca.transform(X_train_)\n",
    "    Z_test = pca.transform(X_test_)\n",
    "    opts = dict(tol = 1e-6, max_iter = int(1e6), verbose = 1)\n",
    "    clf_PCA = LogisticRegression(solver = solver, **opts)\n",
    "    clf_PCA.fit(Z_train, y_train)\n",
    "    y_pred = clf_PCA.predict(Z_test)\n",
    "    print(f\"{clf_PCA.score(Z_test, y_test):.2%}\\n\")\n",
    "    print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92.50%\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.67      0.80         3\n",
      "           1       1.00      1.00      1.00         2\n",
      "           2       0.75      1.00      0.86         3\n",
      "           3       1.00      1.00      1.00         4\n",
      "           4       1.00      1.00      1.00         2\n",
      "           5       1.00      1.00      1.00         4\n",
      "           6       1.00      1.00      1.00         3\n",
      "           7       1.00      0.40      0.57         5\n",
      "           8       1.00      1.00      1.00         2\n",
      "           9       0.50      1.00      0.67         2\n",
      "          10       1.00      1.00      1.00         4\n",
      "          11       1.00      1.00      1.00         3\n",
      "          12       1.00      1.00      1.00         1\n",
      "          13       1.00      1.00      1.00         2\n",
      "          14       1.00      1.00      1.00         3\n",
      "          15       1.00      1.00      1.00         3\n",
      "          16       0.75      0.60      0.67         5\n",
      "          17       1.00      1.00      1.00         5\n",
      "          18       1.00      1.00      1.00         1\n",
      "          19       1.00      1.00      1.00         4\n",
      "          20       1.00      1.00      1.00         3\n",
      "          21       1.00      1.00      1.00         3\n",
      "          22       1.00      1.00      1.00         4\n",
      "          23       1.00      0.80      0.89         5\n",
      "          24       0.00      0.00      0.00         0\n",
      "          25       0.67      1.00      0.80         2\n",
      "          26       1.00      1.00      1.00         3\n",
      "          27       1.00      1.00      1.00         2\n",
      "          28       1.00      1.00      1.00         5\n",
      "          29       0.75      1.00      0.86         3\n",
      "          30       1.00      1.00      1.00         1\n",
      "          31       0.67      1.00      0.80         2\n",
      "          32       1.00      1.00      1.00         3\n",
      "          33       1.00      1.00      1.00         2\n",
      "          34       1.00      1.00      1.00         3\n",
      "          35       1.00      1.00      1.00         2\n",
      "          36       1.00      1.00      1.00         5\n",
      "          37       1.00      1.00      1.00         3\n",
      "          38       1.00      1.00      1.00         4\n",
      "          39       1.00      0.50      0.67         4\n",
      "\n",
      "    accuracy                           0.93       120\n",
      "   macro avg       0.93      0.92      0.91       120\n",
      "weighted avg       0.96      0.93      0.93       120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.8s finished\n",
      "c:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "PCA_LR(n_components = 0.8, solver = 'lbfgs')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- solver = 'liblinear'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]93.33%\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         3\n",
      "           1       0.67      1.00      0.80         2\n",
      "           2       1.00      1.00      1.00         3\n",
      "           3       0.75      0.75      0.75         4\n",
      "           4       0.67      1.00      0.80         2\n",
      "           5       1.00      1.00      1.00         4\n",
      "           6       1.00      0.67      0.80         3\n",
      "           7       1.00      0.60      0.75         5\n",
      "           8       1.00      1.00      1.00         2\n",
      "           9       0.50      1.00      0.67         2\n",
      "          10       1.00      1.00      1.00         4\n",
      "          11       1.00      1.00      1.00         3\n",
      "          12       0.00      0.00      0.00         1\n",
      "          13       1.00      1.00      1.00         2\n",
      "          14       1.00      1.00      1.00         3\n",
      "          15       1.00      1.00      1.00         3\n",
      "          16       1.00      0.80      0.89         5\n",
      "          17       1.00      1.00      1.00         5\n",
      "          18       1.00      1.00      1.00         1\n",
      "          19       1.00      1.00      1.00         4\n",
      "          20       1.00      1.00      1.00         3\n",
      "          21       1.00      1.00      1.00         3\n",
      "          22       1.00      1.00      1.00         4\n",
      "          23       1.00      1.00      1.00         5\n",
      "          24       0.00      0.00      0.00         0\n",
      "          25       1.00      1.00      1.00         2\n",
      "          26       1.00      1.00      1.00         3\n",
      "          27       0.67      1.00      0.80         2\n",
      "          28       1.00      1.00      1.00         5\n",
      "          29       1.00      1.00      1.00         3\n",
      "          30       1.00      1.00      1.00         1\n",
      "          31       1.00      1.00      1.00         2\n",
      "          32       1.00      1.00      1.00         3\n",
      "          33       1.00      1.00      1.00         2\n",
      "          34       0.75      1.00      0.86         3\n",
      "          35       1.00      1.00      1.00         2\n",
      "          36       1.00      1.00      1.00         5\n",
      "          37       1.00      1.00      1.00         3\n",
      "          38       1.00      1.00      1.00         4\n",
      "          39       1.00      0.50      0.67         4\n",
      "\n",
      "    accuracy                           0.93       120\n",
      "   macro avg       0.90      0.91      0.89       120\n",
      "weighted avg       0.95      0.93      0.93       120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "PCA_LR(n_components = 0.8, solver = 'liblinear')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- solver = 'newton-cg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92.50%\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.67      0.80         3\n",
      "           1       1.00      1.00      1.00         2\n",
      "           2       0.75      1.00      0.86         3\n",
      "           3       1.00      1.00      1.00         4\n",
      "           4       1.00      1.00      1.00         2\n",
      "           5       1.00      1.00      1.00         4\n",
      "           6       1.00      1.00      1.00         3\n",
      "           7       1.00      0.40      0.57         5\n",
      "           8       1.00      1.00      1.00         2\n",
      "           9       0.50      1.00      0.67         2\n",
      "          10       1.00      1.00      1.00         4\n",
      "          11       1.00      1.00      1.00         3\n",
      "          12       1.00      1.00      1.00         1\n",
      "          13       1.00      1.00      1.00         2\n",
      "          14       1.00      1.00      1.00         3\n",
      "          15       1.00      1.00      1.00         3\n",
      "          16       0.75      0.60      0.67         5\n",
      "          17       1.00      1.00      1.00         5\n",
      "          18       1.00      1.00      1.00         1\n",
      "          19       1.00      1.00      1.00         4\n",
      "          20       1.00      1.00      1.00         3\n",
      "          21       1.00      1.00      1.00         3\n",
      "          22       1.00      1.00      1.00         4\n",
      "          23       1.00      0.80      0.89         5\n",
      "          24       0.00      0.00      0.00         0\n",
      "          25       0.67      1.00      0.80         2\n",
      "          26       1.00      1.00      1.00         3\n",
      "          27       1.00      1.00      1.00         2\n",
      "          28       1.00      1.00      1.00         5\n",
      "          29       0.75      1.00      0.86         3\n",
      "          30       1.00      1.00      1.00         1\n",
      "          31       0.67      1.00      0.80         2\n",
      "          32       1.00      1.00      1.00         3\n",
      "          33       1.00      1.00      1.00         2\n",
      "          34       1.00      1.00      1.00         3\n",
      "          35       1.00      1.00      1.00         2\n",
      "          36       1.00      1.00      1.00         5\n",
      "          37       1.00      1.00      1.00         3\n",
      "          38       1.00      1.00      1.00         4\n",
      "          39       1.00      0.50      0.67         4\n",
      "\n",
      "    accuracy                           0.93       120\n",
      "   macro avg       0.93      0.92      0.91       120\n",
      "weighted avg       0.96      0.93      0.93       120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.4s finished\n",
      "c:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "PCA_LR(n_components = 0.8, solver = 'newton-cg')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 結論:\n",
    "### PCA的AT&T資料，各個solver('lbfgs','liblinear','newton-cg')表現均差不多，準確率均在90%以上，也對於40筆資料來說表現絕對不差。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 若成分比例採0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75.83%\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.67      0.80         3\n",
      "           1       1.00      1.00      1.00         2\n",
      "           2       1.00      0.67      0.80         3\n",
      "           3       0.75      0.75      0.75         4\n",
      "           4       0.67      1.00      0.80         2\n",
      "           5       1.00      1.00      1.00         4\n",
      "           6       0.50      0.33      0.40         3\n",
      "           7       1.00      0.20      0.33         5\n",
      "           8       1.00      1.00      1.00         2\n",
      "           9       0.67      1.00      0.80         2\n",
      "          10       1.00      1.00      1.00         4\n",
      "          11       1.00      1.00      1.00         3\n",
      "          12       0.00      0.00      0.00         1\n",
      "          13       1.00      1.00      1.00         2\n",
      "          14       0.60      1.00      0.75         3\n",
      "          15       1.00      1.00      1.00         3\n",
      "          16       0.75      0.60      0.67         5\n",
      "          17       0.83      1.00      0.91         5\n",
      "          18       0.50      1.00      0.67         1\n",
      "          19       0.00      0.00      0.00         4\n",
      "          20       0.38      1.00      0.55         3\n",
      "          21       1.00      0.67      0.80         3\n",
      "          22       1.00      0.25      0.40         4\n",
      "          23       1.00      1.00      1.00         5\n",
      "          24       0.00      0.00      0.00         0\n",
      "          25       0.40      1.00      0.57         2\n",
      "          26       1.00      1.00      1.00         3\n",
      "          27       1.00      1.00      1.00         2\n",
      "          28       0.00      0.00      0.00         5\n",
      "          29       0.67      0.67      0.67         3\n",
      "          30       1.00      1.00      1.00         1\n",
      "          31       0.50      1.00      0.67         2\n",
      "          32       1.00      1.00      1.00         3\n",
      "          33       1.00      1.00      1.00         2\n",
      "          34       1.00      1.00      1.00         3\n",
      "          35       1.00      0.50      0.67         2\n",
      "          36       1.00      1.00      1.00         5\n",
      "          37       1.00      1.00      1.00         3\n",
      "          38       0.67      1.00      0.80         4\n",
      "          39       1.00      0.50      0.67         4\n",
      "\n",
      "    accuracy                           0.76       120\n",
      "   macro avg       0.77      0.77      0.74       120\n",
      "weighted avg       0.80      0.76      0.74       120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    2.6s finished\n",
      "c:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "PCA_LR(n_components = 0.6, solver = 'lbfgs')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 結論:\n",
    "### 可以發現當成分比例採0.8時，準確率都還維持在90%以上，當成分比例採0.6時，準確率已經下滑至75%，所以可以推測出當成分比例在0.8左右時，在降低維度的同時保留原本資料的重要資訊，所以接下來的成分比例都將採0.8"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 標準化後原始資料+SVC"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### one vs one"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- kernel = 'linear'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC,LinearSVC\n",
    "\n",
    "def clf_SVC(C, opts, clf_svm):\n",
    "    clf_svm.fit(X_train_,y_train)\n",
    "    predictions = clf_svm.predict(X_test_)\n",
    "    print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         3\n",
      "           1       1.00      1.00      1.00         2\n",
      "           2       1.00      1.00      1.00         3\n",
      "           3       1.00      1.00      1.00         4\n",
      "           4       1.00      1.00      1.00         2\n",
      "           5       1.00      1.00      1.00         4\n",
      "           6       1.00      1.00      1.00         3\n",
      "           7       1.00      0.80      0.89         5\n",
      "           8       1.00      1.00      1.00         2\n",
      "           9       0.67      1.00      0.80         2\n",
      "          10       1.00      1.00      1.00         4\n",
      "          11       1.00      1.00      1.00         3\n",
      "          12       1.00      1.00      1.00         1\n",
      "          13       1.00      1.00      1.00         2\n",
      "          14       1.00      1.00      1.00         3\n",
      "          15       1.00      1.00      1.00         3\n",
      "          16       1.00      0.60      0.75         5\n",
      "          17       1.00      1.00      1.00         5\n",
      "          18       1.00      1.00      1.00         1\n",
      "          19       1.00      1.00      1.00         4\n",
      "          20       1.00      1.00      1.00         3\n",
      "          21       1.00      1.00      1.00         3\n",
      "          22       1.00      1.00      1.00         4\n",
      "          23       1.00      1.00      1.00         5\n",
      "          24       0.00      0.00      0.00         0\n",
      "          25       0.67      1.00      0.80         2\n",
      "          26       1.00      1.00      1.00         3\n",
      "          27       1.00      1.00      1.00         2\n",
      "          28       1.00      1.00      1.00         5\n",
      "          29       1.00      1.00      1.00         3\n",
      "          30       1.00      1.00      1.00         1\n",
      "          31       1.00      1.00      1.00         2\n",
      "          32       1.00      1.00      1.00         3\n",
      "          33       1.00      1.00      1.00         2\n",
      "          34       1.00      1.00      1.00         3\n",
      "          35       1.00      1.00      1.00         2\n",
      "          36       1.00      1.00      1.00         5\n",
      "          37       1.00      1.00      1.00         3\n",
      "          38       1.00      1.00      1.00         4\n",
      "          39       1.00      0.75      0.86         4\n",
      "\n",
      "    accuracy                           0.97       120\n",
      "   macro avg       0.96      0.95      0.95       120\n",
      "weighted avg       0.99      0.97      0.97       120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "C = 1\n",
    "opts = dict(C = C, decision_function_shape = 'ovo', \\\n",
    "             tol = 1e-6, max_iter = int(1e6))\n",
    "clf_svm = SVC(kernel = 'linear', **opts)\n",
    "clf_SVC(C = C, opts = opts, clf_svm = clf_svm)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- kernel = 'rbf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      1.00      0.86         3\n",
      "           1       1.00      1.00      1.00         2\n",
      "           2       1.00      1.00      1.00         3\n",
      "           3       0.75      0.75      0.75         4\n",
      "           4       1.00      1.00      1.00         2\n",
      "           5       1.00      1.00      1.00         4\n",
      "           6       1.00      1.00      1.00         3\n",
      "           7       1.00      0.60      0.75         5\n",
      "           8       0.67      1.00      0.80         2\n",
      "           9       0.50      1.00      0.67         2\n",
      "          10       1.00      1.00      1.00         4\n",
      "          11       1.00      1.00      1.00         3\n",
      "          12       0.33      1.00      0.50         1\n",
      "          13       1.00      1.00      1.00         2\n",
      "          14       1.00      1.00      1.00         3\n",
      "          15       1.00      1.00      1.00         3\n",
      "          16       1.00      0.60      0.75         5\n",
      "          17       1.00      1.00      1.00         5\n",
      "          18       1.00      1.00      1.00         1\n",
      "          19       1.00      1.00      1.00         4\n",
      "          20       0.60      1.00      0.75         3\n",
      "          21       1.00      1.00      1.00         3\n",
      "          22       1.00      0.75      0.86         4\n",
      "          23       1.00      0.80      0.89         5\n",
      "          25       0.50      1.00      0.67         2\n",
      "          26       1.00      1.00      1.00         3\n",
      "          27       1.00      1.00      1.00         2\n",
      "          28       1.00      0.60      0.75         5\n",
      "          29       1.00      1.00      1.00         3\n",
      "          30       1.00      1.00      1.00         1\n",
      "          31       1.00      1.00      1.00         2\n",
      "          32       1.00      1.00      1.00         3\n",
      "          33       1.00      1.00      1.00         2\n",
      "          34       1.00      1.00      1.00         3\n",
      "          35       1.00      1.00      1.00         2\n",
      "          36       1.00      1.00      1.00         5\n",
      "          37       1.00      1.00      1.00         3\n",
      "          38       1.00      1.00      1.00         4\n",
      "          39       1.00      0.50      0.67         4\n",
      "\n",
      "    accuracy                           0.91       120\n",
      "   macro avg       0.93      0.94      0.91       120\n",
      "weighted avg       0.95      0.91      0.91       120\n",
      "\n"
     ]
    }
   ],
   "source": [
    "C = 1\n",
    "opts = dict(C = C, decision_function_shape = 'ovo', \\\n",
    "             tol = 1e-6, max_iter = int(1e6))\n",
    "clf_svm = SVC(kernel = 'rbf', **opts)\n",
    "clf_SVC(C = C, opts = opts, clf_svm = clf_svm)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- kernel = 'poly'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.33      0.40         3\n",
      "           1       1.00      1.00      1.00         2\n",
      "           2       0.00      0.00      0.00         3\n",
      "           3       0.00      0.00      0.00         4\n",
      "           4       1.00      0.50      0.67         2\n",
      "           5       1.00      1.00      1.00         4\n",
      "           6       1.00      0.33      0.50         3\n",
      "           7       1.00      0.40      0.57         5\n",
      "           8       0.00      0.00      0.00         2\n",
      "           9       0.50      0.50      0.50         2\n",
      "          10       1.00      1.00      1.00         4\n",
      "          11       1.00      0.67      0.80         3\n",
      "          12       0.00      0.00      0.00         1\n",
      "          13       0.00      0.00      0.00         2\n",
      "          14       0.00      0.00      0.00         3\n",
      "          15       1.00      0.67      0.80         3\n",
      "          16       0.75      0.60      0.67         5\n",
      "          17       1.00      0.40      0.57         5\n",
      "          18       1.00      1.00      1.00         1\n",
      "          19       1.00      0.25      0.40         4\n",
      "          20       0.00      0.00      0.00         3\n",
      "          21       1.00      1.00      1.00         3\n",
      "          22       0.00      0.00      0.00         4\n",
      "          23       1.00      0.80      0.89         5\n",
      "          24       0.00      0.00      0.00         0\n",
      "          25       1.00      0.50      0.67         2\n",
      "          26       1.00      0.33      0.50         3\n",
      "          27       1.00      0.50      0.67         2\n",
      "          28       1.00      0.20      0.33         5\n",
      "          29       1.00      0.33      0.50         3\n",
      "          30       0.00      0.00      0.00         1\n",
      "          31       1.00      1.00      1.00         2\n",
      "          32       1.00      1.00      1.00         3\n",
      "          33       1.00      1.00      1.00         2\n",
      "          34       0.00      0.00      0.00         3\n",
      "          35       1.00      1.00      1.00         2\n",
      "          36       1.00      1.00      1.00         5\n",
      "          37       0.00      0.00      0.00         3\n",
      "          38       1.00      1.00      1.00         4\n",
      "          39       1.00      0.25      0.40         4\n",
      "\n",
      "    accuracy                           0.48       120\n",
      "   macro avg       0.67      0.46      0.52       120\n",
      "weighted avg       0.73      0.48      0.55       120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "C = 1\n",
    "opts = dict(C = C, decision_function_shape = 'ovo', \\\n",
    "             tol = 1e-6, max_iter = int(1e6))\n",
    "clf_svm = SVC(kernel = 'poly', **opts)\n",
    "clf_SVC(C = C, opts = opts, clf_svm = clf_svm)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 結論:\n",
    "### 在決策函數中選擇一對一(one vs one)的模式，可以看到當 kernel 選擇 'linear' 準確率有最高97%，kernel 選擇 'rbf' 準確率有91%，而 kernel 選擇 'poly' 準確率只有48%，表現得不好"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### one vs the rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         3\n",
      "           1       1.00      1.00      1.00         2\n",
      "           2       1.00      1.00      1.00         3\n",
      "           3       1.00      1.00      1.00         4\n",
      "           4       1.00      1.00      1.00         2\n",
      "           5       1.00      1.00      1.00         4\n",
      "           6       1.00      1.00      1.00         3\n",
      "           7       1.00      1.00      1.00         5\n",
      "           8       0.50      1.00      0.67         2\n",
      "           9       1.00      1.00      1.00         2\n",
      "          10       1.00      1.00      1.00         4\n",
      "          11       1.00      1.00      1.00         3\n",
      "          12       0.00      0.00      0.00         1\n",
      "          13       1.00      1.00      1.00         2\n",
      "          14       1.00      1.00      1.00         3\n",
      "          15       1.00      1.00      1.00         3\n",
      "          16       1.00      0.80      0.89         5\n",
      "          17       1.00      1.00      1.00         5\n",
      "          18       1.00      1.00      1.00         1\n",
      "          19       1.00      1.00      1.00         4\n",
      "          20       1.00      0.67      0.80         3\n",
      "          21       1.00      1.00      1.00         3\n",
      "          22       1.00      1.00      1.00         4\n",
      "          23       1.00      0.60      0.75         5\n",
      "          25       1.00      1.00      1.00         2\n",
      "          26       1.00      1.00      1.00         3\n",
      "          27       1.00      1.00      1.00         2\n",
      "          28       1.00      1.00      1.00         5\n",
      "          29       1.00      1.00      1.00         3\n",
      "          30       0.33      1.00      0.50         1\n",
      "          31       1.00      1.00      1.00         2\n",
      "          32       1.00      1.00      1.00         3\n",
      "          33       1.00      1.00      1.00         2\n",
      "          34       0.75      1.00      0.86         3\n",
      "          35       1.00      1.00      1.00         2\n",
      "          36       1.00      1.00      1.00         5\n",
      "          37       1.00      1.00      1.00         3\n",
      "          38       0.67      1.00      0.80         4\n",
      "          39       1.00      0.50      0.67         4\n",
      "\n",
      "    accuracy                           0.94       120\n",
      "   macro avg       0.93      0.94      0.92       120\n",
      "weighted avg       0.96      0.94      0.94       120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "C = 1\n",
    "opts = dict(C = C,tol = 1e-6,max_iter = int(1e6))\n",
    "clf_svm = LinearSVC(**opts)\n",
    "clf_SVC(C = C, opts = opts, clf_svm = clf_svm)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 結論:\n",
    "### 1. 當模式為一對其他（one vs the rest），準確率為94%\n",
    "### 2. 模式為 一對一 跟 一對其他，可以看到準確率皆在90%以上，相較於一對其他，一對一的模式的準確率較高"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA主成分分析(成分比例採0.8) + SVM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### one vs one"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- kernel = 'linear'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.67      0.80         3\n",
      "           1       1.00      1.00      1.00         2\n",
      "           2       1.00      1.00      1.00         3\n",
      "           3       0.67      1.00      0.80         4\n",
      "           4       1.00      1.00      1.00         2\n",
      "           5       1.00      1.00      1.00         4\n",
      "           6       1.00      1.00      1.00         3\n",
      "           7       1.00      0.40      0.57         5\n",
      "           8       0.67      1.00      0.80         2\n",
      "           9       0.50      1.00      0.67         2\n",
      "          10       1.00      1.00      1.00         4\n",
      "          11       1.00      1.00      1.00         3\n",
      "          12       0.50      1.00      0.67         1\n",
      "          13       1.00      1.00      1.00         2\n",
      "          14       1.00      1.00      1.00         3\n",
      "          15       1.00      1.00      1.00         3\n",
      "          16       0.75      0.60      0.67         5\n",
      "          17       1.00      0.80      0.89         5\n",
      "          18       1.00      1.00      1.00         1\n",
      "          19       1.00      1.00      1.00         4\n",
      "          20       0.75      1.00      0.86         3\n",
      "          21       1.00      1.00      1.00         3\n",
      "          22       1.00      1.00      1.00         4\n",
      "          23       1.00      0.80      0.89         5\n",
      "          24       0.00      0.00      0.00         0\n",
      "          25       1.00      1.00      1.00         2\n",
      "          26       1.00      1.00      1.00         3\n",
      "          27       1.00      1.00      1.00         2\n",
      "          28       1.00      0.80      0.89         5\n",
      "          29       0.75      1.00      0.86         3\n",
      "          30       1.00      1.00      1.00         1\n",
      "          31       1.00      1.00      1.00         2\n",
      "          32       1.00      1.00      1.00         3\n",
      "          33       1.00      1.00      1.00         2\n",
      "          34       1.00      1.00      1.00         3\n",
      "          35       1.00      1.00      1.00         2\n",
      "          36       1.00      1.00      1.00         5\n",
      "          37       1.00      1.00      1.00         3\n",
      "          38       1.00      1.00      1.00         4\n",
      "          39       1.00      0.50      0.67         4\n",
      "\n",
      "    accuracy                           0.91       120\n",
      "   macro avg       0.91      0.91      0.90       120\n",
      "weighted avg       0.95      0.91      0.91       120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=0.8).fit(X_train_)\n",
    "Z_train = pca.transform(X_train_) #降維\n",
    "Z_test = pca.transform(X_test_)\n",
    "C = 1\n",
    "opts = dict(C = C, decision_function_shape = 'ovo', \\\n",
    "             tol = 1e-6, max_iter = int(1e6))\n",
    "clf_svm = SVC(kernel = 'linear', **opts)\n",
    "clf_svm.fit(Z_train,y_train)\n",
    "predictions = clf_svm.predict(Z_test)\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### one vs the rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.67      0.80         3\n",
      "           1       1.00      1.00      1.00         2\n",
      "           2       1.00      1.00      1.00         3\n",
      "           3       0.67      1.00      0.80         4\n",
      "           4       1.00      1.00      1.00         2\n",
      "           5       1.00      1.00      1.00         4\n",
      "           6       1.00      1.00      1.00         3\n",
      "           7       1.00      0.40      0.57         5\n",
      "           8       0.67      1.00      0.80         2\n",
      "           9       0.50      1.00      0.67         2\n",
      "          10       1.00      1.00      1.00         4\n",
      "          11       1.00      1.00      1.00         3\n",
      "          12       0.50      1.00      0.67         1\n",
      "          13       1.00      1.00      1.00         2\n",
      "          14       1.00      1.00      1.00         3\n",
      "          15       1.00      1.00      1.00         3\n",
      "          16       0.75      0.60      0.67         5\n",
      "          17       1.00      0.80      0.89         5\n",
      "          18       1.00      1.00      1.00         1\n",
      "          19       1.00      1.00      1.00         4\n",
      "          20       0.75      1.00      0.86         3\n",
      "          21       1.00      1.00      1.00         3\n",
      "          22       1.00      1.00      1.00         4\n",
      "          23       1.00      0.80      0.89         5\n",
      "          24       0.00      0.00      0.00         0\n",
      "          25       1.00      1.00      1.00         2\n",
      "          26       1.00      1.00      1.00         3\n",
      "          27       1.00      1.00      1.00         2\n",
      "          28       1.00      0.80      0.89         5\n",
      "          29       0.75      1.00      0.86         3\n",
      "          30       1.00      1.00      1.00         1\n",
      "          31       1.00      1.00      1.00         2\n",
      "          32       1.00      1.00      1.00         3\n",
      "          33       1.00      1.00      1.00         2\n",
      "          34       1.00      1.00      1.00         3\n",
      "          35       1.00      1.00      1.00         2\n",
      "          36       1.00      1.00      1.00         5\n",
      "          37       1.00      1.00      1.00         3\n",
      "          38       1.00      1.00      1.00         4\n",
      "          39       1.00      0.50      0.67         4\n",
      "\n",
      "    accuracy                           0.91       120\n",
      "   macro avg       0.91      0.91      0.90       120\n",
      "weighted avg       0.95      0.91      0.91       120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=0.8).fit(X_train_)\n",
    "Z_train = pca.transform(X_train_) #降維\n",
    "Z_test = pca.transform(X_test_)\n",
    "C = 1\n",
    "opts = dict(C=C,tol = 1e-6,max_iter = int(1e6))\n",
    "clf_svm = SVC(kernel = 'linear',**opts)\n",
    "clf_svm.fit(Z_train,y_train)\n",
    "predictions = clf_svm.predict(Z_test)\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 結論:\n",
    "### 當成分比例採0.8時，不管選擇一對一或一對其他都能維持準確率在90%"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 標準化後原始資料 + 神經網路NN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- hidden_layers = (30,) , activation = 'logistic', solver = 'adam'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 3.71675275\n",
      "Iteration 2, loss = 3.42003555\n",
      "Iteration 3, loss = 3.28374889\n",
      "Iteration 4, loss = 3.19359201\n",
      "Iteration 5, loss = 3.12137016\n",
      "Iteration 6, loss = 3.06066795\n",
      "Iteration 7, loss = 3.00514139\n",
      "Iteration 8, loss = 2.95749885\n",
      "Iteration 9, loss = 2.91391547\n",
      "Iteration 10, loss = 2.87297481\n",
      "Iteration 11, loss = 2.83537261\n",
      "Iteration 12, loss = 2.79767338\n",
      "Iteration 13, loss = 2.76317905\n",
      "Iteration 14, loss = 2.73207044\n",
      "Iteration 15, loss = 2.70069477\n",
      "Iteration 16, loss = 2.67061858\n",
      "Iteration 17, loss = 2.64200325\n",
      "Iteration 18, loss = 2.61306638\n",
      "Iteration 19, loss = 2.58360261\n",
      "Iteration 20, loss = 2.55615918\n",
      "Iteration 21, loss = 2.52968527\n",
      "Iteration 22, loss = 2.50283960\n",
      "Iteration 23, loss = 2.47616920\n",
      "Iteration 24, loss = 2.45008440\n",
      "Iteration 25, loss = 2.42436367\n",
      "Iteration 26, loss = 2.39912591\n",
      "Iteration 27, loss = 2.37407067\n",
      "Iteration 28, loss = 2.34926779\n",
      "Iteration 29, loss = 2.32445058\n",
      "Iteration 30, loss = 2.30035931\n",
      "Iteration 31, loss = 2.27677253\n",
      "Iteration 32, loss = 2.25255171\n",
      "Iteration 33, loss = 2.22914597\n",
      "Iteration 34, loss = 2.20629699\n",
      "Iteration 35, loss = 2.18286787\n",
      "Iteration 36, loss = 2.16006340\n",
      "Iteration 37, loss = 2.13719821\n",
      "Iteration 38, loss = 2.11502736\n",
      "Iteration 39, loss = 2.09235825\n",
      "Iteration 40, loss = 2.07015571\n",
      "Iteration 41, loss = 2.04795065\n",
      "Iteration 42, loss = 2.02588415\n",
      "Iteration 43, loss = 2.00448142\n",
      "Iteration 44, loss = 1.98290020\n",
      "Iteration 45, loss = 1.96119077\n",
      "Iteration 46, loss = 1.94008156\n",
      "Iteration 47, loss = 1.91926034\n",
      "Iteration 48, loss = 1.89852691\n",
      "Iteration 49, loss = 1.87787759\n",
      "Iteration 50, loss = 1.85748014\n",
      "Iteration 51, loss = 1.83708636\n",
      "Iteration 52, loss = 1.81683839\n",
      "Iteration 53, loss = 1.79686626\n",
      "Iteration 54, loss = 1.77719257\n",
      "Iteration 55, loss = 1.75743370\n",
      "Iteration 56, loss = 1.73783536\n",
      "Iteration 57, loss = 1.71861623\n",
      "Iteration 58, loss = 1.69926234\n",
      "Iteration 59, loss = 1.68019197\n",
      "Iteration 60, loss = 1.66132329\n",
      "Iteration 61, loss = 1.64256122\n",
      "Iteration 62, loss = 1.62411522\n",
      "Iteration 63, loss = 1.60582277\n",
      "Iteration 64, loss = 1.58775279\n",
      "Iteration 65, loss = 1.56959614\n",
      "Iteration 66, loss = 1.55198911\n",
      "Iteration 67, loss = 1.53446735\n",
      "Iteration 68, loss = 1.51718367\n",
      "Iteration 69, loss = 1.49984177\n",
      "Iteration 70, loss = 1.48301517\n",
      "Iteration 71, loss = 1.46615148\n",
      "Iteration 72, loss = 1.44955152\n",
      "Iteration 73, loss = 1.43290851\n",
      "Iteration 74, loss = 1.41670576\n",
      "Iteration 75, loss = 1.40049501\n",
      "Iteration 76, loss = 1.38441025\n",
      "Iteration 77, loss = 1.36848934\n",
      "Iteration 78, loss = 1.35277273\n",
      "Iteration 79, loss = 1.33716142\n",
      "Iteration 80, loss = 1.32163048\n",
      "Iteration 81, loss = 1.30640478\n",
      "Iteration 82, loss = 1.29136946\n",
      "Iteration 83, loss = 1.27637910\n",
      "Iteration 84, loss = 1.26157959\n",
      "Iteration 85, loss = 1.24721641\n",
      "Iteration 86, loss = 1.23273300\n",
      "Iteration 87, loss = 1.21840034\n",
      "Iteration 88, loss = 1.20446836\n",
      "Iteration 89, loss = 1.19032103\n",
      "Iteration 90, loss = 1.17647464\n",
      "Iteration 91, loss = 1.16286177\n",
      "Iteration 92, loss = 1.14940455\n",
      "Iteration 93, loss = 1.13609325\n",
      "Iteration 94, loss = 1.12302351\n",
      "Iteration 95, loss = 1.11009655\n",
      "Iteration 96, loss = 1.09725241\n",
      "Iteration 97, loss = 1.08455947\n",
      "Iteration 98, loss = 1.07214967\n",
      "Iteration 99, loss = 1.05977465\n",
      "Iteration 100, loss = 1.04761232\n",
      "Iteration 101, loss = 1.03550395\n",
      "Iteration 102, loss = 1.02359075\n",
      "Iteration 103, loss = 1.01171615\n",
      "Iteration 104, loss = 1.00023894\n",
      "Iteration 105, loss = 0.98878520\n",
      "Iteration 106, loss = 0.97732990\n",
      "Iteration 107, loss = 0.96615989\n",
      "Iteration 108, loss = 0.95518352\n",
      "Iteration 109, loss = 0.94430016\n",
      "Iteration 110, loss = 0.93353805\n",
      "Iteration 111, loss = 0.92289279\n",
      "Iteration 112, loss = 0.91245646\n",
      "Iteration 113, loss = 0.90205444\n",
      "Iteration 114, loss = 0.89186604\n",
      "Iteration 115, loss = 0.88179386\n",
      "Iteration 116, loss = 0.87179267\n",
      "Iteration 117, loss = 0.86197423\n",
      "Iteration 118, loss = 0.85220658\n",
      "Iteration 119, loss = 0.84252876\n",
      "Iteration 120, loss = 0.83316904\n",
      "Iteration 121, loss = 0.82376220\n",
      "Iteration 122, loss = 0.81461872\n",
      "Iteration 123, loss = 0.80545257\n",
      "Iteration 124, loss = 0.79654545\n",
      "Iteration 125, loss = 0.78761841\n",
      "Iteration 126, loss = 0.77882267\n",
      "Iteration 127, loss = 0.77025241\n",
      "Iteration 128, loss = 0.76165898\n",
      "Iteration 129, loss = 0.75329761\n",
      "Iteration 130, loss = 0.74495044\n",
      "Iteration 131, loss = 0.73677029\n",
      "Iteration 132, loss = 0.72869587\n",
      "Iteration 133, loss = 0.72071583\n",
      "Iteration 134, loss = 0.71280666\n",
      "Iteration 135, loss = 0.70499632\n",
      "Iteration 136, loss = 0.69718217\n",
      "Iteration 137, loss = 0.68961329\n",
      "Iteration 138, loss = 0.68208610\n",
      "Iteration 139, loss = 0.67475258\n",
      "Iteration 140, loss = 0.66759183\n",
      "Iteration 141, loss = 0.66051367\n",
      "Iteration 142, loss = 0.65347569\n",
      "Iteration 143, loss = 0.64654370\n",
      "Iteration 144, loss = 0.63975470\n",
      "Iteration 145, loss = 0.63304302\n",
      "Iteration 146, loss = 0.62644232\n",
      "Iteration 147, loss = 0.61982691\n",
      "Iteration 148, loss = 0.61330581\n",
      "Iteration 149, loss = 0.60690682\n",
      "Iteration 150, loss = 0.60063974\n",
      "Iteration 151, loss = 0.59426149\n",
      "Iteration 152, loss = 0.58813885\n",
      "Iteration 153, loss = 0.58205263\n",
      "Iteration 154, loss = 0.57606939\n",
      "Iteration 155, loss = 0.57017940\n",
      "Iteration 156, loss = 0.56435254\n",
      "Iteration 157, loss = 0.55858791\n",
      "Iteration 158, loss = 0.55292967\n",
      "Iteration 159, loss = 0.54733413\n",
      "Iteration 160, loss = 0.54186698\n",
      "Iteration 161, loss = 0.53637805\n",
      "Iteration 162, loss = 0.53094704\n",
      "Iteration 163, loss = 0.52560661\n",
      "Iteration 164, loss = 0.52030713\n",
      "Iteration 165, loss = 0.51503983\n",
      "Iteration 166, loss = 0.50994998\n",
      "Iteration 167, loss = 0.50498509\n",
      "Iteration 168, loss = 0.50003183\n",
      "Iteration 169, loss = 0.49515359\n",
      "Iteration 170, loss = 0.49033149\n",
      "Iteration 171, loss = 0.48561517\n",
      "Iteration 172, loss = 0.48095849\n",
      "Iteration 173, loss = 0.47634202\n",
      "Iteration 174, loss = 0.47182363\n",
      "Iteration 175, loss = 0.46727948\n",
      "Iteration 176, loss = 0.46288354\n",
      "Iteration 177, loss = 0.45846145\n",
      "Iteration 178, loss = 0.45411911\n",
      "Iteration 179, loss = 0.44983149\n",
      "Iteration 180, loss = 0.44562404\n",
      "Iteration 181, loss = 0.44142431\n",
      "Iteration 182, loss = 0.43731958\n",
      "Iteration 183, loss = 0.43329379\n",
      "Iteration 184, loss = 0.42932881\n",
      "Iteration 185, loss = 0.42541094\n",
      "Iteration 186, loss = 0.42152023\n",
      "Iteration 187, loss = 0.41768010\n",
      "Iteration 188, loss = 0.41390508\n",
      "Iteration 189, loss = 0.41015341\n",
      "Iteration 190, loss = 0.40646578\n",
      "Iteration 191, loss = 0.40285154\n",
      "Iteration 192, loss = 0.39923251\n",
      "Iteration 193, loss = 0.39569590\n",
      "Iteration 194, loss = 0.39223320\n",
      "Iteration 195, loss = 0.38876255\n",
      "Iteration 196, loss = 0.38535996\n",
      "Iteration 197, loss = 0.38200804\n",
      "Iteration 198, loss = 0.37870156\n",
      "Iteration 199, loss = 0.37542939\n",
      "Iteration 200, loss = 0.37220139\n",
      "Iteration 201, loss = 0.36900583\n",
      "Iteration 202, loss = 0.36585575\n",
      "Iteration 203, loss = 0.36273978\n",
      "Iteration 204, loss = 0.35967544\n",
      "Iteration 205, loss = 0.35663846\n",
      "Iteration 206, loss = 0.35364613\n",
      "Iteration 207, loss = 0.35069099\n",
      "Iteration 208, loss = 0.34776333\n",
      "Iteration 209, loss = 0.34487492\n",
      "Iteration 210, loss = 0.34201788\n",
      "Iteration 211, loss = 0.33919129\n",
      "Iteration 212, loss = 0.33639192\n",
      "Iteration 213, loss = 0.33367872\n",
      "Iteration 214, loss = 0.33092321\n",
      "Iteration 215, loss = 0.32823660\n",
      "Iteration 216, loss = 0.32557409\n",
      "Iteration 217, loss = 0.32295722\n",
      "Iteration 218, loss = 0.32037416\n",
      "Iteration 219, loss = 0.31780838\n",
      "Iteration 220, loss = 0.31528708\n",
      "Iteration 221, loss = 0.31278697\n",
      "Iteration 222, loss = 0.31030272\n",
      "Iteration 223, loss = 0.30786949\n",
      "Iteration 224, loss = 0.30546111\n",
      "Iteration 225, loss = 0.30308102\n",
      "Iteration 226, loss = 0.30072675\n",
      "Iteration 227, loss = 0.29839142\n",
      "Iteration 228, loss = 0.29609163\n",
      "Iteration 229, loss = 0.29382623\n",
      "Iteration 230, loss = 0.29158818\n",
      "Iteration 231, loss = 0.28936340\n",
      "Iteration 232, loss = 0.28716086\n",
      "Iteration 233, loss = 0.28501813\n",
      "Iteration 234, loss = 0.28287080\n",
      "Iteration 235, loss = 0.28074167\n",
      "Iteration 236, loss = 0.27865821\n",
      "Iteration 237, loss = 0.27657573\n",
      "Iteration 238, loss = 0.27452682\n",
      "Iteration 239, loss = 0.27250148\n",
      "Iteration 240, loss = 0.27048578\n",
      "Iteration 241, loss = 0.26849358\n",
      "Iteration 242, loss = 0.26651385\n",
      "Iteration 243, loss = 0.26456931\n",
      "Iteration 244, loss = 0.26264483\n",
      "Iteration 245, loss = 0.26075258\n",
      "Iteration 246, loss = 0.25886953\n",
      "Iteration 247, loss = 0.25700439\n",
      "Iteration 248, loss = 0.25518811\n",
      "Iteration 249, loss = 0.25337030\n",
      "Iteration 250, loss = 0.25157376\n",
      "Iteration 251, loss = 0.24980806\n",
      "Iteration 252, loss = 0.24804721\n",
      "Iteration 253, loss = 0.24631698\n",
      "Iteration 254, loss = 0.24459770\n",
      "Iteration 255, loss = 0.24289755\n",
      "Iteration 256, loss = 0.24120590\n",
      "Iteration 257, loss = 0.23955175\n",
      "Iteration 258, loss = 0.23790044\n",
      "Iteration 259, loss = 0.23627081\n",
      "Iteration 260, loss = 0.23464785\n",
      "Iteration 261, loss = 0.23307041\n",
      "Iteration 262, loss = 0.23146019\n",
      "Iteration 263, loss = 0.22991566\n",
      "Iteration 264, loss = 0.22836266\n",
      "Iteration 265, loss = 0.22682090\n",
      "Iteration 266, loss = 0.22529685\n",
      "Iteration 267, loss = 0.22378485\n",
      "Iteration 268, loss = 0.22230727\n",
      "Iteration 269, loss = 0.22082181\n",
      "Iteration 270, loss = 0.21937006\n",
      "Iteration 271, loss = 0.21792630\n",
      "Iteration 272, loss = 0.21649475\n",
      "Iteration 273, loss = 0.21507777\n",
      "Iteration 274, loss = 0.21369102\n",
      "Iteration 275, loss = 0.21226315\n",
      "Iteration 276, loss = 0.21085558\n",
      "Iteration 277, loss = 0.20947052\n",
      "Iteration 278, loss = 0.20812370\n",
      "Iteration 279, loss = 0.20680369\n",
      "Iteration 280, loss = 0.20547568\n",
      "Iteration 281, loss = 0.20417486\n",
      "Iteration 282, loss = 0.20288555\n",
      "Iteration 283, loss = 0.20160451\n",
      "Iteration 284, loss = 0.20033159\n",
      "Iteration 285, loss = 0.19908847\n",
      "Iteration 286, loss = 0.19784452\n",
      "Iteration 287, loss = 0.19660446\n",
      "Iteration 288, loss = 0.19539290\n",
      "Iteration 289, loss = 0.19417738\n",
      "Iteration 290, loss = 0.19298329\n",
      "Iteration 291, loss = 0.19178151\n",
      "Iteration 292, loss = 0.19059846\n",
      "Iteration 293, loss = 0.18940180\n",
      "Iteration 294, loss = 0.18814643\n",
      "Iteration 295, loss = 0.18695927\n",
      "Iteration 296, loss = 0.18579985\n",
      "Iteration 297, loss = 0.18465209\n",
      "Iteration 298, loss = 0.18344916\n",
      "Iteration 299, loss = 0.18227148\n",
      "Iteration 300, loss = 0.18111407\n",
      "Iteration 301, loss = 0.18000585\n",
      "Iteration 302, loss = 0.17893604\n",
      "Iteration 303, loss = 0.17786561\n",
      "Iteration 304, loss = 0.17680813\n",
      "Iteration 305, loss = 0.17576040\n",
      "Iteration 306, loss = 0.17472900\n",
      "Iteration 307, loss = 0.17370305\n",
      "Iteration 308, loss = 0.17270697\n",
      "Iteration 309, loss = 0.17170672\n",
      "Iteration 310, loss = 0.17071779\n",
      "Iteration 311, loss = 0.16974244\n",
      "Iteration 312, loss = 0.16877715\n",
      "Iteration 313, loss = 0.16781510\n",
      "Iteration 314, loss = 0.16686647\n",
      "Iteration 315, loss = 0.16592768\n",
      "Iteration 316, loss = 0.16499069\n",
      "Iteration 317, loss = 0.16406358\n",
      "Iteration 318, loss = 0.16314519\n",
      "Iteration 319, loss = 0.16224743\n",
      "Iteration 320, loss = 0.16134606\n",
      "Iteration 321, loss = 0.16045448\n",
      "Iteration 322, loss = 0.15956777\n",
      "Iteration 323, loss = 0.15869571\n",
      "Iteration 324, loss = 0.15782729\n",
      "Iteration 325, loss = 0.15695309\n",
      "Iteration 326, loss = 0.15610775\n",
      "Iteration 327, loss = 0.15525026\n",
      "Iteration 328, loss = 0.15441813\n",
      "Iteration 329, loss = 0.15358142\n",
      "Iteration 330, loss = 0.15275488\n",
      "Iteration 331, loss = 0.15192903\n",
      "Iteration 332, loss = 0.15111863\n",
      "Iteration 333, loss = 0.15030708\n",
      "Iteration 334, loss = 0.14949408\n",
      "Iteration 335, loss = 0.14867480\n",
      "Iteration 336, loss = 0.14784664\n",
      "Iteration 337, loss = 0.14700124\n",
      "Iteration 338, loss = 0.14623317\n",
      "Iteration 339, loss = 0.14546345\n",
      "Iteration 340, loss = 0.14470119\n",
      "Iteration 341, loss = 0.14393422\n",
      "Iteration 342, loss = 0.14317488\n",
      "Iteration 343, loss = 0.14242215\n",
      "Iteration 344, loss = 0.14168611\n",
      "Iteration 345, loss = 0.14096005\n",
      "Iteration 346, loss = 0.14024120\n",
      "Iteration 347, loss = 0.13952239\n",
      "Iteration 348, loss = 0.13881319\n",
      "Iteration 349, loss = 0.13810423\n",
      "Iteration 350, loss = 0.13741010\n",
      "Iteration 351, loss = 0.13671951\n",
      "Iteration 352, loss = 0.13602333\n",
      "Iteration 353, loss = 0.13534525\n",
      "Iteration 354, loss = 0.13466267\n",
      "Iteration 355, loss = 0.13398979\n",
      "Iteration 356, loss = 0.13332172\n",
      "Iteration 357, loss = 0.13266129\n",
      "Iteration 358, loss = 0.13200148\n",
      "Iteration 359, loss = 0.13135114\n",
      "Iteration 360, loss = 0.13070404\n",
      "Iteration 361, loss = 0.13006508\n",
      "Iteration 362, loss = 0.12943006\n",
      "Iteration 363, loss = 0.12879801\n",
      "Iteration 364, loss = 0.12817780\n",
      "Iteration 365, loss = 0.12755229\n",
      "Iteration 366, loss = 0.12693804\n",
      "Iteration 367, loss = 0.12632643\n",
      "Iteration 368, loss = 0.12572034\n",
      "Iteration 369, loss = 0.12511661\n",
      "Iteration 370, loss = 0.12451640\n",
      "Iteration 371, loss = 0.12392636\n",
      "Iteration 372, loss = 0.12333603\n",
      "Iteration 373, loss = 0.12275348\n",
      "Iteration 374, loss = 0.12216917\n",
      "Iteration 375, loss = 0.12159833\n",
      "Iteration 376, loss = 0.12102275\n",
      "Iteration 377, loss = 0.12045319\n",
      "Iteration 378, loss = 0.11989609\n",
      "Iteration 379, loss = 0.11933838\n",
      "Iteration 380, loss = 0.11878058\n",
      "Iteration 381, loss = 0.11823101\n",
      "Iteration 382, loss = 0.11768285\n",
      "Iteration 383, loss = 0.11714165\n",
      "Iteration 384, loss = 0.11660122\n",
      "Iteration 385, loss = 0.11607138\n",
      "Iteration 386, loss = 0.11554055\n",
      "Iteration 387, loss = 0.11501567\n",
      "Iteration 388, loss = 0.11449366\n",
      "Iteration 389, loss = 0.11397523\n",
      "Iteration 390, loss = 0.11345950\n",
      "Iteration 391, loss = 0.11294639\n",
      "Iteration 392, loss = 0.11244021\n",
      "Iteration 393, loss = 0.11193287\n",
      "Iteration 394, loss = 0.11142872\n",
      "Iteration 395, loss = 0.11093076\n",
      "Iteration 396, loss = 0.11043539\n",
      "Iteration 397, loss = 0.10993984\n",
      "Iteration 398, loss = 0.10945784\n",
      "Iteration 399, loss = 0.10897052\n",
      "Iteration 400, loss = 0.10849013\n",
      "Iteration 401, loss = 0.10801361\n",
      "Iteration 402, loss = 0.10753447\n",
      "Iteration 403, loss = 0.10706864\n",
      "Iteration 404, loss = 0.10660040\n",
      "Iteration 405, loss = 0.10613287\n",
      "Iteration 406, loss = 0.10567637\n",
      "Iteration 407, loss = 0.10521556\n",
      "Iteration 408, loss = 0.10476259\n",
      "Iteration 409, loss = 0.10431124\n",
      "Iteration 410, loss = 0.10386218\n",
      "Iteration 411, loss = 0.10341562\n",
      "Iteration 412, loss = 0.10297570\n",
      "Iteration 413, loss = 0.10253544\n",
      "Iteration 414, loss = 0.10210165\n",
      "Iteration 415, loss = 0.10166508\n",
      "Iteration 416, loss = 0.10123589\n",
      "Iteration 417, loss = 0.10080618\n",
      "Iteration 418, loss = 0.10038260\n",
      "Iteration 419, loss = 0.09996147\n",
      "Iteration 420, loss = 0.09953589\n",
      "Iteration 421, loss = 0.09912197\n",
      "Iteration 422, loss = 0.09870159\n",
      "Iteration 423, loss = 0.09829643\n",
      "Iteration 424, loss = 0.09788078\n",
      "Iteration 425, loss = 0.09747696\n",
      "Iteration 426, loss = 0.09707350\n",
      "Iteration 427, loss = 0.09666708\n",
      "Iteration 428, loss = 0.09626927\n",
      "Iteration 429, loss = 0.09587389\n",
      "Iteration 430, loss = 0.09547730\n",
      "Iteration 431, loss = 0.09508519\n",
      "Iteration 432, loss = 0.09469965\n",
      "Iteration 433, loss = 0.09431063\n",
      "Iteration 434, loss = 0.09392419\n",
      "Iteration 435, loss = 0.09354305\n",
      "Iteration 436, loss = 0.09316220\n",
      "Iteration 437, loss = 0.09278568\n",
      "Iteration 438, loss = 0.09241008\n",
      "Iteration 439, loss = 0.09203768\n",
      "Iteration 440, loss = 0.09167053\n",
      "Iteration 441, loss = 0.09129711\n",
      "Iteration 442, loss = 0.09093448\n",
      "Iteration 443, loss = 0.09056690\n",
      "Iteration 444, loss = 0.09020760\n",
      "Iteration 445, loss = 0.08984922\n",
      "Iteration 446, loss = 0.08949122\n",
      "Iteration 447, loss = 0.08913761\n",
      "Iteration 448, loss = 0.08878912\n",
      "Iteration 449, loss = 0.08843789\n",
      "Iteration 450, loss = 0.08809261\n",
      "Iteration 451, loss = 0.08774744\n",
      "Iteration 452, loss = 0.08740559\n",
      "Iteration 453, loss = 0.08706538\n",
      "Iteration 454, loss = 0.08672641\n",
      "Iteration 455, loss = 0.08639239\n",
      "Iteration 456, loss = 0.08605761\n",
      "Iteration 457, loss = 0.08572429\n",
      "Iteration 458, loss = 0.08539560\n",
      "Iteration 459, loss = 0.08506681\n",
      "Iteration 460, loss = 0.08474232\n",
      "Iteration 461, loss = 0.08441749\n",
      "Iteration 462, loss = 0.08409708\n",
      "Iteration 463, loss = 0.08377751\n",
      "Iteration 464, loss = 0.08346148\n",
      "Iteration 465, loss = 0.08314271\n",
      "Iteration 466, loss = 0.08282769\n",
      "Iteration 467, loss = 0.08251890\n",
      "Iteration 468, loss = 0.08220801\n",
      "Iteration 469, loss = 0.08189930\n",
      "Iteration 470, loss = 0.08159162\n",
      "Iteration 471, loss = 0.08128803\n",
      "Iteration 472, loss = 0.08098409\n",
      "Iteration 473, loss = 0.08068275\n",
      "Iteration 474, loss = 0.08038120\n",
      "Iteration 475, loss = 0.08008378\n",
      "Iteration 476, loss = 0.07978626\n",
      "Iteration 477, loss = 0.07949312\n",
      "Iteration 478, loss = 0.07919545\n",
      "Iteration 479, loss = 0.07890492\n",
      "Iteration 480, loss = 0.07861513\n",
      "Iteration 481, loss = 0.07832485\n",
      "Iteration 482, loss = 0.07803554\n",
      "Iteration 483, loss = 0.07775155\n",
      "Iteration 484, loss = 0.07746653\n",
      "Iteration 485, loss = 0.07718453\n",
      "Iteration 486, loss = 0.07690373\n",
      "Iteration 487, loss = 0.07662385\n",
      "Iteration 488, loss = 0.07634853\n",
      "Iteration 489, loss = 0.07607087\n",
      "Iteration 490, loss = 0.07579823\n",
      "Iteration 491, loss = 0.07552539\n",
      "Iteration 492, loss = 0.07525482\n",
      "Iteration 493, loss = 0.07498376\n",
      "Iteration 494, loss = 0.07471702\n",
      "Iteration 495, loss = 0.07444996\n",
      "Iteration 496, loss = 0.07418369\n",
      "Iteration 497, loss = 0.07392082\n",
      "Iteration 498, loss = 0.07365608\n",
      "Iteration 499, loss = 0.07339520\n",
      "Iteration 500, loss = 0.07313268\n",
      "Iteration 501, loss = 0.07287429\n",
      "Iteration 502, loss = 0.07261445\n",
      "Iteration 503, loss = 0.07236038\n",
      "Iteration 504, loss = 0.07210277\n",
      "Iteration 505, loss = 0.07184566\n",
      "Iteration 506, loss = 0.07158902\n",
      "Iteration 507, loss = 0.07133031\n",
      "Iteration 508, loss = 0.07107407\n",
      "Iteration 509, loss = 0.07081417\n",
      "Iteration 510, loss = 0.07054923\n",
      "Iteration 511, loss = 0.07029559\n",
      "Iteration 512, loss = 0.07004701\n",
      "Iteration 513, loss = 0.06980149\n",
      "Iteration 514, loss = 0.06956085\n",
      "Iteration 515, loss = 0.06932074\n",
      "Iteration 516, loss = 0.06908367\n",
      "Iteration 517, loss = 0.06884186\n",
      "Iteration 518, loss = 0.06860696\n",
      "Iteration 519, loss = 0.06837154\n",
      "Iteration 520, loss = 0.06813696\n",
      "Iteration 521, loss = 0.06790455\n",
      "Iteration 522, loss = 0.06767382\n",
      "Iteration 523, loss = 0.06744352\n",
      "Iteration 524, loss = 0.06721557\n",
      "Iteration 525, loss = 0.06698790\n",
      "Iteration 526, loss = 0.06676177\n",
      "Iteration 527, loss = 0.06653785\n",
      "Iteration 528, loss = 0.06631640\n",
      "Iteration 529, loss = 0.06609199\n",
      "Iteration 530, loss = 0.06587117\n",
      "Iteration 531, loss = 0.06565191\n",
      "Iteration 532, loss = 0.06543260\n",
      "Iteration 533, loss = 0.06521696\n",
      "Iteration 534, loss = 0.06499949\n",
      "Iteration 535, loss = 0.06478540\n",
      "Iteration 536, loss = 0.06457082\n",
      "Iteration 537, loss = 0.06435812\n",
      "Iteration 538, loss = 0.06414549\n",
      "Iteration 539, loss = 0.06393446\n",
      "Iteration 540, loss = 0.06372626\n",
      "Iteration 541, loss = 0.06351759\n",
      "Iteration 542, loss = 0.06331032\n",
      "Iteration 543, loss = 0.06310313\n",
      "Iteration 544, loss = 0.06289843\n",
      "Iteration 545, loss = 0.06269318\n",
      "Iteration 546, loss = 0.06248918\n",
      "Iteration 547, loss = 0.06228705\n",
      "Iteration 548, loss = 0.06208561\n",
      "Iteration 549, loss = 0.06188411\n",
      "Iteration 550, loss = 0.06168633\n",
      "Iteration 551, loss = 0.06148698\n",
      "Iteration 552, loss = 0.06128918\n",
      "Iteration 553, loss = 0.06109469\n",
      "Iteration 554, loss = 0.06089839\n",
      "Iteration 555, loss = 0.06070442\n",
      "Iteration 556, loss = 0.06051126\n",
      "Iteration 557, loss = 0.06031759\n",
      "Iteration 558, loss = 0.06012785\n",
      "Iteration 559, loss = 0.05993809\n",
      "Iteration 560, loss = 0.05974730\n",
      "Iteration 561, loss = 0.05955987\n",
      "Iteration 562, loss = 0.05937298\n",
      "Iteration 563, loss = 0.05918726\n",
      "Iteration 564, loss = 0.05900122\n",
      "Iteration 565, loss = 0.05881718\n",
      "Iteration 566, loss = 0.05863338\n",
      "Iteration 567, loss = 0.05845015\n",
      "Iteration 568, loss = 0.05826725\n",
      "Iteration 569, loss = 0.05808514\n",
      "Iteration 570, loss = 0.05790685\n",
      "Iteration 571, loss = 0.05772637\n",
      "Iteration 572, loss = 0.05754579\n",
      "Iteration 573, loss = 0.05736851\n",
      "Iteration 574, loss = 0.05719078\n",
      "Iteration 575, loss = 0.05701459\n",
      "Iteration 576, loss = 0.05683898\n",
      "Iteration 577, loss = 0.05666388\n",
      "Iteration 578, loss = 0.05649046\n",
      "Iteration 579, loss = 0.05631584\n",
      "Iteration 580, loss = 0.05614449\n",
      "Iteration 581, loss = 0.05597208\n",
      "Iteration 582, loss = 0.05580247\n",
      "Iteration 583, loss = 0.05563121\n",
      "Iteration 584, loss = 0.05546249\n",
      "Iteration 585, loss = 0.05529544\n",
      "Iteration 586, loss = 0.05512658\n",
      "Iteration 587, loss = 0.05496084\n",
      "Iteration 588, loss = 0.05479322\n",
      "Iteration 589, loss = 0.05462946\n",
      "Iteration 590, loss = 0.05446698\n",
      "Iteration 591, loss = 0.05430191\n",
      "Iteration 592, loss = 0.05413877\n",
      "Iteration 593, loss = 0.05397791\n",
      "Iteration 594, loss = 0.05381676\n",
      "Iteration 595, loss = 0.05365666\n",
      "Iteration 596, loss = 0.05349710\n",
      "Iteration 597, loss = 0.05333858\n",
      "Iteration 598, loss = 0.05318087\n",
      "Iteration 599, loss = 0.05302295\n",
      "Iteration 600, loss = 0.05286639\n",
      "Iteration 601, loss = 0.05271261\n",
      "Iteration 602, loss = 0.05255682\n",
      "Iteration 603, loss = 0.05240192\n",
      "Iteration 604, loss = 0.05224839\n",
      "Iteration 605, loss = 0.05209617\n",
      "Iteration 606, loss = 0.05194373\n",
      "Iteration 607, loss = 0.05179101\n",
      "Iteration 608, loss = 0.05164002\n",
      "Iteration 609, loss = 0.05148911\n",
      "Iteration 610, loss = 0.05133835\n",
      "Iteration 611, loss = 0.05118902\n",
      "Iteration 612, loss = 0.05104040\n",
      "Iteration 613, loss = 0.05089204\n",
      "Iteration 614, loss = 0.05074482\n",
      "Iteration 615, loss = 0.05059729\n",
      "Iteration 616, loss = 0.05045126\n",
      "Iteration 617, loss = 0.05030746\n",
      "Iteration 618, loss = 0.05016167\n",
      "Iteration 619, loss = 0.05001757\n",
      "Iteration 620, loss = 0.04987460\n",
      "Iteration 621, loss = 0.04973197\n",
      "Iteration 622, loss = 0.04958983\n",
      "Iteration 623, loss = 0.04944808\n",
      "Iteration 624, loss = 0.04930716\n",
      "Iteration 625, loss = 0.04916824\n",
      "Iteration 626, loss = 0.04902743\n",
      "Iteration 627, loss = 0.04888837\n",
      "Iteration 628, loss = 0.04874995\n",
      "Iteration 629, loss = 0.04861205\n",
      "Iteration 630, loss = 0.04847436\n",
      "Iteration 631, loss = 0.04833829\n",
      "Iteration 632, loss = 0.04820093\n",
      "Iteration 633, loss = 0.04806694\n",
      "Iteration 634, loss = 0.04793297\n",
      "Iteration 635, loss = 0.04779739\n",
      "Iteration 636, loss = 0.04766480\n",
      "Iteration 637, loss = 0.04753110\n",
      "Iteration 638, loss = 0.04739764\n",
      "Iteration 639, loss = 0.04726590\n",
      "Iteration 640, loss = 0.04713419\n",
      "Iteration 641, loss = 0.04700347\n",
      "Iteration 642, loss = 0.04687335\n",
      "Iteration 643, loss = 0.04674379\n",
      "Iteration 644, loss = 0.04661325\n",
      "Iteration 645, loss = 0.04648507\n",
      "Iteration 646, loss = 0.04635686\n",
      "Iteration 647, loss = 0.04622861\n",
      "Iteration 648, loss = 0.04610131\n",
      "Iteration 649, loss = 0.04597445\n",
      "Iteration 650, loss = 0.04584856\n",
      "Iteration 651, loss = 0.04572319\n",
      "Iteration 652, loss = 0.04559816\n",
      "Iteration 653, loss = 0.04547357\n",
      "Iteration 654, loss = 0.04534900\n",
      "Iteration 655, loss = 0.04522616\n",
      "Iteration 656, loss = 0.04510316\n",
      "Iteration 657, loss = 0.04498002\n",
      "Iteration 658, loss = 0.04485914\n",
      "Iteration 659, loss = 0.04473820\n",
      "Iteration 660, loss = 0.04461674\n",
      "Iteration 661, loss = 0.04449498\n",
      "Iteration 662, loss = 0.04437579\n",
      "Iteration 663, loss = 0.04425676\n",
      "Iteration 664, loss = 0.04413714\n",
      "Iteration 665, loss = 0.04401825\n",
      "Iteration 666, loss = 0.04390110\n",
      "Iteration 667, loss = 0.04378222\n",
      "Iteration 668, loss = 0.04366543\n",
      "Iteration 669, loss = 0.04354798\n",
      "Iteration 670, loss = 0.04343250\n",
      "Iteration 671, loss = 0.04331760\n",
      "Iteration 672, loss = 0.04320172\n",
      "Iteration 673, loss = 0.04308671\n",
      "Iteration 674, loss = 0.04297360\n",
      "Iteration 675, loss = 0.04285944\n",
      "Iteration 676, loss = 0.04274633\n",
      "Iteration 677, loss = 0.04263330\n",
      "Iteration 678, loss = 0.04252080\n",
      "Iteration 679, loss = 0.04240897\n",
      "Iteration 680, loss = 0.04229728\n",
      "Iteration 681, loss = 0.04218602\n",
      "Iteration 682, loss = 0.04207532\n",
      "Iteration 683, loss = 0.04196510\n",
      "Iteration 684, loss = 0.04185559\n",
      "Iteration 685, loss = 0.04174617\n",
      "Iteration 686, loss = 0.04163645\n",
      "Iteration 687, loss = 0.04152913\n",
      "Iteration 688, loss = 0.04142078\n",
      "Iteration 689, loss = 0.04131332\n",
      "Iteration 690, loss = 0.04120577\n",
      "Iteration 691, loss = 0.04109863\n",
      "Iteration 692, loss = 0.04099263\n",
      "Iteration 693, loss = 0.04088817\n",
      "Iteration 694, loss = 0.04078160\n",
      "Iteration 695, loss = 0.04067751\n",
      "Iteration 696, loss = 0.04057203\n",
      "Iteration 697, loss = 0.04046787\n",
      "Iteration 698, loss = 0.04036441\n",
      "Iteration 699, loss = 0.04026119\n",
      "Iteration 700, loss = 0.04015888\n",
      "Iteration 701, loss = 0.04005580\n",
      "Iteration 702, loss = 0.03995355\n",
      "Iteration 703, loss = 0.03985210\n",
      "Iteration 704, loss = 0.03975108\n",
      "Iteration 705, loss = 0.03965011\n",
      "Iteration 706, loss = 0.03954912\n",
      "Iteration 707, loss = 0.03944888\n",
      "Iteration 708, loss = 0.03934923\n",
      "Iteration 709, loss = 0.03924984\n",
      "Iteration 710, loss = 0.03914952\n",
      "Iteration 711, loss = 0.03905075\n",
      "Iteration 712, loss = 0.03895224\n",
      "Iteration 713, loss = 0.03885345\n",
      "Iteration 714, loss = 0.03875665\n",
      "Iteration 715, loss = 0.03865922\n",
      "Iteration 716, loss = 0.03856211\n",
      "Iteration 717, loss = 0.03846563\n",
      "Iteration 718, loss = 0.03836915\n",
      "Iteration 719, loss = 0.03827320\n",
      "Iteration 720, loss = 0.03817844\n",
      "Iteration 721, loss = 0.03808326\n",
      "Iteration 722, loss = 0.03798790\n",
      "Iteration 723, loss = 0.03789435\n",
      "Iteration 724, loss = 0.03780047\n",
      "Iteration 725, loss = 0.03770684\n",
      "Iteration 726, loss = 0.03761413\n",
      "Iteration 727, loss = 0.03752023\n",
      "Iteration 728, loss = 0.03742913\n",
      "Iteration 729, loss = 0.03733581\n",
      "Iteration 730, loss = 0.03724411\n",
      "Iteration 731, loss = 0.03715242\n",
      "Iteration 732, loss = 0.03706094\n",
      "Iteration 733, loss = 0.03697039\n",
      "Iteration 734, loss = 0.03688007\n",
      "Iteration 735, loss = 0.03678987\n",
      "Iteration 736, loss = 0.03669979\n",
      "Iteration 737, loss = 0.03661060\n",
      "Iteration 738, loss = 0.03652076\n",
      "Iteration 739, loss = 0.03643273\n",
      "Iteration 740, loss = 0.03634322\n",
      "Iteration 741, loss = 0.03625574\n",
      "Iteration 742, loss = 0.03616707\n",
      "Iteration 743, loss = 0.03607943\n",
      "Iteration 744, loss = 0.03599166\n",
      "Iteration 745, loss = 0.03590605\n",
      "Iteration 746, loss = 0.03581843\n",
      "Iteration 747, loss = 0.03573137\n",
      "Iteration 748, loss = 0.03564585\n",
      "Iteration 749, loss = 0.03555918\n",
      "Iteration 750, loss = 0.03547423\n",
      "Iteration 751, loss = 0.03538901\n",
      "Iteration 752, loss = 0.03530452\n",
      "Iteration 753, loss = 0.03521932\n",
      "Iteration 754, loss = 0.03513500\n",
      "Iteration 755, loss = 0.03505111\n",
      "Iteration 756, loss = 0.03496737\n",
      "Iteration 757, loss = 0.03488362\n",
      "Iteration 758, loss = 0.03480031\n",
      "Iteration 759, loss = 0.03471776\n",
      "Iteration 760, loss = 0.03463460\n",
      "Iteration 761, loss = 0.03455323\n",
      "Iteration 762, loss = 0.03447076\n",
      "Iteration 763, loss = 0.03438945\n",
      "Iteration 764, loss = 0.03430744\n",
      "Iteration 765, loss = 0.03422654\n",
      "Iteration 766, loss = 0.03414484\n",
      "Iteration 767, loss = 0.03406450\n",
      "Iteration 768, loss = 0.03398391\n",
      "Iteration 769, loss = 0.03390315\n",
      "Iteration 770, loss = 0.03382378\n",
      "Iteration 771, loss = 0.03374374\n",
      "Iteration 772, loss = 0.03366426\n",
      "Iteration 773, loss = 0.03358580\n",
      "Iteration 774, loss = 0.03350689\n",
      "Iteration 775, loss = 0.03342862\n",
      "Iteration 776, loss = 0.03335108\n",
      "Iteration 777, loss = 0.03327185\n",
      "Iteration 778, loss = 0.03319508\n",
      "Iteration 779, loss = 0.03311837\n",
      "Iteration 780, loss = 0.03304172\n",
      "Iteration 781, loss = 0.03296422\n",
      "Iteration 782, loss = 0.03288786\n",
      "Iteration 783, loss = 0.03281151\n",
      "Iteration 784, loss = 0.03273597\n",
      "Iteration 785, loss = 0.03266139\n",
      "Iteration 786, loss = 0.03258509\n",
      "Iteration 787, loss = 0.03251016\n",
      "Iteration 788, loss = 0.03243623\n",
      "Iteration 789, loss = 0.03236128\n",
      "Iteration 790, loss = 0.03228698\n",
      "Iteration 791, loss = 0.03221328\n",
      "Iteration 792, loss = 0.03213954\n",
      "Iteration 793, loss = 0.03206548\n",
      "Iteration 794, loss = 0.03199195\n",
      "Iteration 795, loss = 0.03191947\n",
      "Iteration 796, loss = 0.03184627\n",
      "Iteration 797, loss = 0.03177431\n",
      "Iteration 798, loss = 0.03170219\n",
      "Iteration 799, loss = 0.03163010\n",
      "Iteration 800, loss = 0.03155823\n",
      "Iteration 801, loss = 0.03148750\n",
      "Iteration 802, loss = 0.03141547\n",
      "Iteration 803, loss = 0.03134509\n",
      "Iteration 804, loss = 0.03127400\n",
      "Iteration 805, loss = 0.03120362\n",
      "Iteration 806, loss = 0.03113311\n",
      "Iteration 807, loss = 0.03106323\n",
      "Iteration 808, loss = 0.03099282\n",
      "Iteration 809, loss = 0.03092328\n",
      "Iteration 810, loss = 0.03085326\n",
      "Iteration 811, loss = 0.03078309\n",
      "Iteration 812, loss = 0.03071488\n",
      "Iteration 813, loss = 0.03064536\n",
      "Iteration 814, loss = 0.03057667\n",
      "Iteration 815, loss = 0.03050835\n",
      "Iteration 816, loss = 0.03044041\n",
      "Iteration 817, loss = 0.03037247\n",
      "Iteration 818, loss = 0.03030445\n",
      "Iteration 819, loss = 0.03023703\n",
      "Iteration 820, loss = 0.03016940\n",
      "Iteration 821, loss = 0.03010230\n",
      "Iteration 822, loss = 0.03003533\n",
      "Iteration 823, loss = 0.02996857\n",
      "Iteration 824, loss = 0.02990237\n",
      "Iteration 825, loss = 0.02983609\n",
      "Iteration 826, loss = 0.02976957\n",
      "Iteration 827, loss = 0.02970425\n",
      "Iteration 828, loss = 0.02963824\n",
      "Iteration 829, loss = 0.02957243\n",
      "Iteration 830, loss = 0.02950725\n",
      "Iteration 831, loss = 0.02944207\n",
      "Iteration 832, loss = 0.02937653\n",
      "Iteration 833, loss = 0.02931199\n",
      "Iteration 834, loss = 0.02924782\n",
      "Iteration 835, loss = 0.02918310\n",
      "Iteration 836, loss = 0.02911924\n",
      "Iteration 837, loss = 0.02905480\n",
      "Iteration 838, loss = 0.02898959\n",
      "Iteration 839, loss = 0.02892592\n",
      "Iteration 840, loss = 0.02886081\n",
      "Iteration 841, loss = 0.02879513\n",
      "Iteration 842, loss = 0.02872894\n",
      "Iteration 843, loss = 0.02866316\n",
      "Iteration 844, loss = 0.02859794\n",
      "Iteration 845, loss = 0.02852942\n",
      "Iteration 846, loss = 0.02846219\n",
      "Iteration 847, loss = 0.02839781\n",
      "Iteration 848, loss = 0.02833004\n",
      "Iteration 849, loss = 0.02825774\n",
      "Iteration 850, loss = 0.02817537\n",
      "Iteration 851, loss = 0.02808015\n",
      "Iteration 852, loss = 0.02798621\n",
      "Iteration 853, loss = 0.02790999\n",
      "Iteration 854, loss = 0.02785199\n",
      "Iteration 855, loss = 0.02779355\n",
      "Iteration 856, loss = 0.02773160\n",
      "Iteration 857, loss = 0.02766831\n",
      "Iteration 858, loss = 0.02760455\n",
      "Iteration 859, loss = 0.02754089\n",
      "Iteration 860, loss = 0.02747929\n",
      "Iteration 861, loss = 0.02741838\n",
      "Iteration 862, loss = 0.02735908\n",
      "Iteration 863, loss = 0.02730000\n",
      "Iteration 864, loss = 0.02724176\n",
      "Iteration 865, loss = 0.02718328\n",
      "Iteration 866, loss = 0.02712532\n",
      "Iteration 867, loss = 0.02706774\n",
      "Iteration 868, loss = 0.02701011\n",
      "Iteration 869, loss = 0.02695279\n",
      "Iteration 870, loss = 0.02689599\n",
      "Iteration 871, loss = 0.02683844\n",
      "Iteration 872, loss = 0.02678184\n",
      "Iteration 873, loss = 0.02672564\n",
      "Iteration 874, loss = 0.02666918\n",
      "Iteration 875, loss = 0.02661346\n",
      "Iteration 876, loss = 0.02655768\n",
      "Iteration 877, loss = 0.02650111\n",
      "Iteration 878, loss = 0.02644611\n",
      "Iteration 879, loss = 0.02638970\n",
      "Iteration 880, loss = 0.02633532\n",
      "Iteration 881, loss = 0.02627947\n",
      "Iteration 882, loss = 0.02622490\n",
      "Iteration 883, loss = 0.02616987\n",
      "Iteration 884, loss = 0.02611512\n",
      "Iteration 885, loss = 0.02606045\n",
      "Iteration 886, loss = 0.02600617\n",
      "Iteration 887, loss = 0.02595217\n",
      "Iteration 888, loss = 0.02589784\n",
      "Iteration 889, loss = 0.02584460\n",
      "Iteration 890, loss = 0.02579059\n",
      "Iteration 891, loss = 0.02573727\n",
      "Iteration 892, loss = 0.02568309\n",
      "Iteration 893, loss = 0.02563025\n",
      "Iteration 894, loss = 0.02557761\n",
      "Iteration 895, loss = 0.02552393\n",
      "Iteration 896, loss = 0.02547177\n",
      "Iteration 897, loss = 0.02541856\n",
      "Iteration 898, loss = 0.02536675\n",
      "Iteration 899, loss = 0.02531431\n",
      "Iteration 900, loss = 0.02526260\n",
      "Iteration 901, loss = 0.02521043\n",
      "Iteration 902, loss = 0.02515917\n",
      "Iteration 903, loss = 0.02510745\n",
      "Iteration 904, loss = 0.02505610\n",
      "Iteration 905, loss = 0.02500508\n",
      "Iteration 906, loss = 0.02495408\n",
      "Iteration 907, loss = 0.02490339\n",
      "Iteration 908, loss = 0.02485249\n",
      "Iteration 909, loss = 0.02480159\n",
      "Iteration 910, loss = 0.02475150\n",
      "Iteration 911, loss = 0.02470116\n",
      "Iteration 912, loss = 0.02465058\n",
      "Iteration 913, loss = 0.02460103\n",
      "Iteration 914, loss = 0.02455052\n",
      "Iteration 915, loss = 0.02450081\n",
      "Iteration 916, loss = 0.02445130\n",
      "Iteration 917, loss = 0.02440210\n",
      "Iteration 918, loss = 0.02435278\n",
      "Iteration 919, loss = 0.02430381\n",
      "Iteration 920, loss = 0.02425448\n",
      "Iteration 921, loss = 0.02420580\n",
      "Iteration 922, loss = 0.02415718\n",
      "Iteration 923, loss = 0.02410860\n",
      "Iteration 924, loss = 0.02406011\n",
      "Iteration 925, loss = 0.02401152\n",
      "Iteration 926, loss = 0.02396321\n",
      "Iteration 927, loss = 0.02391542\n",
      "Iteration 928, loss = 0.02386757\n",
      "Iteration 929, loss = 0.02381932\n",
      "Iteration 930, loss = 0.02377145\n",
      "Iteration 931, loss = 0.02372382\n",
      "Iteration 932, loss = 0.02367735\n",
      "Iteration 933, loss = 0.02362927\n",
      "Iteration 934, loss = 0.02358220\n",
      "Iteration 935, loss = 0.02353507\n",
      "Iteration 936, loss = 0.02348797\n",
      "Iteration 937, loss = 0.02344179\n",
      "Iteration 938, loss = 0.02339481\n",
      "Iteration 939, loss = 0.02334848\n",
      "Iteration 940, loss = 0.02330223\n",
      "Iteration 941, loss = 0.02325618\n",
      "Iteration 942, loss = 0.02321013\n",
      "Iteration 943, loss = 0.02316440\n",
      "Iteration 944, loss = 0.02311856\n",
      "Iteration 945, loss = 0.02307302\n",
      "Iteration 946, loss = 0.02302779\n",
      "Iteration 947, loss = 0.02298277\n",
      "Iteration 948, loss = 0.02293756\n",
      "Iteration 949, loss = 0.02289225\n",
      "Iteration 950, loss = 0.02284748\n",
      "Iteration 951, loss = 0.02280223\n",
      "Iteration 952, loss = 0.02275790\n",
      "Iteration 953, loss = 0.02271304\n",
      "Iteration 954, loss = 0.02266915\n",
      "Iteration 955, loss = 0.02262485\n",
      "Iteration 956, loss = 0.02258071\n",
      "Iteration 957, loss = 0.02253670\n",
      "Iteration 958, loss = 0.02249260\n",
      "Iteration 959, loss = 0.02244911\n",
      "Iteration 960, loss = 0.02240536\n",
      "Iteration 961, loss = 0.02236174\n",
      "Iteration 962, loss = 0.02231835\n",
      "Iteration 963, loss = 0.02227538\n",
      "Iteration 964, loss = 0.02223219\n",
      "Iteration 965, loss = 0.02218944\n",
      "Iteration 966, loss = 0.02214630\n",
      "Iteration 967, loss = 0.02210358\n",
      "Iteration 968, loss = 0.02206120\n",
      "Iteration 969, loss = 0.02201846\n",
      "Iteration 970, loss = 0.02197592\n",
      "Iteration 971, loss = 0.02193331\n",
      "Iteration 972, loss = 0.02189090\n",
      "Iteration 973, loss = 0.02184904\n",
      "Iteration 974, loss = 0.02180687\n",
      "Iteration 975, loss = 0.02176494\n",
      "Iteration 976, loss = 0.02172294\n",
      "Iteration 977, loss = 0.02168123\n",
      "Iteration 978, loss = 0.02163932\n",
      "Iteration 979, loss = 0.02159827\n",
      "Iteration 980, loss = 0.02155657\n",
      "Iteration 981, loss = 0.02151559\n",
      "Iteration 982, loss = 0.02147416\n",
      "Iteration 983, loss = 0.02143297\n",
      "Iteration 984, loss = 0.02139216\n",
      "Iteration 985, loss = 0.02135162\n",
      "Iteration 986, loss = 0.02131046\n",
      "Iteration 987, loss = 0.02127042\n",
      "Iteration 988, loss = 0.02122975\n",
      "Iteration 989, loss = 0.02118940\n",
      "Iteration 990, loss = 0.02114960\n",
      "Iteration 991, loss = 0.02110942\n",
      "Iteration 992, loss = 0.02106928\n",
      "Iteration 993, loss = 0.02102987\n",
      "Iteration 994, loss = 0.02099032\n",
      "Iteration 995, loss = 0.02095083\n",
      "Iteration 996, loss = 0.02091076\n",
      "Iteration 997, loss = 0.02087170\n",
      "Iteration 998, loss = 0.02083181\n",
      "Iteration 999, loss = 0.02079273\n",
      "Iteration 1000, loss = 0.02075379\n",
      "Iteration 1001, loss = 0.02071452\n",
      "Iteration 1002, loss = 0.02067572\n",
      "Iteration 1003, loss = 0.02063658\n",
      "Iteration 1004, loss = 0.02059804\n",
      "Iteration 1005, loss = 0.02055942\n",
      "Iteration 1006, loss = 0.02052080\n",
      "Iteration 1007, loss = 0.02048231\n",
      "Iteration 1008, loss = 0.02044421\n",
      "Iteration 1009, loss = 0.02040611\n",
      "Iteration 1010, loss = 0.02036777\n",
      "Iteration 1011, loss = 0.02032962\n",
      "Iteration 1012, loss = 0.02029190\n",
      "Iteration 1013, loss = 0.02025407\n",
      "Iteration 1014, loss = 0.02021649\n",
      "Iteration 1015, loss = 0.02017863\n",
      "Iteration 1016, loss = 0.02014100\n",
      "Iteration 1017, loss = 0.02010342\n",
      "Iteration 1018, loss = 0.02006595\n",
      "Iteration 1019, loss = 0.02002889\n",
      "Iteration 1020, loss = 0.01999119\n",
      "Iteration 1021, loss = 0.01995436\n",
      "Iteration 1022, loss = 0.01991708\n",
      "Iteration 1023, loss = 0.01988042\n",
      "Iteration 1024, loss = 0.01984315\n",
      "Iteration 1025, loss = 0.01980684\n",
      "Iteration 1026, loss = 0.01977013\n",
      "Iteration 1027, loss = 0.01973336\n",
      "Iteration 1028, loss = 0.01969663\n",
      "Iteration 1029, loss = 0.01966044\n",
      "Iteration 1030, loss = 0.01962406\n",
      "Iteration 1031, loss = 0.01958821\n",
      "Iteration 1032, loss = 0.01955171\n",
      "Iteration 1033, loss = 0.01951577\n",
      "Iteration 1034, loss = 0.01947960\n",
      "Iteration 1035, loss = 0.01944362\n",
      "Iteration 1036, loss = 0.01940838\n",
      "Iteration 1037, loss = 0.01937236\n",
      "Iteration 1038, loss = 0.01933702\n",
      "Iteration 1039, loss = 0.01930156\n",
      "Iteration 1040, loss = 0.01926626\n",
      "Iteration 1041, loss = 0.01923071\n",
      "Iteration 1042, loss = 0.01919619\n",
      "Iteration 1043, loss = 0.01916093\n",
      "Iteration 1044, loss = 0.01912588\n",
      "Iteration 1045, loss = 0.01909102\n",
      "Iteration 1046, loss = 0.01905649\n",
      "Iteration 1047, loss = 0.01902155\n",
      "Iteration 1048, loss = 0.01898713\n",
      "Iteration 1049, loss = 0.01895241\n",
      "Iteration 1050, loss = 0.01891805\n",
      "Iteration 1051, loss = 0.01888354\n",
      "Iteration 1052, loss = 0.01884956\n",
      "Iteration 1053, loss = 0.01881588\n",
      "Iteration 1054, loss = 0.01878138\n",
      "Iteration 1055, loss = 0.01874750\n",
      "Iteration 1056, loss = 0.01871344\n",
      "Iteration 1057, loss = 0.01868007\n",
      "Iteration 1058, loss = 0.01864632\n",
      "Iteration 1059, loss = 0.01861228\n",
      "Iteration 1060, loss = 0.01857923\n",
      "Iteration 1061, loss = 0.01854555\n",
      "Iteration 1062, loss = 0.01851223\n",
      "Iteration 1063, loss = 0.01847886\n",
      "Iteration 1064, loss = 0.01844586\n",
      "Iteration 1065, loss = 0.01841276\n",
      "Iteration 1066, loss = 0.01837970\n",
      "Iteration 1067, loss = 0.01834687\n",
      "Iteration 1068, loss = 0.01831439\n",
      "Iteration 1069, loss = 0.01828123\n",
      "Iteration 1070, loss = 0.01824882\n",
      "Iteration 1071, loss = 0.01821648\n",
      "Iteration 1072, loss = 0.01818384\n",
      "Iteration 1073, loss = 0.01815187\n",
      "Iteration 1074, loss = 0.01811925\n",
      "Iteration 1075, loss = 0.01808747\n",
      "Iteration 1076, loss = 0.01805525\n",
      "Iteration 1077, loss = 0.01802321\n",
      "Iteration 1078, loss = 0.01799165\n",
      "Iteration 1079, loss = 0.01795948\n",
      "Iteration 1080, loss = 0.01792775\n",
      "Iteration 1081, loss = 0.01789610\n",
      "Iteration 1082, loss = 0.01786441\n",
      "Iteration 1083, loss = 0.01783300\n",
      "Iteration 1084, loss = 0.01780159\n",
      "Iteration 1085, loss = 0.01777003\n",
      "Iteration 1086, loss = 0.01773867\n",
      "Iteration 1087, loss = 0.01770745\n",
      "Iteration 1088, loss = 0.01767624\n",
      "Iteration 1089, loss = 0.01764517\n",
      "Iteration 1090, loss = 0.01761417\n",
      "Iteration 1091, loss = 0.01758327\n",
      "Iteration 1092, loss = 0.01755248\n",
      "Iteration 1093, loss = 0.01752166\n",
      "Iteration 1094, loss = 0.01749106\n",
      "Iteration 1095, loss = 0.01746056\n",
      "Iteration 1096, loss = 0.01742996\n",
      "Iteration 1097, loss = 0.01739969\n",
      "Iteration 1098, loss = 0.01736942\n",
      "Iteration 1099, loss = 0.01733921\n",
      "Iteration 1100, loss = 0.01730900\n",
      "Iteration 1101, loss = 0.01727885\n",
      "Iteration 1102, loss = 0.01724866\n",
      "Iteration 1103, loss = 0.01721887\n",
      "Iteration 1104, loss = 0.01718863\n",
      "Iteration 1105, loss = 0.01715890\n",
      "Iteration 1106, loss = 0.01712894\n",
      "Iteration 1107, loss = 0.01709929\n",
      "Iteration 1108, loss = 0.01706971\n",
      "Iteration 1109, loss = 0.01704012\n",
      "Iteration 1110, loss = 0.01701061\n",
      "Iteration 1111, loss = 0.01698148\n",
      "Iteration 1112, loss = 0.01695202\n",
      "Iteration 1113, loss = 0.01692250\n",
      "Iteration 1114, loss = 0.01689348\n",
      "Iteration 1115, loss = 0.01686434\n",
      "Iteration 1116, loss = 0.01683521\n",
      "Iteration 1117, loss = 0.01680673\n",
      "Iteration 1118, loss = 0.01677760\n",
      "Iteration 1119, loss = 0.01674888\n",
      "Iteration 1120, loss = 0.01672014\n",
      "Iteration 1121, loss = 0.01669160\n",
      "Iteration 1122, loss = 0.01666261\n",
      "Iteration 1123, loss = 0.01663389\n",
      "Iteration 1124, loss = 0.01660574\n",
      "Iteration 1125, loss = 0.01657699\n",
      "Iteration 1126, loss = 0.01654843\n",
      "Iteration 1127, loss = 0.01652037\n",
      "Iteration 1128, loss = 0.01649193\n",
      "Iteration 1129, loss = 0.01646348\n",
      "Iteration 1130, loss = 0.01643547\n",
      "Iteration 1131, loss = 0.01640733\n",
      "Iteration 1132, loss = 0.01637904\n",
      "Iteration 1133, loss = 0.01635149\n",
      "Iteration 1134, loss = 0.01632336\n",
      "Iteration 1135, loss = 0.01629570\n",
      "Iteration 1136, loss = 0.01626785\n",
      "Iteration 1137, loss = 0.01624010\n",
      "Iteration 1138, loss = 0.01621261\n",
      "Iteration 1139, loss = 0.01618519\n",
      "Iteration 1140, loss = 0.01615727\n",
      "Iteration 1141, loss = 0.01612982\n",
      "Iteration 1142, loss = 0.01610235\n",
      "Iteration 1143, loss = 0.01607457\n",
      "Iteration 1144, loss = 0.01604697\n",
      "Iteration 1145, loss = 0.01601937\n",
      "Iteration 1146, loss = 0.01599135\n",
      "Iteration 1147, loss = 0.01596428\n",
      "Iteration 1148, loss = 0.01593645\n",
      "Iteration 1149, loss = 0.01590874\n",
      "Iteration 1150, loss = 0.01588162\n",
      "Iteration 1151, loss = 0.01585458\n",
      "Iteration 1152, loss = 0.01582711\n",
      "Iteration 1153, loss = 0.01580035\n",
      "Iteration 1154, loss = 0.01577314\n",
      "Iteration 1155, loss = 0.01574626\n",
      "Iteration 1156, loss = 0.01571965\n",
      "Iteration 1157, loss = 0.01569319\n",
      "Iteration 1158, loss = 0.01566614\n",
      "Iteration 1159, loss = 0.01563971\n",
      "Iteration 1160, loss = 0.01561327\n",
      "Iteration 1161, loss = 0.01558663\n",
      "Iteration 1162, loss = 0.01556039\n",
      "Iteration 1163, loss = 0.01553421\n",
      "Iteration 1164, loss = 0.01550814\n",
      "Iteration 1165, loss = 0.01548197\n",
      "Iteration 1166, loss = 0.01545594\n",
      "Iteration 1167, loss = 0.01543013\n",
      "Iteration 1168, loss = 0.01540420\n",
      "Iteration 1169, loss = 0.01537840\n",
      "Iteration 1170, loss = 0.01535268\n",
      "Iteration 1171, loss = 0.01532715\n",
      "Iteration 1172, loss = 0.01530174\n",
      "Iteration 1173, loss = 0.01527600\n",
      "Iteration 1174, loss = 0.01525063\n",
      "Iteration 1175, loss = 0.01522556\n",
      "Iteration 1176, loss = 0.01520018\n",
      "Iteration 1177, loss = 0.01517497\n",
      "Iteration 1178, loss = 0.01514964\n",
      "Iteration 1179, loss = 0.01512451\n",
      "Iteration 1180, loss = 0.01509936\n",
      "Iteration 1181, loss = 0.01507449\n",
      "Iteration 1182, loss = 0.01504951\n",
      "Iteration 1183, loss = 0.01502452\n",
      "Iteration 1184, loss = 0.01499953\n",
      "Iteration 1185, loss = 0.01497496\n",
      "Iteration 1186, loss = 0.01495012\n",
      "Iteration 1187, loss = 0.01492533\n",
      "Iteration 1188, loss = 0.01490077\n",
      "Iteration 1189, loss = 0.01487599\n",
      "Iteration 1190, loss = 0.01485089\n",
      "Iteration 1191, loss = 0.01482629\n",
      "Iteration 1192, loss = 0.01480197\n",
      "Iteration 1193, loss = 0.01477686\n",
      "Iteration 1194, loss = 0.01475245\n",
      "Iteration 1195, loss = 0.01472764\n",
      "Iteration 1196, loss = 0.01470261\n",
      "Iteration 1197, loss = 0.01467822\n",
      "Iteration 1198, loss = 0.01465346\n",
      "Iteration 1199, loss = 0.01462911\n",
      "Iteration 1200, loss = 0.01460437\n",
      "Iteration 1201, loss = 0.01458025\n",
      "Iteration 1202, loss = 0.01455590\n",
      "Iteration 1203, loss = 0.01453168\n",
      "Iteration 1204, loss = 0.01450754\n",
      "Iteration 1205, loss = 0.01448356\n",
      "Iteration 1206, loss = 0.01445997\n",
      "Iteration 1207, loss = 0.01443642\n",
      "Iteration 1208, loss = 0.01441272\n",
      "Iteration 1209, loss = 0.01438904\n",
      "Iteration 1210, loss = 0.01436523\n",
      "Iteration 1211, loss = 0.01434170\n",
      "Iteration 1212, loss = 0.01431826\n",
      "Iteration 1213, loss = 0.01429501\n",
      "Iteration 1214, loss = 0.01427175\n",
      "Iteration 1215, loss = 0.01424864\n",
      "Iteration 1216, loss = 0.01422534\n",
      "Iteration 1217, loss = 0.01420231\n",
      "Iteration 1218, loss = 0.01417931\n",
      "Iteration 1219, loss = 0.01415658\n",
      "Iteration 1220, loss = 0.01413358\n",
      "Iteration 1221, loss = 0.01411078\n",
      "Iteration 1222, loss = 0.01408799\n",
      "Iteration 1223, loss = 0.01406513\n",
      "Iteration 1224, loss = 0.01404238\n",
      "Iteration 1225, loss = 0.01401975\n",
      "Iteration 1226, loss = 0.01399745\n",
      "Iteration 1227, loss = 0.01397454\n",
      "Iteration 1228, loss = 0.01395199\n",
      "Iteration 1229, loss = 0.01392948\n",
      "Iteration 1230, loss = 0.01390761\n",
      "Iteration 1231, loss = 0.01388459\n",
      "Iteration 1232, loss = 0.01386253\n",
      "Iteration 1233, loss = 0.01384017\n",
      "Iteration 1234, loss = 0.01381800\n",
      "Iteration 1235, loss = 0.01379583\n",
      "Iteration 1236, loss = 0.01377364\n",
      "Iteration 1237, loss = 0.01375180\n",
      "Iteration 1238, loss = 0.01372965\n",
      "Iteration 1239, loss = 0.01370787\n",
      "Iteration 1240, loss = 0.01368606\n",
      "Iteration 1241, loss = 0.01366417\n",
      "Iteration 1242, loss = 0.01364257\n",
      "Iteration 1243, loss = 0.01362063\n",
      "Iteration 1244, loss = 0.01359908\n",
      "Iteration 1245, loss = 0.01357742\n",
      "Iteration 1246, loss = 0.01355592\n",
      "Iteration 1247, loss = 0.01353429\n",
      "Iteration 1248, loss = 0.01351303\n",
      "Iteration 1249, loss = 0.01349156\n",
      "Iteration 1250, loss = 0.01347018\n",
      "Iteration 1251, loss = 0.01344901\n",
      "Iteration 1252, loss = 0.01342759\n",
      "Iteration 1253, loss = 0.01340648\n",
      "Iteration 1254, loss = 0.01338540\n",
      "Iteration 1255, loss = 0.01336425\n",
      "Iteration 1256, loss = 0.01334335\n",
      "Iteration 1257, loss = 0.01332200\n",
      "Iteration 1258, loss = 0.01330129\n",
      "Iteration 1259, loss = 0.01328035\n",
      "Iteration 1260, loss = 0.01325934\n",
      "Iteration 1261, loss = 0.01323867\n",
      "Iteration 1262, loss = 0.01321784\n",
      "Iteration 1263, loss = 0.01319709\n",
      "Iteration 1264, loss = 0.01317647\n",
      "Iteration 1265, loss = 0.01315566\n",
      "Iteration 1266, loss = 0.01313514\n",
      "Iteration 1267, loss = 0.01311472\n",
      "Iteration 1268, loss = 0.01309392\n",
      "Iteration 1269, loss = 0.01307356\n",
      "Iteration 1270, loss = 0.01305307\n",
      "Iteration 1271, loss = 0.01303257\n",
      "Iteration 1272, loss = 0.01301214\n",
      "Iteration 1273, loss = 0.01299189\n",
      "Iteration 1274, loss = 0.01297139\n",
      "Iteration 1275, loss = 0.01295129\n",
      "Iteration 1276, loss = 0.01293085\n",
      "Iteration 1277, loss = 0.01291057\n",
      "Iteration 1278, loss = 0.01289038\n",
      "Iteration 1279, loss = 0.01287022\n",
      "Iteration 1280, loss = 0.01284976\n",
      "Iteration 1281, loss = 0.01282966\n",
      "Iteration 1282, loss = 0.01280915\n",
      "Iteration 1283, loss = 0.01278930\n",
      "Iteration 1284, loss = 0.01276852\n",
      "Iteration 1285, loss = 0.01274831\n",
      "Iteration 1286, loss = 0.01272782\n",
      "Iteration 1287, loss = 0.01270721\n",
      "Iteration 1288, loss = 0.01268714\n",
      "Iteration 1289, loss = 0.01266683\n",
      "Iteration 1290, loss = 0.01264640\n",
      "Iteration 1291, loss = 0.01262672\n",
      "Iteration 1292, loss = 0.01260651\n",
      "Iteration 1293, loss = 0.01258655\n",
      "Iteration 1294, loss = 0.01256653\n",
      "Iteration 1295, loss = 0.01254673\n",
      "Iteration 1296, loss = 0.01252707\n",
      "Iteration 1297, loss = 0.01250716\n",
      "Iteration 1298, loss = 0.01248802\n",
      "Iteration 1299, loss = 0.01246834\n",
      "Iteration 1300, loss = 0.01244873\n",
      "Iteration 1301, loss = 0.01242935\n",
      "Iteration 1302, loss = 0.01240976\n",
      "Iteration 1303, loss = 0.01239012\n",
      "Iteration 1304, loss = 0.01237110\n",
      "Iteration 1305, loss = 0.01235163\n",
      "Iteration 1306, loss = 0.01233204\n",
      "Iteration 1307, loss = 0.01231289\n",
      "Iteration 1308, loss = 0.01229381\n",
      "Iteration 1309, loss = 0.01227460\n",
      "Iteration 1310, loss = 0.01225544\n",
      "Iteration 1311, loss = 0.01223636\n",
      "Iteration 1312, loss = 0.01221771\n",
      "Iteration 1313, loss = 0.01219866\n",
      "Iteration 1314, loss = 0.01218005\n",
      "Iteration 1315, loss = 0.01216108\n",
      "Iteration 1316, loss = 0.01214259\n",
      "Iteration 1317, loss = 0.01212374\n",
      "Iteration 1318, loss = 0.01210537\n",
      "Iteration 1319, loss = 0.01208667\n",
      "Iteration 1320, loss = 0.01206817\n",
      "Iteration 1321, loss = 0.01204963\n",
      "Iteration 1322, loss = 0.01203137\n",
      "Iteration 1323, loss = 0.01201278\n",
      "Iteration 1324, loss = 0.01199447\n",
      "Iteration 1325, loss = 0.01197608\n",
      "Iteration 1326, loss = 0.01195787\n",
      "Iteration 1327, loss = 0.01193975\n",
      "Iteration 1328, loss = 0.01192140\n",
      "Iteration 1329, loss = 0.01190350\n",
      "Iteration 1330, loss = 0.01188542\n",
      "Iteration 1331, loss = 0.01186720\n",
      "Iteration 1332, loss = 0.01184920\n",
      "Iteration 1333, loss = 0.01183119\n",
      "Iteration 1334, loss = 0.01181337\n",
      "Iteration 1335, loss = 0.01179534\n",
      "Iteration 1336, loss = 0.01177744\n",
      "Iteration 1337, loss = 0.01175979\n",
      "Iteration 1338, loss = 0.01174185\n",
      "Iteration 1339, loss = 0.01172407\n",
      "Iteration 1340, loss = 0.01170640\n",
      "Iteration 1341, loss = 0.01168876\n",
      "Iteration 1342, loss = 0.01167117\n",
      "Iteration 1343, loss = 0.01165356\n",
      "Iteration 1344, loss = 0.01163621\n",
      "Iteration 1345, loss = 0.01161871\n",
      "Iteration 1346, loss = 0.01160129\n",
      "Iteration 1347, loss = 0.01158383\n",
      "Iteration 1348, loss = 0.01156637\n",
      "Iteration 1349, loss = 0.01154908\n",
      "Iteration 1350, loss = 0.01153154\n",
      "Iteration 1351, loss = 0.01151430\n",
      "Iteration 1352, loss = 0.01149704\n",
      "Iteration 1353, loss = 0.01147998\n",
      "Iteration 1354, loss = 0.01146285\n",
      "Iteration 1355, loss = 0.01144575\n",
      "Iteration 1356, loss = 0.01142875\n",
      "Iteration 1357, loss = 0.01141168\n",
      "Iteration 1358, loss = 0.01139469\n",
      "Iteration 1359, loss = 0.01137762\n",
      "Iteration 1360, loss = 0.01136075\n",
      "Iteration 1361, loss = 0.01134393\n",
      "Iteration 1362, loss = 0.01132695\n",
      "Iteration 1363, loss = 0.01131006\n",
      "Iteration 1364, loss = 0.01129322\n",
      "Iteration 1365, loss = 0.01127631\n",
      "Iteration 1366, loss = 0.01125967\n",
      "Iteration 1367, loss = 0.01124266\n",
      "Iteration 1368, loss = 0.01122598\n",
      "Iteration 1369, loss = 0.01120949\n",
      "Iteration 1370, loss = 0.01119279\n",
      "Iteration 1371, loss = 0.01117625\n",
      "Iteration 1372, loss = 0.01115978\n",
      "Iteration 1373, loss = 0.01114319\n",
      "Iteration 1374, loss = 0.01112666\n",
      "Iteration 1375, loss = 0.01111041\n",
      "Iteration 1376, loss = 0.01109407\n",
      "Iteration 1377, loss = 0.01107768\n",
      "Iteration 1378, loss = 0.01106128\n",
      "Iteration 1379, loss = 0.01104504\n",
      "Iteration 1380, loss = 0.01102869\n",
      "Iteration 1381, loss = 0.01101268\n",
      "Iteration 1382, loss = 0.01099634\n",
      "Iteration 1383, loss = 0.01098014\n",
      "Iteration 1384, loss = 0.01096397\n",
      "Iteration 1385, loss = 0.01094788\n",
      "Iteration 1386, loss = 0.01093171\n",
      "Iteration 1387, loss = 0.01091578\n",
      "Iteration 1388, loss = 0.01089992\n",
      "Iteration 1389, loss = 0.01088385\n",
      "Iteration 1390, loss = 0.01086755\n",
      "Iteration 1391, loss = 0.01085172\n",
      "Iteration 1392, loss = 0.01083593\n",
      "Iteration 1393, loss = 0.01082002\n",
      "Iteration 1394, loss = 0.01080432\n",
      "Iteration 1395, loss = 0.01078844\n",
      "Iteration 1396, loss = 0.01077268\n",
      "Iteration 1397, loss = 0.01075709\n",
      "Iteration 1398, loss = 0.01074154\n",
      "Iteration 1399, loss = 0.01072574\n",
      "Iteration 1400, loss = 0.01071030\n",
      "Iteration 1401, loss = 0.01069463\n",
      "Iteration 1402, loss = 0.01067912\n",
      "Iteration 1403, loss = 0.01066357\n",
      "Iteration 1404, loss = 0.01064820\n",
      "Iteration 1405, loss = 0.01063278\n",
      "Iteration 1406, loss = 0.01061737\n",
      "Iteration 1407, loss = 0.01060198\n",
      "Iteration 1408, loss = 0.01058669\n",
      "Iteration 1409, loss = 0.01057126\n",
      "Iteration 1410, loss = 0.01055617\n",
      "Iteration 1411, loss = 0.01054092\n",
      "Iteration 1412, loss = 0.01052565\n",
      "Iteration 1413, loss = 0.01051057\n",
      "Iteration 1414, loss = 0.01049537\n",
      "Iteration 1415, loss = 0.01048022\n",
      "Iteration 1416, loss = 0.01046516\n",
      "Iteration 1417, loss = 0.01045013\n",
      "Iteration 1418, loss = 0.01043502\n",
      "Iteration 1419, loss = 0.01042002\n",
      "Iteration 1420, loss = 0.01040492\n",
      "Iteration 1421, loss = 0.01038997\n",
      "Iteration 1422, loss = 0.01037502\n",
      "Iteration 1423, loss = 0.01036018\n",
      "Iteration 1424, loss = 0.01034515\n",
      "Iteration 1425, loss = 0.01033036\n",
      "Iteration 1426, loss = 0.01031554\n",
      "Iteration 1427, loss = 0.01030089\n",
      "Iteration 1428, loss = 0.01028602\n",
      "Iteration 1429, loss = 0.01027133\n",
      "Iteration 1430, loss = 0.01025646\n",
      "Iteration 1431, loss = 0.01024180\n",
      "Iteration 1432, loss = 0.01022727\n",
      "Iteration 1433, loss = 0.01021255\n",
      "Iteration 1434, loss = 0.01019782\n",
      "Iteration 1435, loss = 0.01018338\n",
      "Iteration 1436, loss = 0.01016879\n",
      "Iteration 1437, loss = 0.01015422\n",
      "Iteration 1438, loss = 0.01013963\n",
      "Iteration 1439, loss = 0.01012532\n",
      "Iteration 1440, loss = 0.01011082\n",
      "Iteration 1441, loss = 0.01009647\n",
      "Iteration 1442, loss = 0.01008204\n",
      "Iteration 1443, loss = 0.01006773\n",
      "Iteration 1444, loss = 0.01005339\n",
      "Iteration 1445, loss = 0.01003922\n",
      "Iteration 1446, loss = 0.01002486\n",
      "Iteration 1447, loss = 0.01001054\n",
      "Iteration 1448, loss = 0.00999632\n",
      "Iteration 1449, loss = 0.00998222\n",
      "Iteration 1450, loss = 0.00996808\n",
      "Iteration 1451, loss = 0.00995372\n",
      "Iteration 1452, loss = 0.00993972\n",
      "Iteration 1453, loss = 0.00992568\n",
      "Iteration 1454, loss = 0.00991169\n",
      "Iteration 1455, loss = 0.00989755\n",
      "Iteration 1456, loss = 0.00988356\n",
      "Iteration 1457, loss = 0.00986967\n",
      "Iteration 1458, loss = 0.00985572\n",
      "Iteration 1459, loss = 0.00984180\n",
      "Iteration 1460, loss = 0.00982795\n",
      "Iteration 1461, loss = 0.00981409\n",
      "Iteration 1462, loss = 0.00980015\n",
      "Iteration 1463, loss = 0.00978634\n",
      "Iteration 1464, loss = 0.00977259\n",
      "Iteration 1465, loss = 0.00975901\n",
      "Iteration 1466, loss = 0.00974524\n",
      "Iteration 1467, loss = 0.00973139\n",
      "Iteration 1468, loss = 0.00971779\n",
      "Iteration 1469, loss = 0.00970431\n",
      "Iteration 1470, loss = 0.00969065\n",
      "Iteration 1471, loss = 0.00967713\n",
      "Iteration 1472, loss = 0.00966353\n",
      "Iteration 1473, loss = 0.00964998\n",
      "Iteration 1474, loss = 0.00963624\n",
      "Iteration 1475, loss = 0.00962288\n",
      "Iteration 1476, loss = 0.00960943\n",
      "Iteration 1477, loss = 0.00959588\n",
      "Iteration 1478, loss = 0.00958253\n",
      "Iteration 1479, loss = 0.00956902\n",
      "Iteration 1480, loss = 0.00955551\n",
      "Iteration 1481, loss = 0.00954224\n",
      "Iteration 1482, loss = 0.00952904\n",
      "Iteration 1483, loss = 0.00951563\n",
      "Iteration 1484, loss = 0.00950237\n",
      "Iteration 1485, loss = 0.00948916\n",
      "Iteration 1486, loss = 0.00947582\n",
      "Iteration 1487, loss = 0.00946265\n",
      "Iteration 1488, loss = 0.00944939\n",
      "Iteration 1489, loss = 0.00943633\n",
      "Iteration 1490, loss = 0.00942309\n",
      "Iteration 1491, loss = 0.00941006\n",
      "Iteration 1492, loss = 0.00939710\n",
      "Iteration 1493, loss = 0.00938411\n",
      "Iteration 1494, loss = 0.00937129\n",
      "Iteration 1495, loss = 0.00935808\n",
      "Iteration 1496, loss = 0.00934510\n",
      "Iteration 1497, loss = 0.00933225\n",
      "Iteration 1498, loss = 0.00931932\n",
      "Iteration 1499, loss = 0.00930647\n",
      "Iteration 1500, loss = 0.00929356\n",
      "Iteration 1501, loss = 0.00928079\n",
      "Iteration 1502, loss = 0.00926797\n",
      "Iteration 1503, loss = 0.00925532\n",
      "Iteration 1504, loss = 0.00924235\n",
      "Iteration 1505, loss = 0.00922971\n",
      "Iteration 1506, loss = 0.00921701\n",
      "Iteration 1507, loss = 0.00920425\n",
      "Iteration 1508, loss = 0.00919146\n",
      "Iteration 1509, loss = 0.00917891\n",
      "Iteration 1510, loss = 0.00916615\n",
      "Iteration 1511, loss = 0.00915365\n",
      "Iteration 1512, loss = 0.00914098\n",
      "Iteration 1513, loss = 0.00912846\n",
      "Iteration 1514, loss = 0.00911575\n",
      "Iteration 1515, loss = 0.00910332\n",
      "Iteration 1516, loss = 0.00909072\n",
      "Iteration 1517, loss = 0.00907814\n",
      "Iteration 1518, loss = 0.00906572\n",
      "Iteration 1519, loss = 0.00905339\n",
      "Iteration 1520, loss = 0.00904100\n",
      "Iteration 1521, loss = 0.00902849\n",
      "Iteration 1522, loss = 0.00901612\n",
      "Iteration 1523, loss = 0.00900384\n",
      "Iteration 1524, loss = 0.00899140\n",
      "Iteration 1525, loss = 0.00897913\n",
      "Iteration 1526, loss = 0.00896682\n",
      "Iteration 1527, loss = 0.00895457\n",
      "Iteration 1528, loss = 0.00894230\n",
      "Iteration 1529, loss = 0.00892998\n",
      "Iteration 1530, loss = 0.00891770\n",
      "Iteration 1531, loss = 0.00890571\n",
      "Iteration 1532, loss = 0.00889340\n",
      "Iteration 1533, loss = 0.00888127\n",
      "Iteration 1534, loss = 0.00886904\n",
      "Iteration 1535, loss = 0.00885709\n",
      "Iteration 1536, loss = 0.00884497\n",
      "Iteration 1537, loss = 0.00883286\n",
      "Iteration 1538, loss = 0.00882079\n",
      "Iteration 1539, loss = 0.00880891\n",
      "Iteration 1540, loss = 0.00879690\n",
      "Iteration 1541, loss = 0.00878488\n",
      "Iteration 1542, loss = 0.00877299\n",
      "Iteration 1543, loss = 0.00876113\n",
      "Iteration 1544, loss = 0.00874922\n",
      "Iteration 1545, loss = 0.00873739\n",
      "Iteration 1546, loss = 0.00872557\n",
      "Iteration 1547, loss = 0.00871374\n",
      "Iteration 1548, loss = 0.00870197\n",
      "Iteration 1549, loss = 0.00869025\n",
      "Iteration 1550, loss = 0.00867854\n",
      "Iteration 1551, loss = 0.00866673\n",
      "Iteration 1552, loss = 0.00865503\n",
      "Iteration 1553, loss = 0.00864344\n",
      "Iteration 1554, loss = 0.00863156\n",
      "Iteration 1555, loss = 0.00862002\n",
      "Iteration 1556, loss = 0.00860849\n",
      "Iteration 1557, loss = 0.00859680\n",
      "Iteration 1558, loss = 0.00858525\n",
      "Iteration 1559, loss = 0.00857366\n",
      "Iteration 1560, loss = 0.00856201\n",
      "Iteration 1561, loss = 0.00855059\n",
      "Iteration 1562, loss = 0.00853899\n",
      "Iteration 1563, loss = 0.00852751\n",
      "Iteration 1564, loss = 0.00851601\n",
      "Iteration 1565, loss = 0.00850456\n",
      "Iteration 1566, loss = 0.00849311\n",
      "Iteration 1567, loss = 0.00848164\n",
      "Iteration 1568, loss = 0.00847026\n",
      "Iteration 1569, loss = 0.00845901\n",
      "Iteration 1570, loss = 0.00844767\n",
      "Iteration 1571, loss = 0.00843630\n",
      "Iteration 1572, loss = 0.00842501\n",
      "Iteration 1573, loss = 0.00841373\n",
      "Iteration 1574, loss = 0.00840248\n",
      "Iteration 1575, loss = 0.00839145\n",
      "Iteration 1576, loss = 0.00838013\n",
      "Iteration 1577, loss = 0.00836894\n",
      "Iteration 1578, loss = 0.00835777\n",
      "Iteration 1579, loss = 0.00834662\n",
      "Iteration 1580, loss = 0.00833552\n",
      "Iteration 1581, loss = 0.00832434\n",
      "Iteration 1582, loss = 0.00831328\n",
      "Iteration 1583, loss = 0.00830230\n",
      "Iteration 1584, loss = 0.00829125\n",
      "Iteration 1585, loss = 0.00828029\n",
      "Iteration 1586, loss = 0.00826907\n",
      "Iteration 1587, loss = 0.00825815\n",
      "Iteration 1588, loss = 0.00824716\n",
      "Iteration 1589, loss = 0.00823623\n",
      "Iteration 1590, loss = 0.00822524\n",
      "Iteration 1591, loss = 0.00821427\n",
      "Iteration 1592, loss = 0.00820342\n",
      "Iteration 1593, loss = 0.00819247\n",
      "Iteration 1594, loss = 0.00818169\n",
      "Iteration 1595, loss = 0.00817080\n",
      "Iteration 1596, loss = 0.00815981\n",
      "Iteration 1597, loss = 0.00814907\n",
      "Iteration 1598, loss = 0.00813822\n",
      "Iteration 1599, loss = 0.00812750\n",
      "Iteration 1600, loss = 0.00811666\n",
      "Iteration 1601, loss = 0.00810585\n",
      "Iteration 1602, loss = 0.00809518\n",
      "Iteration 1603, loss = 0.00808434\n",
      "Iteration 1604, loss = 0.00807370\n",
      "Iteration 1605, loss = 0.00806289\n",
      "Iteration 1606, loss = 0.00805247\n",
      "Iteration 1607, loss = 0.00804174\n",
      "Iteration 1608, loss = 0.00803110\n",
      "Iteration 1609, loss = 0.00802053\n",
      "Iteration 1610, loss = 0.00800991\n",
      "Iteration 1611, loss = 0.00799939\n",
      "Iteration 1612, loss = 0.00798893\n",
      "Iteration 1613, loss = 0.00797832\n",
      "Iteration 1614, loss = 0.00796786\n",
      "Iteration 1615, loss = 0.00795739\n",
      "Iteration 1616, loss = 0.00794693\n",
      "Iteration 1617, loss = 0.00793650\n",
      "Iteration 1618, loss = 0.00792609\n",
      "Iteration 1619, loss = 0.00791580\n",
      "Iteration 1620, loss = 0.00790527\n",
      "Iteration 1621, loss = 0.00789503\n",
      "Iteration 1622, loss = 0.00788461\n",
      "Iteration 1623, loss = 0.00787429\n",
      "Iteration 1624, loss = 0.00786394\n",
      "Iteration 1625, loss = 0.00785364\n",
      "Iteration 1626, loss = 0.00784330\n",
      "Iteration 1627, loss = 0.00783304\n",
      "Iteration 1628, loss = 0.00782276\n",
      "Iteration 1629, loss = 0.00781263\n",
      "Iteration 1630, loss = 0.00780245\n",
      "Iteration 1631, loss = 0.00779237\n",
      "Iteration 1632, loss = 0.00778210\n",
      "Iteration 1633, loss = 0.00777200\n",
      "Iteration 1634, loss = 0.00776199\n",
      "Iteration 1635, loss = 0.00775187\n",
      "Iteration 1636, loss = 0.00774179\n",
      "Iteration 1637, loss = 0.00773171\n",
      "Iteration 1638, loss = 0.00772163\n",
      "Iteration 1639, loss = 0.00771166\n",
      "Iteration 1640, loss = 0.00770172\n",
      "Iteration 1641, loss = 0.00769172\n",
      "Iteration 1642, loss = 0.00768178\n",
      "Iteration 1643, loss = 0.00767174\n",
      "Iteration 1644, loss = 0.00766186\n",
      "Iteration 1645, loss = 0.00765182\n",
      "Iteration 1646, loss = 0.00764201\n",
      "Iteration 1647, loss = 0.00763220\n",
      "Iteration 1648, loss = 0.00762223\n",
      "Iteration 1649, loss = 0.00761233\n",
      "Iteration 1650, loss = 0.00760257\n",
      "Iteration 1651, loss = 0.00759272\n",
      "Iteration 1652, loss = 0.00758289\n",
      "Iteration 1653, loss = 0.00757303\n",
      "Iteration 1654, loss = 0.00756324\n",
      "Iteration 1655, loss = 0.00755346\n",
      "Iteration 1656, loss = 0.00754377\n",
      "Iteration 1657, loss = 0.00753402\n",
      "Iteration 1658, loss = 0.00752425\n",
      "Iteration 1659, loss = 0.00751464\n",
      "Iteration 1660, loss = 0.00750492\n",
      "Iteration 1661, loss = 0.00749515\n",
      "Iteration 1662, loss = 0.00748568\n",
      "Iteration 1663, loss = 0.00747594\n",
      "Iteration 1664, loss = 0.00746632\n",
      "Iteration 1665, loss = 0.00745682\n",
      "Iteration 1666, loss = 0.00744716\n",
      "Iteration 1667, loss = 0.00743766\n",
      "Iteration 1668, loss = 0.00742808\n",
      "Iteration 1669, loss = 0.00741858\n",
      "Iteration 1670, loss = 0.00740910\n",
      "Iteration 1671, loss = 0.00739959\n",
      "Iteration 1672, loss = 0.00738999\n",
      "Iteration 1673, loss = 0.00738054\n",
      "Iteration 1674, loss = 0.00737122\n",
      "Iteration 1675, loss = 0.00736155\n",
      "Iteration 1676, loss = 0.00735221\n",
      "Iteration 1677, loss = 0.00734275\n",
      "Iteration 1678, loss = 0.00733340\n",
      "Iteration 1679, loss = 0.00732398\n",
      "Iteration 1680, loss = 0.00731457\n",
      "Iteration 1681, loss = 0.00730521\n",
      "Iteration 1682, loss = 0.00729597\n",
      "Iteration 1683, loss = 0.00728658\n",
      "Iteration 1684, loss = 0.00727722\n",
      "Iteration 1685, loss = 0.00726796\n",
      "Iteration 1686, loss = 0.00725875\n",
      "Iteration 1687, loss = 0.00724937\n",
      "Iteration 1688, loss = 0.00724013\n",
      "Iteration 1689, loss = 0.00723090\n",
      "Iteration 1690, loss = 0.00722166\n",
      "Iteration 1691, loss = 0.00721248\n",
      "Iteration 1692, loss = 0.00720335\n",
      "Iteration 1693, loss = 0.00719415\n",
      "Iteration 1694, loss = 0.00718501\n",
      "Iteration 1695, loss = 0.00717583\n",
      "Iteration 1696, loss = 0.00716668\n",
      "Iteration 1697, loss = 0.00715767\n",
      "Iteration 1698, loss = 0.00714856\n",
      "Iteration 1699, loss = 0.00713954\n",
      "Iteration 1700, loss = 0.00713050\n",
      "Iteration 1701, loss = 0.00712151\n",
      "Iteration 1702, loss = 0.00711243\n",
      "Iteration 1703, loss = 0.00710338\n",
      "Iteration 1704, loss = 0.00709443\n",
      "Iteration 1705, loss = 0.00708551\n",
      "Iteration 1706, loss = 0.00707657\n",
      "Iteration 1707, loss = 0.00706764\n",
      "Iteration 1708, loss = 0.00705874\n",
      "Iteration 1709, loss = 0.00704969\n",
      "Iteration 1710, loss = 0.00704087\n",
      "Iteration 1711, loss = 0.00703202\n",
      "Iteration 1712, loss = 0.00702310\n",
      "Iteration 1713, loss = 0.00701420\n",
      "Iteration 1714, loss = 0.00700554\n",
      "Iteration 1715, loss = 0.00699661\n",
      "Iteration 1716, loss = 0.00698777\n",
      "Iteration 1717, loss = 0.00697896\n",
      "Iteration 1718, loss = 0.00697024\n",
      "Iteration 1719, loss = 0.00696155\n",
      "Iteration 1720, loss = 0.00695273\n",
      "Iteration 1721, loss = 0.00694391\n",
      "Iteration 1722, loss = 0.00693525\n",
      "Iteration 1723, loss = 0.00692658\n",
      "Iteration 1724, loss = 0.00691785\n",
      "Iteration 1725, loss = 0.00690917\n",
      "Iteration 1726, loss = 0.00690041\n",
      "Iteration 1727, loss = 0.00689182\n",
      "Iteration 1728, loss = 0.00688314\n",
      "Iteration 1729, loss = 0.00687447\n",
      "Iteration 1730, loss = 0.00686586\n",
      "Iteration 1731, loss = 0.00685728\n",
      "Iteration 1732, loss = 0.00684867\n",
      "Iteration 1733, loss = 0.00684024\n",
      "Iteration 1734, loss = 0.00683154\n",
      "Iteration 1735, loss = 0.00682304\n",
      "Iteration 1736, loss = 0.00681452\n",
      "Iteration 1737, loss = 0.00680605\n",
      "Iteration 1738, loss = 0.00679749\n",
      "Iteration 1739, loss = 0.00678894\n",
      "Iteration 1740, loss = 0.00678051\n",
      "Iteration 1741, loss = 0.00677214\n",
      "Iteration 1742, loss = 0.00676367\n",
      "Iteration 1743, loss = 0.00675520\n",
      "Iteration 1744, loss = 0.00674681\n",
      "Iteration 1745, loss = 0.00673831\n",
      "Iteration 1746, loss = 0.00672993\n",
      "Iteration 1747, loss = 0.00672168\n",
      "Iteration 1748, loss = 0.00671327\n",
      "Iteration 1749, loss = 0.00670500\n",
      "Iteration 1750, loss = 0.00669664\n",
      "Iteration 1751, loss = 0.00668841\n",
      "Iteration 1752, loss = 0.00668001\n",
      "Iteration 1753, loss = 0.00667187\n",
      "Iteration 1754, loss = 0.00666350\n",
      "Iteration 1755, loss = 0.00665541\n",
      "Iteration 1756, loss = 0.00664707\n",
      "Iteration 1757, loss = 0.00663882\n",
      "Iteration 1758, loss = 0.00663068\n",
      "Iteration 1759, loss = 0.00662247\n",
      "Iteration 1760, loss = 0.00661422\n",
      "Iteration 1761, loss = 0.00660611\n",
      "Iteration 1762, loss = 0.00659798\n",
      "Iteration 1763, loss = 0.00658974\n",
      "Iteration 1764, loss = 0.00658157\n",
      "Iteration 1765, loss = 0.00657350\n",
      "Iteration 1766, loss = 0.00656528\n",
      "Iteration 1767, loss = 0.00655728\n",
      "Iteration 1768, loss = 0.00654907\n",
      "Iteration 1769, loss = 0.00654093\n",
      "Iteration 1770, loss = 0.00653294\n",
      "Iteration 1771, loss = 0.00652482\n",
      "Iteration 1772, loss = 0.00651673\n",
      "Iteration 1773, loss = 0.00650876\n",
      "Iteration 1774, loss = 0.00650070\n",
      "Iteration 1775, loss = 0.00649261\n",
      "Iteration 1776, loss = 0.00648465\n",
      "Iteration 1777, loss = 0.00647660\n",
      "Iteration 1778, loss = 0.00646873\n",
      "Iteration 1779, loss = 0.00646077\n",
      "Iteration 1780, loss = 0.00645278\n",
      "Iteration 1781, loss = 0.00644483\n",
      "Iteration 1782, loss = 0.00643692\n",
      "Iteration 1783, loss = 0.00642904\n",
      "Iteration 1784, loss = 0.00642118\n",
      "Iteration 1785, loss = 0.00641326\n",
      "Iteration 1786, loss = 0.00640541\n",
      "Iteration 1787, loss = 0.00639763\n",
      "Iteration 1788, loss = 0.00638972\n",
      "Iteration 1789, loss = 0.00638187\n",
      "Iteration 1790, loss = 0.00637411\n",
      "Iteration 1791, loss = 0.00636630\n",
      "Iteration 1792, loss = 0.00635842\n",
      "Iteration 1793, loss = 0.00635074\n",
      "Iteration 1794, loss = 0.00634297\n",
      "Iteration 1795, loss = 0.00633513\n",
      "Iteration 1796, loss = 0.00632747\n",
      "Iteration 1797, loss = 0.00631967\n",
      "Iteration 1798, loss = 0.00631196\n",
      "Iteration 1799, loss = 0.00630429\n",
      "Iteration 1800, loss = 0.00629646\n",
      "Iteration 1801, loss = 0.00628884\n",
      "Iteration 1802, loss = 0.00628121\n",
      "Iteration 1803, loss = 0.00627355\n",
      "Iteration 1804, loss = 0.00626583\n",
      "Iteration 1805, loss = 0.00625821\n",
      "Iteration 1806, loss = 0.00625059\n",
      "Iteration 1807, loss = 0.00624290\n",
      "Iteration 1808, loss = 0.00623534\n",
      "Iteration 1809, loss = 0.00622770\n",
      "Iteration 1810, loss = 0.00622011\n",
      "Iteration 1811, loss = 0.00621259\n",
      "Iteration 1812, loss = 0.00620493\n",
      "Iteration 1813, loss = 0.00619747\n",
      "Iteration 1814, loss = 0.00618994\n",
      "Iteration 1815, loss = 0.00618240\n",
      "Iteration 1816, loss = 0.00617499\n",
      "Iteration 1817, loss = 0.00616747\n",
      "Iteration 1818, loss = 0.00616007\n",
      "Iteration 1819, loss = 0.00615264\n",
      "Iteration 1820, loss = 0.00614518\n",
      "Iteration 1821, loss = 0.00613771\n",
      "Iteration 1822, loss = 0.00613036\n",
      "Iteration 1823, loss = 0.00612294\n",
      "Iteration 1824, loss = 0.00611550\n",
      "Iteration 1825, loss = 0.00610810\n",
      "Iteration 1826, loss = 0.00610078\n",
      "Iteration 1827, loss = 0.00609339\n",
      "Iteration 1828, loss = 0.00608598\n",
      "Iteration 1829, loss = 0.00607872\n",
      "Iteration 1830, loss = 0.00607131\n",
      "Iteration 1831, loss = 0.00606399\n",
      "Iteration 1832, loss = 0.00605666\n",
      "Iteration 1833, loss = 0.00604928\n",
      "Iteration 1834, loss = 0.00604205\n",
      "Iteration 1835, loss = 0.00603469\n",
      "Iteration 1836, loss = 0.00602747\n",
      "Iteration 1837, loss = 0.00602019\n",
      "Iteration 1838, loss = 0.00601295\n",
      "Iteration 1839, loss = 0.00600575\n",
      "Iteration 1840, loss = 0.00599844\n",
      "Iteration 1841, loss = 0.00599132\n",
      "Iteration 1842, loss = 0.00598426\n",
      "Iteration 1843, loss = 0.00597688\n",
      "Iteration 1844, loss = 0.00596978\n",
      "Iteration 1845, loss = 0.00596252\n",
      "Iteration 1846, loss = 0.00595548\n",
      "Iteration 1847, loss = 0.00594836\n",
      "Iteration 1848, loss = 0.00594115\n",
      "Iteration 1849, loss = 0.00593406\n",
      "Iteration 1850, loss = 0.00592687\n",
      "Iteration 1851, loss = 0.00591983\n",
      "Iteration 1852, loss = 0.00591265\n",
      "Iteration 1853, loss = 0.00590557\n",
      "Iteration 1854, loss = 0.00589853\n",
      "Iteration 1855, loss = 0.00589151\n",
      "Iteration 1856, loss = 0.00588436\n",
      "Iteration 1857, loss = 0.00587736\n",
      "Iteration 1858, loss = 0.00587029\n",
      "Iteration 1859, loss = 0.00586328\n",
      "Iteration 1860, loss = 0.00585629\n",
      "Iteration 1861, loss = 0.00584928\n",
      "Iteration 1862, loss = 0.00584232\n",
      "Iteration 1863, loss = 0.00583533\n",
      "Iteration 1864, loss = 0.00582836\n",
      "Iteration 1865, loss = 0.00582143\n",
      "Iteration 1866, loss = 0.00581448\n",
      "Iteration 1867, loss = 0.00580757\n",
      "Iteration 1868, loss = 0.00580070\n",
      "Iteration 1869, loss = 0.00579374\n",
      "Iteration 1870, loss = 0.00578684\n",
      "Iteration 1871, loss = 0.00578006\n",
      "Iteration 1872, loss = 0.00577317\n",
      "Iteration 1873, loss = 0.00576623\n",
      "Iteration 1874, loss = 0.00575941\n",
      "Iteration 1875, loss = 0.00575258\n",
      "Iteration 1876, loss = 0.00574582\n",
      "Iteration 1877, loss = 0.00573889\n",
      "Iteration 1878, loss = 0.00573208\n",
      "Iteration 1879, loss = 0.00572531\n",
      "Iteration 1880, loss = 0.00571850\n",
      "Iteration 1881, loss = 0.00571173\n",
      "Iteration 1882, loss = 0.00570492\n",
      "Iteration 1883, loss = 0.00569823\n",
      "Iteration 1884, loss = 0.00569144\n",
      "Iteration 1885, loss = 0.00568478\n",
      "Iteration 1886, loss = 0.00567798\n",
      "Iteration 1887, loss = 0.00567127\n",
      "Iteration 1888, loss = 0.00566460\n",
      "Iteration 1889, loss = 0.00565791\n",
      "Iteration 1890, loss = 0.00565119\n",
      "Iteration 1891, loss = 0.00564451\n",
      "Iteration 1892, loss = 0.00563790\n",
      "Iteration 1893, loss = 0.00563120\n",
      "Iteration 1894, loss = 0.00562459\n",
      "Iteration 1895, loss = 0.00561790\n",
      "Iteration 1896, loss = 0.00561128\n",
      "Iteration 1897, loss = 0.00560471\n",
      "Iteration 1898, loss = 0.00559806\n",
      "Iteration 1899, loss = 0.00559146\n",
      "Iteration 1900, loss = 0.00558489\n",
      "Iteration 1901, loss = 0.00557827\n",
      "Iteration 1902, loss = 0.00557174\n",
      "Iteration 1903, loss = 0.00556515\n",
      "Iteration 1904, loss = 0.00555862\n",
      "Iteration 1905, loss = 0.00555203\n",
      "Iteration 1906, loss = 0.00554561\n",
      "Iteration 1907, loss = 0.00553902\n",
      "Iteration 1908, loss = 0.00553252\n",
      "Iteration 1909, loss = 0.00552605\n",
      "Iteration 1910, loss = 0.00551957\n",
      "Iteration 1911, loss = 0.00551315\n",
      "Iteration 1912, loss = 0.00550669\n",
      "Iteration 1913, loss = 0.00550019\n",
      "Iteration 1914, loss = 0.00549369\n",
      "Iteration 1915, loss = 0.00548728\n",
      "Iteration 1916, loss = 0.00548079\n",
      "Iteration 1917, loss = 0.00547446\n",
      "Iteration 1918, loss = 0.00546803\n",
      "Iteration 1919, loss = 0.00546157\n",
      "Iteration 1920, loss = 0.00545529\n",
      "Iteration 1921, loss = 0.00544883\n",
      "Iteration 1922, loss = 0.00544245\n",
      "Iteration 1923, loss = 0.00543609\n",
      "Iteration 1924, loss = 0.00542975\n",
      "Iteration 1925, loss = 0.00542334\n",
      "Iteration 1926, loss = 0.00541706\n",
      "Iteration 1927, loss = 0.00541069\n",
      "Iteration 1928, loss = 0.00540438\n",
      "Iteration 1929, loss = 0.00539806\n",
      "Iteration 1930, loss = 0.00539174\n",
      "Iteration 1931, loss = 0.00538548\n",
      "Iteration 1932, loss = 0.00537916\n",
      "Iteration 1933, loss = 0.00537295\n",
      "Iteration 1934, loss = 0.00536665\n",
      "Iteration 1935, loss = 0.00536042\n",
      "Iteration 1936, loss = 0.00535421\n",
      "Iteration 1937, loss = 0.00534804\n",
      "Iteration 1938, loss = 0.00534181\n",
      "Iteration 1939, loss = 0.00533561\n",
      "Iteration 1940, loss = 0.00532937\n",
      "Iteration 1941, loss = 0.00532319\n",
      "Iteration 1942, loss = 0.00531704\n",
      "Iteration 1943, loss = 0.00531088\n",
      "Iteration 1944, loss = 0.00530466\n",
      "Iteration 1945, loss = 0.00529851\n",
      "Iteration 1946, loss = 0.00529242\n",
      "Iteration 1947, loss = 0.00528627\n",
      "Iteration 1948, loss = 0.00528015\n",
      "Iteration 1949, loss = 0.00527410\n",
      "Iteration 1950, loss = 0.00526797\n",
      "Iteration 1951, loss = 0.00526192\n",
      "Iteration 1952, loss = 0.00525581\n",
      "Iteration 1953, loss = 0.00524979\n",
      "Iteration 1954, loss = 0.00524375\n",
      "Iteration 1955, loss = 0.00523770\n",
      "Iteration 1956, loss = 0.00523161\n",
      "Iteration 1957, loss = 0.00522561\n",
      "Iteration 1958, loss = 0.00521954\n",
      "Iteration 1959, loss = 0.00521358\n",
      "Iteration 1960, loss = 0.00520754\n",
      "Iteration 1961, loss = 0.00520158\n",
      "Iteration 1962, loss = 0.00519558\n",
      "Iteration 1963, loss = 0.00518964\n",
      "Iteration 1964, loss = 0.00518366\n",
      "Iteration 1965, loss = 0.00517773\n",
      "Iteration 1966, loss = 0.00517173\n",
      "Iteration 1967, loss = 0.00516587\n",
      "Iteration 1968, loss = 0.00515989\n",
      "Iteration 1969, loss = 0.00515404\n",
      "Iteration 1970, loss = 0.00514811\n",
      "Iteration 1971, loss = 0.00514222\n",
      "Iteration 1972, loss = 0.00513634\n",
      "Iteration 1973, loss = 0.00513045\n",
      "Iteration 1974, loss = 0.00512456\n",
      "Iteration 1975, loss = 0.00511873\n",
      "Iteration 1976, loss = 0.00511286\n",
      "Iteration 1977, loss = 0.00510698\n",
      "Iteration 1978, loss = 0.00510118\n",
      "Iteration 1979, loss = 0.00509537\n",
      "Iteration 1980, loss = 0.00508951\n",
      "Iteration 1981, loss = 0.00508371\n",
      "Iteration 1982, loss = 0.00507788\n",
      "Iteration 1983, loss = 0.00507206\n",
      "Iteration 1984, loss = 0.00506628\n",
      "Iteration 1985, loss = 0.00506044\n",
      "Iteration 1986, loss = 0.00505471\n",
      "Iteration 1987, loss = 0.00504891\n",
      "Iteration 1988, loss = 0.00504314\n",
      "Iteration 1989, loss = 0.00503737\n",
      "Iteration 1990, loss = 0.00503168\n",
      "Iteration 1991, loss = 0.00502588\n",
      "Iteration 1992, loss = 0.00502021\n",
      "Iteration 1993, loss = 0.00501450\n",
      "Iteration 1994, loss = 0.00500880\n",
      "Iteration 1995, loss = 0.00500313\n",
      "Iteration 1996, loss = 0.00499745\n",
      "Iteration 1997, loss = 0.00499170\n",
      "Iteration 1998, loss = 0.00498605\n",
      "Iteration 1999, loss = 0.00498040\n",
      "Iteration 2000, loss = 0.00497482\n",
      "Iteration 2001, loss = 0.00496910\n",
      "Iteration 2002, loss = 0.00496350\n",
      "Iteration 2003, loss = 0.00495791\n",
      "Iteration 2004, loss = 0.00495225\n",
      "Iteration 2005, loss = 0.00494658\n",
      "Iteration 2006, loss = 0.00494100\n",
      "Iteration 2007, loss = 0.00493531\n",
      "Iteration 2008, loss = 0.00492968\n",
      "Iteration 2009, loss = 0.00492417\n",
      "Iteration 2010, loss = 0.00491851\n",
      "Iteration 2011, loss = 0.00491281\n",
      "Iteration 2012, loss = 0.00490731\n",
      "Iteration 2013, loss = 0.00490159\n",
      "Iteration 2014, loss = 0.00489585\n",
      "Iteration 2015, loss = 0.00489003\n",
      "Iteration 2016, loss = 0.00488432\n",
      "Iteration 2017, loss = 0.00487790\n",
      "Iteration 2018, loss = 0.00487120\n",
      "Iteration 2019, loss = 0.00486447\n",
      "Iteration 2020, loss = 0.00485578\n",
      "Iteration 2021, loss = 0.00484701\n",
      "Iteration 2022, loss = 0.00483931\n",
      "Iteration 2023, loss = 0.00483267\n",
      "Iteration 2024, loss = 0.00482654\n",
      "Iteration 2025, loss = 0.00482110\n",
      "Iteration 2026, loss = 0.00481552\n",
      "Iteration 2027, loss = 0.00481009\n",
      "Iteration 2028, loss = 0.00480472\n",
      "Iteration 2029, loss = 0.00479928\n",
      "Iteration 2030, loss = 0.00479400\n",
      "Iteration 2031, loss = 0.00478851\n",
      "Iteration 2032, loss = 0.00478315\n",
      "Iteration 2033, loss = 0.00477783\n",
      "Iteration 2034, loss = 0.00477240\n",
      "Iteration 2035, loss = 0.00476708\n",
      "Iteration 2036, loss = 0.00476168\n",
      "Iteration 2037, loss = 0.00475632\n",
      "Iteration 2038, loss = 0.00475099\n",
      "Iteration 2039, loss = 0.00474565\n",
      "Iteration 2040, loss = 0.00474026\n",
      "Iteration 2041, loss = 0.00473497\n",
      "Iteration 2042, loss = 0.00472960\n",
      "Iteration 2043, loss = 0.00472427\n",
      "Iteration 2044, loss = 0.00471899\n",
      "Iteration 2045, loss = 0.00471365\n",
      "Iteration 2046, loss = 0.00470844\n",
      "Iteration 2047, loss = 0.00470315\n",
      "Iteration 2048, loss = 0.00469789\n",
      "Iteration 2049, loss = 0.00469266\n",
      "Iteration 2050, loss = 0.00468738\n",
      "Iteration 2051, loss = 0.00468219\n",
      "Iteration 2052, loss = 0.00467694\n",
      "Iteration 2053, loss = 0.00467170\n",
      "Iteration 2054, loss = 0.00466647\n",
      "Iteration 2055, loss = 0.00466136\n",
      "Iteration 2056, loss = 0.00465610\n",
      "Iteration 2057, loss = 0.00465097\n",
      "Iteration 2058, loss = 0.00464585\n",
      "Iteration 2059, loss = 0.00464064\n",
      "Iteration 2060, loss = 0.00463549\n",
      "Iteration 2061, loss = 0.00463034\n",
      "Iteration 2062, loss = 0.00462527\n",
      "Iteration 2063, loss = 0.00462010\n",
      "Iteration 2064, loss = 0.00461499\n",
      "Iteration 2065, loss = 0.00460984\n",
      "Iteration 2066, loss = 0.00460480\n",
      "Iteration 2067, loss = 0.00459963\n",
      "Iteration 2068, loss = 0.00459452\n",
      "Iteration 2069, loss = 0.00458940\n",
      "Iteration 2070, loss = 0.00458435\n",
      "Iteration 2071, loss = 0.00457924\n",
      "Iteration 2072, loss = 0.00457420\n",
      "Iteration 2073, loss = 0.00456915\n",
      "Iteration 2074, loss = 0.00456402\n",
      "Iteration 2075, loss = 0.00455898\n",
      "Iteration 2076, loss = 0.00455393\n",
      "Iteration 2077, loss = 0.00454885\n",
      "Iteration 2078, loss = 0.00454390\n",
      "Iteration 2079, loss = 0.00453885\n",
      "Iteration 2080, loss = 0.00453382\n",
      "Iteration 2081, loss = 0.00452879\n",
      "Iteration 2082, loss = 0.00452379\n",
      "Iteration 2083, loss = 0.00451878\n",
      "Iteration 2084, loss = 0.00451380\n",
      "Iteration 2085, loss = 0.00450876\n",
      "Iteration 2086, loss = 0.00450376\n",
      "Iteration 2087, loss = 0.00449886\n",
      "Iteration 2088, loss = 0.00449379\n",
      "Iteration 2089, loss = 0.00448882\n",
      "Iteration 2090, loss = 0.00448387\n",
      "Iteration 2091, loss = 0.00447892\n",
      "Iteration 2092, loss = 0.00447402\n",
      "Iteration 2093, loss = 0.00446910\n",
      "Iteration 2094, loss = 0.00446415\n",
      "Iteration 2095, loss = 0.00445925\n",
      "Iteration 2096, loss = 0.00445436\n",
      "Iteration 2097, loss = 0.00444944\n",
      "Iteration 2098, loss = 0.00444458\n",
      "Iteration 2099, loss = 0.00443971\n",
      "Iteration 2100, loss = 0.00443479\n",
      "Iteration 2101, loss = 0.00442996\n",
      "Iteration 2102, loss = 0.00442505\n",
      "Iteration 2103, loss = 0.00442020\n",
      "Iteration 2104, loss = 0.00441537\n",
      "Iteration 2105, loss = 0.00441060\n",
      "Iteration 2106, loss = 0.00440568\n",
      "Iteration 2107, loss = 0.00440090\n",
      "Iteration 2108, loss = 0.00439608\n",
      "Iteration 2109, loss = 0.00439126\n",
      "Iteration 2110, loss = 0.00438652\n",
      "Iteration 2111, loss = 0.00438167\n",
      "Iteration 2112, loss = 0.00437686\n",
      "Iteration 2113, loss = 0.00437210\n",
      "Iteration 2114, loss = 0.00436732\n",
      "Iteration 2115, loss = 0.00436257\n",
      "Iteration 2116, loss = 0.00435777\n",
      "Iteration 2117, loss = 0.00435306\n",
      "Iteration 2118, loss = 0.00434832\n",
      "Iteration 2119, loss = 0.00434354\n",
      "Iteration 2120, loss = 0.00433883\n",
      "Iteration 2121, loss = 0.00433410\n",
      "Iteration 2122, loss = 0.00432936\n",
      "Iteration 2123, loss = 0.00432466\n",
      "Iteration 2124, loss = 0.00431992\n",
      "Iteration 2125, loss = 0.00431521\n",
      "Iteration 2126, loss = 0.00431055\n",
      "Iteration 2127, loss = 0.00430587\n",
      "Iteration 2128, loss = 0.00430117\n",
      "Iteration 2129, loss = 0.00429655\n",
      "Iteration 2130, loss = 0.00429185\n",
      "Iteration 2131, loss = 0.00428719\n",
      "Iteration 2132, loss = 0.00428253\n",
      "Iteration 2133, loss = 0.00427789\n",
      "Iteration 2134, loss = 0.00427323\n",
      "Iteration 2135, loss = 0.00426862\n",
      "Iteration 2136, loss = 0.00426396\n",
      "Iteration 2137, loss = 0.00425932\n",
      "Iteration 2138, loss = 0.00425478\n",
      "Iteration 2139, loss = 0.00425010\n",
      "Iteration 2140, loss = 0.00424548\n",
      "Iteration 2141, loss = 0.00424092\n",
      "Iteration 2142, loss = 0.00423636\n",
      "Iteration 2143, loss = 0.00423174\n",
      "Iteration 2144, loss = 0.00422719\n",
      "Iteration 2145, loss = 0.00422262\n",
      "Iteration 2146, loss = 0.00421798\n",
      "Iteration 2147, loss = 0.00421351\n",
      "Iteration 2148, loss = 0.00420888\n",
      "Iteration 2149, loss = 0.00420438\n",
      "Iteration 2150, loss = 0.00419980\n",
      "Iteration 2151, loss = 0.00419528\n",
      "Iteration 2152, loss = 0.00419072\n",
      "Iteration 2153, loss = 0.00418625\n",
      "Iteration 2154, loss = 0.00418173\n",
      "Iteration 2155, loss = 0.00417720\n",
      "Iteration 2156, loss = 0.00417274\n",
      "Iteration 2157, loss = 0.00416820\n",
      "Iteration 2158, loss = 0.00416375\n",
      "Iteration 2159, loss = 0.00415930\n",
      "Iteration 2160, loss = 0.00415483\n",
      "Iteration 2161, loss = 0.00415038\n",
      "Iteration 2162, loss = 0.00414593\n",
      "Iteration 2163, loss = 0.00414148\n",
      "Iteration 2164, loss = 0.00413706\n",
      "Iteration 2165, loss = 0.00413259\n",
      "Iteration 2166, loss = 0.00412817\n",
      "Iteration 2167, loss = 0.00412373\n",
      "Iteration 2168, loss = 0.00411927\n",
      "Iteration 2169, loss = 0.00411487\n",
      "Iteration 2170, loss = 0.00411046\n",
      "Iteration 2171, loss = 0.00410608\n",
      "Iteration 2172, loss = 0.00410163\n",
      "Iteration 2173, loss = 0.00409722\n",
      "Iteration 2174, loss = 0.00409282\n",
      "Iteration 2175, loss = 0.00408846\n",
      "Iteration 2176, loss = 0.00408405\n",
      "Iteration 2177, loss = 0.00407971\n",
      "Iteration 2178, loss = 0.00407533\n",
      "Iteration 2179, loss = 0.00407099\n",
      "Iteration 2180, loss = 0.00406668\n",
      "Iteration 2181, loss = 0.00406231\n",
      "Iteration 2182, loss = 0.00405799\n",
      "Iteration 2183, loss = 0.00405364\n",
      "Iteration 2184, loss = 0.00404931\n",
      "Iteration 2185, loss = 0.00404498\n",
      "Iteration 2186, loss = 0.00404071\n",
      "Iteration 2187, loss = 0.00403636\n",
      "Iteration 2188, loss = 0.00403204\n",
      "Iteration 2189, loss = 0.00402778\n",
      "Iteration 2190, loss = 0.00402348\n",
      "Iteration 2191, loss = 0.00401922\n",
      "Iteration 2192, loss = 0.00401496\n",
      "Iteration 2193, loss = 0.00401067\n",
      "Iteration 2194, loss = 0.00400642\n",
      "Iteration 2195, loss = 0.00400216\n",
      "Iteration 2196, loss = 0.00399791\n",
      "Iteration 2197, loss = 0.00399363\n",
      "Iteration 2198, loss = 0.00398945\n",
      "Iteration 2199, loss = 0.00398517\n",
      "Iteration 2200, loss = 0.00398094\n",
      "Iteration 2201, loss = 0.00397673\n",
      "Iteration 2202, loss = 0.00397252\n",
      "Iteration 2203, loss = 0.00396835\n",
      "Iteration 2204, loss = 0.00396413\n",
      "Iteration 2205, loss = 0.00395991\n",
      "Iteration 2206, loss = 0.00395577\n",
      "Iteration 2207, loss = 0.00395157\n",
      "Iteration 2208, loss = 0.00394740\n",
      "Iteration 2209, loss = 0.00394319\n",
      "Iteration 2210, loss = 0.00393903\n",
      "Iteration 2211, loss = 0.00393485\n",
      "Iteration 2212, loss = 0.00393070\n",
      "Iteration 2213, loss = 0.00392655\n",
      "Iteration 2214, loss = 0.00392233\n",
      "Iteration 2215, loss = 0.00391820\n",
      "Iteration 2216, loss = 0.00391410\n",
      "Iteration 2217, loss = 0.00390993\n",
      "Iteration 2218, loss = 0.00390584\n",
      "Iteration 2219, loss = 0.00390166\n",
      "Iteration 2220, loss = 0.00389760\n",
      "Iteration 2221, loss = 0.00389345\n",
      "Iteration 2222, loss = 0.00388937\n",
      "Iteration 2223, loss = 0.00388528\n",
      "Iteration 2224, loss = 0.00388115\n",
      "Iteration 2225, loss = 0.00387710\n",
      "Iteration 2226, loss = 0.00387305\n",
      "Iteration 2227, loss = 0.00386895\n",
      "Iteration 2228, loss = 0.00386487\n",
      "Iteration 2229, loss = 0.00386083\n",
      "Iteration 2230, loss = 0.00385671\n",
      "Iteration 2231, loss = 0.00385265\n",
      "Iteration 2232, loss = 0.00384863\n",
      "Iteration 2233, loss = 0.00384459\n",
      "Iteration 2234, loss = 0.00384050\n",
      "Iteration 2235, loss = 0.00383646\n",
      "Iteration 2236, loss = 0.00383241\n",
      "Iteration 2237, loss = 0.00382843\n",
      "Iteration 2238, loss = 0.00382435\n",
      "Iteration 2239, loss = 0.00382039\n",
      "Iteration 2240, loss = 0.00381639\n",
      "Iteration 2241, loss = 0.00381238\n",
      "Iteration 2242, loss = 0.00380835\n",
      "Iteration 2243, loss = 0.00380441\n",
      "Iteration 2244, loss = 0.00380035\n",
      "Iteration 2245, loss = 0.00379638\n",
      "Iteration 2246, loss = 0.00379237\n",
      "Iteration 2247, loss = 0.00378839\n",
      "Iteration 2248, loss = 0.00378445\n",
      "Iteration 2249, loss = 0.00378042\n",
      "Iteration 2250, loss = 0.00377651\n",
      "Iteration 2251, loss = 0.00377252\n",
      "Iteration 2252, loss = 0.00376861\n",
      "Iteration 2253, loss = 0.00376463\n",
      "Iteration 2254, loss = 0.00376075\n",
      "Iteration 2255, loss = 0.00375677\n",
      "Iteration 2256, loss = 0.00375284\n",
      "Iteration 2257, loss = 0.00374897\n",
      "Iteration 2258, loss = 0.00374503\n",
      "Iteration 2259, loss = 0.00374117\n",
      "Iteration 2260, loss = 0.00373723\n",
      "Iteration 2261, loss = 0.00373335\n",
      "Iteration 2262, loss = 0.00372947\n",
      "Iteration 2263, loss = 0.00372559\n",
      "Iteration 2264, loss = 0.00372174\n",
      "Iteration 2265, loss = 0.00371782\n",
      "Iteration 2266, loss = 0.00371398\n",
      "Iteration 2267, loss = 0.00371015\n",
      "Iteration 2268, loss = 0.00370622\n",
      "Iteration 2269, loss = 0.00370239\n",
      "Iteration 2270, loss = 0.00369855\n",
      "Iteration 2271, loss = 0.00369472\n",
      "Iteration 2272, loss = 0.00369086\n",
      "Iteration 2273, loss = 0.00368702\n",
      "Iteration 2274, loss = 0.00368319\n",
      "Iteration 2275, loss = 0.00367937\n",
      "Iteration 2276, loss = 0.00367559\n",
      "Iteration 2277, loss = 0.00367174\n",
      "Iteration 2278, loss = 0.00366794\n",
      "Iteration 2279, loss = 0.00366412\n",
      "Iteration 2280, loss = 0.00366025\n",
      "Iteration 2281, loss = 0.00365651\n",
      "Iteration 2282, loss = 0.00365269\n",
      "Iteration 2283, loss = 0.00364886\n",
      "Iteration 2284, loss = 0.00364507\n",
      "Iteration 2285, loss = 0.00364127\n",
      "Iteration 2286, loss = 0.00363750\n",
      "Iteration 2287, loss = 0.00363374\n",
      "Iteration 2288, loss = 0.00362996\n",
      "Iteration 2289, loss = 0.00362619\n",
      "Iteration 2290, loss = 0.00362246\n",
      "Iteration 2291, loss = 0.00361873\n",
      "Iteration 2292, loss = 0.00361496\n",
      "Iteration 2293, loss = 0.00361128\n",
      "Iteration 2294, loss = 0.00360758\n",
      "Iteration 2295, loss = 0.00360383\n",
      "Iteration 2296, loss = 0.00360012\n",
      "Iteration 2297, loss = 0.00359640\n",
      "Iteration 2298, loss = 0.00359275\n",
      "Iteration 2299, loss = 0.00358902\n",
      "Iteration 2300, loss = 0.00358531\n",
      "Iteration 2301, loss = 0.00358160\n",
      "Iteration 2302, loss = 0.00357792\n",
      "Iteration 2303, loss = 0.00357421\n",
      "Iteration 2304, loss = 0.00357054\n",
      "Iteration 2305, loss = 0.00356684\n",
      "Iteration 2306, loss = 0.00356315\n",
      "Iteration 2307, loss = 0.00355950\n",
      "Iteration 2308, loss = 0.00355581\n",
      "Iteration 2309, loss = 0.00355215\n",
      "Iteration 2310, loss = 0.00354847\n",
      "Iteration 2311, loss = 0.00354484\n",
      "Iteration 2312, loss = 0.00354115\n",
      "Iteration 2313, loss = 0.00353755\n",
      "Iteration 2314, loss = 0.00353385\n",
      "Iteration 2315, loss = 0.00353023\n",
      "Iteration 2316, loss = 0.00352656\n",
      "Iteration 2317, loss = 0.00352293\n",
      "Iteration 2318, loss = 0.00351927\n",
      "Iteration 2319, loss = 0.00351567\n",
      "Iteration 2320, loss = 0.00351204\n",
      "Iteration 2321, loss = 0.00350838\n",
      "Iteration 2322, loss = 0.00350479\n",
      "Iteration 2323, loss = 0.00350116\n",
      "Iteration 2324, loss = 0.00349753\n",
      "Iteration 2325, loss = 0.00349390\n",
      "Iteration 2326, loss = 0.00349033\n",
      "Iteration 2327, loss = 0.00348670\n",
      "Iteration 2328, loss = 0.00348311\n",
      "Iteration 2329, loss = 0.00347952\n",
      "Iteration 2330, loss = 0.00347596\n",
      "Iteration 2331, loss = 0.00347241\n",
      "Iteration 2332, loss = 0.00346885\n",
      "Iteration 2333, loss = 0.00346534\n",
      "Iteration 2334, loss = 0.00346178\n",
      "Iteration 2335, loss = 0.00345827\n",
      "Iteration 2336, loss = 0.00345475\n",
      "Iteration 2337, loss = 0.00345126\n",
      "Iteration 2338, loss = 0.00344770\n",
      "Iteration 2339, loss = 0.00344420\n",
      "Iteration 2340, loss = 0.00344072\n",
      "Iteration 2341, loss = 0.00343720\n",
      "Iteration 2342, loss = 0.00343373\n",
      "Iteration 2343, loss = 0.00343025\n",
      "Iteration 2344, loss = 0.00342677\n",
      "Iteration 2345, loss = 0.00342332\n",
      "Iteration 2346, loss = 0.00341985\n",
      "Iteration 2347, loss = 0.00341637\n",
      "Iteration 2348, loss = 0.00341295\n",
      "Iteration 2349, loss = 0.00340948\n",
      "Iteration 2350, loss = 0.00340602\n",
      "Iteration 2351, loss = 0.00340259\n",
      "Iteration 2352, loss = 0.00339913\n",
      "Iteration 2353, loss = 0.00339568\n",
      "Iteration 2354, loss = 0.00339228\n",
      "Iteration 2355, loss = 0.00338885\n",
      "Iteration 2356, loss = 0.00338544\n",
      "Iteration 2357, loss = 0.00338201\n",
      "Iteration 2358, loss = 0.00337860\n",
      "Iteration 2359, loss = 0.00337513\n",
      "Iteration 2360, loss = 0.00337178\n",
      "Iteration 2361, loss = 0.00336834\n",
      "Iteration 2362, loss = 0.00336496\n",
      "Iteration 2363, loss = 0.00336156\n",
      "Iteration 2364, loss = 0.00335815\n",
      "Iteration 2365, loss = 0.00335476\n",
      "Iteration 2366, loss = 0.00335138\n",
      "Iteration 2367, loss = 0.00334797\n",
      "Iteration 2368, loss = 0.00334460\n",
      "Iteration 2369, loss = 0.00334124\n",
      "Iteration 2370, loss = 0.00333785\n",
      "Iteration 2371, loss = 0.00333451\n",
      "Iteration 2372, loss = 0.00333111\n",
      "Iteration 2373, loss = 0.00332779\n",
      "Iteration 2374, loss = 0.00332442\n",
      "Iteration 2375, loss = 0.00332108\n",
      "Iteration 2376, loss = 0.00331773\n",
      "Iteration 2377, loss = 0.00331439\n",
      "Iteration 2378, loss = 0.00331107\n",
      "Iteration 2379, loss = 0.00330777\n",
      "Iteration 2380, loss = 0.00330440\n",
      "Iteration 2381, loss = 0.00330112\n",
      "Iteration 2382, loss = 0.00329779\n",
      "Iteration 2383, loss = 0.00329445\n",
      "Iteration 2384, loss = 0.00329118\n",
      "Iteration 2385, loss = 0.00328785\n",
      "Iteration 2386, loss = 0.00328462\n",
      "Iteration 2387, loss = 0.00328131\n",
      "Iteration 2388, loss = 0.00327802\n",
      "Iteration 2389, loss = 0.00327477\n",
      "Iteration 2390, loss = 0.00327146\n",
      "Iteration 2391, loss = 0.00326820\n",
      "Iteration 2392, loss = 0.00326495\n",
      "Iteration 2393, loss = 0.00326163\n",
      "Iteration 2394, loss = 0.00325834\n",
      "Iteration 2395, loss = 0.00325508\n",
      "Iteration 2396, loss = 0.00325183\n",
      "Iteration 2397, loss = 0.00324860\n",
      "Iteration 2398, loss = 0.00324535\n",
      "Iteration 2399, loss = 0.00324206\n",
      "Iteration 2400, loss = 0.00323886\n",
      "Iteration 2401, loss = 0.00323557\n",
      "Iteration 2402, loss = 0.00323236\n",
      "Iteration 2403, loss = 0.00322913\n",
      "Iteration 2404, loss = 0.00322590\n",
      "Iteration 2405, loss = 0.00322266\n",
      "Iteration 2406, loss = 0.00321943\n",
      "Iteration 2407, loss = 0.00321623\n",
      "Iteration 2408, loss = 0.00321297\n",
      "Iteration 2409, loss = 0.00320982\n",
      "Iteration 2410, loss = 0.00320660\n",
      "Iteration 2411, loss = 0.00320341\n",
      "Iteration 2412, loss = 0.00320022\n",
      "Iteration 2413, loss = 0.00319702\n",
      "Iteration 2414, loss = 0.00319386\n",
      "Iteration 2415, loss = 0.00319071\n",
      "Iteration 2416, loss = 0.00318753\n",
      "Iteration 2417, loss = 0.00318438\n",
      "Iteration 2418, loss = 0.00318119\n",
      "Iteration 2419, loss = 0.00317802\n",
      "Iteration 2420, loss = 0.00317489\n",
      "Iteration 2421, loss = 0.00317173\n",
      "Iteration 2422, loss = 0.00316858\n",
      "Iteration 2423, loss = 0.00316545\n",
      "Iteration 2424, loss = 0.00316232\n",
      "Iteration 2425, loss = 0.00315916\n",
      "Iteration 2426, loss = 0.00315606\n",
      "Iteration 2427, loss = 0.00315290\n",
      "Iteration 2428, loss = 0.00314983\n",
      "Iteration 2429, loss = 0.00314669\n",
      "Iteration 2430, loss = 0.00314355\n",
      "Iteration 2431, loss = 0.00314039\n",
      "Iteration 2432, loss = 0.00313732\n",
      "Iteration 2433, loss = 0.00313421\n",
      "Iteration 2434, loss = 0.00313111\n",
      "Iteration 2435, loss = 0.00312798\n",
      "Iteration 2436, loss = 0.00312490\n",
      "Iteration 2437, loss = 0.00312177\n",
      "Iteration 2438, loss = 0.00311869\n",
      "Iteration 2439, loss = 0.00311564\n",
      "Iteration 2440, loss = 0.00311254\n",
      "Iteration 2441, loss = 0.00310946\n",
      "Iteration 2442, loss = 0.00310637\n",
      "Iteration 2443, loss = 0.00310330\n",
      "Iteration 2444, loss = 0.00310022\n",
      "Iteration 2445, loss = 0.00309717\n",
      "Iteration 2446, loss = 0.00309412\n",
      "Iteration 2447, loss = 0.00309106\n",
      "Iteration 2448, loss = 0.00308800\n",
      "Iteration 2449, loss = 0.00308497\n",
      "Iteration 2450, loss = 0.00308192\n",
      "Iteration 2451, loss = 0.00307890\n",
      "Iteration 2452, loss = 0.00307587\n",
      "Iteration 2453, loss = 0.00307285\n",
      "Iteration 2454, loss = 0.00306985\n",
      "Iteration 2455, loss = 0.00306681\n",
      "Iteration 2456, loss = 0.00306381\n",
      "Iteration 2457, loss = 0.00306080\n",
      "Iteration 2458, loss = 0.00305782\n",
      "Iteration 2459, loss = 0.00305483\n",
      "Iteration 2460, loss = 0.00305185\n",
      "Iteration 2461, loss = 0.00304886\n",
      "Iteration 2462, loss = 0.00304585\n",
      "Iteration 2463, loss = 0.00304287\n",
      "Iteration 2464, loss = 0.00303989\n",
      "Iteration 2465, loss = 0.00303692\n",
      "Iteration 2466, loss = 0.00303394\n",
      "Iteration 2467, loss = 0.00303094\n",
      "Iteration 2468, loss = 0.00302798\n",
      "Iteration 2469, loss = 0.00302504\n",
      "Iteration 2470, loss = 0.00302210\n",
      "Iteration 2471, loss = 0.00301914\n",
      "Iteration 2472, loss = 0.00301616\n",
      "Iteration 2473, loss = 0.00301323\n",
      "Iteration 2474, loss = 0.00301028\n",
      "Iteration 2475, loss = 0.00300735\n",
      "Iteration 2476, loss = 0.00300441\n",
      "Iteration 2477, loss = 0.00300146\n",
      "Iteration 2478, loss = 0.00299850\n",
      "Iteration 2479, loss = 0.00299559\n",
      "Iteration 2480, loss = 0.00299263\n",
      "Iteration 2481, loss = 0.00298975\n",
      "Iteration 2482, loss = 0.00298682\n",
      "Iteration 2483, loss = 0.00298391\n",
      "Iteration 2484, loss = 0.00298099\n",
      "Iteration 2485, loss = 0.00297808\n",
      "Iteration 2486, loss = 0.00297520\n",
      "Iteration 2487, loss = 0.00297234\n",
      "Iteration 2488, loss = 0.00296942\n",
      "Iteration 2489, loss = 0.00296653\n",
      "Iteration 2490, loss = 0.00296364\n",
      "Iteration 2491, loss = 0.00296076\n",
      "Iteration 2492, loss = 0.00295788\n",
      "Iteration 2493, loss = 0.00295502\n",
      "Iteration 2494, loss = 0.00295212\n",
      "Iteration 2495, loss = 0.00294928\n",
      "Iteration 2496, loss = 0.00294644\n",
      "Iteration 2497, loss = 0.00294357\n",
      "Iteration 2498, loss = 0.00294075\n",
      "Iteration 2499, loss = 0.00293786\n",
      "Iteration 2500, loss = 0.00293505\n",
      "Iteration 2501, loss = 0.00293221\n",
      "Iteration 2502, loss = 0.00292938\n",
      "Iteration 2503, loss = 0.00292656\n",
      "Iteration 2504, loss = 0.00292375\n",
      "Iteration 2505, loss = 0.00292093\n",
      "Iteration 2506, loss = 0.00291812\n",
      "Iteration 2507, loss = 0.00291533\n",
      "Iteration 2508, loss = 0.00291250\n",
      "Iteration 2509, loss = 0.00290972\n",
      "Iteration 2510, loss = 0.00290694\n",
      "Iteration 2511, loss = 0.00290412\n",
      "Iteration 2512, loss = 0.00290135\n",
      "Iteration 2513, loss = 0.00289853\n",
      "Iteration 2514, loss = 0.00289575\n",
      "Iteration 2515, loss = 0.00289295\n",
      "Iteration 2516, loss = 0.00289017\n",
      "Iteration 2517, loss = 0.00288739\n",
      "Iteration 2518, loss = 0.00288458\n",
      "Iteration 2519, loss = 0.00288182\n",
      "Iteration 2520, loss = 0.00287902\n",
      "Iteration 2521, loss = 0.00287623\n",
      "Iteration 2522, loss = 0.00287347\n",
      "Iteration 2523, loss = 0.00287071\n",
      "Iteration 2524, loss = 0.00286796\n",
      "Iteration 2525, loss = 0.00286517\n",
      "Iteration 2526, loss = 0.00286243\n",
      "Iteration 2527, loss = 0.00285967\n",
      "Iteration 2528, loss = 0.00285698\n",
      "Iteration 2529, loss = 0.00285420\n",
      "Iteration 2530, loss = 0.00285148\n",
      "Iteration 2531, loss = 0.00284877\n",
      "Iteration 2532, loss = 0.00284603\n",
      "Iteration 2533, loss = 0.00284329\n",
      "Iteration 2534, loss = 0.00284056\n",
      "Iteration 2535, loss = 0.00283785\n",
      "Iteration 2536, loss = 0.00283512\n",
      "Iteration 2537, loss = 0.00283237\n",
      "Iteration 2538, loss = 0.00282969\n",
      "Iteration 2539, loss = 0.00282699\n",
      "Iteration 2540, loss = 0.00282426\n",
      "Iteration 2541, loss = 0.00282157\n",
      "Iteration 2542, loss = 0.00281887\n",
      "Iteration 2543, loss = 0.00281615\n",
      "Iteration 2544, loss = 0.00281346\n",
      "Iteration 2545, loss = 0.00281078\n",
      "Iteration 2546, loss = 0.00280810\n",
      "Iteration 2547, loss = 0.00280538\n",
      "Iteration 2548, loss = 0.00280273\n",
      "Iteration 2549, loss = 0.00280006\n",
      "Iteration 2550, loss = 0.00279740\n",
      "Iteration 2551, loss = 0.00279473\n",
      "Iteration 2552, loss = 0.00279207\n",
      "Iteration 2553, loss = 0.00278941\n",
      "Iteration 2554, loss = 0.00278674\n",
      "Iteration 2555, loss = 0.00278409\n",
      "Iteration 2556, loss = 0.00278144\n",
      "Iteration 2557, loss = 0.00277880\n",
      "Iteration 2558, loss = 0.00277617\n",
      "Iteration 2559, loss = 0.00277353\n",
      "Iteration 2560, loss = 0.00277090\n",
      "Iteration 2561, loss = 0.00276827\n",
      "Iteration 2562, loss = 0.00276563\n",
      "Iteration 2563, loss = 0.00276302\n",
      "Iteration 2564, loss = 0.00276040\n",
      "Iteration 2565, loss = 0.00275776\n",
      "Iteration 2566, loss = 0.00275514\n",
      "Iteration 2567, loss = 0.00275252\n",
      "Iteration 2568, loss = 0.00274991\n",
      "Iteration 2569, loss = 0.00274729\n",
      "Iteration 2570, loss = 0.00274472\n",
      "Iteration 2571, loss = 0.00274213\n",
      "Iteration 2572, loss = 0.00273952\n",
      "Iteration 2573, loss = 0.00273696\n",
      "Iteration 2574, loss = 0.00273437\n",
      "Iteration 2575, loss = 0.00273181\n",
      "Iteration 2576, loss = 0.00272922\n",
      "Iteration 2577, loss = 0.00272666\n",
      "Iteration 2578, loss = 0.00272412\n",
      "Iteration 2579, loss = 0.00272153\n",
      "Iteration 2580, loss = 0.00271900\n",
      "Iteration 2581, loss = 0.00271645\n",
      "Iteration 2582, loss = 0.00271391\n",
      "Iteration 2583, loss = 0.00271133\n",
      "Iteration 2584, loss = 0.00270877\n",
      "Iteration 2585, loss = 0.00270622\n",
      "Iteration 2586, loss = 0.00270369\n",
      "Iteration 2587, loss = 0.00270112\n",
      "Iteration 2588, loss = 0.00269855\n",
      "Iteration 2589, loss = 0.00269602\n",
      "Iteration 2590, loss = 0.00269345\n",
      "Iteration 2591, loss = 0.00269091\n",
      "Iteration 2592, loss = 0.00268839\n",
      "Iteration 2593, loss = 0.00268586\n",
      "Iteration 2594, loss = 0.00268331\n",
      "Iteration 2595, loss = 0.00268078\n",
      "Iteration 2596, loss = 0.00267826\n",
      "Iteration 2597, loss = 0.00267575\n",
      "Iteration 2598, loss = 0.00267324\n",
      "Iteration 2599, loss = 0.00267072\n",
      "Iteration 2600, loss = 0.00266822\n",
      "Iteration 2601, loss = 0.00266572\n",
      "Iteration 2602, loss = 0.00266320\n",
      "Iteration 2603, loss = 0.00266070\n",
      "Iteration 2604, loss = 0.00265824\n",
      "Iteration 2605, loss = 0.00265572\n",
      "Iteration 2606, loss = 0.00265326\n",
      "Iteration 2607, loss = 0.00265075\n",
      "Iteration 2608, loss = 0.00264828\n",
      "Iteration 2609, loss = 0.00264585\n",
      "Iteration 2610, loss = 0.00264335\n",
      "Iteration 2611, loss = 0.00264088\n",
      "Iteration 2612, loss = 0.00263843\n",
      "Iteration 2613, loss = 0.00263600\n",
      "Iteration 2614, loss = 0.00263353\n",
      "Iteration 2615, loss = 0.00263108\n",
      "Iteration 2616, loss = 0.00262861\n",
      "Iteration 2617, loss = 0.00262615\n",
      "Iteration 2618, loss = 0.00262370\n",
      "Iteration 2619, loss = 0.00262127\n",
      "Iteration 2620, loss = 0.00261880\n",
      "Iteration 2621, loss = 0.00261635\n",
      "Iteration 2622, loss = 0.00261390\n",
      "Iteration 2623, loss = 0.00261147\n",
      "Iteration 2624, loss = 0.00260904\n",
      "Iteration 2625, loss = 0.00260659\n",
      "Iteration 2626, loss = 0.00260420\n",
      "Iteration 2627, loss = 0.00260174\n",
      "Iteration 2628, loss = 0.00259931\n",
      "Iteration 2629, loss = 0.00259691\n",
      "Iteration 2630, loss = 0.00259445\n",
      "Iteration 2631, loss = 0.00259203\n",
      "Iteration 2632, loss = 0.00258963\n",
      "Iteration 2633, loss = 0.00258717\n",
      "Iteration 2634, loss = 0.00258475\n",
      "Iteration 2635, loss = 0.00258233\n",
      "Iteration 2636, loss = 0.00257991\n",
      "Iteration 2637, loss = 0.00257751\n",
      "Iteration 2638, loss = 0.00257506\n",
      "Iteration 2639, loss = 0.00257266\n",
      "Iteration 2640, loss = 0.00257023\n",
      "Iteration 2641, loss = 0.00256779\n",
      "Iteration 2642, loss = 0.00256537\n",
      "Iteration 2643, loss = 0.00256290\n",
      "Iteration 2644, loss = 0.00256039\n",
      "Iteration 2645, loss = 0.00255789\n",
      "Iteration 2646, loss = 0.00255535\n",
      "Iteration 2647, loss = 0.00255288\n",
      "Iteration 2648, loss = 0.00255030\n",
      "Iteration 2649, loss = 0.00254773\n",
      "Iteration 2650, loss = 0.00254517\n",
      "Iteration 2651, loss = 0.00254265\n",
      "Iteration 2652, loss = 0.00254010\n",
      "Iteration 2653, loss = 0.00253760\n",
      "Iteration 2654, loss = 0.00253508\n",
      "Iteration 2655, loss = 0.00253265\n",
      "Iteration 2656, loss = 0.00253017\n",
      "Iteration 2657, loss = 0.00252781\n",
      "Iteration 2658, loss = 0.00252540\n",
      "Iteration 2659, loss = 0.00252301\n",
      "Iteration 2660, loss = 0.00252062\n",
      "Iteration 2661, loss = 0.00251828\n",
      "Iteration 2662, loss = 0.00251591\n",
      "Iteration 2663, loss = 0.00251360\n",
      "Iteration 2664, loss = 0.00251123\n",
      "Iteration 2665, loss = 0.00250890\n",
      "Iteration 2666, loss = 0.00250655\n",
      "Iteration 2667, loss = 0.00250425\n",
      "Iteration 2668, loss = 0.00250192\n",
      "Iteration 2669, loss = 0.00249959\n",
      "Iteration 2670, loss = 0.00249729\n",
      "Iteration 2671, loss = 0.00249498\n",
      "Iteration 2672, loss = 0.00249265\n",
      "Iteration 2673, loss = 0.00249039\n",
      "Iteration 2674, loss = 0.00248807\n",
      "Iteration 2675, loss = 0.00248577\n",
      "Iteration 2676, loss = 0.00248351\n",
      "Iteration 2677, loss = 0.00248121\n",
      "Iteration 2678, loss = 0.00247894\n",
      "Iteration 2679, loss = 0.00247664\n",
      "Iteration 2680, loss = 0.00247439\n",
      "Iteration 2681, loss = 0.00247208\n",
      "Iteration 2682, loss = 0.00246981\n",
      "Iteration 2683, loss = 0.00246755\n",
      "Iteration 2684, loss = 0.00246528\n",
      "Iteration 2685, loss = 0.00246301\n",
      "Iteration 2686, loss = 0.00246074\n",
      "Iteration 2687, loss = 0.00245846\n",
      "Iteration 2688, loss = 0.00245622\n",
      "Iteration 2689, loss = 0.00245397\n",
      "Iteration 2690, loss = 0.00245172\n",
      "Iteration 2691, loss = 0.00244949\n",
      "Iteration 2692, loss = 0.00244721\n",
      "Iteration 2693, loss = 0.00244499\n",
      "Iteration 2694, loss = 0.00244278\n",
      "Iteration 2695, loss = 0.00244051\n",
      "Iteration 2696, loss = 0.00243828\n",
      "Iteration 2697, loss = 0.00243607\n",
      "Iteration 2698, loss = 0.00243386\n",
      "Iteration 2699, loss = 0.00243161\n",
      "Iteration 2700, loss = 0.00242941\n",
      "Iteration 2701, loss = 0.00242717\n",
      "Iteration 2702, loss = 0.00242495\n",
      "Iteration 2703, loss = 0.00242278\n",
      "Iteration 2704, loss = 0.00242056\n",
      "Iteration 2705, loss = 0.00241837\n",
      "Iteration 2706, loss = 0.00241615\n",
      "Iteration 2707, loss = 0.00241397\n",
      "Iteration 2708, loss = 0.00241175\n",
      "Iteration 2709, loss = 0.00240959\n",
      "Iteration 2710, loss = 0.00240738\n",
      "Iteration 2711, loss = 0.00240517\n",
      "Iteration 2712, loss = 0.00240299\n",
      "Iteration 2713, loss = 0.00240080\n",
      "Iteration 2714, loss = 0.00239863\n",
      "Iteration 2715, loss = 0.00239643\n",
      "Iteration 2716, loss = 0.00239424\n",
      "Iteration 2717, loss = 0.00239210\n",
      "Iteration 2718, loss = 0.00238991\n",
      "Iteration 2719, loss = 0.00238775\n",
      "Iteration 2720, loss = 0.00238560\n",
      "Iteration 2721, loss = 0.00238344\n",
      "Iteration 2722, loss = 0.00238129\n",
      "Iteration 2723, loss = 0.00237912\n",
      "Iteration 2724, loss = 0.00237700\n",
      "Iteration 2725, loss = 0.00237484\n",
      "Iteration 2726, loss = 0.00237268\n",
      "Iteration 2727, loss = 0.00237055\n",
      "Iteration 2728, loss = 0.00236838\n",
      "Iteration 2729, loss = 0.00236622\n",
      "Iteration 2730, loss = 0.00236413\n",
      "Iteration 2731, loss = 0.00236196\n",
      "Iteration 2732, loss = 0.00235984\n",
      "Iteration 2733, loss = 0.00235767\n",
      "Iteration 2734, loss = 0.00235555\n",
      "Iteration 2735, loss = 0.00235341\n",
      "Iteration 2736, loss = 0.00235127\n",
      "Iteration 2737, loss = 0.00234916\n",
      "Iteration 2738, loss = 0.00234702\n",
      "Iteration 2739, loss = 0.00234490\n",
      "Iteration 2740, loss = 0.00234280\n",
      "Iteration 2741, loss = 0.00234065\n",
      "Iteration 2742, loss = 0.00233855\n",
      "Iteration 2743, loss = 0.00233648\n",
      "Iteration 2744, loss = 0.00233433\n",
      "Iteration 2745, loss = 0.00233227\n",
      "Iteration 2746, loss = 0.00233016\n",
      "Iteration 2747, loss = 0.00232807\n",
      "Iteration 2748, loss = 0.00232597\n",
      "Iteration 2749, loss = 0.00232388\n",
      "Iteration 2750, loss = 0.00232179\n",
      "Iteration 2751, loss = 0.00231974\n",
      "Iteration 2752, loss = 0.00231765\n",
      "Iteration 2753, loss = 0.00231556\n",
      "Iteration 2754, loss = 0.00231349\n",
      "Iteration 2755, loss = 0.00231140\n",
      "Iteration 2756, loss = 0.00230935\n",
      "Iteration 2757, loss = 0.00230728\n",
      "Iteration 2758, loss = 0.00230521\n",
      "Iteration 2759, loss = 0.00230314\n",
      "Iteration 2760, loss = 0.00230111\n",
      "Iteration 2761, loss = 0.00229902\n",
      "Iteration 2762, loss = 0.00229697\n",
      "Iteration 2763, loss = 0.00229494\n",
      "Iteration 2764, loss = 0.00229287\n",
      "Iteration 2765, loss = 0.00229084\n",
      "Iteration 2766, loss = 0.00228880\n",
      "Iteration 2767, loss = 0.00228675\n",
      "Iteration 2768, loss = 0.00228472\n",
      "Iteration 2769, loss = 0.00228273\n",
      "Iteration 2770, loss = 0.00228068\n",
      "Iteration 2771, loss = 0.00227869\n",
      "Iteration 2772, loss = 0.00227666\n",
      "Iteration 2773, loss = 0.00227464\n",
      "Iteration 2774, loss = 0.00227263\n",
      "Iteration 2775, loss = 0.00227063\n",
      "Iteration 2776, loss = 0.00226863\n",
      "Iteration 2777, loss = 0.00226663\n",
      "Iteration 2778, loss = 0.00226465\n",
      "Iteration 2779, loss = 0.00226263\n",
      "Iteration 2780, loss = 0.00226061\n",
      "Iteration 2781, loss = 0.00225863\n",
      "Iteration 2782, loss = 0.00225663\n",
      "Iteration 2783, loss = 0.00225461\n",
      "Iteration 2784, loss = 0.00225263\n",
      "Iteration 2785, loss = 0.00225060\n",
      "Iteration 2786, loss = 0.00224865\n",
      "Iteration 2787, loss = 0.00224663\n",
      "Iteration 2788, loss = 0.00224468\n",
      "Iteration 2789, loss = 0.00224267\n",
      "Iteration 2790, loss = 0.00224071\n",
      "Iteration 2791, loss = 0.00223871\n",
      "Iteration 2792, loss = 0.00223675\n",
      "Iteration 2793, loss = 0.00223477\n",
      "Iteration 2794, loss = 0.00223280\n",
      "Iteration 2795, loss = 0.00223081\n",
      "Iteration 2796, loss = 0.00222885\n",
      "Iteration 2797, loss = 0.00222688\n",
      "Iteration 2798, loss = 0.00222491\n",
      "Iteration 2799, loss = 0.00222297\n",
      "Iteration 2800, loss = 0.00222100\n",
      "Iteration 2801, loss = 0.00221904\n",
      "Iteration 2802, loss = 0.00221709\n",
      "Iteration 2803, loss = 0.00221512\n",
      "Iteration 2804, loss = 0.00221318\n",
      "Iteration 2805, loss = 0.00221124\n",
      "Iteration 2806, loss = 0.00220929\n",
      "Iteration 2807, loss = 0.00220736\n",
      "Iteration 2808, loss = 0.00220542\n",
      "Iteration 2809, loss = 0.00220345\n",
      "Iteration 2810, loss = 0.00220154\n",
      "Iteration 2811, loss = 0.00219962\n",
      "Iteration 2812, loss = 0.00219765\n",
      "Iteration 2813, loss = 0.00219571\n",
      "Iteration 2814, loss = 0.00219379\n",
      "Iteration 2815, loss = 0.00219185\n",
      "Iteration 2816, loss = 0.00218992\n",
      "Iteration 2817, loss = 0.00218801\n",
      "Iteration 2818, loss = 0.00218609\n",
      "Iteration 2819, loss = 0.00218416\n",
      "Iteration 2820, loss = 0.00218222\n",
      "Iteration 2821, loss = 0.00218034\n",
      "Iteration 2822, loss = 0.00217844\n",
      "Iteration 2823, loss = 0.00217651\n",
      "Iteration 2824, loss = 0.00217460\n",
      "Iteration 2825, loss = 0.00217268\n",
      "Iteration 2826, loss = 0.00217081\n",
      "Iteration 2827, loss = 0.00216892\n",
      "Iteration 2828, loss = 0.00216701\n",
      "Iteration 2829, loss = 0.00216510\n",
      "Iteration 2830, loss = 0.00216323\n",
      "Iteration 2831, loss = 0.00216134\n",
      "Iteration 2832, loss = 0.00215943\n",
      "Iteration 2833, loss = 0.00215758\n",
      "Iteration 2834, loss = 0.00215569\n",
      "Iteration 2835, loss = 0.00215384\n",
      "Iteration 2836, loss = 0.00215194\n",
      "Iteration 2837, loss = 0.00215008\n",
      "Iteration 2838, loss = 0.00214819\n",
      "Iteration 2839, loss = 0.00214633\n",
      "Iteration 2840, loss = 0.00214447\n",
      "Iteration 2841, loss = 0.00214259\n",
      "Iteration 2842, loss = 0.00214072\n",
      "Iteration 2843, loss = 0.00213886\n",
      "Iteration 2844, loss = 0.00213700\n",
      "Iteration 2845, loss = 0.00213514\n",
      "Iteration 2846, loss = 0.00213329\n",
      "Iteration 2847, loss = 0.00213143\n",
      "Iteration 2848, loss = 0.00212955\n",
      "Iteration 2849, loss = 0.00212769\n",
      "Iteration 2850, loss = 0.00212583\n",
      "Iteration 2851, loss = 0.00212396\n",
      "Iteration 2852, loss = 0.00212207\n",
      "Iteration 2853, loss = 0.00212021\n",
      "Iteration 2854, loss = 0.00211830\n",
      "Iteration 2855, loss = 0.00211644\n",
      "Iteration 2856, loss = 0.00211453\n",
      "Iteration 2857, loss = 0.00211268\n",
      "Iteration 2858, loss = 0.00211080\n",
      "Iteration 2859, loss = 0.00210896\n",
      "Iteration 2860, loss = 0.00210714\n",
      "Iteration 2861, loss = 0.00210531\n",
      "Iteration 2862, loss = 0.00210352\n",
      "Iteration 2863, loss = 0.00210168\n",
      "Iteration 2864, loss = 0.00209987\n",
      "Iteration 2865, loss = 0.00209807\n",
      "Iteration 2866, loss = 0.00209624\n",
      "Iteration 2867, loss = 0.00209444\n",
      "Iteration 2868, loss = 0.00209266\n",
      "Iteration 2869, loss = 0.00209084\n",
      "Iteration 2870, loss = 0.00208904\n",
      "Iteration 2871, loss = 0.00208725\n",
      "Iteration 2872, loss = 0.00208545\n",
      "Iteration 2873, loss = 0.00208365\n",
      "Iteration 2874, loss = 0.00208188\n",
      "Iteration 2875, loss = 0.00208008\n",
      "Iteration 2876, loss = 0.00207830\n",
      "Iteration 2877, loss = 0.00207650\n",
      "Iteration 2878, loss = 0.00207472\n",
      "Iteration 2879, loss = 0.00207295\n",
      "Iteration 2880, loss = 0.00207117\n",
      "Iteration 2881, loss = 0.00206938\n",
      "Iteration 2882, loss = 0.00206763\n",
      "Iteration 2883, loss = 0.00206586\n",
      "Iteration 2884, loss = 0.00206408\n",
      "Iteration 2885, loss = 0.00206232\n",
      "Iteration 2886, loss = 0.00206054\n",
      "Iteration 2887, loss = 0.00205879\n",
      "Iteration 2888, loss = 0.00205702\n",
      "Iteration 2889, loss = 0.00205526\n",
      "Iteration 2890, loss = 0.00205348\n",
      "Iteration 2891, loss = 0.00205175\n",
      "Iteration 2892, loss = 0.00204999\n",
      "Iteration 2893, loss = 0.00204823\n",
      "Iteration 2894, loss = 0.00204649\n",
      "Iteration 2895, loss = 0.00204474\n",
      "Iteration 2896, loss = 0.00204299\n",
      "Iteration 2897, loss = 0.00204127\n",
      "Iteration 2898, loss = 0.00203953\n",
      "Iteration 2899, loss = 0.00203780\n",
      "Iteration 2900, loss = 0.00203605\n",
      "Iteration 2901, loss = 0.00203432\n",
      "Iteration 2902, loss = 0.00203257\n",
      "Iteration 2903, loss = 0.00203085\n",
      "Iteration 2904, loss = 0.00202910\n",
      "Iteration 2905, loss = 0.00202740\n",
      "Iteration 2906, loss = 0.00202566\n",
      "Iteration 2907, loss = 0.00202393\n",
      "Iteration 2908, loss = 0.00202222\n",
      "Iteration 2909, loss = 0.00202050\n",
      "Iteration 2910, loss = 0.00201878\n",
      "Iteration 2911, loss = 0.00201704\n",
      "Iteration 2912, loss = 0.00201535\n",
      "Iteration 2913, loss = 0.00201363\n",
      "Iteration 2914, loss = 0.00201191\n",
      "Iteration 2915, loss = 0.00201020\n",
      "Iteration 2916, loss = 0.00200849\n",
      "Iteration 2917, loss = 0.00200679\n",
      "Iteration 2918, loss = 0.00200510\n",
      "Iteration 2919, loss = 0.00200339\n",
      "Iteration 2920, loss = 0.00200167\n",
      "Iteration 2921, loss = 0.00199999\n",
      "Iteration 2922, loss = 0.00199828\n",
      "Iteration 2923, loss = 0.00199659\n",
      "Iteration 2924, loss = 0.00199489\n",
      "Iteration 2925, loss = 0.00199321\n",
      "Iteration 2926, loss = 0.00199149\n",
      "Iteration 2927, loss = 0.00198983\n",
      "Iteration 2928, loss = 0.00198813\n",
      "Iteration 2929, loss = 0.00198645\n",
      "Iteration 2930, loss = 0.00198477\n",
      "Iteration 2931, loss = 0.00198307\n",
      "Iteration 2932, loss = 0.00198142\n",
      "Iteration 2933, loss = 0.00197974\n",
      "Iteration 2934, loss = 0.00197808\n",
      "Iteration 2935, loss = 0.00197641\n",
      "Iteration 2936, loss = 0.00197473\n",
      "Iteration 2937, loss = 0.00197308\n",
      "Iteration 2938, loss = 0.00197142\n",
      "Iteration 2939, loss = 0.00196975\n",
      "Iteration 2940, loss = 0.00196810\n",
      "Iteration 2941, loss = 0.00196644\n",
      "Iteration 2942, loss = 0.00196477\n",
      "Iteration 2943, loss = 0.00196312\n",
      "Iteration 2944, loss = 0.00196148\n",
      "Iteration 2945, loss = 0.00195983\n",
      "Iteration 2946, loss = 0.00195817\n",
      "Iteration 2947, loss = 0.00195652\n",
      "Iteration 2948, loss = 0.00195488\n",
      "Iteration 2949, loss = 0.00195323\n",
      "Iteration 2950, loss = 0.00195159\n",
      "Iteration 2951, loss = 0.00194994\n",
      "Iteration 2952, loss = 0.00194831\n",
      "Iteration 2953, loss = 0.00194668\n",
      "Iteration 2954, loss = 0.00194503\n",
      "Iteration 2955, loss = 0.00194341\n",
      "Iteration 2956, loss = 0.00194179\n",
      "Iteration 2957, loss = 0.00194016\n",
      "Iteration 2958, loss = 0.00193853\n",
      "Iteration 2959, loss = 0.00193691\n",
      "Iteration 2960, loss = 0.00193530\n",
      "Iteration 2961, loss = 0.00193368\n",
      "Iteration 2962, loss = 0.00193205\n",
      "Iteration 2963, loss = 0.00193045\n",
      "Iteration 2964, loss = 0.00192883\n",
      "Iteration 2965, loss = 0.00192721\n",
      "Iteration 2966, loss = 0.00192562\n",
      "Iteration 2967, loss = 0.00192402\n",
      "Iteration 2968, loss = 0.00192243\n",
      "Iteration 2969, loss = 0.00192082\n",
      "Iteration 2970, loss = 0.00191922\n",
      "Iteration 2971, loss = 0.00191762\n",
      "Iteration 2972, loss = 0.00191604\n",
      "Iteration 2973, loss = 0.00191445\n",
      "Iteration 2974, loss = 0.00191285\n",
      "Iteration 2975, loss = 0.00191126\n",
      "Iteration 2976, loss = 0.00190968\n",
      "Iteration 2977, loss = 0.00190809\n",
      "Iteration 2978, loss = 0.00190653\n",
      "Iteration 2979, loss = 0.00190494\n",
      "Iteration 2980, loss = 0.00190336\n",
      "Iteration 2981, loss = 0.00190180\n",
      "Iteration 2982, loss = 0.00190022\n",
      "Iteration 2983, loss = 0.00189865\n",
      "Iteration 2984, loss = 0.00189709\n",
      "Iteration 2985, loss = 0.00189550\n",
      "Iteration 2986, loss = 0.00189394\n",
      "Iteration 2987, loss = 0.00189239\n",
      "Iteration 2988, loss = 0.00189081\n",
      "Iteration 2989, loss = 0.00188925\n",
      "Iteration 2990, loss = 0.00188770\n",
      "Iteration 2991, loss = 0.00188613\n",
      "Iteration 2992, loss = 0.00188459\n",
      "Iteration 2993, loss = 0.00188303\n",
      "Iteration 2994, loss = 0.00188147\n",
      "Iteration 2995, loss = 0.00187994\n",
      "Iteration 2996, loss = 0.00187839\n",
      "Iteration 2997, loss = 0.00187684\n",
      "Iteration 2998, loss = 0.00187530\n",
      "Iteration 2999, loss = 0.00187375\n",
      "Iteration 3000, loss = 0.00187222\n",
      "Iteration 3001, loss = 0.00187067\n",
      "Iteration 3002, loss = 0.00186913\n",
      "Iteration 3003, loss = 0.00186761\n",
      "Iteration 3004, loss = 0.00186607\n",
      "Iteration 3005, loss = 0.00186456\n",
      "Iteration 3006, loss = 0.00186301\n",
      "Iteration 3007, loss = 0.00186149\n",
      "Iteration 3008, loss = 0.00185995\n",
      "Iteration 3009, loss = 0.00185845\n",
      "Iteration 3010, loss = 0.00185692\n",
      "Iteration 3011, loss = 0.00185542\n",
      "Iteration 3012, loss = 0.00185390\n",
      "Iteration 3013, loss = 0.00185238\n",
      "Iteration 3014, loss = 0.00185088\n",
      "Iteration 3015, loss = 0.00184936\n",
      "Iteration 3016, loss = 0.00184785\n",
      "Iteration 3017, loss = 0.00184635\n",
      "Iteration 3018, loss = 0.00184484\n",
      "Iteration 3019, loss = 0.00184333\n",
      "Iteration 3020, loss = 0.00184182\n",
      "Iteration 3021, loss = 0.00184031\n",
      "Iteration 3022, loss = 0.00183882\n",
      "Iteration 3023, loss = 0.00183729\n",
      "Iteration 3024, loss = 0.00183579\n",
      "Iteration 3025, loss = 0.00183431\n",
      "Iteration 3026, loss = 0.00183281\n",
      "Iteration 3027, loss = 0.00183131\n",
      "Iteration 3028, loss = 0.00182982\n",
      "Iteration 3029, loss = 0.00182832\n",
      "Iteration 3030, loss = 0.00182684\n",
      "Iteration 3031, loss = 0.00182535\n",
      "Iteration 3032, loss = 0.00182384\n",
      "Iteration 3033, loss = 0.00182236\n",
      "Iteration 3034, loss = 0.00182085\n",
      "Iteration 3035, loss = 0.00181939\n",
      "Iteration 3036, loss = 0.00181790\n",
      "Iteration 3037, loss = 0.00181643\n",
      "Iteration 3038, loss = 0.00181492\n",
      "Iteration 3039, loss = 0.00181345\n",
      "Iteration 3040, loss = 0.00181199\n",
      "Iteration 3041, loss = 0.00181050\n",
      "Iteration 3042, loss = 0.00180905\n",
      "Iteration 3043, loss = 0.00180757\n",
      "Iteration 3044, loss = 0.00180611\n",
      "Iteration 3045, loss = 0.00180463\n",
      "Iteration 3046, loss = 0.00180317\n",
      "Iteration 3047, loss = 0.00180173\n",
      "Iteration 3048, loss = 0.00180027\n",
      "Iteration 3049, loss = 0.00179881\n",
      "Iteration 3050, loss = 0.00179736\n",
      "Iteration 3051, loss = 0.00179592\n",
      "Iteration 3052, loss = 0.00179447\n",
      "Iteration 3053, loss = 0.00179303\n",
      "Iteration 3054, loss = 0.00179157\n",
      "Iteration 3055, loss = 0.00179013\n",
      "Iteration 3056, loss = 0.00178870\n",
      "Iteration 3057, loss = 0.00178725\n",
      "Iteration 3058, loss = 0.00178580\n",
      "Iteration 3059, loss = 0.00178438\n",
      "Iteration 3060, loss = 0.00178293\n",
      "Iteration 3061, loss = 0.00178150\n",
      "Iteration 3062, loss = 0.00178005\n",
      "Iteration 3063, loss = 0.00177863\n",
      "Iteration 3064, loss = 0.00177718\n",
      "Iteration 3065, loss = 0.00177576\n",
      "Iteration 3066, loss = 0.00177431\n",
      "Iteration 3067, loss = 0.00177290\n",
      "Iteration 3068, loss = 0.00177148\n",
      "Iteration 3069, loss = 0.00177006\n",
      "Iteration 3070, loss = 0.00176863\n",
      "Iteration 3071, loss = 0.00176723\n",
      "Iteration 3072, loss = 0.00176580\n",
      "Iteration 3073, loss = 0.00176440\n",
      "Iteration 3074, loss = 0.00176297\n",
      "Iteration 3075, loss = 0.00176156\n",
      "Iteration 3076, loss = 0.00176015\n",
      "Iteration 3077, loss = 0.00175876\n",
      "Iteration 3078, loss = 0.00175735\n",
      "Iteration 3079, loss = 0.00175596\n",
      "Iteration 3080, loss = 0.00175456\n",
      "Iteration 3081, loss = 0.00175317\n",
      "Iteration 3082, loss = 0.00175179\n",
      "Iteration 3083, loss = 0.00175038\n",
      "Iteration 3084, loss = 0.00174897\n",
      "Iteration 3085, loss = 0.00174759\n",
      "Iteration 3086, loss = 0.00174619\n",
      "Iteration 3087, loss = 0.00174481\n",
      "Iteration 3088, loss = 0.00174342\n",
      "Iteration 3089, loss = 0.00174201\n",
      "Iteration 3090, loss = 0.00174062\n",
      "Iteration 3091, loss = 0.00173925\n",
      "Iteration 3092, loss = 0.00173787\n",
      "Iteration 3093, loss = 0.00173646\n",
      "Iteration 3094, loss = 0.00173511\n",
      "Iteration 3095, loss = 0.00173372\n",
      "Iteration 3096, loss = 0.00173234\n",
      "Iteration 3097, loss = 0.00173097\n",
      "Iteration 3098, loss = 0.00172959\n",
      "Iteration 3099, loss = 0.00172822\n",
      "Iteration 3100, loss = 0.00172685\n",
      "Iteration 3101, loss = 0.00172548\n",
      "Iteration 3102, loss = 0.00172410\n",
      "Iteration 3103, loss = 0.00172274\n",
      "Iteration 3104, loss = 0.00172139\n",
      "Iteration 3105, loss = 0.00172002\n",
      "Iteration 3106, loss = 0.00171865\n",
      "Iteration 3107, loss = 0.00171729\n",
      "Iteration 3108, loss = 0.00171595\n",
      "Iteration 3109, loss = 0.00171460\n",
      "Iteration 3110, loss = 0.00171321\n",
      "Iteration 3111, loss = 0.00171188\n",
      "Iteration 3112, loss = 0.00171052\n",
      "Iteration 3113, loss = 0.00170915\n",
      "Iteration 3114, loss = 0.00170781\n",
      "Iteration 3115, loss = 0.00170644\n",
      "Iteration 3116, loss = 0.00170512\n",
      "Iteration 3117, loss = 0.00170376\n",
      "Iteration 3118, loss = 0.00170242\n",
      "Iteration 3119, loss = 0.00170107\n",
      "Iteration 3120, loss = 0.00169974\n",
      "Iteration 3121, loss = 0.00169840\n",
      "Iteration 3122, loss = 0.00169707\n",
      "Iteration 3123, loss = 0.00169575\n",
      "Iteration 3124, loss = 0.00169442\n",
      "Iteration 3125, loss = 0.00169311\n",
      "Iteration 3126, loss = 0.00169178\n",
      "Iteration 3127, loss = 0.00169046\n",
      "Iteration 3128, loss = 0.00168914\n",
      "Iteration 3129, loss = 0.00168782\n",
      "Iteration 3130, loss = 0.00168651\n",
      "Iteration 3131, loss = 0.00168519\n",
      "Iteration 3132, loss = 0.00168387\n",
      "Iteration 3133, loss = 0.00168254\n",
      "Iteration 3134, loss = 0.00168123\n",
      "Iteration 3135, loss = 0.00167989\n",
      "Iteration 3136, loss = 0.00167860\n",
      "Iteration 3137, loss = 0.00167728\n",
      "Iteration 3138, loss = 0.00167596\n",
      "Iteration 3139, loss = 0.00167467\n",
      "Iteration 3140, loss = 0.00167335\n",
      "Iteration 3141, loss = 0.00167203\n",
      "Iteration 3142, loss = 0.00167073\n",
      "Iteration 3143, loss = 0.00166940\n",
      "Iteration 3144, loss = 0.00166810\n",
      "Iteration 3145, loss = 0.00166680\n",
      "Iteration 3146, loss = 0.00166548\n",
      "Iteration 3147, loss = 0.00166419\n",
      "Iteration 3148, loss = 0.00166288\n",
      "Iteration 3149, loss = 0.00166157\n",
      "Iteration 3150, loss = 0.00166027\n",
      "Iteration 3151, loss = 0.00165898\n",
      "Iteration 3152, loss = 0.00165769\n",
      "Iteration 3153, loss = 0.00165639\n",
      "Iteration 3154, loss = 0.00165511\n",
      "Iteration 3155, loss = 0.00165381\n",
      "Iteration 3156, loss = 0.00165253\n",
      "Iteration 3157, loss = 0.00165124\n",
      "Iteration 3158, loss = 0.00164996\n",
      "Iteration 3159, loss = 0.00164865\n",
      "Iteration 3160, loss = 0.00164739\n",
      "Iteration 3161, loss = 0.00164610\n",
      "Iteration 3162, loss = 0.00164483\n",
      "Iteration 3163, loss = 0.00164353\n",
      "Iteration 3164, loss = 0.00164224\n",
      "Iteration 3165, loss = 0.00164097\n",
      "Iteration 3166, loss = 0.00163969\n",
      "Iteration 3167, loss = 0.00163841\n",
      "Iteration 3168, loss = 0.00163712\n",
      "Iteration 3169, loss = 0.00163585\n",
      "Iteration 3170, loss = 0.00163459\n",
      "Iteration 3171, loss = 0.00163331\n",
      "Iteration 3172, loss = 0.00163205\n",
      "Iteration 3173, loss = 0.00163078\n",
      "Iteration 3174, loss = 0.00162952\n",
      "Iteration 3175, loss = 0.00162825\n",
      "Iteration 3176, loss = 0.00162702\n",
      "Iteration 3177, loss = 0.00162573\n",
      "Iteration 3178, loss = 0.00162449\n",
      "Iteration 3179, loss = 0.00162325\n",
      "Iteration 3180, loss = 0.00162198\n",
      "Iteration 3181, loss = 0.00162074\n",
      "Iteration 3182, loss = 0.00161950\n",
      "Iteration 3183, loss = 0.00161825\n",
      "Iteration 3184, loss = 0.00161700\n",
      "Iteration 3185, loss = 0.00161576\n",
      "Iteration 3186, loss = 0.00161452\n",
      "Iteration 3187, loss = 0.00161329\n",
      "Iteration 3188, loss = 0.00161205\n",
      "Iteration 3189, loss = 0.00161081\n",
      "Iteration 3190, loss = 0.00160958\n",
      "Iteration 3191, loss = 0.00160835\n",
      "Iteration 3192, loss = 0.00160711\n",
      "Iteration 3193, loss = 0.00160590\n",
      "Iteration 3194, loss = 0.00160466\n",
      "Iteration 3195, loss = 0.00160344\n",
      "Iteration 3196, loss = 0.00160223\n",
      "Iteration 3197, loss = 0.00160102\n",
      "Iteration 3198, loss = 0.00159979\n",
      "Iteration 3199, loss = 0.00159856\n",
      "Iteration 3200, loss = 0.00159734\n",
      "Iteration 3201, loss = 0.00159614\n",
      "Iteration 3202, loss = 0.00159490\n",
      "Iteration 3203, loss = 0.00159369\n",
      "Iteration 3204, loss = 0.00159248\n",
      "Iteration 3205, loss = 0.00159125\n",
      "Iteration 3206, loss = 0.00159004\n",
      "Iteration 3207, loss = 0.00158882\n",
      "Iteration 3208, loss = 0.00158762\n",
      "Iteration 3209, loss = 0.00158642\n",
      "Iteration 3210, loss = 0.00158520\n",
      "Iteration 3211, loss = 0.00158400\n",
      "Iteration 3212, loss = 0.00158279\n",
      "Iteration 3213, loss = 0.00158158\n",
      "Iteration 3214, loss = 0.00158039\n",
      "Iteration 3215, loss = 0.00157918\n",
      "Iteration 3216, loss = 0.00157797\n",
      "Iteration 3217, loss = 0.00157678\n",
      "Iteration 3218, loss = 0.00157558\n",
      "Iteration 3219, loss = 0.00157439\n",
      "Iteration 3220, loss = 0.00157319\n",
      "Iteration 3221, loss = 0.00157199\n",
      "Iteration 3222, loss = 0.00157080\n",
      "Iteration 3223, loss = 0.00156963\n",
      "Iteration 3224, loss = 0.00156841\n",
      "Iteration 3225, loss = 0.00156724\n",
      "Iteration 3226, loss = 0.00156604\n",
      "Iteration 3227, loss = 0.00156487\n",
      "Iteration 3228, loss = 0.00156369\n",
      "Iteration 3229, loss = 0.00156251\n",
      "Iteration 3230, loss = 0.00156132\n",
      "Iteration 3231, loss = 0.00156015\n",
      "Iteration 3232, loss = 0.00155897\n",
      "Iteration 3233, loss = 0.00155781\n",
      "Iteration 3234, loss = 0.00155663\n",
      "Iteration 3235, loss = 0.00155546\n",
      "Iteration 3236, loss = 0.00155430\n",
      "Iteration 3237, loss = 0.00155312\n",
      "Iteration 3238, loss = 0.00155197\n",
      "Iteration 3239, loss = 0.00155078\n",
      "Iteration 3240, loss = 0.00154963\n",
      "Iteration 3241, loss = 0.00154845\n",
      "Iteration 3242, loss = 0.00154731\n",
      "Iteration 3243, loss = 0.00154613\n",
      "Iteration 3244, loss = 0.00154498\n",
      "Iteration 3245, loss = 0.00154381\n",
      "Iteration 3246, loss = 0.00154268\n",
      "Iteration 3247, loss = 0.00154151\n",
      "Iteration 3248, loss = 0.00154037\n",
      "Iteration 3249, loss = 0.00153921\n",
      "Iteration 3250, loss = 0.00153806\n",
      "Iteration 3251, loss = 0.00153690\n",
      "Iteration 3252, loss = 0.00153576\n",
      "Iteration 3253, loss = 0.00153460\n",
      "Iteration 3254, loss = 0.00153345\n",
      "Iteration 3255, loss = 0.00153230\n",
      "Iteration 3256, loss = 0.00153116\n",
      "Iteration 3257, loss = 0.00153002\n",
      "Iteration 3258, loss = 0.00152887\n",
      "Iteration 3259, loss = 0.00152772\n",
      "Iteration 3260, loss = 0.00152659\n",
      "Iteration 3261, loss = 0.00152545\n",
      "Iteration 3262, loss = 0.00152430\n",
      "Iteration 3263, loss = 0.00152315\n",
      "Iteration 3264, loss = 0.00152201\n",
      "Iteration 3265, loss = 0.00152089\n",
      "Iteration 3266, loss = 0.00151974\n",
      "Iteration 3267, loss = 0.00151860\n",
      "Iteration 3268, loss = 0.00151747\n",
      "Iteration 3269, loss = 0.00151634\n",
      "Iteration 3270, loss = 0.00151521\n",
      "Iteration 3271, loss = 0.00151407\n",
      "Iteration 3272, loss = 0.00151294\n",
      "Iteration 3273, loss = 0.00151183\n",
      "Iteration 3274, loss = 0.00151070\n",
      "Iteration 3275, loss = 0.00150959\n",
      "Iteration 3276, loss = 0.00150846\n",
      "Iteration 3277, loss = 0.00150734\n",
      "Iteration 3278, loss = 0.00150623\n",
      "Iteration 3279, loss = 0.00150510\n",
      "Iteration 3280, loss = 0.00150402\n",
      "Iteration 3281, loss = 0.00150288\n",
      "Iteration 3282, loss = 0.00150177\n",
      "Iteration 3283, loss = 0.00150066\n",
      "Iteration 3284, loss = 0.00149955\n",
      "Iteration 3285, loss = 0.00149845\n",
      "Iteration 3286, loss = 0.00149733\n",
      "Iteration 3287, loss = 0.00149622\n",
      "Iteration 3288, loss = 0.00149512\n",
      "Iteration 3289, loss = 0.00149402\n",
      "Iteration 3290, loss = 0.00149292\n",
      "Iteration 3291, loss = 0.00149180\n",
      "Iteration 3292, loss = 0.00149071\n",
      "Iteration 3293, loss = 0.00148960\n",
      "Iteration 3294, loss = 0.00148851\n",
      "Iteration 3295, loss = 0.00148741\n",
      "Iteration 3296, loss = 0.00148632\n",
      "Iteration 3297, loss = 0.00148520\n",
      "Iteration 3298, loss = 0.00148411\n",
      "Iteration 3299, loss = 0.00148302\n",
      "Iteration 3300, loss = 0.00148194\n",
      "Iteration 3301, loss = 0.00148084\n",
      "Iteration 3302, loss = 0.00147975\n",
      "Iteration 3303, loss = 0.00147867\n",
      "Iteration 3304, loss = 0.00147758\n",
      "Iteration 3305, loss = 0.00147649\n",
      "Iteration 3306, loss = 0.00147542\n",
      "Iteration 3307, loss = 0.00147433\n",
      "Iteration 3308, loss = 0.00147324\n",
      "Iteration 3309, loss = 0.00147218\n",
      "Iteration 3310, loss = 0.00147110\n",
      "Iteration 3311, loss = 0.00147002\n",
      "Iteration 3312, loss = 0.00146897\n",
      "Iteration 3313, loss = 0.00146789\n",
      "Iteration 3314, loss = 0.00146682\n",
      "Iteration 3315, loss = 0.00146575\n",
      "Iteration 3316, loss = 0.00146469\n",
      "Iteration 3317, loss = 0.00146362\n",
      "Iteration 3318, loss = 0.00146255\n",
      "Iteration 3319, loss = 0.00146149\n",
      "Iteration 3320, loss = 0.00146042\n",
      "Iteration 3321, loss = 0.00145935\n",
      "Iteration 3322, loss = 0.00145830\n",
      "Iteration 3323, loss = 0.00145723\n",
      "Iteration 3324, loss = 0.00145619\n",
      "Iteration 3325, loss = 0.00145512\n",
      "Iteration 3326, loss = 0.00145407\n",
      "Iteration 3327, loss = 0.00145300\n",
      "Iteration 3328, loss = 0.00145195\n",
      "Iteration 3329, loss = 0.00145089\n",
      "Iteration 3330, loss = 0.00144985\n",
      "Iteration 3331, loss = 0.00144880\n",
      "Iteration 3332, loss = 0.00144775\n",
      "Iteration 3333, loss = 0.00144670\n",
      "Iteration 3334, loss = 0.00144566\n",
      "Iteration 3335, loss = 0.00144461\n",
      "Iteration 3336, loss = 0.00144357\n",
      "Iteration 3337, loss = 0.00144253\n",
      "Iteration 3338, loss = 0.00144150\n",
      "Iteration 3339, loss = 0.00144045\n",
      "Iteration 3340, loss = 0.00143941\n",
      "Iteration 3341, loss = 0.00143839\n",
      "Iteration 3342, loss = 0.00143734\n",
      "Iteration 3343, loss = 0.00143630\n",
      "Iteration 3344, loss = 0.00143529\n",
      "Iteration 3345, loss = 0.00143424\n",
      "Iteration 3346, loss = 0.00143321\n",
      "Iteration 3347, loss = 0.00143217\n",
      "Iteration 3348, loss = 0.00143115\n",
      "Iteration 3349, loss = 0.00143013\n",
      "Iteration 3350, loss = 0.00142909\n",
      "Iteration 3351, loss = 0.00142807\n",
      "Iteration 3352, loss = 0.00142704\n",
      "Iteration 3353, loss = 0.00142602\n",
      "Iteration 3354, loss = 0.00142499\n",
      "Iteration 3355, loss = 0.00142397\n",
      "Iteration 3356, loss = 0.00142295\n",
      "Iteration 3357, loss = 0.00142193\n",
      "Iteration 3358, loss = 0.00142090\n",
      "Iteration 3359, loss = 0.00141989\n",
      "Iteration 3360, loss = 0.00141886\n",
      "Iteration 3361, loss = 0.00141784\n",
      "Iteration 3362, loss = 0.00141683\n",
      "Iteration 3363, loss = 0.00141583\n",
      "Iteration 3364, loss = 0.00141482\n",
      "Iteration 3365, loss = 0.00141380\n",
      "Iteration 3366, loss = 0.00141279\n",
      "Iteration 3367, loss = 0.00141178\n",
      "Iteration 3368, loss = 0.00141079\n",
      "Iteration 3369, loss = 0.00140977\n",
      "Iteration 3370, loss = 0.00140877\n",
      "Iteration 3371, loss = 0.00140777\n",
      "Iteration 3372, loss = 0.00140676\n",
      "Iteration 3373, loss = 0.00140576\n",
      "Iteration 3374, loss = 0.00140476\n",
      "Iteration 3375, loss = 0.00140376\n",
      "Iteration 3376, loss = 0.00140277\n",
      "Iteration 3377, loss = 0.00140177\n",
      "Iteration 3378, loss = 0.00140078\n",
      "Iteration 3379, loss = 0.00139978\n",
      "Iteration 3380, loss = 0.00139878\n",
      "Iteration 3381, loss = 0.00139779\n",
      "Iteration 3382, loss = 0.00139681\n",
      "Iteration 3383, loss = 0.00139583\n",
      "Iteration 3384, loss = 0.00139484\n",
      "Iteration 3385, loss = 0.00139384\n",
      "Iteration 3386, loss = 0.00139286\n",
      "Iteration 3387, loss = 0.00139186\n",
      "Iteration 3388, loss = 0.00139090\n",
      "Iteration 3389, loss = 0.00138991\n",
      "Iteration 3390, loss = 0.00138891\n",
      "Iteration 3391, loss = 0.00138795\n",
      "Iteration 3392, loss = 0.00138696\n",
      "Iteration 3393, loss = 0.00138598\n",
      "Iteration 3394, loss = 0.00138501\n",
      "Iteration 3395, loss = 0.00138404\n",
      "Iteration 3396, loss = 0.00138308\n",
      "Iteration 3397, loss = 0.00138210\n",
      "Iteration 3398, loss = 0.00138114\n",
      "Training loss did not improve more than tol=0.000001 for 10 consecutive epochs. Stopping.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         3\n",
      "           1       1.00      1.00      1.00         2\n",
      "           2       1.00      1.00      1.00         3\n",
      "           3       1.00      0.75      0.86         4\n",
      "           4       1.00      1.00      1.00         2\n",
      "           5       1.00      1.00      1.00         4\n",
      "           6       0.60      1.00      0.75         3\n",
      "           7       1.00      0.80      0.89         5\n",
      "           8       1.00      1.00      1.00         2\n",
      "           9       0.67      1.00      0.80         2\n",
      "          10       1.00      1.00      1.00         4\n",
      "          11       1.00      1.00      1.00         3\n",
      "          12       1.00      1.00      1.00         1\n",
      "          13       1.00      1.00      1.00         2\n",
      "          14       1.00      1.00      1.00         3\n",
      "          15       0.75      1.00      0.86         3\n",
      "          16       1.00      0.60      0.75         5\n",
      "          17       1.00      1.00      1.00         5\n",
      "          18       0.50      1.00      0.67         1\n",
      "          19       1.00      1.00      1.00         4\n",
      "          20       1.00      1.00      1.00         3\n",
      "          21       1.00      1.00      1.00         3\n",
      "          22       1.00      1.00      1.00         4\n",
      "          23       1.00      1.00      1.00         5\n",
      "          25       1.00      1.00      1.00         2\n",
      "          26       1.00      1.00      1.00         3\n",
      "          27       1.00      1.00      1.00         2\n",
      "          28       1.00      1.00      1.00         5\n",
      "          29       1.00      1.00      1.00         3\n",
      "          30       1.00      1.00      1.00         1\n",
      "          31       1.00      1.00      1.00         2\n",
      "          32       1.00      1.00      1.00         3\n",
      "          33       1.00      1.00      1.00         2\n",
      "          34       1.00      1.00      1.00         3\n",
      "          35       1.00      0.50      0.67         2\n",
      "          36       1.00      1.00      1.00         5\n",
      "          37       1.00      1.00      1.00         3\n",
      "          38       1.00      1.00      1.00         4\n",
      "          39       1.00      1.00      1.00         4\n",
      "\n",
      "    accuracy                           0.96       120\n",
      "   macro avg       0.96      0.97      0.95       120\n",
      "weighted avg       0.97      0.96      0.96       120\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "hidden_layers = (30,)\n",
    "activation = 'logistic'\n",
    "opts = dict(hidden_layer_sizes = hidden_layers,verbose = True,activation = activation,tol = 1e-6,max_iter = int(1e6))\n",
    "solver = 'adam'\n",
    "clf_MLP = MLPClassifier(solver = solver, **opts)\n",
    "clf_MLP.fit(X_train_,y_train)\n",
    "predictions = clf_MLP.predict(X_test_)\n",
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- hidden_layers = (30,) , activation = 'relu', solver = 'adam'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 3.92284120\n",
      "Iteration 2, loss = 2.70205202\n",
      "Iteration 3, loss = 2.03790412\n",
      "Iteration 4, loss = 1.54289766\n",
      "Iteration 5, loss = 1.19753175\n",
      "Iteration 6, loss = 0.93280993\n",
      "Iteration 7, loss = 0.73391345\n",
      "Iteration 8, loss = 0.58356599\n",
      "Iteration 9, loss = 0.47199727\n",
      "Iteration 10, loss = 0.38114793\n",
      "Iteration 11, loss = 0.31257131\n",
      "Iteration 12, loss = 0.26098446\n",
      "Iteration 13, loss = 0.22026617\n",
      "Iteration 14, loss = 0.18549616\n",
      "Iteration 15, loss = 0.15715687\n",
      "Iteration 16, loss = 0.13392923\n",
      "Iteration 17, loss = 0.11164844\n",
      "Iteration 18, loss = 0.09406502\n",
      "Iteration 19, loss = 0.07661063\n",
      "Iteration 20, loss = 0.06265050\n",
      "Iteration 21, loss = 0.05098174\n",
      "Iteration 22, loss = 0.04196640\n",
      "Iteration 23, loss = 0.03555432\n",
      "Iteration 24, loss = 0.03034489\n",
      "Iteration 25, loss = 0.02692195\n",
      "Iteration 26, loss = 0.02385549\n",
      "Iteration 27, loss = 0.02146664\n",
      "Iteration 28, loss = 0.01950457\n",
      "Iteration 29, loss = 0.01790892\n",
      "Iteration 30, loss = 0.01653100\n",
      "Iteration 31, loss = 0.01530961\n",
      "Iteration 32, loss = 0.01435112\n",
      "Iteration 33, loss = 0.01349316\n",
      "Iteration 34, loss = 0.01268050\n",
      "Iteration 35, loss = 0.01203657\n",
      "Iteration 36, loss = 0.01144168\n",
      "Iteration 37, loss = 0.01088678\n",
      "Iteration 38, loss = 0.01040512\n",
      "Iteration 39, loss = 0.00995845\n",
      "Iteration 40, loss = 0.00952960\n",
      "Iteration 41, loss = 0.00917831\n",
      "Iteration 42, loss = 0.00884646\n",
      "Iteration 43, loss = 0.00854435\n",
      "Iteration 44, loss = 0.00825849\n",
      "Iteration 45, loss = 0.00799346\n",
      "Iteration 46, loss = 0.00772415\n",
      "Iteration 47, loss = 0.00750433\n",
      "Iteration 48, loss = 0.00728348\n",
      "Iteration 49, loss = 0.00706963\n",
      "Iteration 50, loss = 0.00688001\n",
      "Iteration 51, loss = 0.00670047\n",
      "Iteration 52, loss = 0.00652045\n",
      "Iteration 53, loss = 0.00636123\n",
      "Iteration 54, loss = 0.00619893\n",
      "Iteration 55, loss = 0.00604904\n",
      "Iteration 56, loss = 0.00590712\n",
      "Iteration 57, loss = 0.00577488\n",
      "Iteration 58, loss = 0.00564385\n",
      "Iteration 59, loss = 0.00551629\n",
      "Iteration 60, loss = 0.00539429\n",
      "Iteration 61, loss = 0.00528469\n",
      "Iteration 62, loss = 0.00517350\n",
      "Iteration 63, loss = 0.00506754\n",
      "Iteration 64, loss = 0.00495863\n",
      "Iteration 65, loss = 0.00486087\n",
      "Iteration 66, loss = 0.00476152\n",
      "Iteration 67, loss = 0.00466454\n",
      "Iteration 68, loss = 0.00457862\n",
      "Iteration 69, loss = 0.00448688\n",
      "Iteration 70, loss = 0.00440334\n",
      "Iteration 71, loss = 0.00432080\n",
      "Iteration 72, loss = 0.00424265\n",
      "Iteration 73, loss = 0.00416239\n",
      "Iteration 74, loss = 0.00409055\n",
      "Iteration 75, loss = 0.00401588\n",
      "Iteration 76, loss = 0.00394833\n",
      "Iteration 77, loss = 0.00388120\n",
      "Iteration 78, loss = 0.00381410\n",
      "Iteration 79, loss = 0.00375305\n",
      "Iteration 80, loss = 0.00368986\n",
      "Iteration 81, loss = 0.00362940\n",
      "Iteration 82, loss = 0.00356936\n",
      "Iteration 83, loss = 0.00351238\n",
      "Iteration 84, loss = 0.00345704\n",
      "Iteration 85, loss = 0.00340346\n",
      "Iteration 86, loss = 0.00334938\n",
      "Iteration 87, loss = 0.00329656\n",
      "Iteration 88, loss = 0.00324506\n",
      "Iteration 89, loss = 0.00319612\n",
      "Iteration 90, loss = 0.00314696\n",
      "Iteration 91, loss = 0.00309947\n",
      "Iteration 92, loss = 0.00305231\n",
      "Iteration 93, loss = 0.00300811\n",
      "Iteration 94, loss = 0.00296442\n",
      "Iteration 95, loss = 0.00292195\n",
      "Iteration 96, loss = 0.00287897\n",
      "Iteration 97, loss = 0.00284017\n",
      "Iteration 98, loss = 0.00280056\n",
      "Iteration 99, loss = 0.00276085\n",
      "Iteration 100, loss = 0.00272184\n",
      "Iteration 101, loss = 0.00268523\n",
      "Iteration 102, loss = 0.00264750\n",
      "Iteration 103, loss = 0.00261222\n",
      "Iteration 104, loss = 0.00257570\n",
      "Iteration 105, loss = 0.00254289\n",
      "Iteration 106, loss = 0.00250891\n",
      "Iteration 107, loss = 0.00247706\n",
      "Iteration 108, loss = 0.00244363\n",
      "Iteration 109, loss = 0.00241251\n",
      "Iteration 110, loss = 0.00238149\n",
      "Iteration 111, loss = 0.00235252\n",
      "Iteration 112, loss = 0.00232190\n",
      "Iteration 113, loss = 0.00229246\n",
      "Iteration 114, loss = 0.00226489\n",
      "Iteration 115, loss = 0.00223678\n",
      "Iteration 116, loss = 0.00221051\n",
      "Iteration 117, loss = 0.00218378\n",
      "Iteration 118, loss = 0.00215744\n",
      "Iteration 119, loss = 0.00213177\n",
      "Iteration 120, loss = 0.00210684\n",
      "Iteration 121, loss = 0.00208109\n",
      "Iteration 122, loss = 0.00205723\n",
      "Iteration 123, loss = 0.00203175\n",
      "Iteration 124, loss = 0.00200918\n",
      "Iteration 125, loss = 0.00198570\n",
      "Iteration 126, loss = 0.00196190\n",
      "Iteration 127, loss = 0.00193928\n",
      "Iteration 128, loss = 0.00191705\n",
      "Iteration 129, loss = 0.00189539\n",
      "Iteration 130, loss = 0.00187473\n",
      "Iteration 131, loss = 0.00185334\n",
      "Iteration 132, loss = 0.00183317\n",
      "Iteration 133, loss = 0.00181247\n",
      "Iteration 134, loss = 0.00179270\n",
      "Iteration 135, loss = 0.00177348\n",
      "Iteration 136, loss = 0.00175474\n",
      "Iteration 137, loss = 0.00173543\n",
      "Iteration 138, loss = 0.00171708\n",
      "Iteration 139, loss = 0.00169943\n",
      "Iteration 140, loss = 0.00168196\n",
      "Iteration 141, loss = 0.00166366\n",
      "Iteration 142, loss = 0.00164731\n",
      "Iteration 143, loss = 0.00162986\n",
      "Iteration 144, loss = 0.00161394\n",
      "Iteration 145, loss = 0.00159771\n",
      "Iteration 146, loss = 0.00158251\n",
      "Iteration 147, loss = 0.00156652\n",
      "Iteration 148, loss = 0.00155151\n",
      "Iteration 149, loss = 0.00153673\n",
      "Iteration 150, loss = 0.00152200\n",
      "Iteration 151, loss = 0.00150709\n",
      "Iteration 152, loss = 0.00149254\n",
      "Iteration 153, loss = 0.00147856\n",
      "Iteration 154, loss = 0.00146460\n",
      "Iteration 155, loss = 0.00145081\n",
      "Iteration 156, loss = 0.00143719\n",
      "Iteration 157, loss = 0.00142442\n",
      "Iteration 158, loss = 0.00141083\n",
      "Iteration 159, loss = 0.00139758\n",
      "Iteration 160, loss = 0.00138494\n",
      "Iteration 161, loss = 0.00137215\n",
      "Iteration 162, loss = 0.00135933\n",
      "Iteration 163, loss = 0.00134752\n",
      "Iteration 164, loss = 0.00133565\n",
      "Iteration 165, loss = 0.00132354\n",
      "Iteration 166, loss = 0.00131212\n",
      "Iteration 167, loss = 0.00130068\n",
      "Iteration 168, loss = 0.00128961\n",
      "Iteration 169, loss = 0.00127830\n",
      "Iteration 170, loss = 0.00126730\n",
      "Iteration 171, loss = 0.00125659\n",
      "Iteration 172, loss = 0.00124581\n",
      "Iteration 173, loss = 0.00123513\n",
      "Iteration 174, loss = 0.00122495\n",
      "Iteration 175, loss = 0.00121449\n",
      "Iteration 176, loss = 0.00120412\n",
      "Iteration 177, loss = 0.00119414\n",
      "Iteration 178, loss = 0.00118431\n",
      "Iteration 179, loss = 0.00117424\n",
      "Iteration 180, loss = 0.00116469\n",
      "Iteration 181, loss = 0.00115504\n",
      "Iteration 182, loss = 0.00114554\n",
      "Iteration 183, loss = 0.00113587\n",
      "Iteration 184, loss = 0.00112698\n",
      "Iteration 185, loss = 0.00111772\n",
      "Iteration 186, loss = 0.00110877\n",
      "Iteration 187, loss = 0.00109997\n",
      "Iteration 188, loss = 0.00109148\n",
      "Iteration 189, loss = 0.00108291\n",
      "Iteration 190, loss = 0.00107430\n",
      "Iteration 191, loss = 0.00106616\n",
      "Iteration 192, loss = 0.00105805\n",
      "Iteration 193, loss = 0.00104989\n",
      "Iteration 194, loss = 0.00104214\n",
      "Iteration 195, loss = 0.00103429\n",
      "Iteration 196, loss = 0.00102622\n",
      "Iteration 197, loss = 0.00101859\n",
      "Iteration 198, loss = 0.00101083\n",
      "Iteration 199, loss = 0.00100354\n",
      "Iteration 200, loss = 0.00099588\n",
      "Iteration 201, loss = 0.00098893\n",
      "Iteration 202, loss = 0.00098162\n",
      "Iteration 203, loss = 0.00097429\n",
      "Iteration 204, loss = 0.00096727\n",
      "Iteration 205, loss = 0.00096018\n",
      "Iteration 206, loss = 0.00095327\n",
      "Iteration 207, loss = 0.00094640\n",
      "Iteration 208, loss = 0.00093965\n",
      "Iteration 209, loss = 0.00093302\n",
      "Iteration 210, loss = 0.00092634\n",
      "Iteration 211, loss = 0.00092009\n",
      "Iteration 212, loss = 0.00091373\n",
      "Iteration 213, loss = 0.00090733\n",
      "Iteration 214, loss = 0.00090103\n",
      "Iteration 215, loss = 0.00089479\n",
      "Iteration 216, loss = 0.00088881\n",
      "Iteration 217, loss = 0.00088271\n",
      "Iteration 218, loss = 0.00087669\n",
      "Iteration 219, loss = 0.00087069\n",
      "Iteration 220, loss = 0.00086487\n",
      "Iteration 221, loss = 0.00085912\n",
      "Iteration 222, loss = 0.00085324\n",
      "Iteration 223, loss = 0.00084753\n",
      "Iteration 224, loss = 0.00084197\n",
      "Iteration 225, loss = 0.00083644\n",
      "Iteration 226, loss = 0.00083087\n",
      "Iteration 227, loss = 0.00082538\n",
      "Iteration 228, loss = 0.00082009\n",
      "Iteration 229, loss = 0.00081473\n",
      "Iteration 230, loss = 0.00080941\n",
      "Iteration 231, loss = 0.00080413\n",
      "Iteration 232, loss = 0.00079898\n",
      "Iteration 233, loss = 0.00079393\n",
      "Iteration 234, loss = 0.00078885\n",
      "Iteration 235, loss = 0.00078378\n",
      "Iteration 236, loss = 0.00077891\n",
      "Iteration 237, loss = 0.00077400\n",
      "Iteration 238, loss = 0.00076909\n",
      "Iteration 239, loss = 0.00076422\n",
      "Iteration 240, loss = 0.00075946\n",
      "Iteration 241, loss = 0.00075469\n",
      "Iteration 242, loss = 0.00075007\n",
      "Iteration 243, loss = 0.00074537\n",
      "Iteration 244, loss = 0.00074078\n",
      "Iteration 245, loss = 0.00073617\n",
      "Iteration 246, loss = 0.00073178\n",
      "Iteration 247, loss = 0.00072730\n",
      "Iteration 248, loss = 0.00072290\n",
      "Iteration 249, loss = 0.00071854\n",
      "Iteration 250, loss = 0.00071428\n",
      "Iteration 251, loss = 0.00071000\n",
      "Iteration 252, loss = 0.00070586\n",
      "Iteration 253, loss = 0.00070165\n",
      "Iteration 254, loss = 0.00069748\n",
      "Iteration 255, loss = 0.00069346\n",
      "Iteration 256, loss = 0.00068933\n",
      "Iteration 257, loss = 0.00068533\n",
      "Iteration 258, loss = 0.00068135\n",
      "Iteration 259, loss = 0.00067744\n",
      "Iteration 260, loss = 0.00067354\n",
      "Iteration 261, loss = 0.00066967\n",
      "Iteration 262, loss = 0.00066588\n",
      "Iteration 263, loss = 0.00066216\n",
      "Iteration 264, loss = 0.00065850\n",
      "Iteration 265, loss = 0.00065475\n",
      "Iteration 266, loss = 0.00065102\n",
      "Iteration 267, loss = 0.00064739\n",
      "Iteration 268, loss = 0.00064375\n",
      "Iteration 269, loss = 0.00064013\n",
      "Iteration 270, loss = 0.00063671\n",
      "Iteration 271, loss = 0.00063309\n",
      "Iteration 272, loss = 0.00062963\n",
      "Iteration 273, loss = 0.00062627\n",
      "Iteration 274, loss = 0.00062289\n",
      "Iteration 275, loss = 0.00061949\n",
      "Iteration 276, loss = 0.00061626\n",
      "Iteration 277, loss = 0.00061294\n",
      "Iteration 278, loss = 0.00060966\n",
      "Iteration 279, loss = 0.00060639\n",
      "Iteration 280, loss = 0.00060313\n",
      "Iteration 281, loss = 0.00059989\n",
      "Iteration 282, loss = 0.00059666\n",
      "Iteration 283, loss = 0.00059346\n",
      "Iteration 284, loss = 0.00059036\n",
      "Iteration 285, loss = 0.00058719\n",
      "Iteration 286, loss = 0.00058415\n",
      "Iteration 287, loss = 0.00058104\n",
      "Iteration 288, loss = 0.00057802\n",
      "Iteration 289, loss = 0.00057498\n",
      "Iteration 290, loss = 0.00057209\n",
      "Iteration 291, loss = 0.00056905\n",
      "Iteration 292, loss = 0.00056616\n",
      "Iteration 293, loss = 0.00056333\n",
      "Iteration 294, loss = 0.00056047\n",
      "Iteration 295, loss = 0.00055758\n",
      "Iteration 296, loss = 0.00055476\n",
      "Iteration 297, loss = 0.00055196\n",
      "Iteration 298, loss = 0.00054915\n",
      "Iteration 299, loss = 0.00054644\n",
      "Iteration 300, loss = 0.00054368\n",
      "Iteration 301, loss = 0.00054099\n",
      "Iteration 302, loss = 0.00053829\n",
      "Iteration 303, loss = 0.00053568\n",
      "Iteration 304, loss = 0.00053297\n",
      "Iteration 305, loss = 0.00053039\n",
      "Iteration 306, loss = 0.00052777\n",
      "Iteration 307, loss = 0.00052523\n",
      "Iteration 308, loss = 0.00052264\n",
      "Iteration 309, loss = 0.00052009\n",
      "Iteration 310, loss = 0.00051760\n",
      "Iteration 311, loss = 0.00051504\n",
      "Iteration 312, loss = 0.00051259\n",
      "Iteration 313, loss = 0.00051016\n",
      "Iteration 314, loss = 0.00050777\n",
      "Iteration 315, loss = 0.00050529\n",
      "Iteration 316, loss = 0.00050284\n",
      "Iteration 317, loss = 0.00050048\n",
      "Iteration 318, loss = 0.00049810\n",
      "Iteration 319, loss = 0.00049578\n",
      "Iteration 320, loss = 0.00049341\n",
      "Iteration 321, loss = 0.00049110\n",
      "Iteration 322, loss = 0.00048876\n",
      "Iteration 323, loss = 0.00048655\n",
      "Iteration 324, loss = 0.00048429\n",
      "Iteration 325, loss = 0.00048201\n",
      "Iteration 326, loss = 0.00047983\n",
      "Iteration 327, loss = 0.00047762\n",
      "Iteration 328, loss = 0.00047538\n",
      "Iteration 329, loss = 0.00047331\n",
      "Iteration 330, loss = 0.00047112\n",
      "Iteration 331, loss = 0.00046902\n",
      "Iteration 332, loss = 0.00046692\n",
      "Iteration 333, loss = 0.00046485\n",
      "Iteration 334, loss = 0.00046280\n",
      "Iteration 335, loss = 0.00046075\n",
      "Iteration 336, loss = 0.00045869\n",
      "Iteration 337, loss = 0.00045669\n",
      "Iteration 338, loss = 0.00045468\n",
      "Iteration 339, loss = 0.00045258\n",
      "Iteration 340, loss = 0.00045058\n",
      "Iteration 341, loss = 0.00044855\n",
      "Iteration 342, loss = 0.00044661\n",
      "Iteration 343, loss = 0.00044463\n",
      "Iteration 344, loss = 0.00044270\n",
      "Iteration 345, loss = 0.00044069\n",
      "Iteration 346, loss = 0.00043878\n",
      "Iteration 347, loss = 0.00043694\n",
      "Iteration 348, loss = 0.00043503\n",
      "Iteration 349, loss = 0.00043317\n",
      "Iteration 350, loss = 0.00043132\n",
      "Iteration 351, loss = 0.00042948\n",
      "Iteration 352, loss = 0.00042762\n",
      "Iteration 353, loss = 0.00042585\n",
      "Iteration 354, loss = 0.00042406\n",
      "Iteration 355, loss = 0.00042227\n",
      "Iteration 356, loss = 0.00042052\n",
      "Iteration 357, loss = 0.00041882\n",
      "Iteration 358, loss = 0.00041711\n",
      "Iteration 359, loss = 0.00041537\n",
      "Iteration 360, loss = 0.00041362\n",
      "Iteration 361, loss = 0.00041196\n",
      "Iteration 362, loss = 0.00041029\n",
      "Iteration 363, loss = 0.00040863\n",
      "Iteration 364, loss = 0.00040697\n",
      "Iteration 365, loss = 0.00040534\n",
      "Iteration 366, loss = 0.00040372\n",
      "Iteration 367, loss = 0.00040210\n",
      "Iteration 368, loss = 0.00040051\n",
      "Iteration 369, loss = 0.00039892\n",
      "Iteration 370, loss = 0.00039734\n",
      "Iteration 371, loss = 0.00039572\n",
      "Iteration 372, loss = 0.00039420\n",
      "Iteration 373, loss = 0.00039259\n",
      "Iteration 374, loss = 0.00039107\n",
      "Iteration 375, loss = 0.00038950\n",
      "Iteration 376, loss = 0.00038796\n",
      "Iteration 377, loss = 0.00038645\n",
      "Iteration 378, loss = 0.00038494\n",
      "Iteration 379, loss = 0.00038345\n",
      "Iteration 380, loss = 0.00038197\n",
      "Iteration 381, loss = 0.00038047\n",
      "Iteration 382, loss = 0.00037901\n",
      "Iteration 383, loss = 0.00037752\n",
      "Iteration 384, loss = 0.00037608\n",
      "Iteration 385, loss = 0.00037463\n",
      "Iteration 386, loss = 0.00037319\n",
      "Iteration 387, loss = 0.00037179\n",
      "Iteration 388, loss = 0.00037037\n",
      "Iteration 389, loss = 0.00036895\n",
      "Iteration 390, loss = 0.00036752\n",
      "Iteration 391, loss = 0.00036615\n",
      "Iteration 392, loss = 0.00036474\n",
      "Iteration 393, loss = 0.00036337\n",
      "Iteration 394, loss = 0.00036200\n",
      "Iteration 395, loss = 0.00036064\n",
      "Iteration 396, loss = 0.00035929\n",
      "Iteration 397, loss = 0.00035798\n",
      "Iteration 398, loss = 0.00035658\n",
      "Iteration 399, loss = 0.00035531\n",
      "Iteration 400, loss = 0.00035398\n",
      "Iteration 401, loss = 0.00035267\n",
      "Iteration 402, loss = 0.00035137\n",
      "Iteration 403, loss = 0.00035008\n",
      "Iteration 404, loss = 0.00034880\n",
      "Iteration 405, loss = 0.00034755\n",
      "Iteration 406, loss = 0.00034626\n",
      "Iteration 407, loss = 0.00034498\n",
      "Iteration 408, loss = 0.00034373\n",
      "Iteration 409, loss = 0.00034246\n",
      "Iteration 410, loss = 0.00034123\n",
      "Iteration 411, loss = 0.00034000\n",
      "Iteration 412, loss = 0.00033875\n",
      "Iteration 413, loss = 0.00033757\n",
      "Iteration 414, loss = 0.00033637\n",
      "Iteration 415, loss = 0.00033518\n",
      "Iteration 416, loss = 0.00033396\n",
      "Iteration 417, loss = 0.00033280\n",
      "Iteration 418, loss = 0.00033164\n",
      "Iteration 419, loss = 0.00033048\n",
      "Iteration 420, loss = 0.00032930\n",
      "Iteration 421, loss = 0.00032816\n",
      "Iteration 422, loss = 0.00032703\n",
      "Iteration 423, loss = 0.00032587\n",
      "Iteration 424, loss = 0.00032476\n",
      "Iteration 425, loss = 0.00032365\n",
      "Iteration 426, loss = 0.00032250\n",
      "Iteration 427, loss = 0.00032139\n",
      "Iteration 428, loss = 0.00032029\n",
      "Iteration 429, loss = 0.00031918\n",
      "Iteration 430, loss = 0.00031805\n",
      "Iteration 431, loss = 0.00031699\n",
      "Iteration 432, loss = 0.00031590\n",
      "Iteration 433, loss = 0.00031482\n",
      "Iteration 434, loss = 0.00031375\n",
      "Iteration 435, loss = 0.00031270\n",
      "Iteration 436, loss = 0.00031166\n",
      "Iteration 437, loss = 0.00031062\n",
      "Iteration 438, loss = 0.00030960\n",
      "Iteration 439, loss = 0.00030854\n",
      "Iteration 440, loss = 0.00030749\n",
      "Iteration 441, loss = 0.00030644\n",
      "Iteration 442, loss = 0.00030546\n",
      "Iteration 443, loss = 0.00030444\n",
      "Iteration 444, loss = 0.00030341\n",
      "Iteration 445, loss = 0.00030239\n",
      "Iteration 446, loss = 0.00030141\n",
      "Iteration 447, loss = 0.00030041\n",
      "Iteration 448, loss = 0.00029943\n",
      "Iteration 449, loss = 0.00029847\n",
      "Iteration 450, loss = 0.00029748\n",
      "Iteration 451, loss = 0.00029654\n",
      "Iteration 452, loss = 0.00029557\n",
      "Iteration 453, loss = 0.00029460\n",
      "Iteration 454, loss = 0.00029364\n",
      "Iteration 455, loss = 0.00029269\n",
      "Iteration 456, loss = 0.00029177\n",
      "Iteration 457, loss = 0.00029084\n",
      "Iteration 458, loss = 0.00028986\n",
      "Training loss did not improve more than tol=0.000001 for 10 consecutive epochs. Stopping.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         3\n",
      "           1       1.00      1.00      1.00         2\n",
      "           2       1.00      1.00      1.00         3\n",
      "           3       1.00      0.75      0.86         4\n",
      "           4       1.00      1.00      1.00         2\n",
      "           5       1.00      1.00      1.00         4\n",
      "           6       0.60      1.00      0.75         3\n",
      "           7       1.00      1.00      1.00         5\n",
      "           8       1.00      1.00      1.00         2\n",
      "           9       1.00      1.00      1.00         2\n",
      "          10       1.00      1.00      1.00         4\n",
      "          11       0.60      1.00      0.75         3\n",
      "          12       1.00      1.00      1.00         1\n",
      "          13       1.00      1.00      1.00         2\n",
      "          14       0.75      1.00      0.86         3\n",
      "          15       1.00      1.00      1.00         3\n",
      "          16       1.00      0.60      0.75         5\n",
      "          17       1.00      1.00      1.00         5\n",
      "          18       1.00      1.00      1.00         1\n",
      "          19       1.00      1.00      1.00         4\n",
      "          20       1.00      0.67      0.80         3\n",
      "          21       1.00      1.00      1.00         3\n",
      "          22       1.00      1.00      1.00         4\n",
      "          23       0.80      0.80      0.80         5\n",
      "          24       0.00      0.00      0.00         0\n",
      "          25       1.00      1.00      1.00         2\n",
      "          26       1.00      1.00      1.00         3\n",
      "          27       1.00      1.00      1.00         2\n",
      "          28       1.00      1.00      1.00         5\n",
      "          29       1.00      1.00      1.00         3\n",
      "          30       1.00      1.00      1.00         1\n",
      "          31       1.00      1.00      1.00         2\n",
      "          32       1.00      0.67      0.80         3\n",
      "          33       1.00      1.00      1.00         2\n",
      "          34       1.00      1.00      1.00         3\n",
      "          35       0.50      0.50      0.50         2\n",
      "          36       1.00      1.00      1.00         5\n",
      "          37       1.00      1.00      1.00         3\n",
      "          38       0.80      1.00      0.89         4\n",
      "          39       1.00      0.50      0.67         4\n",
      "\n",
      "    accuracy                           0.93       120\n",
      "   macro avg       0.93      0.91      0.91       120\n",
      "weighted avg       0.95      0.93      0.93       120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "hidden_layers = (30,)\n",
    "activation = 'relu'\n",
    "opts = dict(hidden_layer_sizes = hidden_layers,verbose = True,activation = activation,tol = 1e-6,max_iter = int(1e6))\n",
    "solver = 'adam'\n",
    "clf_MLP = MLPClassifier(solver = solver, **opts)\n",
    "clf_MLP.fit(X_train_,y_train)\n",
    "predictions = clf_MLP.predict(X_test_)\n",
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- hidden_layers = (30,) , solver = 'lbfgs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         3\n",
      "           1       1.00      1.00      1.00         2\n",
      "           2       1.00      1.00      1.00         3\n",
      "           3       1.00      1.00      1.00         4\n",
      "           4       1.00      1.00      1.00         2\n",
      "           5       0.80      1.00      0.89         4\n",
      "           6       1.00      1.00      1.00         3\n",
      "           7       1.00      0.80      0.89         5\n",
      "           8       1.00      1.00      1.00         2\n",
      "           9       0.67      1.00      0.80         2\n",
      "          10       1.00      1.00      1.00         4\n",
      "          11       0.60      1.00      0.75         3\n",
      "          12       0.00      0.00      0.00         1\n",
      "          13       1.00      1.00      1.00         2\n",
      "          14       1.00      1.00      1.00         3\n",
      "          15       1.00      1.00      1.00         3\n",
      "          16       1.00      0.60      0.75         5\n",
      "          17       1.00      1.00      1.00         5\n",
      "          18       1.00      1.00      1.00         1\n",
      "          19       1.00      1.00      1.00         4\n",
      "          20       1.00      1.00      1.00         3\n",
      "          21       1.00      1.00      1.00         3\n",
      "          22       1.00      1.00      1.00         4\n",
      "          23       1.00      1.00      1.00         5\n",
      "          25       0.50      1.00      0.67         2\n",
      "          26       1.00      1.00      1.00         3\n",
      "          27       1.00      1.00      1.00         2\n",
      "          28       1.00      1.00      1.00         5\n",
      "          29       1.00      1.00      1.00         3\n",
      "          30       1.00      1.00      1.00         1\n",
      "          31       1.00      1.00      1.00         2\n",
      "          32       1.00      1.00      1.00         3\n",
      "          33       1.00      1.00      1.00         2\n",
      "          34       1.00      1.00      1.00         3\n",
      "          35       1.00      1.00      1.00         2\n",
      "          36       1.00      1.00      1.00         5\n",
      "          37       1.00      1.00      1.00         3\n",
      "          38       1.00      1.00      1.00         4\n",
      "          39       1.00      0.50      0.67         4\n",
      "\n",
      "    accuracy                           0.95       120\n",
      "   macro avg       0.94      0.95      0.93       120\n",
      "weighted avg       0.96      0.95      0.95       120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "hidden_layers = (30,)\n",
    "activation = 'logistic'\n",
    "opts = dict(hidden_layer_sizes = hidden_layers,verbose = True,activation = activation,tol = 1e-6,max_iter = int(1e6))\n",
    "solver = 'lbfgs'\n",
    "clf_MLP = MLPClassifier(solver = solver, **opts)\n",
    "clf_MLP.fit(X_train_,y_train)\n",
    "predictions = clf_MLP.predict(X_test_)\n",
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- hidden_layers = (512,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 3.59783294\n",
      "Iteration 2, loss = 2.01007045\n",
      "Iteration 3, loss = 1.38300530\n",
      "Iteration 4, loss = 0.99612100\n",
      "Iteration 5, loss = 0.74502940\n",
      "Iteration 6, loss = 0.56160761\n",
      "Iteration 7, loss = 0.42544535\n",
      "Iteration 8, loss = 0.32880152\n",
      "Iteration 9, loss = 0.25910717\n",
      "Iteration 10, loss = 0.20695003\n",
      "Iteration 11, loss = 0.16794196\n",
      "Iteration 12, loss = 0.13874900\n",
      "Iteration 13, loss = 0.11522481\n",
      "Iteration 14, loss = 0.09745428\n",
      "Iteration 15, loss = 0.08377847\n",
      "Iteration 16, loss = 0.07266671\n",
      "Iteration 17, loss = 0.06362277\n",
      "Iteration 18, loss = 0.05643220\n",
      "Iteration 19, loss = 0.05051096\n",
      "Iteration 20, loss = 0.04554897\n",
      "Iteration 21, loss = 0.04149164\n",
      "Iteration 22, loss = 0.03807717\n",
      "Iteration 23, loss = 0.03516921\n",
      "Iteration 24, loss = 0.03267920\n",
      "Iteration 25, loss = 0.03054056\n",
      "Iteration 26, loss = 0.02866553\n",
      "Iteration 27, loss = 0.02700638\n",
      "Iteration 28, loss = 0.02549777\n",
      "Iteration 29, loss = 0.02422493\n",
      "Iteration 30, loss = 0.02308095\n",
      "Iteration 31, loss = 0.02202897\n",
      "Iteration 32, loss = 0.02108223\n",
      "Iteration 33, loss = 0.02024540\n",
      "Iteration 34, loss = 0.01945122\n",
      "Iteration 35, loss = 0.01873919\n",
      "Iteration 36, loss = 0.01806261\n",
      "Iteration 37, loss = 0.01745442\n",
      "Iteration 38, loss = 0.01687535\n",
      "Iteration 39, loss = 0.01634000\n",
      "Iteration 40, loss = 0.01583487\n",
      "Iteration 41, loss = 0.01535859\n",
      "Iteration 42, loss = 0.01491194\n",
      "Iteration 43, loss = 0.01448697\n",
      "Iteration 44, loss = 0.01407504\n",
      "Iteration 45, loss = 0.01369478\n",
      "Iteration 46, loss = 0.01333525\n",
      "Iteration 47, loss = 0.01298854\n",
      "Iteration 48, loss = 0.01266194\n",
      "Iteration 49, loss = 0.01235192\n",
      "Iteration 50, loss = 0.01205204\n",
      "Iteration 51, loss = 0.01176877\n",
      "Iteration 52, loss = 0.01149716\n",
      "Iteration 53, loss = 0.01123024\n",
      "Iteration 54, loss = 0.01098394\n",
      "Iteration 55, loss = 0.01073744\n",
      "Iteration 56, loss = 0.01050496\n",
      "Iteration 57, loss = 0.01028026\n",
      "Iteration 58, loss = 0.01006253\n",
      "Iteration 59, loss = 0.00985519\n",
      "Iteration 60, loss = 0.00964849\n",
      "Iteration 61, loss = 0.00945094\n",
      "Iteration 62, loss = 0.00926446\n",
      "Iteration 63, loss = 0.00907785\n",
      "Iteration 64, loss = 0.00890560\n",
      "Iteration 65, loss = 0.00873406\n",
      "Iteration 66, loss = 0.00856679\n",
      "Iteration 67, loss = 0.00840978\n",
      "Iteration 68, loss = 0.00825283\n",
      "Iteration 69, loss = 0.00810332\n",
      "Iteration 70, loss = 0.00795702\n",
      "Iteration 71, loss = 0.00781787\n",
      "Iteration 72, loss = 0.00768022\n",
      "Iteration 73, loss = 0.00754682\n",
      "Iteration 74, loss = 0.00741702\n",
      "Iteration 75, loss = 0.00729041\n",
      "Iteration 76, loss = 0.00716344\n",
      "Iteration 77, loss = 0.00704471\n",
      "Iteration 78, loss = 0.00692849\n",
      "Iteration 79, loss = 0.00681700\n",
      "Iteration 80, loss = 0.00670525\n",
      "Iteration 81, loss = 0.00659838\n",
      "Iteration 82, loss = 0.00649404\n",
      "Iteration 83, loss = 0.00639146\n",
      "Iteration 84, loss = 0.00629384\n",
      "Iteration 85, loss = 0.00619711\n",
      "Iteration 86, loss = 0.00610282\n",
      "Iteration 87, loss = 0.00601262\n",
      "Iteration 88, loss = 0.00592326\n",
      "Iteration 89, loss = 0.00583757\n",
      "Iteration 90, loss = 0.00575475\n",
      "Iteration 91, loss = 0.00567134\n",
      "Iteration 92, loss = 0.00559100\n",
      "Iteration 93, loss = 0.00551335\n",
      "Iteration 94, loss = 0.00543738\n",
      "Iteration 95, loss = 0.00536171\n",
      "Iteration 96, loss = 0.00528731\n",
      "Iteration 97, loss = 0.00521645\n",
      "Iteration 98, loss = 0.00514666\n",
      "Iteration 99, loss = 0.00507755\n",
      "Iteration 100, loss = 0.00501047\n",
      "Iteration 101, loss = 0.00494507\n",
      "Iteration 102, loss = 0.00487923\n",
      "Iteration 103, loss = 0.00481865\n",
      "Iteration 104, loss = 0.00475714\n",
      "Iteration 105, loss = 0.00469790\n",
      "Iteration 106, loss = 0.00463923\n",
      "Iteration 107, loss = 0.00458148\n",
      "Iteration 108, loss = 0.00452562\n",
      "Iteration 109, loss = 0.00447024\n",
      "Iteration 110, loss = 0.00441595\n",
      "Iteration 111, loss = 0.00436337\n",
      "Iteration 112, loss = 0.00431100\n",
      "Iteration 113, loss = 0.00425979\n",
      "Iteration 114, loss = 0.00421042\n",
      "Iteration 115, loss = 0.00416156\n",
      "Iteration 116, loss = 0.00411323\n",
      "Iteration 117, loss = 0.00406622\n",
      "Iteration 118, loss = 0.00402034\n",
      "Iteration 119, loss = 0.00397573\n",
      "Iteration 120, loss = 0.00393110\n",
      "Iteration 121, loss = 0.00388834\n",
      "Iteration 122, loss = 0.00384594\n",
      "Iteration 123, loss = 0.00380380\n",
      "Iteration 124, loss = 0.00376291\n",
      "Iteration 125, loss = 0.00372215\n",
      "Iteration 126, loss = 0.00368267\n",
      "Iteration 127, loss = 0.00364370\n",
      "Iteration 128, loss = 0.00360563\n",
      "Iteration 129, loss = 0.00356786\n",
      "Iteration 130, loss = 0.00353070\n",
      "Iteration 131, loss = 0.00349446\n",
      "Iteration 132, loss = 0.00345876\n",
      "Iteration 133, loss = 0.00342360\n",
      "Iteration 134, loss = 0.00338946\n",
      "Iteration 135, loss = 0.00335516\n",
      "Iteration 136, loss = 0.00332220\n",
      "Iteration 137, loss = 0.00328926\n",
      "Iteration 138, loss = 0.00325749\n",
      "Iteration 139, loss = 0.00322628\n",
      "Iteration 140, loss = 0.00319566\n",
      "Iteration 141, loss = 0.00316548\n",
      "Iteration 142, loss = 0.00313552\n",
      "Iteration 143, loss = 0.00310546\n",
      "Iteration 144, loss = 0.00307665\n",
      "Iteration 145, loss = 0.00304835\n",
      "Iteration 146, loss = 0.00302067\n",
      "Iteration 147, loss = 0.00299357\n",
      "Iteration 148, loss = 0.00296661\n",
      "Iteration 149, loss = 0.00294001\n",
      "Iteration 150, loss = 0.00291436\n",
      "Iteration 151, loss = 0.00288840\n",
      "Iteration 152, loss = 0.00286307\n",
      "Iteration 153, loss = 0.00283864\n",
      "Iteration 154, loss = 0.00281379\n",
      "Iteration 155, loss = 0.00278956\n",
      "Iteration 156, loss = 0.00276577\n",
      "Iteration 157, loss = 0.00274230\n",
      "Iteration 158, loss = 0.00271893\n",
      "Iteration 159, loss = 0.00269629\n",
      "Iteration 160, loss = 0.00267349\n",
      "Iteration 161, loss = 0.00265149\n",
      "Iteration 162, loss = 0.00262965\n",
      "Iteration 163, loss = 0.00260796\n",
      "Iteration 164, loss = 0.00258727\n",
      "Iteration 165, loss = 0.00256591\n",
      "Iteration 166, loss = 0.00254533\n",
      "Iteration 167, loss = 0.00252470\n",
      "Iteration 168, loss = 0.00250476\n",
      "Iteration 169, loss = 0.00248472\n",
      "Iteration 170, loss = 0.00246520\n",
      "Iteration 171, loss = 0.00244584\n",
      "Iteration 172, loss = 0.00242714\n",
      "Iteration 173, loss = 0.00240865\n",
      "Iteration 174, loss = 0.00238983\n",
      "Iteration 175, loss = 0.00237162\n",
      "Iteration 176, loss = 0.00235381\n",
      "Iteration 177, loss = 0.00233578\n",
      "Iteration 178, loss = 0.00231818\n",
      "Iteration 179, loss = 0.00230078\n",
      "Iteration 180, loss = 0.00228379\n",
      "Iteration 181, loss = 0.00226706\n",
      "Iteration 182, loss = 0.00225024\n",
      "Iteration 183, loss = 0.00223393\n",
      "Iteration 184, loss = 0.00221775\n",
      "Iteration 185, loss = 0.00220172\n",
      "Iteration 186, loss = 0.00218600\n",
      "Iteration 187, loss = 0.00217045\n",
      "Iteration 188, loss = 0.00215507\n",
      "Iteration 189, loss = 0.00214014\n",
      "Iteration 190, loss = 0.00212521\n",
      "Iteration 191, loss = 0.00211053\n",
      "Iteration 192, loss = 0.00209596\n",
      "Iteration 193, loss = 0.00208167\n",
      "Iteration 194, loss = 0.00206744\n",
      "Iteration 195, loss = 0.00205346\n",
      "Iteration 196, loss = 0.00203967\n",
      "Iteration 197, loss = 0.00202590\n",
      "Iteration 198, loss = 0.00201249\n",
      "Iteration 199, loss = 0.00199906\n",
      "Iteration 200, loss = 0.00198591\n",
      "Iteration 201, loss = 0.00197265\n",
      "Iteration 202, loss = 0.00195983\n",
      "Iteration 203, loss = 0.00194687\n",
      "Iteration 204, loss = 0.00193424\n",
      "Iteration 205, loss = 0.00192204\n",
      "Iteration 206, loss = 0.00190945\n",
      "Iteration 207, loss = 0.00189732\n",
      "Iteration 208, loss = 0.00188517\n",
      "Iteration 209, loss = 0.00187306\n",
      "Iteration 210, loss = 0.00186111\n",
      "Iteration 211, loss = 0.00184953\n",
      "Iteration 212, loss = 0.00183794\n",
      "Iteration 213, loss = 0.00182645\n",
      "Iteration 214, loss = 0.00181506\n",
      "Iteration 215, loss = 0.00180389\n",
      "Iteration 216, loss = 0.00179300\n",
      "Iteration 217, loss = 0.00178189\n",
      "Iteration 218, loss = 0.00177118\n",
      "Iteration 219, loss = 0.00176031\n",
      "Iteration 220, loss = 0.00174987\n",
      "Iteration 221, loss = 0.00173930\n",
      "Iteration 222, loss = 0.00172884\n",
      "Iteration 223, loss = 0.00171873\n",
      "Iteration 224, loss = 0.00170860\n",
      "Iteration 225, loss = 0.00169856\n",
      "Iteration 226, loss = 0.00168884\n",
      "Iteration 227, loss = 0.00167879\n",
      "Iteration 228, loss = 0.00166921\n",
      "Iteration 229, loss = 0.00165973\n",
      "Iteration 230, loss = 0.00165015\n",
      "Iteration 231, loss = 0.00164068\n",
      "Iteration 232, loss = 0.00163160\n",
      "Iteration 233, loss = 0.00162234\n",
      "Iteration 234, loss = 0.00161313\n",
      "Iteration 235, loss = 0.00160417\n",
      "Iteration 236, loss = 0.00159543\n",
      "Iteration 237, loss = 0.00158660\n",
      "Iteration 238, loss = 0.00157789\n",
      "Iteration 239, loss = 0.00156937\n",
      "Iteration 240, loss = 0.00156086\n",
      "Iteration 241, loss = 0.00155251\n",
      "Iteration 242, loss = 0.00154419\n",
      "Iteration 243, loss = 0.00153596\n",
      "Iteration 244, loss = 0.00152779\n",
      "Iteration 245, loss = 0.00151985\n",
      "Iteration 246, loss = 0.00151197\n",
      "Iteration 247, loss = 0.00150411\n",
      "Iteration 248, loss = 0.00149626\n",
      "Iteration 249, loss = 0.00148850\n",
      "Iteration 250, loss = 0.00148085\n",
      "Iteration 251, loss = 0.00147331\n",
      "Iteration 252, loss = 0.00146564\n",
      "Iteration 253, loss = 0.00145815\n",
      "Iteration 254, loss = 0.00145094\n",
      "Iteration 255, loss = 0.00144340\n",
      "Iteration 256, loss = 0.00143599\n",
      "Iteration 257, loss = 0.00142887\n",
      "Iteration 258, loss = 0.00142168\n",
      "Iteration 259, loss = 0.00141460\n",
      "Iteration 260, loss = 0.00140764\n",
      "Iteration 261, loss = 0.00140059\n",
      "Iteration 262, loss = 0.00139372\n",
      "Iteration 263, loss = 0.00138680\n",
      "Iteration 264, loss = 0.00138012\n",
      "Iteration 265, loss = 0.00137339\n",
      "Iteration 266, loss = 0.00136667\n",
      "Iteration 267, loss = 0.00136017\n",
      "Iteration 268, loss = 0.00135369\n",
      "Iteration 269, loss = 0.00134730\n",
      "Iteration 270, loss = 0.00134089\n",
      "Iteration 271, loss = 0.00133465\n",
      "Iteration 272, loss = 0.00132835\n",
      "Iteration 273, loss = 0.00132219\n",
      "Iteration 274, loss = 0.00131609\n",
      "Iteration 275, loss = 0.00131001\n",
      "Iteration 276, loss = 0.00130399\n",
      "Iteration 277, loss = 0.00129808\n",
      "Iteration 278, loss = 0.00129211\n",
      "Iteration 279, loss = 0.00128633\n",
      "Iteration 280, loss = 0.00128049\n",
      "Iteration 281, loss = 0.00127474\n",
      "Iteration 282, loss = 0.00126905\n",
      "Iteration 283, loss = 0.00126344\n",
      "Iteration 284, loss = 0.00125788\n",
      "Iteration 285, loss = 0.00125240\n",
      "Iteration 286, loss = 0.00124689\n",
      "Iteration 287, loss = 0.00124149\n",
      "Iteration 288, loss = 0.00123609\n",
      "Iteration 289, loss = 0.00123074\n",
      "Iteration 290, loss = 0.00122547\n",
      "Iteration 291, loss = 0.00122022\n",
      "Iteration 292, loss = 0.00121500\n",
      "Iteration 293, loss = 0.00120979\n",
      "Iteration 294, loss = 0.00120465\n",
      "Iteration 295, loss = 0.00119973\n",
      "Iteration 296, loss = 0.00119464\n",
      "Iteration 297, loss = 0.00118958\n",
      "Iteration 298, loss = 0.00118468\n",
      "Iteration 299, loss = 0.00117971\n",
      "Iteration 300, loss = 0.00117492\n",
      "Iteration 301, loss = 0.00116996\n",
      "Iteration 302, loss = 0.00116517\n",
      "Iteration 303, loss = 0.00116035\n",
      "Iteration 304, loss = 0.00115566\n",
      "Iteration 305, loss = 0.00115091\n",
      "Iteration 306, loss = 0.00114627\n",
      "Iteration 307, loss = 0.00114161\n",
      "Iteration 308, loss = 0.00113704\n",
      "Iteration 309, loss = 0.00113244\n",
      "Iteration 310, loss = 0.00112786\n",
      "Iteration 311, loss = 0.00112346\n",
      "Iteration 312, loss = 0.00111901\n",
      "Iteration 313, loss = 0.00111457\n",
      "Iteration 314, loss = 0.00111019\n",
      "Iteration 315, loss = 0.00110591\n",
      "Iteration 316, loss = 0.00110155\n",
      "Iteration 317, loss = 0.00109725\n",
      "Iteration 318, loss = 0.00109302\n",
      "Iteration 319, loss = 0.00108886\n",
      "Iteration 320, loss = 0.00108468\n",
      "Iteration 321, loss = 0.00108054\n",
      "Iteration 322, loss = 0.00107645\n",
      "Iteration 323, loss = 0.00107234\n",
      "Iteration 324, loss = 0.00106837\n",
      "Iteration 325, loss = 0.00106425\n",
      "Iteration 326, loss = 0.00106037\n",
      "Iteration 327, loss = 0.00105639\n",
      "Iteration 328, loss = 0.00105247\n",
      "Iteration 329, loss = 0.00104858\n",
      "Iteration 330, loss = 0.00104466\n",
      "Iteration 331, loss = 0.00104088\n",
      "Iteration 332, loss = 0.00103700\n",
      "Iteration 333, loss = 0.00103323\n",
      "Iteration 334, loss = 0.00102945\n",
      "Iteration 335, loss = 0.00102577\n",
      "Iteration 336, loss = 0.00102205\n",
      "Iteration 337, loss = 0.00101839\n",
      "Iteration 338, loss = 0.00101471\n",
      "Iteration 339, loss = 0.00101111\n",
      "Iteration 340, loss = 0.00100749\n",
      "Iteration 341, loss = 0.00100395\n",
      "Iteration 342, loss = 0.00100047\n",
      "Iteration 343, loss = 0.00099696\n",
      "Iteration 344, loss = 0.00099342\n",
      "Iteration 345, loss = 0.00098999\n",
      "Iteration 346, loss = 0.00098653\n",
      "Iteration 347, loss = 0.00098312\n",
      "Iteration 348, loss = 0.00097969\n",
      "Iteration 349, loss = 0.00097635\n",
      "Iteration 350, loss = 0.00097299\n",
      "Iteration 351, loss = 0.00096969\n",
      "Iteration 352, loss = 0.00096637\n",
      "Iteration 353, loss = 0.00096307\n",
      "Iteration 354, loss = 0.00095983\n",
      "Iteration 355, loss = 0.00095658\n",
      "Iteration 356, loss = 0.00095340\n",
      "Iteration 357, loss = 0.00095013\n",
      "Iteration 358, loss = 0.00094701\n",
      "Iteration 359, loss = 0.00094382\n",
      "Iteration 360, loss = 0.00094074\n",
      "Iteration 361, loss = 0.00093764\n",
      "Iteration 362, loss = 0.00093461\n",
      "Iteration 363, loss = 0.00093150\n",
      "Iteration 364, loss = 0.00092845\n",
      "Iteration 365, loss = 0.00092548\n",
      "Iteration 366, loss = 0.00092249\n",
      "Iteration 367, loss = 0.00091947\n",
      "Iteration 368, loss = 0.00091657\n",
      "Iteration 369, loss = 0.00091363\n",
      "Iteration 370, loss = 0.00091076\n",
      "Iteration 371, loss = 0.00090782\n",
      "Iteration 372, loss = 0.00090499\n",
      "Iteration 373, loss = 0.00090214\n",
      "Iteration 374, loss = 0.00089934\n",
      "Iteration 375, loss = 0.00089650\n",
      "Iteration 376, loss = 0.00089377\n",
      "Iteration 377, loss = 0.00089101\n",
      "Iteration 378, loss = 0.00088830\n",
      "Iteration 379, loss = 0.00088558\n",
      "Iteration 380, loss = 0.00088286\n",
      "Iteration 381, loss = 0.00088015\n",
      "Iteration 382, loss = 0.00087750\n",
      "Iteration 383, loss = 0.00087482\n",
      "Iteration 384, loss = 0.00087219\n",
      "Iteration 385, loss = 0.00086960\n",
      "Iteration 386, loss = 0.00086700\n",
      "Iteration 387, loss = 0.00086436\n",
      "Iteration 388, loss = 0.00086181\n",
      "Iteration 389, loss = 0.00085927\n",
      "Iteration 390, loss = 0.00085675\n",
      "Iteration 391, loss = 0.00085421\n",
      "Iteration 392, loss = 0.00085176\n",
      "Iteration 393, loss = 0.00084925\n",
      "Iteration 394, loss = 0.00084676\n",
      "Iteration 395, loss = 0.00084433\n",
      "Iteration 396, loss = 0.00084189\n",
      "Iteration 397, loss = 0.00083949\n",
      "Iteration 398, loss = 0.00083709\n",
      "Iteration 399, loss = 0.00083466\n",
      "Iteration 400, loss = 0.00083234\n",
      "Iteration 401, loss = 0.00082996\n",
      "Iteration 402, loss = 0.00082766\n",
      "Iteration 403, loss = 0.00082532\n",
      "Iteration 404, loss = 0.00082297\n",
      "Iteration 405, loss = 0.00082069\n",
      "Iteration 406, loss = 0.00081840\n",
      "Iteration 407, loss = 0.00081610\n",
      "Iteration 408, loss = 0.00081387\n",
      "Iteration 409, loss = 0.00081160\n",
      "Iteration 410, loss = 0.00080932\n",
      "Iteration 411, loss = 0.00080712\n",
      "Iteration 412, loss = 0.00080486\n",
      "Iteration 413, loss = 0.00080269\n",
      "Iteration 414, loss = 0.00080050\n",
      "Iteration 415, loss = 0.00079831\n",
      "Iteration 416, loss = 0.00079614\n",
      "Iteration 417, loss = 0.00079400\n",
      "Iteration 418, loss = 0.00079189\n",
      "Iteration 419, loss = 0.00078979\n",
      "Iteration 420, loss = 0.00078765\n",
      "Iteration 421, loss = 0.00078558\n",
      "Iteration 422, loss = 0.00078351\n",
      "Iteration 423, loss = 0.00078146\n",
      "Iteration 424, loss = 0.00077939\n",
      "Iteration 425, loss = 0.00077738\n",
      "Iteration 426, loss = 0.00077532\n",
      "Iteration 427, loss = 0.00077335\n",
      "Iteration 428, loss = 0.00077134\n",
      "Iteration 429, loss = 0.00076938\n",
      "Iteration 430, loss = 0.00076738\n",
      "Iteration 431, loss = 0.00076542\n",
      "Iteration 432, loss = 0.00076346\n",
      "Iteration 433, loss = 0.00076151\n",
      "Iteration 434, loss = 0.00075959\n",
      "Iteration 435, loss = 0.00075771\n",
      "Iteration 436, loss = 0.00075579\n",
      "Iteration 437, loss = 0.00075388\n",
      "Iteration 438, loss = 0.00075200\n",
      "Iteration 439, loss = 0.00075014\n",
      "Iteration 440, loss = 0.00074826\n",
      "Iteration 441, loss = 0.00074643\n",
      "Iteration 442, loss = 0.00074460\n",
      "Iteration 443, loss = 0.00074273\n",
      "Iteration 444, loss = 0.00074091\n",
      "Iteration 445, loss = 0.00073913\n",
      "Iteration 446, loss = 0.00073732\n",
      "Iteration 447, loss = 0.00073552\n",
      "Iteration 448, loss = 0.00073376\n",
      "Iteration 449, loss = 0.00073196\n",
      "Iteration 450, loss = 0.00073018\n",
      "Iteration 451, loss = 0.00072845\n",
      "Iteration 452, loss = 0.00072669\n",
      "Iteration 453, loss = 0.00072498\n",
      "Iteration 454, loss = 0.00072325\n",
      "Iteration 455, loss = 0.00072154\n",
      "Iteration 456, loss = 0.00071983\n",
      "Iteration 457, loss = 0.00071814\n",
      "Iteration 458, loss = 0.00071646\n",
      "Iteration 459, loss = 0.00071479\n",
      "Iteration 460, loss = 0.00071314\n",
      "Iteration 461, loss = 0.00071148\n",
      "Iteration 462, loss = 0.00070983\n",
      "Iteration 463, loss = 0.00070820\n",
      "Iteration 464, loss = 0.00070658\n",
      "Iteration 465, loss = 0.00070494\n",
      "Iteration 466, loss = 0.00070335\n",
      "Iteration 467, loss = 0.00070176\n",
      "Iteration 468, loss = 0.00070015\n",
      "Iteration 469, loss = 0.00069859\n",
      "Iteration 470, loss = 0.00069699\n",
      "Iteration 471, loss = 0.00069542\n",
      "Iteration 472, loss = 0.00069385\n",
      "Iteration 473, loss = 0.00069231\n",
      "Iteration 474, loss = 0.00069075\n",
      "Iteration 475, loss = 0.00068923\n",
      "Iteration 476, loss = 0.00068773\n",
      "Iteration 477, loss = 0.00068616\n",
      "Iteration 478, loss = 0.00068467\n",
      "Iteration 479, loss = 0.00068313\n",
      "Iteration 480, loss = 0.00068164\n",
      "Iteration 481, loss = 0.00068015\n",
      "Iteration 482, loss = 0.00067863\n",
      "Iteration 483, loss = 0.00067715\n",
      "Iteration 484, loss = 0.00067569\n",
      "Iteration 485, loss = 0.00067420\n",
      "Iteration 486, loss = 0.00067273\n",
      "Iteration 487, loss = 0.00067131\n",
      "Iteration 488, loss = 0.00066986\n",
      "Iteration 489, loss = 0.00066844\n",
      "Iteration 490, loss = 0.00066703\n",
      "Iteration 491, loss = 0.00066563\n",
      "Iteration 492, loss = 0.00066418\n",
      "Iteration 493, loss = 0.00066281\n",
      "Iteration 494, loss = 0.00066142\n",
      "Iteration 495, loss = 0.00066003\n",
      "Iteration 496, loss = 0.00065866\n",
      "Iteration 497, loss = 0.00065729\n",
      "Iteration 498, loss = 0.00065591\n",
      "Iteration 499, loss = 0.00065457\n",
      "Iteration 500, loss = 0.00065322\n",
      "Iteration 501, loss = 0.00065189\n",
      "Iteration 502, loss = 0.00065054\n",
      "Iteration 503, loss = 0.00064921\n",
      "Iteration 504, loss = 0.00064788\n",
      "Iteration 505, loss = 0.00064658\n",
      "Iteration 506, loss = 0.00064528\n",
      "Iteration 507, loss = 0.00064399\n",
      "Iteration 508, loss = 0.00064269\n",
      "Iteration 509, loss = 0.00064141\n",
      "Iteration 510, loss = 0.00064014\n",
      "Iteration 511, loss = 0.00063887\n",
      "Iteration 512, loss = 0.00063760\n",
      "Iteration 513, loss = 0.00063634\n",
      "Iteration 514, loss = 0.00063511\n",
      "Iteration 515, loss = 0.00063386\n",
      "Iteration 516, loss = 0.00063262\n",
      "Iteration 517, loss = 0.00063139\n",
      "Iteration 518, loss = 0.00063013\n",
      "Iteration 519, loss = 0.00062893\n",
      "Iteration 520, loss = 0.00062772\n",
      "Iteration 521, loss = 0.00062649\n",
      "Iteration 522, loss = 0.00062529\n",
      "Iteration 523, loss = 0.00062408\n",
      "Iteration 524, loss = 0.00062289\n",
      "Iteration 525, loss = 0.00062172\n",
      "Iteration 526, loss = 0.00062054\n",
      "Iteration 527, loss = 0.00061936\n",
      "Iteration 528, loss = 0.00061819\n",
      "Iteration 529, loss = 0.00061702\n",
      "Iteration 530, loss = 0.00061587\n",
      "Iteration 531, loss = 0.00061471\n",
      "Iteration 532, loss = 0.00061357\n",
      "Iteration 533, loss = 0.00061243\n",
      "Iteration 534, loss = 0.00061128\n",
      "Iteration 535, loss = 0.00061016\n",
      "Iteration 536, loss = 0.00060902\n",
      "Iteration 537, loss = 0.00060788\n",
      "Iteration 538, loss = 0.00060677\n",
      "Iteration 539, loss = 0.00060568\n",
      "Iteration 540, loss = 0.00060455\n",
      "Iteration 541, loss = 0.00060346\n",
      "Iteration 542, loss = 0.00060236\n",
      "Iteration 543, loss = 0.00060127\n",
      "Iteration 544, loss = 0.00060018\n",
      "Iteration 545, loss = 0.00059910\n",
      "Iteration 546, loss = 0.00059805\n",
      "Iteration 547, loss = 0.00059696\n",
      "Iteration 548, loss = 0.00059591\n",
      "Iteration 549, loss = 0.00059484\n",
      "Iteration 550, loss = 0.00059376\n",
      "Iteration 551, loss = 0.00059271\n",
      "Iteration 552, loss = 0.00059167\n",
      "Iteration 553, loss = 0.00059062\n",
      "Iteration 554, loss = 0.00058958\n",
      "Iteration 555, loss = 0.00058856\n",
      "Iteration 556, loss = 0.00058751\n",
      "Iteration 557, loss = 0.00058649\n",
      "Iteration 558, loss = 0.00058547\n",
      "Iteration 559, loss = 0.00058448\n",
      "Iteration 560, loss = 0.00058346\n",
      "Iteration 561, loss = 0.00058245\n",
      "Iteration 562, loss = 0.00058147\n",
      "Iteration 563, loss = 0.00058048\n",
      "Iteration 564, loss = 0.00057949\n",
      "Iteration 565, loss = 0.00057850\n",
      "Iteration 566, loss = 0.00057753\n",
      "Iteration 567, loss = 0.00057655\n",
      "Iteration 568, loss = 0.00057557\n",
      "Iteration 569, loss = 0.00057460\n",
      "Iteration 570, loss = 0.00057365\n",
      "Iteration 571, loss = 0.00057268\n",
      "Iteration 572, loss = 0.00057172\n",
      "Training loss did not improve more than tol=0.000001 for 10 consecutive epochs. Stopping.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         3\n",
      "           1       1.00      1.00      1.00         2\n",
      "           2       1.00      1.00      1.00         3\n",
      "           3       1.00      1.00      1.00         4\n",
      "           4       1.00      1.00      1.00         2\n",
      "           5       1.00      1.00      1.00         4\n",
      "           6       1.00      1.00      1.00         3\n",
      "           7       1.00      0.80      0.89         5\n",
      "           8       1.00      1.00      1.00         2\n",
      "           9       0.67      1.00      0.80         2\n",
      "          10       1.00      1.00      1.00         4\n",
      "          11       1.00      1.00      1.00         3\n",
      "          12       1.00      1.00      1.00         1\n",
      "          13       1.00      1.00      1.00         2\n",
      "          14       1.00      1.00      1.00         3\n",
      "          15       1.00      1.00      1.00         3\n",
      "          16       1.00      0.60      0.75         5\n",
      "          17       1.00      1.00      1.00         5\n",
      "          18       1.00      1.00      1.00         1\n",
      "          19       1.00      1.00      1.00         4\n",
      "          20       1.00      1.00      1.00         3\n",
      "          21       1.00      1.00      1.00         3\n",
      "          22       1.00      1.00      1.00         4\n",
      "          23       1.00      1.00      1.00         5\n",
      "          24       0.00      0.00      0.00         0\n",
      "          25       0.67      1.00      0.80         2\n",
      "          26       1.00      1.00      1.00         3\n",
      "          27       1.00      1.00      1.00         2\n",
      "          28       1.00      1.00      1.00         5\n",
      "          29       1.00      1.00      1.00         3\n",
      "          30       1.00      1.00      1.00         1\n",
      "          31       1.00      1.00      1.00         2\n",
      "          32       1.00      1.00      1.00         3\n",
      "          33       1.00      1.00      1.00         2\n",
      "          34       0.75      1.00      0.86         3\n",
      "          35       1.00      1.00      1.00         2\n",
      "          36       1.00      1.00      1.00         5\n",
      "          37       1.00      1.00      1.00         3\n",
      "          38       1.00      1.00      1.00         4\n",
      "          39       1.00      0.75      0.86         4\n",
      "\n",
      "    accuracy                           0.97       120\n",
      "   macro avg       0.95      0.95      0.95       120\n",
      "weighted avg       0.98      0.97      0.97       120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "hidden_layers = (512,)\n",
    "activation = 'logistic'\n",
    "opts = dict(hidden_layer_sizes = hidden_layers,verbose = True,activation = activation,tol = 1e-6,max_iter = int(1e6))\n",
    "solver = 'adam'\n",
    "clf_MLP = MLPClassifier(solver = solver, **opts)\n",
    "clf_MLP.fit(X_train_,y_train)\n",
    "predictions = clf_MLP.predict(X_test_)\n",
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "訓練資料的classification report "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 3.58026335\n",
      "Iteration 2, loss = 1.98147730\n",
      "Iteration 3, loss = 1.37545512\n",
      "Iteration 4, loss = 0.98457755\n",
      "Iteration 5, loss = 0.72261990\n",
      "Iteration 6, loss = 0.54409384\n",
      "Iteration 7, loss = 0.41794984\n",
      "Iteration 8, loss = 0.32115604\n",
      "Iteration 9, loss = 0.25327760\n",
      "Iteration 10, loss = 0.20235390\n",
      "Iteration 11, loss = 0.16494933\n",
      "Iteration 12, loss = 0.13660005\n",
      "Iteration 13, loss = 0.11390252\n",
      "Iteration 14, loss = 0.09672891\n",
      "Iteration 15, loss = 0.08284534\n",
      "Iteration 16, loss = 0.07165431\n",
      "Iteration 17, loss = 0.06236537\n",
      "Iteration 18, loss = 0.05529642\n",
      "Iteration 19, loss = 0.04937488\n",
      "Iteration 20, loss = 0.04464491\n",
      "Iteration 21, loss = 0.04068999\n",
      "Iteration 22, loss = 0.03734484\n",
      "Iteration 23, loss = 0.03448664\n",
      "Iteration 24, loss = 0.03201388\n",
      "Iteration 25, loss = 0.02991032\n",
      "Iteration 26, loss = 0.02814038\n",
      "Iteration 27, loss = 0.02651698\n",
      "Iteration 28, loss = 0.02510839\n",
      "Iteration 29, loss = 0.02386637\n",
      "Iteration 30, loss = 0.02272688\n",
      "Iteration 31, loss = 0.02170027\n",
      "Iteration 32, loss = 0.02077873\n",
      "Iteration 33, loss = 0.01994436\n",
      "Iteration 34, loss = 0.01917153\n",
      "Iteration 35, loss = 0.01848796\n",
      "Iteration 36, loss = 0.01783155\n",
      "Iteration 37, loss = 0.01723610\n",
      "Iteration 38, loss = 0.01667981\n",
      "Iteration 39, loss = 0.01616400\n",
      "Iteration 40, loss = 0.01567353\n",
      "Iteration 41, loss = 0.01521279\n",
      "Iteration 42, loss = 0.01478356\n",
      "Iteration 43, loss = 0.01437421\n",
      "Iteration 44, loss = 0.01397956\n",
      "Iteration 45, loss = 0.01360985\n",
      "Iteration 46, loss = 0.01326239\n",
      "Iteration 47, loss = 0.01292812\n",
      "Iteration 48, loss = 0.01260858\n",
      "Iteration 49, loss = 0.01230450\n",
      "Iteration 50, loss = 0.01201064\n",
      "Iteration 51, loss = 0.01172629\n",
      "Iteration 52, loss = 0.01145994\n",
      "Iteration 53, loss = 0.01119880\n",
      "Iteration 54, loss = 0.01095003\n",
      "Iteration 55, loss = 0.01071112\n",
      "Iteration 56, loss = 0.01047627\n",
      "Iteration 57, loss = 0.01025146\n",
      "Iteration 58, loss = 0.01003342\n",
      "Iteration 59, loss = 0.00982417\n",
      "Iteration 60, loss = 0.00962528\n",
      "Iteration 61, loss = 0.00943162\n",
      "Iteration 62, loss = 0.00924336\n",
      "Iteration 63, loss = 0.00906081\n",
      "Iteration 64, loss = 0.00888476\n",
      "Iteration 65, loss = 0.00871670\n",
      "Iteration 66, loss = 0.00855123\n",
      "Iteration 67, loss = 0.00839054\n",
      "Iteration 68, loss = 0.00823784\n",
      "Iteration 69, loss = 0.00808882\n",
      "Iteration 70, loss = 0.00794816\n",
      "Iteration 71, loss = 0.00780830\n",
      "Iteration 72, loss = 0.00767149\n",
      "Iteration 73, loss = 0.00754215\n",
      "Iteration 74, loss = 0.00741253\n",
      "Iteration 75, loss = 0.00728627\n",
      "Iteration 76, loss = 0.00716645\n",
      "Iteration 77, loss = 0.00704870\n",
      "Iteration 78, loss = 0.00693317\n",
      "Iteration 79, loss = 0.00682338\n",
      "Iteration 80, loss = 0.00671450\n",
      "Iteration 81, loss = 0.00660986\n",
      "Iteration 82, loss = 0.00650593\n",
      "Iteration 83, loss = 0.00640590\n",
      "Iteration 84, loss = 0.00630833\n",
      "Iteration 85, loss = 0.00621419\n",
      "Iteration 86, loss = 0.00612009\n",
      "Iteration 87, loss = 0.00603047\n",
      "Iteration 88, loss = 0.00594256\n",
      "Iteration 89, loss = 0.00585718\n",
      "Iteration 90, loss = 0.00577269\n",
      "Iteration 91, loss = 0.00569238\n",
      "Iteration 92, loss = 0.00561146\n",
      "Iteration 93, loss = 0.00553359\n",
      "Iteration 94, loss = 0.00545960\n",
      "Iteration 95, loss = 0.00538518\n",
      "Iteration 96, loss = 0.00531398\n",
      "Iteration 97, loss = 0.00524217\n",
      "Iteration 98, loss = 0.00517368\n",
      "Iteration 99, loss = 0.00510547\n",
      "Iteration 100, loss = 0.00504030\n",
      "Iteration 101, loss = 0.00497535\n",
      "Iteration 102, loss = 0.00491112\n",
      "Iteration 103, loss = 0.00484923\n",
      "Iteration 104, loss = 0.00478792\n",
      "Iteration 105, loss = 0.00472839\n",
      "Iteration 106, loss = 0.00466922\n",
      "Iteration 107, loss = 0.00461112\n",
      "Iteration 108, loss = 0.00455523\n",
      "Iteration 109, loss = 0.00450092\n",
      "Iteration 110, loss = 0.00444730\n",
      "Iteration 111, loss = 0.00439420\n",
      "Iteration 112, loss = 0.00434243\n",
      "Iteration 113, loss = 0.00429125\n",
      "Iteration 114, loss = 0.00424199\n",
      "Iteration 115, loss = 0.00419233\n",
      "Iteration 116, loss = 0.00414484\n",
      "Iteration 117, loss = 0.00409780\n",
      "Iteration 118, loss = 0.00405194\n",
      "Iteration 119, loss = 0.00400643\n",
      "Iteration 120, loss = 0.00396290\n",
      "Iteration 121, loss = 0.00391825\n",
      "Iteration 122, loss = 0.00387576\n",
      "Iteration 123, loss = 0.00383404\n",
      "Iteration 124, loss = 0.00379280\n",
      "Iteration 125, loss = 0.00375289\n",
      "Iteration 126, loss = 0.00371330\n",
      "Iteration 127, loss = 0.00367471\n",
      "Iteration 128, loss = 0.00363650\n",
      "Iteration 129, loss = 0.00359912\n",
      "Iteration 130, loss = 0.00356208\n",
      "Iteration 131, loss = 0.00352601\n",
      "Iteration 132, loss = 0.00349075\n",
      "Iteration 133, loss = 0.00345572\n",
      "Iteration 134, loss = 0.00342117\n",
      "Iteration 135, loss = 0.00338827\n",
      "Iteration 136, loss = 0.00335421\n",
      "Iteration 137, loss = 0.00332216\n",
      "Iteration 138, loss = 0.00328961\n",
      "Iteration 139, loss = 0.00325810\n",
      "Iteration 140, loss = 0.00322670\n",
      "Iteration 141, loss = 0.00319649\n",
      "Iteration 142, loss = 0.00316632\n",
      "Iteration 143, loss = 0.00313618\n",
      "Iteration 144, loss = 0.00310728\n",
      "Iteration 145, loss = 0.00307862\n",
      "Iteration 146, loss = 0.00305052\n",
      "Iteration 147, loss = 0.00302275\n",
      "Iteration 148, loss = 0.00299577\n",
      "Iteration 149, loss = 0.00296874\n",
      "Iteration 150, loss = 0.00294233\n",
      "Iteration 151, loss = 0.00291654\n",
      "Iteration 152, loss = 0.00289053\n",
      "Iteration 153, loss = 0.00286565\n",
      "Iteration 154, loss = 0.00284031\n",
      "Iteration 155, loss = 0.00281598\n",
      "Iteration 156, loss = 0.00279200\n",
      "Iteration 157, loss = 0.00276797\n",
      "Iteration 158, loss = 0.00274482\n",
      "Iteration 159, loss = 0.00272204\n",
      "Iteration 160, loss = 0.00269921\n",
      "Iteration 161, loss = 0.00267734\n",
      "Iteration 162, loss = 0.00265533\n",
      "Iteration 163, loss = 0.00263364\n",
      "Iteration 164, loss = 0.00261232\n",
      "Iteration 165, loss = 0.00259150\n",
      "Iteration 166, loss = 0.00257071\n",
      "Iteration 167, loss = 0.00255061\n",
      "Iteration 168, loss = 0.00253000\n",
      "Iteration 169, loss = 0.00251006\n",
      "Iteration 170, loss = 0.00249048\n",
      "Iteration 171, loss = 0.00247117\n",
      "Iteration 172, loss = 0.00245199\n",
      "Iteration 173, loss = 0.00243304\n",
      "Iteration 174, loss = 0.00241440\n",
      "Iteration 175, loss = 0.00239599\n",
      "Iteration 176, loss = 0.00237780\n",
      "Iteration 177, loss = 0.00235978\n",
      "Iteration 178, loss = 0.00234244\n",
      "Iteration 179, loss = 0.00232515\n",
      "Iteration 180, loss = 0.00230766\n",
      "Iteration 181, loss = 0.00229076\n",
      "Iteration 182, loss = 0.00227437\n",
      "Iteration 183, loss = 0.00225770\n",
      "Iteration 184, loss = 0.00224138\n",
      "Iteration 185, loss = 0.00222542\n",
      "Iteration 186, loss = 0.00220939\n",
      "Iteration 187, loss = 0.00219370\n",
      "Iteration 188, loss = 0.00217814\n",
      "Iteration 189, loss = 0.00216286\n",
      "Iteration 190, loss = 0.00214773\n",
      "Iteration 191, loss = 0.00213268\n",
      "Iteration 192, loss = 0.00211785\n",
      "Iteration 193, loss = 0.00210322\n",
      "Iteration 194, loss = 0.00208883\n",
      "Iteration 195, loss = 0.00207460\n",
      "Iteration 196, loss = 0.00206047\n",
      "Iteration 197, loss = 0.00204665\n",
      "Iteration 198, loss = 0.00203303\n",
      "Iteration 199, loss = 0.00201963\n",
      "Iteration 200, loss = 0.00200615\n",
      "Iteration 201, loss = 0.00199279\n",
      "Iteration 202, loss = 0.00197998\n",
      "Iteration 203, loss = 0.00196688\n",
      "Iteration 204, loss = 0.00195418\n",
      "Iteration 205, loss = 0.00194158\n",
      "Iteration 206, loss = 0.00192906\n",
      "Iteration 207, loss = 0.00191684\n",
      "Iteration 208, loss = 0.00190453\n",
      "Iteration 209, loss = 0.00189267\n",
      "Iteration 210, loss = 0.00188085\n",
      "Iteration 211, loss = 0.00186902\n",
      "Iteration 212, loss = 0.00185739\n",
      "Iteration 213, loss = 0.00184606\n",
      "Iteration 214, loss = 0.00183471\n",
      "Iteration 215, loss = 0.00182335\n",
      "Iteration 216, loss = 0.00181242\n",
      "Iteration 217, loss = 0.00180163\n",
      "Iteration 218, loss = 0.00179079\n",
      "Iteration 219, loss = 0.00178012\n",
      "Iteration 220, loss = 0.00176937\n",
      "Iteration 221, loss = 0.00175900\n",
      "Iteration 222, loss = 0.00174873\n",
      "Iteration 223, loss = 0.00173840\n",
      "Iteration 224, loss = 0.00172823\n",
      "Iteration 225, loss = 0.00171815\n",
      "Iteration 226, loss = 0.00170831\n",
      "Iteration 227, loss = 0.00169843\n",
      "Iteration 228, loss = 0.00168894\n",
      "Iteration 229, loss = 0.00167925\n",
      "Iteration 230, loss = 0.00166970\n",
      "Iteration 231, loss = 0.00166035\n",
      "Iteration 232, loss = 0.00165112\n",
      "Iteration 233, loss = 0.00164190\n",
      "Iteration 234, loss = 0.00163270\n",
      "Iteration 235, loss = 0.00162366\n",
      "Iteration 236, loss = 0.00161474\n",
      "Iteration 237, loss = 0.00160597\n",
      "Iteration 238, loss = 0.00159713\n",
      "Iteration 239, loss = 0.00158843\n",
      "Iteration 240, loss = 0.00157982\n",
      "Iteration 241, loss = 0.00157134\n",
      "Iteration 242, loss = 0.00156275\n",
      "Iteration 243, loss = 0.00155443\n",
      "Iteration 244, loss = 0.00154627\n",
      "Iteration 245, loss = 0.00153801\n",
      "Iteration 246, loss = 0.00152993\n",
      "Iteration 247, loss = 0.00152196\n",
      "Iteration 248, loss = 0.00151384\n",
      "Iteration 249, loss = 0.00150607\n",
      "Iteration 250, loss = 0.00149824\n",
      "Iteration 251, loss = 0.00149054\n",
      "Iteration 252, loss = 0.00148290\n",
      "Iteration 253, loss = 0.00147530\n",
      "Iteration 254, loss = 0.00146776\n",
      "Iteration 255, loss = 0.00146028\n",
      "Iteration 256, loss = 0.00145284\n",
      "Iteration 257, loss = 0.00144560\n",
      "Iteration 258, loss = 0.00143832\n",
      "Iteration 259, loss = 0.00143108\n",
      "Iteration 260, loss = 0.00142402\n",
      "Iteration 261, loss = 0.00141699\n",
      "Iteration 262, loss = 0.00140994\n",
      "Iteration 263, loss = 0.00140316\n",
      "Iteration 264, loss = 0.00139634\n",
      "Iteration 265, loss = 0.00138957\n",
      "Iteration 266, loss = 0.00138294\n",
      "Iteration 267, loss = 0.00137635\n",
      "Iteration 268, loss = 0.00136982\n",
      "Iteration 269, loss = 0.00136332\n",
      "Iteration 270, loss = 0.00135695\n",
      "Iteration 271, loss = 0.00135058\n",
      "Iteration 272, loss = 0.00134429\n",
      "Iteration 273, loss = 0.00133810\n",
      "Iteration 274, loss = 0.00133184\n",
      "Iteration 275, loss = 0.00132570\n",
      "Iteration 276, loss = 0.00131977\n",
      "Iteration 277, loss = 0.00131369\n",
      "Iteration 278, loss = 0.00130765\n",
      "Iteration 279, loss = 0.00130175\n",
      "Iteration 280, loss = 0.00129584\n",
      "Iteration 281, loss = 0.00129004\n",
      "Iteration 282, loss = 0.00128435\n",
      "Iteration 283, loss = 0.00127864\n",
      "Iteration 284, loss = 0.00127290\n",
      "Iteration 285, loss = 0.00126726\n",
      "Iteration 286, loss = 0.00126178\n",
      "Iteration 287, loss = 0.00125618\n",
      "Iteration 288, loss = 0.00125073\n",
      "Iteration 289, loss = 0.00124525\n",
      "Iteration 290, loss = 0.00123984\n",
      "Iteration 291, loss = 0.00123453\n",
      "Iteration 292, loss = 0.00122924\n",
      "Iteration 293, loss = 0.00122405\n",
      "Iteration 294, loss = 0.00121880\n",
      "Iteration 295, loss = 0.00121368\n",
      "Iteration 296, loss = 0.00120863\n",
      "Iteration 297, loss = 0.00120354\n",
      "Iteration 298, loss = 0.00119851\n",
      "Iteration 299, loss = 0.00119350\n",
      "Iteration 300, loss = 0.00118854\n",
      "Iteration 301, loss = 0.00118371\n",
      "Iteration 302, loss = 0.00117881\n",
      "Iteration 303, loss = 0.00117391\n",
      "Iteration 304, loss = 0.00116916\n",
      "Iteration 305, loss = 0.00116443\n",
      "Iteration 306, loss = 0.00115969\n",
      "Iteration 307, loss = 0.00115501\n",
      "Iteration 308, loss = 0.00115034\n",
      "Iteration 309, loss = 0.00114574\n",
      "Iteration 310, loss = 0.00114112\n",
      "Iteration 311, loss = 0.00113667\n",
      "Iteration 312, loss = 0.00113215\n",
      "Iteration 313, loss = 0.00112766\n",
      "Iteration 314, loss = 0.00112326\n",
      "Iteration 315, loss = 0.00111886\n",
      "Iteration 316, loss = 0.00111457\n",
      "Iteration 317, loss = 0.00111021\n",
      "Iteration 318, loss = 0.00110589\n",
      "Iteration 319, loss = 0.00110161\n",
      "Iteration 320, loss = 0.00109744\n",
      "Iteration 321, loss = 0.00109319\n",
      "Iteration 322, loss = 0.00108905\n",
      "Iteration 323, loss = 0.00108492\n",
      "Iteration 324, loss = 0.00108077\n",
      "Iteration 325, loss = 0.00107675\n",
      "Iteration 326, loss = 0.00107271\n",
      "Iteration 327, loss = 0.00106866\n",
      "Iteration 328, loss = 0.00106471\n",
      "Iteration 329, loss = 0.00106072\n",
      "Iteration 330, loss = 0.00105679\n",
      "Iteration 331, loss = 0.00105291\n",
      "Iteration 332, loss = 0.00104909\n",
      "Iteration 333, loss = 0.00104522\n",
      "Iteration 334, loss = 0.00104146\n",
      "Iteration 335, loss = 0.00103769\n",
      "Iteration 336, loss = 0.00103395\n",
      "Iteration 337, loss = 0.00103022\n",
      "Iteration 338, loss = 0.00102658\n",
      "Iteration 339, loss = 0.00102294\n",
      "Iteration 340, loss = 0.00101929\n",
      "Iteration 341, loss = 0.00101574\n",
      "Iteration 342, loss = 0.00101214\n",
      "Iteration 343, loss = 0.00100860\n",
      "Iteration 344, loss = 0.00100512\n",
      "Iteration 345, loss = 0.00100161\n",
      "Iteration 346, loss = 0.00099818\n",
      "Iteration 347, loss = 0.00099477\n",
      "Iteration 348, loss = 0.00099131\n",
      "Iteration 349, loss = 0.00098788\n",
      "Iteration 350, loss = 0.00098458\n",
      "Iteration 351, loss = 0.00098119\n",
      "Iteration 352, loss = 0.00097792\n",
      "Iteration 353, loss = 0.00097464\n",
      "Iteration 354, loss = 0.00097138\n",
      "Iteration 355, loss = 0.00096811\n",
      "Iteration 356, loss = 0.00096491\n",
      "Iteration 357, loss = 0.00096165\n",
      "Iteration 358, loss = 0.00095851\n",
      "Iteration 359, loss = 0.00095532\n",
      "Iteration 360, loss = 0.00095214\n",
      "Iteration 361, loss = 0.00094898\n",
      "Iteration 362, loss = 0.00094589\n",
      "Iteration 363, loss = 0.00094277\n",
      "Iteration 364, loss = 0.00093976\n",
      "Iteration 365, loss = 0.00093668\n",
      "Iteration 366, loss = 0.00093367\n",
      "Iteration 367, loss = 0.00093068\n",
      "Iteration 368, loss = 0.00092769\n",
      "Iteration 369, loss = 0.00092469\n",
      "Iteration 370, loss = 0.00092180\n",
      "Iteration 371, loss = 0.00091886\n",
      "Iteration 372, loss = 0.00091598\n",
      "Iteration 373, loss = 0.00091308\n",
      "Iteration 374, loss = 0.00091025\n",
      "Iteration 375, loss = 0.00090742\n",
      "Iteration 376, loss = 0.00090458\n",
      "Iteration 377, loss = 0.00090174\n",
      "Iteration 378, loss = 0.00089897\n",
      "Iteration 379, loss = 0.00089620\n",
      "Iteration 380, loss = 0.00089346\n",
      "Iteration 381, loss = 0.00089072\n",
      "Iteration 382, loss = 0.00088797\n",
      "Iteration 383, loss = 0.00088532\n",
      "Iteration 384, loss = 0.00088260\n",
      "Iteration 385, loss = 0.00087993\n",
      "Iteration 386, loss = 0.00087728\n",
      "Iteration 387, loss = 0.00087467\n",
      "Iteration 388, loss = 0.00087212\n",
      "Iteration 389, loss = 0.00086945\n",
      "Iteration 390, loss = 0.00086691\n",
      "Iteration 391, loss = 0.00086434\n",
      "Iteration 392, loss = 0.00086186\n",
      "Iteration 393, loss = 0.00085931\n",
      "Iteration 394, loss = 0.00085683\n",
      "Iteration 395, loss = 0.00085435\n",
      "Iteration 396, loss = 0.00085186\n",
      "Iteration 397, loss = 0.00084940\n",
      "Iteration 398, loss = 0.00084698\n",
      "Iteration 399, loss = 0.00084454\n",
      "Iteration 400, loss = 0.00084212\n",
      "Iteration 401, loss = 0.00083971\n",
      "Iteration 402, loss = 0.00083732\n",
      "Iteration 403, loss = 0.00083498\n",
      "Iteration 404, loss = 0.00083258\n",
      "Iteration 405, loss = 0.00083025\n",
      "Iteration 406, loss = 0.00082791\n",
      "Iteration 407, loss = 0.00082561\n",
      "Iteration 408, loss = 0.00082329\n",
      "Iteration 409, loss = 0.00082102\n",
      "Iteration 410, loss = 0.00081879\n",
      "Iteration 411, loss = 0.00081653\n",
      "Iteration 412, loss = 0.00081426\n",
      "Iteration 413, loss = 0.00081206\n",
      "Iteration 414, loss = 0.00080986\n",
      "Iteration 415, loss = 0.00080764\n",
      "Iteration 416, loss = 0.00080545\n",
      "Iteration 417, loss = 0.00080328\n",
      "Iteration 418, loss = 0.00080108\n",
      "Iteration 419, loss = 0.00079891\n",
      "Iteration 420, loss = 0.00079676\n",
      "Iteration 421, loss = 0.00079463\n",
      "Iteration 422, loss = 0.00079248\n",
      "Iteration 423, loss = 0.00079042\n",
      "Iteration 424, loss = 0.00078833\n",
      "Iteration 425, loss = 0.00078621\n",
      "Iteration 426, loss = 0.00078418\n",
      "Iteration 427, loss = 0.00078211\n",
      "Iteration 428, loss = 0.00078008\n",
      "Iteration 429, loss = 0.00077808\n",
      "Iteration 430, loss = 0.00077605\n",
      "Iteration 431, loss = 0.00077406\n",
      "Iteration 432, loss = 0.00077208\n",
      "Iteration 433, loss = 0.00077012\n",
      "Iteration 434, loss = 0.00076814\n",
      "Iteration 435, loss = 0.00076619\n",
      "Iteration 436, loss = 0.00076423\n",
      "Iteration 437, loss = 0.00076232\n",
      "Iteration 438, loss = 0.00076038\n",
      "Iteration 439, loss = 0.00075849\n",
      "Iteration 440, loss = 0.00075660\n",
      "Iteration 441, loss = 0.00075471\n",
      "Iteration 442, loss = 0.00075284\n",
      "Iteration 443, loss = 0.00075099\n",
      "Iteration 444, loss = 0.00074911\n",
      "Iteration 445, loss = 0.00074727\n",
      "Iteration 446, loss = 0.00074547\n",
      "Iteration 447, loss = 0.00074363\n",
      "Iteration 448, loss = 0.00074183\n",
      "Iteration 449, loss = 0.00074003\n",
      "Iteration 450, loss = 0.00073823\n",
      "Iteration 451, loss = 0.00073645\n",
      "Iteration 452, loss = 0.00073468\n",
      "Iteration 453, loss = 0.00073294\n",
      "Iteration 454, loss = 0.00073118\n",
      "Iteration 455, loss = 0.00072945\n",
      "Iteration 456, loss = 0.00072770\n",
      "Iteration 457, loss = 0.00072598\n",
      "Iteration 458, loss = 0.00072428\n",
      "Iteration 459, loss = 0.00072254\n",
      "Iteration 460, loss = 0.00072086\n",
      "Iteration 461, loss = 0.00071915\n",
      "Iteration 462, loss = 0.00071746\n",
      "Iteration 463, loss = 0.00071579\n",
      "Iteration 464, loss = 0.00071412\n",
      "Iteration 465, loss = 0.00071246\n",
      "Iteration 466, loss = 0.00071083\n",
      "Iteration 467, loss = 0.00070919\n",
      "Iteration 468, loss = 0.00070756\n",
      "Iteration 469, loss = 0.00070593\n",
      "Iteration 470, loss = 0.00070436\n",
      "Iteration 471, loss = 0.00070275\n",
      "Iteration 472, loss = 0.00070114\n",
      "Iteration 473, loss = 0.00069958\n",
      "Iteration 474, loss = 0.00069802\n",
      "Iteration 475, loss = 0.00069646\n",
      "Iteration 476, loss = 0.00069491\n",
      "Iteration 477, loss = 0.00069334\n",
      "Iteration 478, loss = 0.00069183\n",
      "Iteration 479, loss = 0.00069027\n",
      "Iteration 480, loss = 0.00068876\n",
      "Iteration 481, loss = 0.00068724\n",
      "Iteration 482, loss = 0.00068575\n",
      "Iteration 483, loss = 0.00068424\n",
      "Iteration 484, loss = 0.00068274\n",
      "Iteration 485, loss = 0.00068126\n",
      "Iteration 486, loss = 0.00067977\n",
      "Iteration 487, loss = 0.00067832\n",
      "Iteration 488, loss = 0.00067686\n",
      "Iteration 489, loss = 0.00067539\n",
      "Iteration 490, loss = 0.00067398\n",
      "Iteration 491, loss = 0.00067252\n",
      "Iteration 492, loss = 0.00067114\n",
      "Iteration 493, loss = 0.00066969\n",
      "Iteration 494, loss = 0.00066830\n",
      "Iteration 495, loss = 0.00066690\n",
      "Iteration 496, loss = 0.00066550\n",
      "Iteration 497, loss = 0.00066413\n",
      "Iteration 498, loss = 0.00066274\n",
      "Iteration 499, loss = 0.00066138\n",
      "Iteration 500, loss = 0.00066003\n",
      "Iteration 501, loss = 0.00065865\n",
      "Iteration 502, loss = 0.00065729\n",
      "Iteration 503, loss = 0.00065596\n",
      "Iteration 504, loss = 0.00065462\n",
      "Iteration 505, loss = 0.00065328\n",
      "Iteration 506, loss = 0.00065196\n",
      "Iteration 507, loss = 0.00065063\n",
      "Iteration 508, loss = 0.00064932\n",
      "Iteration 509, loss = 0.00064799\n",
      "Iteration 510, loss = 0.00064669\n",
      "Iteration 511, loss = 0.00064540\n",
      "Iteration 512, loss = 0.00064412\n",
      "Iteration 513, loss = 0.00064284\n",
      "Iteration 514, loss = 0.00064156\n",
      "Iteration 515, loss = 0.00064030\n",
      "Iteration 516, loss = 0.00063904\n",
      "Iteration 517, loss = 0.00063779\n",
      "Iteration 518, loss = 0.00063654\n",
      "Iteration 519, loss = 0.00063533\n",
      "Iteration 520, loss = 0.00063407\n",
      "Iteration 521, loss = 0.00063286\n",
      "Iteration 522, loss = 0.00063164\n",
      "Iteration 523, loss = 0.00063042\n",
      "Iteration 524, loss = 0.00062920\n",
      "Iteration 525, loss = 0.00062800\n",
      "Iteration 526, loss = 0.00062679\n",
      "Iteration 527, loss = 0.00062559\n",
      "Iteration 528, loss = 0.00062441\n",
      "Iteration 529, loss = 0.00062323\n",
      "Iteration 530, loss = 0.00062203\n",
      "Iteration 531, loss = 0.00062085\n",
      "Iteration 532, loss = 0.00061967\n",
      "Iteration 533, loss = 0.00061853\n",
      "Iteration 534, loss = 0.00061734\n",
      "Iteration 535, loss = 0.00061620\n",
      "Iteration 536, loss = 0.00061505\n",
      "Iteration 537, loss = 0.00061392\n",
      "Iteration 538, loss = 0.00061279\n",
      "Iteration 539, loss = 0.00061165\n",
      "Iteration 540, loss = 0.00061052\n",
      "Iteration 541, loss = 0.00060939\n",
      "Iteration 542, loss = 0.00060828\n",
      "Iteration 543, loss = 0.00060717\n",
      "Iteration 544, loss = 0.00060606\n",
      "Iteration 545, loss = 0.00060497\n",
      "Iteration 546, loss = 0.00060387\n",
      "Iteration 547, loss = 0.00060279\n",
      "Iteration 548, loss = 0.00060172\n",
      "Iteration 549, loss = 0.00060062\n",
      "Iteration 550, loss = 0.00059956\n",
      "Iteration 551, loss = 0.00059850\n",
      "Iteration 552, loss = 0.00059742\n",
      "Iteration 553, loss = 0.00059635\n",
      "Iteration 554, loss = 0.00059533\n",
      "Iteration 555, loss = 0.00059426\n",
      "Iteration 556, loss = 0.00059321\n",
      "Iteration 557, loss = 0.00059216\n",
      "Iteration 558, loss = 0.00059112\n",
      "Iteration 559, loss = 0.00059010\n",
      "Iteration 560, loss = 0.00058907\n",
      "Iteration 561, loss = 0.00058805\n",
      "Iteration 562, loss = 0.00058703\n",
      "Iteration 563, loss = 0.00058601\n",
      "Iteration 564, loss = 0.00058500\n",
      "Iteration 565, loss = 0.00058400\n",
      "Iteration 566, loss = 0.00058301\n",
      "Iteration 567, loss = 0.00058200\n",
      "Iteration 568, loss = 0.00058101\n",
      "Iteration 569, loss = 0.00058003\n",
      "Iteration 570, loss = 0.00057904\n",
      "Iteration 571, loss = 0.00057807\n",
      "Iteration 572, loss = 0.00057711\n",
      "Iteration 573, loss = 0.00057614\n",
      "Iteration 574, loss = 0.00057520\n",
      "Iteration 575, loss = 0.00057423\n",
      "Iteration 576, loss = 0.00057330\n",
      "Iteration 577, loss = 0.00057234\n",
      "Iteration 578, loss = 0.00057140\n",
      "Training loss did not improve more than tol=0.000001 for 10 consecutive epochs. Stopping.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         7\n",
      "           1       1.00      1.00      1.00         8\n",
      "           2       1.00      1.00      1.00         7\n",
      "           3       1.00      1.00      1.00         6\n",
      "           4       1.00      1.00      1.00         8\n",
      "           5       1.00      1.00      1.00         6\n",
      "           6       1.00      1.00      1.00         7\n",
      "           7       1.00      1.00      1.00         5\n",
      "           8       1.00      1.00      1.00         8\n",
      "           9       1.00      1.00      1.00         8\n",
      "          10       1.00      1.00      1.00         6\n",
      "          11       1.00      1.00      1.00         7\n",
      "          12       1.00      1.00      1.00         9\n",
      "          13       1.00      1.00      1.00         8\n",
      "          14       1.00      1.00      1.00         7\n",
      "          15       1.00      1.00      1.00         7\n",
      "          16       1.00      1.00      1.00         5\n",
      "          17       1.00      1.00      1.00         5\n",
      "          18       1.00      1.00      1.00         9\n",
      "          19       1.00      1.00      1.00         6\n",
      "          20       1.00      1.00      1.00         7\n",
      "          21       1.00      1.00      1.00         7\n",
      "          22       1.00      1.00      1.00         6\n",
      "          23       1.00      1.00      1.00         5\n",
      "          24       1.00      1.00      1.00        10\n",
      "          25       1.00      1.00      1.00         8\n",
      "          26       1.00      1.00      1.00         7\n",
      "          27       1.00      1.00      1.00         8\n",
      "          28       1.00      1.00      1.00         5\n",
      "          29       1.00      1.00      1.00         7\n",
      "          30       1.00      1.00      1.00         9\n",
      "          31       1.00      1.00      1.00         8\n",
      "          32       1.00      1.00      1.00         7\n",
      "          33       1.00      1.00      1.00         8\n",
      "          34       1.00      1.00      1.00         7\n",
      "          35       1.00      1.00      1.00         8\n",
      "          36       1.00      1.00      1.00         5\n",
      "          37       1.00      1.00      1.00         7\n",
      "          38       1.00      1.00      1.00         6\n",
      "          39       1.00      1.00      1.00         6\n",
      "\n",
      "    accuracy                           1.00       280\n",
      "   macro avg       1.00      1.00      1.00       280\n",
      "weighted avg       1.00      1.00      1.00       280\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "hidden_layers = (512,)\n",
    "activation = 'logistic'\n",
    "opts = dict(hidden_layer_sizes = hidden_layers,verbose = True,activation = activation,tol = 1e-6,max_iter = int(1e6))\n",
    "solver = 'adam'\n",
    "clf_MLP = MLPClassifier(solver = solver, **opts)\n",
    "clf_MLP.fit(X_train_,y_train)\n",
    "pred = clf_MLP.predict(X_train_)\n",
    "print(classification_report(y_train,pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- hidden_layers = (60, 60, 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 3.75694497\n",
      "Iteration 2, loss = 3.72770521\n",
      "Iteration 3, loss = 3.70857108\n",
      "Iteration 4, loss = 3.69493432\n",
      "Iteration 5, loss = 3.68350455\n",
      "Iteration 6, loss = 3.67276668\n",
      "Iteration 7, loss = 3.66606760\n",
      "Iteration 8, loss = 3.65904794\n",
      "Iteration 9, loss = 3.65402024\n",
      "Iteration 10, loss = 3.64854411\n",
      "Iteration 11, loss = 3.64433902\n",
      "Iteration 12, loss = 3.64074859\n",
      "Iteration 13, loss = 3.63639017\n",
      "Iteration 14, loss = 3.63320657\n",
      "Iteration 15, loss = 3.62965533\n",
      "Iteration 16, loss = 3.62604179\n",
      "Iteration 17, loss = 3.62277868\n",
      "Iteration 18, loss = 3.61969745\n",
      "Iteration 19, loss = 3.61567279\n",
      "Iteration 20, loss = 3.61201167\n",
      "Iteration 21, loss = 3.60817236\n",
      "Iteration 22, loss = 3.60452827\n",
      "Iteration 23, loss = 3.60009472\n",
      "Iteration 24, loss = 3.59541978\n",
      "Iteration 25, loss = 3.59079532\n",
      "Iteration 26, loss = 3.58592309\n",
      "Iteration 27, loss = 3.58074838\n",
      "Iteration 28, loss = 3.57550263\n",
      "Iteration 29, loss = 3.56931398\n",
      "Iteration 30, loss = 3.56362338\n",
      "Iteration 31, loss = 3.55693451\n",
      "Iteration 32, loss = 3.55013557\n",
      "Iteration 33, loss = 3.54326185\n",
      "Iteration 34, loss = 3.53582161\n",
      "Iteration 35, loss = 3.52806900\n",
      "Iteration 36, loss = 3.51984027\n",
      "Iteration 37, loss = 3.51125336\n",
      "Iteration 38, loss = 3.50195581\n",
      "Iteration 39, loss = 3.49221792\n",
      "Iteration 40, loss = 3.48275099\n",
      "Iteration 41, loss = 3.47184577\n",
      "Iteration 42, loss = 3.46114132\n",
      "Iteration 43, loss = 3.44952504\n",
      "Iteration 44, loss = 3.43753439\n",
      "Iteration 45, loss = 3.42504741\n",
      "Iteration 46, loss = 3.41226231\n",
      "Iteration 47, loss = 3.39882153\n",
      "Iteration 48, loss = 3.38509920\n",
      "Iteration 49, loss = 3.37046043\n",
      "Iteration 50, loss = 3.35553029\n",
      "Iteration 51, loss = 3.33991330\n",
      "Iteration 52, loss = 3.32401239\n",
      "Iteration 53, loss = 3.30743662\n",
      "Iteration 54, loss = 3.29046467\n",
      "Iteration 55, loss = 3.27290677\n",
      "Iteration 56, loss = 3.25566711\n",
      "Iteration 57, loss = 3.23735883\n",
      "Iteration 58, loss = 3.21892505\n",
      "Iteration 59, loss = 3.19977286\n",
      "Iteration 60, loss = 3.18066858\n",
      "Iteration 61, loss = 3.16130508\n",
      "Iteration 62, loss = 3.14211989\n",
      "Iteration 63, loss = 3.12203718\n",
      "Iteration 64, loss = 3.10182388\n",
      "Iteration 65, loss = 3.08178461\n",
      "Iteration 66, loss = 3.06145095\n",
      "Iteration 67, loss = 3.04130645\n",
      "Iteration 68, loss = 3.02092255\n",
      "Iteration 69, loss = 3.00000725\n",
      "Iteration 70, loss = 2.97940169\n",
      "Iteration 71, loss = 2.95862483\n",
      "Iteration 72, loss = 2.93804176\n",
      "Iteration 73, loss = 2.91681135\n",
      "Iteration 74, loss = 2.89586797\n",
      "Iteration 75, loss = 2.87501408\n",
      "Iteration 76, loss = 2.85395582\n",
      "Iteration 77, loss = 2.83307031\n",
      "Iteration 78, loss = 2.81194625\n",
      "Iteration 79, loss = 2.79077321\n",
      "Iteration 80, loss = 2.76976277\n",
      "Iteration 81, loss = 2.74895338\n",
      "Iteration 82, loss = 2.72791634\n",
      "Iteration 83, loss = 2.70670355\n",
      "Iteration 84, loss = 2.68604583\n",
      "Iteration 85, loss = 2.66475334\n",
      "Iteration 86, loss = 2.64380934\n",
      "Iteration 87, loss = 2.62291249\n",
      "Iteration 88, loss = 2.60204477\n",
      "Iteration 89, loss = 2.58145880\n",
      "Iteration 90, loss = 2.56127640\n",
      "Iteration 91, loss = 2.54028091\n",
      "Iteration 92, loss = 2.51956052\n",
      "Iteration 93, loss = 2.49906029\n",
      "Iteration 94, loss = 2.47822532\n",
      "Iteration 95, loss = 2.45799542\n",
      "Iteration 96, loss = 2.43750591\n",
      "Iteration 97, loss = 2.41734164\n",
      "Iteration 98, loss = 2.39717928\n",
      "Iteration 99, loss = 2.37715933\n",
      "Iteration 100, loss = 2.35720041\n",
      "Iteration 101, loss = 2.33755186\n",
      "Iteration 102, loss = 2.31787464\n",
      "Iteration 103, loss = 2.29842865\n",
      "Iteration 104, loss = 2.27915794\n",
      "Iteration 105, loss = 2.25984671\n",
      "Iteration 106, loss = 2.24056589\n",
      "Iteration 107, loss = 2.22147618\n",
      "Iteration 108, loss = 2.20270903\n",
      "Iteration 109, loss = 2.18365248\n",
      "Iteration 110, loss = 2.16505361\n",
      "Iteration 111, loss = 2.14671569\n",
      "Iteration 112, loss = 2.12808704\n",
      "Iteration 113, loss = 2.10998176\n",
      "Iteration 114, loss = 2.09146812\n",
      "Iteration 115, loss = 2.07328345\n",
      "Iteration 116, loss = 2.05534929\n",
      "Iteration 117, loss = 2.03758887\n",
      "Iteration 118, loss = 2.01974546\n",
      "Iteration 119, loss = 2.00220152\n",
      "Iteration 120, loss = 1.98477922\n",
      "Iteration 121, loss = 1.96750924\n",
      "Iteration 122, loss = 1.95032634\n",
      "Iteration 123, loss = 1.93349732\n",
      "Iteration 124, loss = 1.91641225\n",
      "Iteration 125, loss = 1.89962933\n",
      "Iteration 126, loss = 1.88270475\n",
      "Iteration 127, loss = 1.86613168\n",
      "Iteration 128, loss = 1.84960185\n",
      "Iteration 129, loss = 1.83334654\n",
      "Iteration 130, loss = 1.81680193\n",
      "Iteration 131, loss = 1.80108846\n",
      "Iteration 132, loss = 1.78488313\n",
      "Iteration 133, loss = 1.76912869\n",
      "Iteration 134, loss = 1.75376662\n",
      "Iteration 135, loss = 1.73808946\n",
      "Iteration 136, loss = 1.72261073\n",
      "Iteration 137, loss = 1.70729815\n",
      "Iteration 138, loss = 1.69191606\n",
      "Iteration 139, loss = 1.67734284\n",
      "Iteration 140, loss = 1.66212852\n",
      "Iteration 141, loss = 1.64745947\n",
      "Iteration 142, loss = 1.63260481\n",
      "Iteration 143, loss = 1.61796701\n",
      "Iteration 144, loss = 1.60351288\n",
      "Iteration 145, loss = 1.58912853\n",
      "Iteration 146, loss = 1.57465522\n",
      "Iteration 147, loss = 1.56015478\n",
      "Iteration 148, loss = 1.54588721\n",
      "Iteration 149, loss = 1.53161815\n",
      "Iteration 150, loss = 1.51795563\n",
      "Iteration 151, loss = 1.50407505\n",
      "Iteration 152, loss = 1.49052963\n",
      "Iteration 153, loss = 1.47733487\n",
      "Iteration 154, loss = 1.46419486\n",
      "Iteration 155, loss = 1.45068346\n",
      "Iteration 156, loss = 1.43746270\n",
      "Iteration 157, loss = 1.42466493\n",
      "Iteration 158, loss = 1.41172323\n",
      "Iteration 159, loss = 1.39918422\n",
      "Iteration 160, loss = 1.38617544\n",
      "Iteration 161, loss = 1.37355120\n",
      "Iteration 162, loss = 1.36110645\n",
      "Iteration 163, loss = 1.34883984\n",
      "Iteration 164, loss = 1.33665249\n",
      "Iteration 165, loss = 1.32432938\n",
      "Iteration 166, loss = 1.31236165\n",
      "Iteration 167, loss = 1.30055954\n",
      "Iteration 168, loss = 1.28859870\n",
      "Iteration 169, loss = 1.27704143\n",
      "Iteration 170, loss = 1.26581443\n",
      "Iteration 171, loss = 1.25418059\n",
      "Iteration 172, loss = 1.24298511\n",
      "Iteration 173, loss = 1.23144300\n",
      "Iteration 174, loss = 1.22003915\n",
      "Iteration 175, loss = 1.20902437\n",
      "Iteration 176, loss = 1.19784312\n",
      "Iteration 177, loss = 1.18669263\n",
      "Iteration 178, loss = 1.17587940\n",
      "Iteration 179, loss = 1.16511954\n",
      "Iteration 180, loss = 1.15465706\n",
      "Iteration 181, loss = 1.14398920\n",
      "Iteration 182, loss = 1.13341409\n",
      "Iteration 183, loss = 1.12293407\n",
      "Iteration 184, loss = 1.11254089\n",
      "Iteration 185, loss = 1.10234409\n",
      "Iteration 186, loss = 1.09214776\n",
      "Iteration 187, loss = 1.08222694\n",
      "Iteration 188, loss = 1.07228526\n",
      "Iteration 189, loss = 1.06235584\n",
      "Iteration 190, loss = 1.05265371\n",
      "Iteration 191, loss = 1.04291925\n",
      "Iteration 192, loss = 1.03317973\n",
      "Iteration 193, loss = 1.02361233\n",
      "Iteration 194, loss = 1.01437772\n",
      "Iteration 195, loss = 1.00489222\n",
      "Iteration 196, loss = 0.99582261\n",
      "Iteration 197, loss = 0.98661271\n",
      "Iteration 198, loss = 0.97735814\n",
      "Iteration 199, loss = 0.96837203\n",
      "Iteration 200, loss = 0.95904964\n",
      "Iteration 201, loss = 0.95007690\n",
      "Iteration 202, loss = 0.94093041\n",
      "Iteration 203, loss = 0.93204495\n",
      "Iteration 204, loss = 0.92337851\n",
      "Iteration 205, loss = 0.91469943\n",
      "Iteration 206, loss = 0.90593795\n",
      "Iteration 207, loss = 0.89748097\n",
      "Iteration 208, loss = 0.88882030\n",
      "Iteration 209, loss = 0.88023372\n",
      "Iteration 210, loss = 0.87185779\n",
      "Iteration 211, loss = 0.86377160\n",
      "Iteration 212, loss = 0.85547225\n",
      "Iteration 213, loss = 0.84738690\n",
      "Iteration 214, loss = 0.83925443\n",
      "Iteration 215, loss = 0.83141443\n",
      "Iteration 216, loss = 0.82344045\n",
      "Iteration 217, loss = 0.81560721\n",
      "Iteration 218, loss = 0.80766385\n",
      "Iteration 219, loss = 0.79985655\n",
      "Iteration 220, loss = 0.79214891\n",
      "Iteration 221, loss = 0.78460342\n",
      "Iteration 222, loss = 0.77694843\n",
      "Iteration 223, loss = 0.76963945\n",
      "Iteration 224, loss = 0.76207357\n",
      "Iteration 225, loss = 0.75490222\n",
      "Iteration 226, loss = 0.74756984\n",
      "Iteration 227, loss = 0.74032808\n",
      "Iteration 228, loss = 0.73334187\n",
      "Iteration 229, loss = 0.72631513\n",
      "Iteration 230, loss = 0.71953084\n",
      "Iteration 231, loss = 0.71264172\n",
      "Iteration 232, loss = 0.70585950\n",
      "Iteration 233, loss = 0.69912700\n",
      "Iteration 234, loss = 0.69232515\n",
      "Iteration 235, loss = 0.68555435\n",
      "Iteration 236, loss = 0.67893156\n",
      "Iteration 237, loss = 0.67213076\n",
      "Iteration 238, loss = 0.66562500\n",
      "Iteration 239, loss = 0.65914484\n",
      "Iteration 240, loss = 0.65273068\n",
      "Iteration 241, loss = 0.64614476\n",
      "Iteration 242, loss = 0.63994129\n",
      "Iteration 243, loss = 0.63368408\n",
      "Iteration 244, loss = 0.62748701\n",
      "Iteration 245, loss = 0.62139065\n",
      "Iteration 246, loss = 0.61533064\n",
      "Iteration 247, loss = 0.60945308\n",
      "Iteration 248, loss = 0.60349524\n",
      "Iteration 249, loss = 0.59764615\n",
      "Iteration 250, loss = 0.59182843\n",
      "Iteration 251, loss = 0.58609538\n",
      "Iteration 252, loss = 0.58041188\n",
      "Iteration 253, loss = 0.57461425\n",
      "Iteration 254, loss = 0.56908731\n",
      "Iteration 255, loss = 0.56364236\n",
      "Iteration 256, loss = 0.55812842\n",
      "Iteration 257, loss = 0.55275181\n",
      "Iteration 258, loss = 0.54738669\n",
      "Iteration 259, loss = 0.54197542\n",
      "Iteration 260, loss = 0.53684817\n",
      "Iteration 261, loss = 0.53171321\n",
      "Iteration 262, loss = 0.52642142\n",
      "Iteration 263, loss = 0.52139939\n",
      "Iteration 264, loss = 0.51627154\n",
      "Iteration 265, loss = 0.51143849\n",
      "Iteration 266, loss = 0.50649816\n",
      "Iteration 267, loss = 0.50170394\n",
      "Iteration 268, loss = 0.49678092\n",
      "Iteration 269, loss = 0.49196580\n",
      "Iteration 270, loss = 0.48725631\n",
      "Iteration 271, loss = 0.48261762\n",
      "Iteration 272, loss = 0.47795772\n",
      "Iteration 273, loss = 0.47319241\n",
      "Iteration 274, loss = 0.46884290\n",
      "Iteration 275, loss = 0.46429700\n",
      "Iteration 276, loss = 0.45976365\n",
      "Iteration 277, loss = 0.45529682\n",
      "Iteration 278, loss = 0.45094677\n",
      "Iteration 279, loss = 0.44688659\n",
      "Iteration 280, loss = 0.44245886\n",
      "Iteration 281, loss = 0.43831849\n",
      "Iteration 282, loss = 0.43403739\n",
      "Iteration 283, loss = 0.43013992\n",
      "Iteration 284, loss = 0.42598456\n",
      "Iteration 285, loss = 0.42198478\n",
      "Iteration 286, loss = 0.41806912\n",
      "Iteration 287, loss = 0.41414489\n",
      "Iteration 288, loss = 0.41018470\n",
      "Iteration 289, loss = 0.40636633\n",
      "Iteration 290, loss = 0.40253320\n",
      "Iteration 291, loss = 0.39862673\n",
      "Iteration 292, loss = 0.39491192\n",
      "Iteration 293, loss = 0.39108749\n",
      "Iteration 294, loss = 0.38751377\n",
      "Iteration 295, loss = 0.38379999\n",
      "Iteration 296, loss = 0.38019460\n",
      "Iteration 297, loss = 0.37672063\n",
      "Iteration 298, loss = 0.37315616\n",
      "Iteration 299, loss = 0.36977524\n",
      "Iteration 300, loss = 0.36636811\n",
      "Iteration 301, loss = 0.36304188\n",
      "Iteration 302, loss = 0.35974822\n",
      "Iteration 303, loss = 0.35642800\n",
      "Iteration 304, loss = 0.35327512\n",
      "Iteration 305, loss = 0.35004150\n",
      "Iteration 306, loss = 0.34682512\n",
      "Iteration 307, loss = 0.34365111\n",
      "Iteration 308, loss = 0.34048108\n",
      "Iteration 309, loss = 0.33750654\n",
      "Iteration 310, loss = 0.33442814\n",
      "Iteration 311, loss = 0.33136615\n",
      "Iteration 312, loss = 0.32846182\n",
      "Iteration 313, loss = 0.32540287\n",
      "Iteration 314, loss = 0.32255083\n",
      "Iteration 315, loss = 0.31960735\n",
      "Iteration 316, loss = 0.31666352\n",
      "Iteration 317, loss = 0.31386842\n",
      "Iteration 318, loss = 0.31098585\n",
      "Iteration 319, loss = 0.30826251\n",
      "Iteration 320, loss = 0.30549938\n",
      "Iteration 321, loss = 0.30269740\n",
      "Iteration 322, loss = 0.30005570\n",
      "Iteration 323, loss = 0.29740365\n",
      "Iteration 324, loss = 0.29481643\n",
      "Iteration 325, loss = 0.29214441\n",
      "Iteration 326, loss = 0.28955305\n",
      "Iteration 327, loss = 0.28702767\n",
      "Iteration 328, loss = 0.28457033\n",
      "Iteration 329, loss = 0.28200926\n",
      "Iteration 330, loss = 0.27965280\n",
      "Iteration 331, loss = 0.27710971\n",
      "Iteration 332, loss = 0.27480623\n",
      "Iteration 333, loss = 0.27240735\n",
      "Iteration 334, loss = 0.27002241\n",
      "Iteration 335, loss = 0.26767025\n",
      "Iteration 336, loss = 0.26533214\n",
      "Iteration 337, loss = 0.26302231\n",
      "Iteration 338, loss = 0.26079175\n",
      "Iteration 339, loss = 0.25853820\n",
      "Iteration 340, loss = 0.25634999\n",
      "Iteration 341, loss = 0.25415102\n",
      "Iteration 342, loss = 0.25205033\n",
      "Iteration 343, loss = 0.24994719\n",
      "Iteration 344, loss = 0.24779714\n",
      "Iteration 345, loss = 0.24575969\n",
      "Iteration 346, loss = 0.24372367\n",
      "Iteration 347, loss = 0.24168368\n",
      "Iteration 348, loss = 0.23970394\n",
      "Iteration 349, loss = 0.23773228\n",
      "Iteration 350, loss = 0.23577889\n",
      "Iteration 351, loss = 0.23384382\n",
      "Iteration 352, loss = 0.23190726\n",
      "Iteration 353, loss = 0.22993917\n",
      "Iteration 354, loss = 0.22814910\n",
      "Iteration 355, loss = 0.22622793\n",
      "Iteration 356, loss = 0.22436452\n",
      "Iteration 357, loss = 0.22256833\n",
      "Iteration 358, loss = 0.22074271\n",
      "Iteration 359, loss = 0.21894675\n",
      "Iteration 360, loss = 0.21718736\n",
      "Iteration 361, loss = 0.21544474\n",
      "Iteration 362, loss = 0.21370337\n",
      "Iteration 363, loss = 0.21204292\n",
      "Iteration 364, loss = 0.21028538\n",
      "Iteration 365, loss = 0.20868426\n",
      "Iteration 366, loss = 0.20703777\n",
      "Iteration 367, loss = 0.20533629\n",
      "Iteration 368, loss = 0.20371998\n",
      "Iteration 369, loss = 0.20209538\n",
      "Iteration 370, loss = 0.20044003\n",
      "Iteration 371, loss = 0.19884545\n",
      "Iteration 372, loss = 0.19722565\n",
      "Iteration 373, loss = 0.19570728\n",
      "Iteration 374, loss = 0.19414711\n",
      "Iteration 375, loss = 0.19260312\n",
      "Iteration 376, loss = 0.19109923\n",
      "Iteration 377, loss = 0.18961148\n",
      "Iteration 378, loss = 0.18817095\n",
      "Iteration 379, loss = 0.18673095\n",
      "Iteration 380, loss = 0.18529987\n",
      "Iteration 381, loss = 0.18389279\n",
      "Iteration 382, loss = 0.18248640\n",
      "Iteration 383, loss = 0.18109865\n",
      "Iteration 384, loss = 0.17975540\n",
      "Iteration 385, loss = 0.17839475\n",
      "Iteration 386, loss = 0.17707913\n",
      "Iteration 387, loss = 0.17573780\n",
      "Iteration 388, loss = 0.17444666\n",
      "Iteration 389, loss = 0.17315948\n",
      "Iteration 390, loss = 0.17189188\n",
      "Iteration 391, loss = 0.17061257\n",
      "Iteration 392, loss = 0.16933863\n",
      "Iteration 393, loss = 0.16808806\n",
      "Iteration 394, loss = 0.16684845\n",
      "Iteration 395, loss = 0.16559911\n",
      "Iteration 396, loss = 0.16437805\n",
      "Iteration 397, loss = 0.16313214\n",
      "Iteration 398, loss = 0.16198406\n",
      "Iteration 399, loss = 0.16078518\n",
      "Iteration 400, loss = 0.15964965\n",
      "Iteration 401, loss = 0.15845063\n",
      "Iteration 402, loss = 0.15727987\n",
      "Iteration 403, loss = 0.15617864\n",
      "Iteration 404, loss = 0.15501817\n",
      "Iteration 405, loss = 0.15390006\n",
      "Iteration 406, loss = 0.15280274\n",
      "Iteration 407, loss = 0.15171955\n",
      "Iteration 408, loss = 0.15064754\n",
      "Iteration 409, loss = 0.14961078\n",
      "Iteration 410, loss = 0.14856274\n",
      "Iteration 411, loss = 0.14754636\n",
      "Iteration 412, loss = 0.14650791\n",
      "Iteration 413, loss = 0.14549388\n",
      "Iteration 414, loss = 0.14449027\n",
      "Iteration 415, loss = 0.14348350\n",
      "Iteration 416, loss = 0.14248844\n",
      "Iteration 417, loss = 0.14150620\n",
      "Iteration 418, loss = 0.14052633\n",
      "Iteration 419, loss = 0.13958851\n",
      "Iteration 420, loss = 0.13860997\n",
      "Iteration 421, loss = 0.13772670\n",
      "Iteration 422, loss = 0.13679980\n",
      "Iteration 423, loss = 0.13588668\n",
      "Iteration 424, loss = 0.13497837\n",
      "Iteration 425, loss = 0.13404858\n",
      "Iteration 426, loss = 0.13314647\n",
      "Iteration 427, loss = 0.13227389\n",
      "Iteration 428, loss = 0.13141868\n",
      "Iteration 429, loss = 0.13055175\n",
      "Iteration 430, loss = 0.12969753\n",
      "Iteration 431, loss = 0.12881383\n",
      "Iteration 432, loss = 0.12801391\n",
      "Iteration 433, loss = 0.12716023\n",
      "Iteration 434, loss = 0.12629708\n",
      "Iteration 435, loss = 0.12547042\n",
      "Iteration 436, loss = 0.12465537\n",
      "Iteration 437, loss = 0.12384488\n",
      "Iteration 438, loss = 0.12306412\n",
      "Iteration 439, loss = 0.12227837\n",
      "Iteration 440, loss = 0.12147079\n",
      "Iteration 441, loss = 0.12071727\n",
      "Iteration 442, loss = 0.11994355\n",
      "Iteration 443, loss = 0.11917490\n",
      "Iteration 444, loss = 0.11842792\n",
      "Iteration 445, loss = 0.11768547\n",
      "Iteration 446, loss = 0.11696278\n",
      "Iteration 447, loss = 0.11618966\n",
      "Iteration 448, loss = 0.11550198\n",
      "Iteration 449, loss = 0.11477028\n",
      "Iteration 450, loss = 0.11405054\n",
      "Iteration 451, loss = 0.11333373\n",
      "Iteration 452, loss = 0.11266237\n",
      "Iteration 453, loss = 0.11195392\n",
      "Iteration 454, loss = 0.11126885\n",
      "Iteration 455, loss = 0.11060344\n",
      "Iteration 456, loss = 0.10991688\n",
      "Iteration 457, loss = 0.10928284\n",
      "Iteration 458, loss = 0.10857668\n",
      "Iteration 459, loss = 0.10794531\n",
      "Iteration 460, loss = 0.10729379\n",
      "Iteration 461, loss = 0.10664313\n",
      "Iteration 462, loss = 0.10599380\n",
      "Iteration 463, loss = 0.10538948\n",
      "Iteration 464, loss = 0.10472897\n",
      "Iteration 465, loss = 0.10416689\n",
      "Iteration 466, loss = 0.10352154\n",
      "Iteration 467, loss = 0.10292398\n",
      "Iteration 468, loss = 0.10229484\n",
      "Iteration 469, loss = 0.10175196\n",
      "Iteration 470, loss = 0.10117245\n",
      "Iteration 471, loss = 0.10058633\n",
      "Iteration 472, loss = 0.09999051\n",
      "Iteration 473, loss = 0.09937220\n",
      "Iteration 474, loss = 0.09884248\n",
      "Iteration 475, loss = 0.09823067\n",
      "Iteration 476, loss = 0.09767400\n",
      "Iteration 477, loss = 0.09709168\n",
      "Iteration 478, loss = 0.09653352\n",
      "Iteration 479, loss = 0.09597445\n",
      "Iteration 480, loss = 0.09544335\n",
      "Iteration 481, loss = 0.09487726\n",
      "Iteration 482, loss = 0.09432433\n",
      "Iteration 483, loss = 0.09376356\n",
      "Iteration 484, loss = 0.09322390\n",
      "Iteration 485, loss = 0.09269506\n",
      "Iteration 486, loss = 0.09218865\n",
      "Iteration 487, loss = 0.09165930\n",
      "Iteration 488, loss = 0.09114245\n",
      "Iteration 489, loss = 0.09063387\n",
      "Iteration 490, loss = 0.09013332\n",
      "Iteration 491, loss = 0.08962734\n",
      "Iteration 492, loss = 0.08912962\n",
      "Iteration 493, loss = 0.08864481\n",
      "Iteration 494, loss = 0.08816009\n",
      "Iteration 495, loss = 0.08767409\n",
      "Iteration 496, loss = 0.08721992\n",
      "Iteration 497, loss = 0.08672696\n",
      "Iteration 498, loss = 0.08627522\n",
      "Iteration 499, loss = 0.08580615\n",
      "Iteration 500, loss = 0.08536234\n",
      "Iteration 501, loss = 0.08489496\n",
      "Iteration 502, loss = 0.08446525\n",
      "Iteration 503, loss = 0.08399618\n",
      "Iteration 504, loss = 0.08355213\n",
      "Iteration 505, loss = 0.08310445\n",
      "Iteration 506, loss = 0.08269449\n",
      "Iteration 507, loss = 0.08222995\n",
      "Iteration 508, loss = 0.08178600\n",
      "Iteration 509, loss = 0.08135701\n",
      "Iteration 510, loss = 0.08094210\n",
      "Iteration 511, loss = 0.08051371\n",
      "Iteration 512, loss = 0.08009881\n",
      "Iteration 513, loss = 0.07968175\n",
      "Iteration 514, loss = 0.07926476\n",
      "Iteration 515, loss = 0.07886317\n",
      "Iteration 516, loss = 0.07845454\n",
      "Iteration 517, loss = 0.07805399\n",
      "Iteration 518, loss = 0.07765460\n",
      "Iteration 519, loss = 0.07724847\n",
      "Iteration 520, loss = 0.07685506\n",
      "Iteration 521, loss = 0.07647199\n",
      "Iteration 522, loss = 0.07607084\n",
      "Iteration 523, loss = 0.07570716\n",
      "Iteration 524, loss = 0.07531924\n",
      "Iteration 525, loss = 0.07494433\n",
      "Iteration 526, loss = 0.07456122\n",
      "Iteration 527, loss = 0.07419845\n",
      "Iteration 528, loss = 0.07382160\n",
      "Iteration 529, loss = 0.07345558\n",
      "Iteration 530, loss = 0.07309588\n",
      "Iteration 531, loss = 0.07273897\n",
      "Iteration 532, loss = 0.07238052\n",
      "Iteration 533, loss = 0.07202536\n",
      "Iteration 534, loss = 0.07167967\n",
      "Iteration 535, loss = 0.07132686\n",
      "Iteration 536, loss = 0.07098362\n",
      "Iteration 537, loss = 0.07064011\n",
      "Iteration 538, loss = 0.07030338\n",
      "Iteration 539, loss = 0.06995377\n",
      "Iteration 540, loss = 0.06962004\n",
      "Iteration 541, loss = 0.06928392\n",
      "Iteration 542, loss = 0.06895169\n",
      "Iteration 543, loss = 0.06861678\n",
      "Iteration 544, loss = 0.06829435\n",
      "Iteration 545, loss = 0.06796517\n",
      "Iteration 546, loss = 0.06764514\n",
      "Iteration 547, loss = 0.06731767\n",
      "Iteration 548, loss = 0.06699867\n",
      "Iteration 549, loss = 0.06668571\n",
      "Iteration 550, loss = 0.06637040\n",
      "Iteration 551, loss = 0.06605824\n",
      "Iteration 552, loss = 0.06574945\n",
      "Iteration 553, loss = 0.06544293\n",
      "Iteration 554, loss = 0.06514092\n",
      "Iteration 555, loss = 0.06483660\n",
      "Iteration 556, loss = 0.06453781\n",
      "Iteration 557, loss = 0.06424721\n",
      "Iteration 558, loss = 0.06394781\n",
      "Iteration 559, loss = 0.06366277\n",
      "Iteration 560, loss = 0.06337181\n",
      "Iteration 561, loss = 0.06308993\n",
      "Iteration 562, loss = 0.06281147\n",
      "Iteration 563, loss = 0.06252889\n",
      "Iteration 564, loss = 0.06223780\n",
      "Iteration 565, loss = 0.06196609\n",
      "Iteration 566, loss = 0.06168197\n",
      "Iteration 567, loss = 0.06140026\n",
      "Iteration 568, loss = 0.06111578\n",
      "Iteration 569, loss = 0.06083975\n",
      "Iteration 570, loss = 0.06055692\n",
      "Iteration 571, loss = 0.06026534\n",
      "Iteration 572, loss = 0.05999158\n",
      "Iteration 573, loss = 0.05970780\n",
      "Iteration 574, loss = 0.05944699\n",
      "Iteration 575, loss = 0.05916564\n",
      "Iteration 576, loss = 0.05891316\n",
      "Iteration 577, loss = 0.05864580\n",
      "Iteration 578, loss = 0.05839714\n",
      "Iteration 579, loss = 0.05813960\n",
      "Iteration 580, loss = 0.05788195\n",
      "Iteration 581, loss = 0.05762902\n",
      "Iteration 582, loss = 0.05738235\n",
      "Iteration 583, loss = 0.05713572\n",
      "Iteration 584, loss = 0.05688053\n",
      "Iteration 585, loss = 0.05662913\n",
      "Iteration 586, loss = 0.05638854\n",
      "Iteration 587, loss = 0.05615695\n",
      "Iteration 588, loss = 0.05590870\n",
      "Iteration 589, loss = 0.05567156\n",
      "Iteration 590, loss = 0.05543373\n",
      "Iteration 591, loss = 0.05519330\n",
      "Iteration 592, loss = 0.05495516\n",
      "Iteration 593, loss = 0.05472553\n",
      "Iteration 594, loss = 0.05448606\n",
      "Iteration 595, loss = 0.05425796\n",
      "Iteration 596, loss = 0.05403028\n",
      "Iteration 597, loss = 0.05379743\n",
      "Iteration 598, loss = 0.05357345\n",
      "Iteration 599, loss = 0.05334965\n",
      "Iteration 600, loss = 0.05312670\n",
      "Iteration 601, loss = 0.05290418\n",
      "Iteration 602, loss = 0.05268276\n",
      "Iteration 603, loss = 0.05246817\n",
      "Iteration 604, loss = 0.05224262\n",
      "Iteration 605, loss = 0.05203205\n",
      "Iteration 606, loss = 0.05181281\n",
      "Iteration 607, loss = 0.05160093\n",
      "Iteration 608, loss = 0.05138625\n",
      "Iteration 609, loss = 0.05118101\n",
      "Iteration 610, loss = 0.05096880\n",
      "Iteration 611, loss = 0.05076084\n",
      "Iteration 612, loss = 0.05055413\n",
      "Iteration 613, loss = 0.05034837\n",
      "Iteration 614, loss = 0.05014808\n",
      "Iteration 615, loss = 0.04994310\n",
      "Iteration 616, loss = 0.04974229\n",
      "Iteration 617, loss = 0.04954151\n",
      "Iteration 618, loss = 0.04934505\n",
      "Iteration 619, loss = 0.04914323\n",
      "Iteration 620, loss = 0.04895026\n",
      "Iteration 621, loss = 0.04875397\n",
      "Iteration 622, loss = 0.04856331\n",
      "Iteration 623, loss = 0.04836598\n",
      "Iteration 624, loss = 0.04817675\n",
      "Iteration 625, loss = 0.04798712\n",
      "Iteration 626, loss = 0.04779501\n",
      "Iteration 627, loss = 0.04761019\n",
      "Iteration 628, loss = 0.04742196\n",
      "Iteration 629, loss = 0.04723482\n",
      "Iteration 630, loss = 0.04705219\n",
      "Iteration 631, loss = 0.04687584\n",
      "Iteration 632, loss = 0.04668920\n",
      "Iteration 633, loss = 0.04650852\n",
      "Iteration 634, loss = 0.04632747\n",
      "Iteration 635, loss = 0.04615020\n",
      "Iteration 636, loss = 0.04596867\n",
      "Iteration 637, loss = 0.04578997\n",
      "Iteration 638, loss = 0.04561306\n",
      "Iteration 639, loss = 0.04543639\n",
      "Iteration 640, loss = 0.04526340\n",
      "Iteration 641, loss = 0.04508939\n",
      "Iteration 642, loss = 0.04491810\n",
      "Iteration 643, loss = 0.04475030\n",
      "Iteration 644, loss = 0.04457904\n",
      "Iteration 645, loss = 0.04440944\n",
      "Iteration 646, loss = 0.04424104\n",
      "Iteration 647, loss = 0.04407504\n",
      "Iteration 648, loss = 0.04390749\n",
      "Iteration 649, loss = 0.04374227\n",
      "Iteration 650, loss = 0.04357684\n",
      "Iteration 651, loss = 0.04341410\n",
      "Iteration 652, loss = 0.04325483\n",
      "Iteration 653, loss = 0.04309397\n",
      "Iteration 654, loss = 0.04293100\n",
      "Iteration 655, loss = 0.04277608\n",
      "Iteration 656, loss = 0.04261583\n",
      "Iteration 657, loss = 0.04245959\n",
      "Iteration 658, loss = 0.04230320\n",
      "Iteration 659, loss = 0.04215337\n",
      "Iteration 660, loss = 0.04199859\n",
      "Iteration 661, loss = 0.04184487\n",
      "Iteration 662, loss = 0.04168962\n",
      "Iteration 663, loss = 0.04153924\n",
      "Iteration 664, loss = 0.04138173\n",
      "Iteration 665, loss = 0.04123180\n",
      "Iteration 666, loss = 0.04108247\n",
      "Iteration 667, loss = 0.04092829\n",
      "Iteration 668, loss = 0.04077885\n",
      "Iteration 669, loss = 0.04063682\n",
      "Iteration 670, loss = 0.04048986\n",
      "Iteration 671, loss = 0.04033833\n",
      "Iteration 672, loss = 0.04019725\n",
      "Iteration 673, loss = 0.04005147\n",
      "Iteration 674, loss = 0.03990407\n",
      "Iteration 675, loss = 0.03976466\n",
      "Iteration 676, loss = 0.03962597\n",
      "Iteration 677, loss = 0.03948273\n",
      "Iteration 678, loss = 0.03935004\n",
      "Iteration 679, loss = 0.03920703\n",
      "Iteration 680, loss = 0.03906982\n",
      "Iteration 681, loss = 0.03893409\n",
      "Iteration 682, loss = 0.03879411\n",
      "Iteration 683, loss = 0.03866398\n",
      "Iteration 684, loss = 0.03852451\n",
      "Iteration 685, loss = 0.03838886\n",
      "Iteration 686, loss = 0.03825203\n",
      "Iteration 687, loss = 0.03812200\n",
      "Iteration 688, loss = 0.03798529\n",
      "Iteration 689, loss = 0.03787359\n",
      "Iteration 690, loss = 0.03772812\n",
      "Iteration 691, loss = 0.03762375\n",
      "Iteration 692, loss = 0.03748110\n",
      "Iteration 693, loss = 0.03735286\n",
      "Iteration 694, loss = 0.03723126\n",
      "Iteration 695, loss = 0.03709454\n",
      "Iteration 696, loss = 0.03697181\n",
      "Iteration 697, loss = 0.03684676\n",
      "Iteration 698, loss = 0.03671806\n",
      "Iteration 699, loss = 0.03659088\n",
      "Iteration 700, loss = 0.03645762\n",
      "Iteration 701, loss = 0.03633674\n",
      "Iteration 702, loss = 0.03621382\n",
      "Iteration 703, loss = 0.03608900\n",
      "Iteration 704, loss = 0.03596631\n",
      "Iteration 705, loss = 0.03584219\n",
      "Iteration 706, loss = 0.03572019\n",
      "Iteration 707, loss = 0.03560059\n",
      "Iteration 708, loss = 0.03548097\n",
      "Iteration 709, loss = 0.03535944\n",
      "Iteration 710, loss = 0.03524656\n",
      "Iteration 711, loss = 0.03512384\n",
      "Iteration 712, loss = 0.03500794\n",
      "Iteration 713, loss = 0.03489477\n",
      "Iteration 714, loss = 0.03477650\n",
      "Iteration 715, loss = 0.03465651\n",
      "Iteration 716, loss = 0.03454351\n",
      "Iteration 717, loss = 0.03442899\n",
      "Iteration 718, loss = 0.03431744\n",
      "Iteration 719, loss = 0.03420278\n",
      "Iteration 720, loss = 0.03409556\n",
      "Iteration 721, loss = 0.03398292\n",
      "Iteration 722, loss = 0.03386870\n",
      "Iteration 723, loss = 0.03375824\n",
      "Iteration 724, loss = 0.03365183\n",
      "Iteration 725, loss = 0.03353961\n",
      "Iteration 726, loss = 0.03343147\n",
      "Iteration 727, loss = 0.03332411\n",
      "Iteration 728, loss = 0.03321269\n",
      "Iteration 729, loss = 0.03310828\n",
      "Iteration 730, loss = 0.03300212\n",
      "Iteration 731, loss = 0.03289388\n",
      "Iteration 732, loss = 0.03278749\n",
      "Iteration 733, loss = 0.03268520\n",
      "Iteration 734, loss = 0.03257692\n",
      "Iteration 735, loss = 0.03247407\n",
      "Iteration 736, loss = 0.03236901\n",
      "Iteration 737, loss = 0.03226418\n",
      "Iteration 738, loss = 0.03216499\n",
      "Iteration 739, loss = 0.03206161\n",
      "Iteration 740, loss = 0.03195891\n",
      "Iteration 741, loss = 0.03185830\n",
      "Iteration 742, loss = 0.03175654\n",
      "Iteration 743, loss = 0.03165663\n",
      "Iteration 744, loss = 0.03155810\n",
      "Iteration 745, loss = 0.03145854\n",
      "Iteration 746, loss = 0.03135886\n",
      "Iteration 747, loss = 0.03126148\n",
      "Iteration 748, loss = 0.03116327\n",
      "Iteration 749, loss = 0.03106694\n",
      "Iteration 750, loss = 0.03096965\n",
      "Iteration 751, loss = 0.03087279\n",
      "Iteration 752, loss = 0.03077452\n",
      "Iteration 753, loss = 0.03067957\n",
      "Iteration 754, loss = 0.03058366\n",
      "Iteration 755, loss = 0.03048786\n",
      "Iteration 756, loss = 0.03039492\n",
      "Iteration 757, loss = 0.03030030\n",
      "Iteration 758, loss = 0.03020851\n",
      "Iteration 759, loss = 0.03011595\n",
      "Iteration 760, loss = 0.03002185\n",
      "Iteration 761, loss = 0.02993063\n",
      "Iteration 762, loss = 0.02983896\n",
      "Iteration 763, loss = 0.02974779\n",
      "Iteration 764, loss = 0.02965931\n",
      "Iteration 765, loss = 0.02956533\n",
      "Iteration 766, loss = 0.02947884\n",
      "Iteration 767, loss = 0.02938604\n",
      "Iteration 768, loss = 0.02929930\n",
      "Iteration 769, loss = 0.02920793\n",
      "Iteration 770, loss = 0.02912313\n",
      "Iteration 771, loss = 0.02903168\n",
      "Iteration 772, loss = 0.02894594\n",
      "Iteration 773, loss = 0.02886024\n",
      "Iteration 774, loss = 0.02877347\n",
      "Iteration 775, loss = 0.02868577\n",
      "Iteration 776, loss = 0.02859898\n",
      "Iteration 777, loss = 0.02851175\n",
      "Iteration 778, loss = 0.02842905\n",
      "Iteration 779, loss = 0.02834175\n",
      "Iteration 780, loss = 0.02825878\n",
      "Iteration 781, loss = 0.02817496\n",
      "Iteration 782, loss = 0.02809135\n",
      "Iteration 783, loss = 0.02800794\n",
      "Iteration 784, loss = 0.02792476\n",
      "Iteration 785, loss = 0.02784501\n",
      "Iteration 786, loss = 0.02776090\n",
      "Iteration 787, loss = 0.02768010\n",
      "Iteration 788, loss = 0.02760017\n",
      "Iteration 789, loss = 0.02751778\n",
      "Iteration 790, loss = 0.02743788\n",
      "Iteration 791, loss = 0.02735775\n",
      "Iteration 792, loss = 0.02727797\n",
      "Iteration 793, loss = 0.02719865\n",
      "Iteration 794, loss = 0.02711941\n",
      "Iteration 795, loss = 0.02704229\n",
      "Iteration 796, loss = 0.02696327\n",
      "Iteration 797, loss = 0.02688467\n",
      "Iteration 798, loss = 0.02680634\n",
      "Iteration 799, loss = 0.02672905\n",
      "Iteration 800, loss = 0.02665227\n",
      "Iteration 801, loss = 0.02657427\n",
      "Iteration 802, loss = 0.02649826\n",
      "Iteration 803, loss = 0.02642227\n",
      "Iteration 804, loss = 0.02634735\n",
      "Iteration 805, loss = 0.02627081\n",
      "Iteration 806, loss = 0.02619691\n",
      "Iteration 807, loss = 0.02612248\n",
      "Iteration 808, loss = 0.02604827\n",
      "Iteration 809, loss = 0.02597519\n",
      "Iteration 810, loss = 0.02590143\n",
      "Iteration 811, loss = 0.02582878\n",
      "Iteration 812, loss = 0.02575603\n",
      "Iteration 813, loss = 0.02568394\n",
      "Iteration 814, loss = 0.02561210\n",
      "Iteration 815, loss = 0.02553897\n",
      "Iteration 816, loss = 0.02546996\n",
      "Iteration 817, loss = 0.02539783\n",
      "Iteration 818, loss = 0.02532785\n",
      "Iteration 819, loss = 0.02525754\n",
      "Iteration 820, loss = 0.02518775\n",
      "Iteration 821, loss = 0.02511779\n",
      "Iteration 822, loss = 0.02504796\n",
      "Iteration 823, loss = 0.02497946\n",
      "Iteration 824, loss = 0.02490971\n",
      "Iteration 825, loss = 0.02484217\n",
      "Iteration 826, loss = 0.02477276\n",
      "Iteration 827, loss = 0.02470433\n",
      "Iteration 828, loss = 0.02463623\n",
      "Iteration 829, loss = 0.02456885\n",
      "Iteration 830, loss = 0.02450245\n",
      "Iteration 831, loss = 0.02443427\n",
      "Iteration 832, loss = 0.02436731\n",
      "Iteration 833, loss = 0.02430088\n",
      "Iteration 834, loss = 0.02423394\n",
      "Iteration 835, loss = 0.02416784\n",
      "Iteration 836, loss = 0.02410352\n",
      "Iteration 837, loss = 0.02403698\n",
      "Iteration 838, loss = 0.02397146\n",
      "Iteration 839, loss = 0.02390678\n",
      "Iteration 840, loss = 0.02384227\n",
      "Iteration 841, loss = 0.02377739\n",
      "Iteration 842, loss = 0.02371229\n",
      "Iteration 843, loss = 0.02364959\n",
      "Iteration 844, loss = 0.02358533\n",
      "Iteration 845, loss = 0.02352167\n",
      "Iteration 846, loss = 0.02345826\n",
      "Iteration 847, loss = 0.02339481\n",
      "Iteration 848, loss = 0.02333273\n",
      "Iteration 849, loss = 0.02327039\n",
      "Iteration 850, loss = 0.02320835\n",
      "Iteration 851, loss = 0.02314650\n",
      "Iteration 852, loss = 0.02308405\n",
      "Iteration 853, loss = 0.02302286\n",
      "Iteration 854, loss = 0.02296141\n",
      "Iteration 855, loss = 0.02290041\n",
      "Iteration 856, loss = 0.02283955\n",
      "Iteration 857, loss = 0.02277858\n",
      "Iteration 858, loss = 0.02271859\n",
      "Iteration 859, loss = 0.02265837\n",
      "Iteration 860, loss = 0.02259862\n",
      "Iteration 861, loss = 0.02253975\n",
      "Iteration 862, loss = 0.02248045\n",
      "Iteration 863, loss = 0.02242087\n",
      "Iteration 864, loss = 0.02236257\n",
      "Iteration 865, loss = 0.02230491\n",
      "Iteration 866, loss = 0.02224623\n",
      "Iteration 867, loss = 0.02218843\n",
      "Iteration 868, loss = 0.02212987\n",
      "Iteration 869, loss = 0.02207388\n",
      "Iteration 870, loss = 0.02201634\n",
      "Iteration 871, loss = 0.02195876\n",
      "Iteration 872, loss = 0.02190221\n",
      "Iteration 873, loss = 0.02184591\n",
      "Iteration 874, loss = 0.02179023\n",
      "Iteration 875, loss = 0.02173391\n",
      "Iteration 876, loss = 0.02167756\n",
      "Iteration 877, loss = 0.02162169\n",
      "Iteration 878, loss = 0.02156602\n",
      "Iteration 879, loss = 0.02151180\n",
      "Iteration 880, loss = 0.02145651\n",
      "Iteration 881, loss = 0.02140133\n",
      "Iteration 882, loss = 0.02134678\n",
      "Iteration 883, loss = 0.02129174\n",
      "Iteration 884, loss = 0.02123861\n",
      "Iteration 885, loss = 0.02118333\n",
      "Iteration 886, loss = 0.02113036\n",
      "Iteration 887, loss = 0.02107623\n",
      "Iteration 888, loss = 0.02102310\n",
      "Iteration 889, loss = 0.02096933\n",
      "Iteration 890, loss = 0.02091579\n",
      "Iteration 891, loss = 0.02086320\n",
      "Iteration 892, loss = 0.02081083\n",
      "Iteration 893, loss = 0.02075828\n",
      "Iteration 894, loss = 0.02070606\n",
      "Iteration 895, loss = 0.02065407\n",
      "Iteration 896, loss = 0.02060152\n",
      "Iteration 897, loss = 0.02055058\n",
      "Iteration 898, loss = 0.02049971\n",
      "Iteration 899, loss = 0.02044878\n",
      "Iteration 900, loss = 0.02039722\n",
      "Iteration 901, loss = 0.02034611\n",
      "Iteration 902, loss = 0.02029594\n",
      "Iteration 903, loss = 0.02024506\n",
      "Iteration 904, loss = 0.02019536\n",
      "Iteration 905, loss = 0.02014505\n",
      "Iteration 906, loss = 0.02009518\n",
      "Iteration 907, loss = 0.02004586\n",
      "Iteration 908, loss = 0.01999641\n",
      "Iteration 909, loss = 0.01994740\n",
      "Iteration 910, loss = 0.01989814\n",
      "Iteration 911, loss = 0.01984930\n",
      "Iteration 912, loss = 0.01980049\n",
      "Iteration 913, loss = 0.01975310\n",
      "Iteration 914, loss = 0.01970368\n",
      "Iteration 915, loss = 0.01965595\n",
      "Iteration 916, loss = 0.01960790\n",
      "Iteration 917, loss = 0.01956037\n",
      "Iteration 918, loss = 0.01951256\n",
      "Iteration 919, loss = 0.01946548\n",
      "Iteration 920, loss = 0.01941769\n",
      "Iteration 921, loss = 0.01937144\n",
      "Iteration 922, loss = 0.01932396\n",
      "Iteration 923, loss = 0.01927787\n",
      "Iteration 924, loss = 0.01923090\n",
      "Iteration 925, loss = 0.01918497\n",
      "Iteration 926, loss = 0.01913852\n",
      "Iteration 927, loss = 0.01909236\n",
      "Iteration 928, loss = 0.01904636\n",
      "Iteration 929, loss = 0.01900069\n",
      "Iteration 930, loss = 0.01895575\n",
      "Iteration 931, loss = 0.01890977\n",
      "Iteration 932, loss = 0.01886537\n",
      "Iteration 933, loss = 0.01882013\n",
      "Iteration 934, loss = 0.01877517\n",
      "Iteration 935, loss = 0.01873037\n",
      "Iteration 936, loss = 0.01868527\n",
      "Iteration 937, loss = 0.01864103\n",
      "Iteration 938, loss = 0.01859651\n",
      "Iteration 939, loss = 0.01855296\n",
      "Iteration 940, loss = 0.01850816\n",
      "Iteration 941, loss = 0.01846462\n",
      "Iteration 942, loss = 0.01842114\n",
      "Iteration 943, loss = 0.01837707\n",
      "Iteration 944, loss = 0.01833391\n",
      "Iteration 945, loss = 0.01829054\n",
      "Iteration 946, loss = 0.01824760\n",
      "Iteration 947, loss = 0.01820476\n",
      "Iteration 948, loss = 0.01816194\n",
      "Iteration 949, loss = 0.01811924\n",
      "Iteration 950, loss = 0.01807701\n",
      "Iteration 951, loss = 0.01803463\n",
      "Iteration 952, loss = 0.01799267\n",
      "Iteration 953, loss = 0.01795018\n",
      "Iteration 954, loss = 0.01790883\n",
      "Iteration 955, loss = 0.01786704\n",
      "Iteration 956, loss = 0.01782527\n",
      "Iteration 957, loss = 0.01778308\n",
      "Iteration 958, loss = 0.01774258\n",
      "Iteration 959, loss = 0.01770149\n",
      "Iteration 960, loss = 0.01766041\n",
      "Iteration 961, loss = 0.01761930\n",
      "Iteration 962, loss = 0.01757886\n",
      "Iteration 963, loss = 0.01753825\n",
      "Iteration 964, loss = 0.01749754\n",
      "Iteration 965, loss = 0.01745761\n",
      "Iteration 966, loss = 0.01741727\n",
      "Iteration 967, loss = 0.01737751\n",
      "Iteration 968, loss = 0.01733731\n",
      "Iteration 969, loss = 0.01729741\n",
      "Iteration 970, loss = 0.01725816\n",
      "Iteration 971, loss = 0.01721891\n",
      "Iteration 972, loss = 0.01718016\n",
      "Iteration 973, loss = 0.01714095\n",
      "Iteration 974, loss = 0.01710228\n",
      "Iteration 975, loss = 0.01706338\n",
      "Iteration 976, loss = 0.01702439\n",
      "Iteration 977, loss = 0.01698603\n",
      "Iteration 978, loss = 0.01694768\n",
      "Iteration 979, loss = 0.01690923\n",
      "Iteration 980, loss = 0.01687104\n",
      "Iteration 981, loss = 0.01683303\n",
      "Iteration 982, loss = 0.01679508\n",
      "Iteration 983, loss = 0.01675733\n",
      "Iteration 984, loss = 0.01671960\n",
      "Iteration 985, loss = 0.01668226\n",
      "Iteration 986, loss = 0.01664512\n",
      "Iteration 987, loss = 0.01660817\n",
      "Iteration 988, loss = 0.01657034\n",
      "Iteration 989, loss = 0.01653367\n",
      "Iteration 990, loss = 0.01649643\n",
      "Iteration 991, loss = 0.01645959\n",
      "Iteration 992, loss = 0.01642322\n",
      "Iteration 993, loss = 0.01638638\n",
      "Iteration 994, loss = 0.01635026\n",
      "Iteration 995, loss = 0.01631375\n",
      "Iteration 996, loss = 0.01627743\n",
      "Iteration 997, loss = 0.01624149\n",
      "Iteration 998, loss = 0.01620497\n",
      "Iteration 999, loss = 0.01616888\n",
      "Iteration 1000, loss = 0.01613423\n",
      "Iteration 1001, loss = 0.01609796\n",
      "Iteration 1002, loss = 0.01606193\n",
      "Iteration 1003, loss = 0.01602655\n",
      "Iteration 1004, loss = 0.01599128\n",
      "Iteration 1005, loss = 0.01595585\n",
      "Iteration 1006, loss = 0.01592036\n",
      "Iteration 1007, loss = 0.01588595\n",
      "Iteration 1008, loss = 0.01585094\n",
      "Iteration 1009, loss = 0.01581612\n",
      "Iteration 1010, loss = 0.01578131\n",
      "Iteration 1011, loss = 0.01574689\n",
      "Iteration 1012, loss = 0.01571314\n",
      "Iteration 1013, loss = 0.01567796\n",
      "Iteration 1014, loss = 0.01564410\n",
      "Iteration 1015, loss = 0.01560995\n",
      "Iteration 1016, loss = 0.01557547\n",
      "Iteration 1017, loss = 0.01554191\n",
      "Iteration 1018, loss = 0.01550830\n",
      "Iteration 1019, loss = 0.01547452\n",
      "Iteration 1020, loss = 0.01544139\n",
      "Iteration 1021, loss = 0.01540779\n",
      "Iteration 1022, loss = 0.01537492\n",
      "Iteration 1023, loss = 0.01534211\n",
      "Iteration 1024, loss = 0.01530861\n",
      "Iteration 1025, loss = 0.01527605\n",
      "Iteration 1026, loss = 0.01524361\n",
      "Iteration 1027, loss = 0.01521035\n",
      "Iteration 1028, loss = 0.01517792\n",
      "Iteration 1029, loss = 0.01514562\n",
      "Iteration 1030, loss = 0.01511319\n",
      "Iteration 1031, loss = 0.01508052\n",
      "Iteration 1032, loss = 0.01504905\n",
      "Iteration 1033, loss = 0.01501686\n",
      "Iteration 1034, loss = 0.01498503\n",
      "Iteration 1035, loss = 0.01495302\n",
      "Iteration 1036, loss = 0.01492114\n",
      "Iteration 1037, loss = 0.01488953\n",
      "Iteration 1038, loss = 0.01485828\n",
      "Iteration 1039, loss = 0.01482691\n",
      "Iteration 1040, loss = 0.01479507\n",
      "Iteration 1041, loss = 0.01476373\n",
      "Iteration 1042, loss = 0.01473285\n",
      "Iteration 1043, loss = 0.01470213\n",
      "Iteration 1044, loss = 0.01467105\n",
      "Iteration 1045, loss = 0.01464060\n",
      "Iteration 1046, loss = 0.01460966\n",
      "Iteration 1047, loss = 0.01457929\n",
      "Iteration 1048, loss = 0.01454861\n",
      "Iteration 1049, loss = 0.01451826\n",
      "Iteration 1050, loss = 0.01448810\n",
      "Iteration 1051, loss = 0.01445866\n",
      "Iteration 1052, loss = 0.01442783\n",
      "Iteration 1053, loss = 0.01439782\n",
      "Iteration 1054, loss = 0.01436789\n",
      "Iteration 1055, loss = 0.01433838\n",
      "Iteration 1056, loss = 0.01430860\n",
      "Iteration 1057, loss = 0.01427928\n",
      "Iteration 1058, loss = 0.01425010\n",
      "Iteration 1059, loss = 0.01422029\n",
      "Iteration 1060, loss = 0.01419112\n",
      "Iteration 1061, loss = 0.01416176\n",
      "Iteration 1062, loss = 0.01413279\n",
      "Iteration 1063, loss = 0.01410359\n",
      "Iteration 1064, loss = 0.01407452\n",
      "Iteration 1065, loss = 0.01404595\n",
      "Iteration 1066, loss = 0.01401727\n",
      "Iteration 1067, loss = 0.01398834\n",
      "Iteration 1068, loss = 0.01395965\n",
      "Iteration 1069, loss = 0.01393121\n",
      "Iteration 1070, loss = 0.01390263\n",
      "Iteration 1071, loss = 0.01387416\n",
      "Iteration 1072, loss = 0.01384607\n",
      "Iteration 1073, loss = 0.01381756\n",
      "Iteration 1074, loss = 0.01378914\n",
      "Iteration 1075, loss = 0.01376062\n",
      "Iteration 1076, loss = 0.01373214\n",
      "Iteration 1077, loss = 0.01370255\n",
      "Iteration 1078, loss = 0.01367399\n",
      "Iteration 1079, loss = 0.01364475\n",
      "Iteration 1080, loss = 0.01361714\n",
      "Iteration 1081, loss = 0.01358951\n",
      "Iteration 1082, loss = 0.01356188\n",
      "Iteration 1083, loss = 0.01353413\n",
      "Iteration 1084, loss = 0.01350692\n",
      "Iteration 1085, loss = 0.01347961\n",
      "Iteration 1086, loss = 0.01345221\n",
      "Iteration 1087, loss = 0.01342515\n",
      "Iteration 1088, loss = 0.01339820\n",
      "Iteration 1089, loss = 0.01337129\n",
      "Iteration 1090, loss = 0.01334433\n",
      "Iteration 1091, loss = 0.01331763\n",
      "Iteration 1092, loss = 0.01329103\n",
      "Iteration 1093, loss = 0.01326415\n",
      "Iteration 1094, loss = 0.01323801\n",
      "Iteration 1095, loss = 0.01321212\n",
      "Iteration 1096, loss = 0.01318493\n",
      "Iteration 1097, loss = 0.01315873\n",
      "Iteration 1098, loss = 0.01313271\n",
      "Iteration 1099, loss = 0.01310667\n",
      "Iteration 1100, loss = 0.01308131\n",
      "Iteration 1101, loss = 0.01305522\n",
      "Iteration 1102, loss = 0.01302915\n",
      "Iteration 1103, loss = 0.01300379\n",
      "Iteration 1104, loss = 0.01297852\n",
      "Iteration 1105, loss = 0.01295235\n",
      "Iteration 1106, loss = 0.01292717\n",
      "Iteration 1107, loss = 0.01290162\n",
      "Iteration 1108, loss = 0.01287647\n",
      "Iteration 1109, loss = 0.01285170\n",
      "Iteration 1110, loss = 0.01282637\n",
      "Iteration 1111, loss = 0.01280069\n",
      "Iteration 1112, loss = 0.01277684\n",
      "Iteration 1113, loss = 0.01275240\n",
      "Iteration 1114, loss = 0.01272583\n",
      "Iteration 1115, loss = 0.01270078\n",
      "Iteration 1116, loss = 0.01267689\n",
      "Iteration 1117, loss = 0.01265205\n",
      "Iteration 1118, loss = 0.01262700\n",
      "Iteration 1119, loss = 0.01260242\n",
      "Iteration 1120, loss = 0.01257836\n",
      "Iteration 1121, loss = 0.01255343\n",
      "Iteration 1122, loss = 0.01252913\n",
      "Iteration 1123, loss = 0.01250552\n",
      "Iteration 1124, loss = 0.01248118\n",
      "Iteration 1125, loss = 0.01245674\n",
      "Iteration 1126, loss = 0.01243300\n",
      "Iteration 1127, loss = 0.01240932\n",
      "Iteration 1128, loss = 0.01238503\n",
      "Iteration 1129, loss = 0.01236076\n",
      "Iteration 1130, loss = 0.01233729\n",
      "Iteration 1131, loss = 0.01231322\n",
      "Iteration 1132, loss = 0.01228988\n",
      "Iteration 1133, loss = 0.01226659\n",
      "Iteration 1134, loss = 0.01224276\n",
      "Iteration 1135, loss = 0.01221939\n",
      "Iteration 1136, loss = 0.01219586\n",
      "Iteration 1137, loss = 0.01217277\n",
      "Iteration 1138, loss = 0.01214971\n",
      "Iteration 1139, loss = 0.01212642\n",
      "Iteration 1140, loss = 0.01210347\n",
      "Iteration 1141, loss = 0.01208050\n",
      "Iteration 1142, loss = 0.01205765\n",
      "Iteration 1143, loss = 0.01203473\n",
      "Iteration 1144, loss = 0.01201248\n",
      "Iteration 1145, loss = 0.01198935\n",
      "Iteration 1146, loss = 0.01196717\n",
      "Iteration 1147, loss = 0.01194464\n",
      "Iteration 1148, loss = 0.01192197\n",
      "Iteration 1149, loss = 0.01189922\n",
      "Iteration 1150, loss = 0.01187713\n",
      "Iteration 1151, loss = 0.01185473\n",
      "Iteration 1152, loss = 0.01183229\n",
      "Iteration 1153, loss = 0.01181005\n",
      "Iteration 1154, loss = 0.01178816\n",
      "Iteration 1155, loss = 0.01176574\n",
      "Iteration 1156, loss = 0.01174372\n",
      "Iteration 1157, loss = 0.01172181\n",
      "Iteration 1158, loss = 0.01169960\n",
      "Iteration 1159, loss = 0.01167762\n",
      "Iteration 1160, loss = 0.01165594\n",
      "Iteration 1161, loss = 0.01163379\n",
      "Iteration 1162, loss = 0.01161192\n",
      "Iteration 1163, loss = 0.01159039\n",
      "Iteration 1164, loss = 0.01156870\n",
      "Iteration 1165, loss = 0.01154704\n",
      "Iteration 1166, loss = 0.01152528\n",
      "Iteration 1167, loss = 0.01150364\n",
      "Iteration 1168, loss = 0.01148235\n",
      "Iteration 1169, loss = 0.01146078\n",
      "Iteration 1170, loss = 0.01143944\n",
      "Iteration 1171, loss = 0.01141849\n",
      "Iteration 1172, loss = 0.01139725\n",
      "Iteration 1173, loss = 0.01137620\n",
      "Iteration 1174, loss = 0.01135530\n",
      "Iteration 1175, loss = 0.01133432\n",
      "Iteration 1176, loss = 0.01131339\n",
      "Iteration 1177, loss = 0.01129248\n",
      "Iteration 1178, loss = 0.01127181\n",
      "Iteration 1179, loss = 0.01125090\n",
      "Iteration 1180, loss = 0.01123045\n",
      "Iteration 1181, loss = 0.01120988\n",
      "Iteration 1182, loss = 0.01118897\n",
      "Iteration 1183, loss = 0.01116868\n",
      "Iteration 1184, loss = 0.01114805\n",
      "Iteration 1185, loss = 0.01112765\n",
      "Iteration 1186, loss = 0.01110750\n",
      "Iteration 1187, loss = 0.01108733\n",
      "Iteration 1188, loss = 0.01106705\n",
      "Iteration 1189, loss = 0.01104693\n",
      "Iteration 1190, loss = 0.01102703\n",
      "Iteration 1191, loss = 0.01100713\n",
      "Iteration 1192, loss = 0.01098719\n",
      "Iteration 1193, loss = 0.01096720\n",
      "Iteration 1194, loss = 0.01094738\n",
      "Iteration 1195, loss = 0.01092768\n",
      "Iteration 1196, loss = 0.01090791\n",
      "Iteration 1197, loss = 0.01088829\n",
      "Iteration 1198, loss = 0.01086888\n",
      "Iteration 1199, loss = 0.01084929\n",
      "Iteration 1200, loss = 0.01082974\n",
      "Iteration 1201, loss = 0.01081057\n",
      "Iteration 1202, loss = 0.01079111\n",
      "Iteration 1203, loss = 0.01077201\n",
      "Iteration 1204, loss = 0.01075263\n",
      "Iteration 1205, loss = 0.01073336\n",
      "Iteration 1206, loss = 0.01071413\n",
      "Iteration 1207, loss = 0.01069492\n",
      "Iteration 1208, loss = 0.01067589\n",
      "Iteration 1209, loss = 0.01065703\n",
      "Iteration 1210, loss = 0.01063798\n",
      "Iteration 1211, loss = 0.01061913\n",
      "Iteration 1212, loss = 0.01060030\n",
      "Iteration 1213, loss = 0.01058159\n",
      "Iteration 1214, loss = 0.01056288\n",
      "Iteration 1215, loss = 0.01054406\n",
      "Iteration 1216, loss = 0.01052537\n",
      "Iteration 1217, loss = 0.01050679\n",
      "Iteration 1218, loss = 0.01048827\n",
      "Iteration 1219, loss = 0.01046956\n",
      "Iteration 1220, loss = 0.01045111\n",
      "Iteration 1221, loss = 0.01043289\n",
      "Iteration 1222, loss = 0.01041434\n",
      "Iteration 1223, loss = 0.01039632\n",
      "Iteration 1224, loss = 0.01037750\n",
      "Iteration 1225, loss = 0.01035958\n",
      "Iteration 1226, loss = 0.01034118\n",
      "Iteration 1227, loss = 0.01032308\n",
      "Iteration 1228, loss = 0.01030468\n",
      "Iteration 1229, loss = 0.01028683\n",
      "Iteration 1230, loss = 0.01026882\n",
      "Iteration 1231, loss = 0.01025093\n",
      "Iteration 1232, loss = 0.01023317\n",
      "Iteration 1233, loss = 0.01021516\n",
      "Iteration 1234, loss = 0.01019738\n",
      "Iteration 1235, loss = 0.01017972\n",
      "Iteration 1236, loss = 0.01016195\n",
      "Iteration 1237, loss = 0.01014421\n",
      "Iteration 1238, loss = 0.01012659\n",
      "Iteration 1239, loss = 0.01010894\n",
      "Iteration 1240, loss = 0.01009161\n",
      "Iteration 1241, loss = 0.01007404\n",
      "Iteration 1242, loss = 0.01005661\n",
      "Iteration 1243, loss = 0.01003917\n",
      "Iteration 1244, loss = 0.01002196\n",
      "Iteration 1245, loss = 0.01000451\n",
      "Iteration 1246, loss = 0.00998744\n",
      "Iteration 1247, loss = 0.00997048\n",
      "Iteration 1248, loss = 0.00995329\n",
      "Iteration 1249, loss = 0.00993621\n",
      "Iteration 1250, loss = 0.00991923\n",
      "Iteration 1251, loss = 0.00990218\n",
      "Iteration 1252, loss = 0.00988537\n",
      "Iteration 1253, loss = 0.00986842\n",
      "Iteration 1254, loss = 0.00985162\n",
      "Iteration 1255, loss = 0.00983485\n",
      "Iteration 1256, loss = 0.00981807\n",
      "Iteration 1257, loss = 0.00980117\n",
      "Iteration 1258, loss = 0.00978464\n",
      "Iteration 1259, loss = 0.00976778\n",
      "Iteration 1260, loss = 0.00975117\n",
      "Iteration 1261, loss = 0.00973477\n",
      "Iteration 1262, loss = 0.00971833\n",
      "Iteration 1263, loss = 0.00970157\n",
      "Iteration 1264, loss = 0.00968531\n",
      "Iteration 1265, loss = 0.00966889\n",
      "Iteration 1266, loss = 0.00965247\n",
      "Iteration 1267, loss = 0.00963613\n",
      "Iteration 1268, loss = 0.00961989\n",
      "Iteration 1269, loss = 0.00960359\n",
      "Iteration 1270, loss = 0.00958738\n",
      "Iteration 1271, loss = 0.00957136\n",
      "Iteration 1272, loss = 0.00955518\n",
      "Iteration 1273, loss = 0.00953916\n",
      "Iteration 1274, loss = 0.00952307\n",
      "Iteration 1275, loss = 0.00950708\n",
      "Iteration 1276, loss = 0.00949113\n",
      "Iteration 1277, loss = 0.00947522\n",
      "Iteration 1278, loss = 0.00945914\n",
      "Iteration 1279, loss = 0.00944341\n",
      "Iteration 1280, loss = 0.00942751\n",
      "Iteration 1281, loss = 0.00941174\n",
      "Iteration 1282, loss = 0.00939606\n",
      "Iteration 1283, loss = 0.00938030\n",
      "Iteration 1284, loss = 0.00936467\n",
      "Iteration 1285, loss = 0.00934923\n",
      "Iteration 1286, loss = 0.00933368\n",
      "Iteration 1287, loss = 0.00931811\n",
      "Iteration 1288, loss = 0.00930266\n",
      "Iteration 1289, loss = 0.00928728\n",
      "Iteration 1290, loss = 0.00927188\n",
      "Iteration 1291, loss = 0.00925648\n",
      "Iteration 1292, loss = 0.00924142\n",
      "Iteration 1293, loss = 0.00922611\n",
      "Iteration 1294, loss = 0.00921092\n",
      "Iteration 1295, loss = 0.00919579\n",
      "Iteration 1296, loss = 0.00918064\n",
      "Iteration 1297, loss = 0.00916537\n",
      "Iteration 1298, loss = 0.00915018\n",
      "Iteration 1299, loss = 0.00913519\n",
      "Iteration 1300, loss = 0.00912022\n",
      "Iteration 1301, loss = 0.00910529\n",
      "Iteration 1302, loss = 0.00909028\n",
      "Iteration 1303, loss = 0.00907533\n",
      "Iteration 1304, loss = 0.00906037\n",
      "Iteration 1305, loss = 0.00904560\n",
      "Iteration 1306, loss = 0.00903087\n",
      "Iteration 1307, loss = 0.00901604\n",
      "Iteration 1308, loss = 0.00900124\n",
      "Iteration 1309, loss = 0.00898660\n",
      "Iteration 1310, loss = 0.00897203\n",
      "Iteration 1311, loss = 0.00895748\n",
      "Iteration 1312, loss = 0.00894290\n",
      "Iteration 1313, loss = 0.00892839\n",
      "Iteration 1314, loss = 0.00891396\n",
      "Iteration 1315, loss = 0.00889953\n",
      "Iteration 1316, loss = 0.00888511\n",
      "Iteration 1317, loss = 0.00887062\n",
      "Iteration 1318, loss = 0.00885630\n",
      "Iteration 1319, loss = 0.00884213\n",
      "Iteration 1320, loss = 0.00882760\n",
      "Iteration 1321, loss = 0.00881351\n",
      "Iteration 1322, loss = 0.00879926\n",
      "Iteration 1323, loss = 0.00878504\n",
      "Iteration 1324, loss = 0.00877099\n",
      "Iteration 1325, loss = 0.00875704\n",
      "Iteration 1326, loss = 0.00874286\n",
      "Iteration 1327, loss = 0.00872864\n",
      "Iteration 1328, loss = 0.00871463\n",
      "Iteration 1329, loss = 0.00870079\n",
      "Iteration 1330, loss = 0.00868678\n",
      "Iteration 1331, loss = 0.00867294\n",
      "Iteration 1332, loss = 0.00865878\n",
      "Iteration 1333, loss = 0.00864481\n",
      "Iteration 1334, loss = 0.00863109\n",
      "Iteration 1335, loss = 0.00861713\n",
      "Iteration 1336, loss = 0.00860327\n",
      "Iteration 1337, loss = 0.00858962\n",
      "Iteration 1338, loss = 0.00857567\n",
      "Iteration 1339, loss = 0.00856218\n",
      "Iteration 1340, loss = 0.00854853\n",
      "Iteration 1341, loss = 0.00853466\n",
      "Iteration 1342, loss = 0.00852136\n",
      "Iteration 1343, loss = 0.00850776\n",
      "Iteration 1344, loss = 0.00849420\n",
      "Iteration 1345, loss = 0.00848068\n",
      "Iteration 1346, loss = 0.00846737\n",
      "Iteration 1347, loss = 0.00845392\n",
      "Iteration 1348, loss = 0.00844043\n",
      "Iteration 1349, loss = 0.00842717\n",
      "Iteration 1350, loss = 0.00841397\n",
      "Iteration 1351, loss = 0.00840082\n",
      "Iteration 1352, loss = 0.00838757\n",
      "Iteration 1353, loss = 0.00837448\n",
      "Iteration 1354, loss = 0.00836111\n",
      "Iteration 1355, loss = 0.00834788\n",
      "Iteration 1356, loss = 0.00833501\n",
      "Iteration 1357, loss = 0.00832183\n",
      "Iteration 1358, loss = 0.00830872\n",
      "Iteration 1359, loss = 0.00829580\n",
      "Iteration 1360, loss = 0.00828280\n",
      "Iteration 1361, loss = 0.00827001\n",
      "Iteration 1362, loss = 0.00825699\n",
      "Iteration 1363, loss = 0.00824415\n",
      "Iteration 1364, loss = 0.00823137\n",
      "Iteration 1365, loss = 0.00821844\n",
      "Iteration 1366, loss = 0.00820572\n",
      "Iteration 1367, loss = 0.00819295\n",
      "Iteration 1368, loss = 0.00818019\n",
      "Iteration 1369, loss = 0.00816755\n",
      "Iteration 1370, loss = 0.00815498\n",
      "Iteration 1371, loss = 0.00814221\n",
      "Iteration 1372, loss = 0.00812966\n",
      "Iteration 1373, loss = 0.00811706\n",
      "Iteration 1374, loss = 0.00810455\n",
      "Iteration 1375, loss = 0.00809201\n",
      "Iteration 1376, loss = 0.00807935\n",
      "Iteration 1377, loss = 0.00806707\n",
      "Iteration 1378, loss = 0.00805473\n",
      "Iteration 1379, loss = 0.00804209\n",
      "Iteration 1380, loss = 0.00802967\n",
      "Iteration 1381, loss = 0.00801739\n",
      "Iteration 1382, loss = 0.00800485\n",
      "Iteration 1383, loss = 0.00799264\n",
      "Iteration 1384, loss = 0.00798042\n",
      "Iteration 1385, loss = 0.00796790\n",
      "Iteration 1386, loss = 0.00795585\n",
      "Iteration 1387, loss = 0.00794369\n",
      "Iteration 1388, loss = 0.00793147\n",
      "Iteration 1389, loss = 0.00791916\n",
      "Iteration 1390, loss = 0.00790701\n",
      "Iteration 1391, loss = 0.00789503\n",
      "Iteration 1392, loss = 0.00788299\n",
      "Iteration 1393, loss = 0.00787096\n",
      "Iteration 1394, loss = 0.00785896\n",
      "Iteration 1395, loss = 0.00784713\n",
      "Iteration 1396, loss = 0.00783521\n",
      "Iteration 1397, loss = 0.00782334\n",
      "Iteration 1398, loss = 0.00781145\n",
      "Iteration 1399, loss = 0.00779969\n",
      "Iteration 1400, loss = 0.00778784\n",
      "Iteration 1401, loss = 0.00777606\n",
      "Iteration 1402, loss = 0.00776429\n",
      "Iteration 1403, loss = 0.00775258\n",
      "Iteration 1404, loss = 0.00774095\n",
      "Iteration 1405, loss = 0.00772902\n",
      "Iteration 1406, loss = 0.00771750\n",
      "Iteration 1407, loss = 0.00770596\n",
      "Iteration 1408, loss = 0.00769434\n",
      "Iteration 1409, loss = 0.00768277\n",
      "Iteration 1410, loss = 0.00767112\n",
      "Iteration 1411, loss = 0.00765955\n",
      "Iteration 1412, loss = 0.00764798\n",
      "Iteration 1413, loss = 0.00763648\n",
      "Iteration 1414, loss = 0.00762479\n",
      "Iteration 1415, loss = 0.00761359\n",
      "Iteration 1416, loss = 0.00760207\n",
      "Iteration 1417, loss = 0.00759069\n",
      "Iteration 1418, loss = 0.00757937\n",
      "Iteration 1419, loss = 0.00756800\n",
      "Iteration 1420, loss = 0.00755666\n",
      "Iteration 1421, loss = 0.00754548\n",
      "Iteration 1422, loss = 0.00753424\n",
      "Iteration 1423, loss = 0.00752290\n",
      "Iteration 1424, loss = 0.00751179\n",
      "Iteration 1425, loss = 0.00750067\n",
      "Iteration 1426, loss = 0.00748948\n",
      "Iteration 1427, loss = 0.00747842\n",
      "Iteration 1428, loss = 0.00746732\n",
      "Iteration 1429, loss = 0.00745626\n",
      "Iteration 1430, loss = 0.00744534\n",
      "Iteration 1431, loss = 0.00743428\n",
      "Iteration 1432, loss = 0.00742318\n",
      "Iteration 1433, loss = 0.00741226\n",
      "Iteration 1434, loss = 0.00740142\n",
      "Iteration 1435, loss = 0.00739024\n",
      "Iteration 1436, loss = 0.00737950\n",
      "Iteration 1437, loss = 0.00736855\n",
      "Iteration 1438, loss = 0.00735766\n",
      "Iteration 1439, loss = 0.00734689\n",
      "Iteration 1440, loss = 0.00733605\n",
      "Iteration 1441, loss = 0.00732534\n",
      "Iteration 1442, loss = 0.00731450\n",
      "Iteration 1443, loss = 0.00730371\n",
      "Iteration 1444, loss = 0.00729313\n",
      "Iteration 1445, loss = 0.00728251\n",
      "Iteration 1446, loss = 0.00727181\n",
      "Iteration 1447, loss = 0.00726121\n",
      "Iteration 1448, loss = 0.00725068\n",
      "Iteration 1449, loss = 0.00724016\n",
      "Iteration 1450, loss = 0.00722965\n",
      "Iteration 1451, loss = 0.00721909\n",
      "Iteration 1452, loss = 0.00720857\n",
      "Iteration 1453, loss = 0.00719816\n",
      "Iteration 1454, loss = 0.00718763\n",
      "Iteration 1455, loss = 0.00717729\n",
      "Iteration 1456, loss = 0.00716682\n",
      "Iteration 1457, loss = 0.00715646\n",
      "Iteration 1458, loss = 0.00714601\n",
      "Iteration 1459, loss = 0.00713565\n",
      "Iteration 1460, loss = 0.00712547\n",
      "Iteration 1461, loss = 0.00711506\n",
      "Iteration 1462, loss = 0.00710484\n",
      "Iteration 1463, loss = 0.00709452\n",
      "Iteration 1464, loss = 0.00708443\n",
      "Iteration 1465, loss = 0.00707410\n",
      "Iteration 1466, loss = 0.00706395\n",
      "Iteration 1467, loss = 0.00705382\n",
      "Iteration 1468, loss = 0.00704350\n",
      "Iteration 1469, loss = 0.00703338\n",
      "Iteration 1470, loss = 0.00702344\n",
      "Iteration 1471, loss = 0.00701327\n",
      "Iteration 1472, loss = 0.00700320\n",
      "Iteration 1473, loss = 0.00699325\n",
      "Iteration 1474, loss = 0.00698304\n",
      "Iteration 1475, loss = 0.00697315\n",
      "Iteration 1476, loss = 0.00696329\n",
      "Iteration 1477, loss = 0.00695335\n",
      "Iteration 1478, loss = 0.00694346\n",
      "Iteration 1479, loss = 0.00693352\n",
      "Iteration 1480, loss = 0.00692367\n",
      "Iteration 1481, loss = 0.00691378\n",
      "Iteration 1482, loss = 0.00690411\n",
      "Iteration 1483, loss = 0.00689420\n",
      "Iteration 1484, loss = 0.00688447\n",
      "Iteration 1485, loss = 0.00687482\n",
      "Iteration 1486, loss = 0.00686507\n",
      "Iteration 1487, loss = 0.00685523\n",
      "Iteration 1488, loss = 0.00684560\n",
      "Iteration 1489, loss = 0.00683600\n",
      "Iteration 1490, loss = 0.00682635\n",
      "Iteration 1491, loss = 0.00681666\n",
      "Iteration 1492, loss = 0.00680704\n",
      "Iteration 1493, loss = 0.00679744\n",
      "Iteration 1494, loss = 0.00678781\n",
      "Iteration 1495, loss = 0.00677835\n",
      "Iteration 1496, loss = 0.00676868\n",
      "Iteration 1497, loss = 0.00675921\n",
      "Iteration 1498, loss = 0.00674968\n",
      "Iteration 1499, loss = 0.00674013\n",
      "Iteration 1500, loss = 0.00673073\n",
      "Iteration 1501, loss = 0.00672135\n",
      "Iteration 1502, loss = 0.00671182\n",
      "Iteration 1503, loss = 0.00670257\n",
      "Iteration 1504, loss = 0.00669320\n",
      "Iteration 1505, loss = 0.00668392\n",
      "Iteration 1506, loss = 0.00667455\n",
      "Iteration 1507, loss = 0.00666527\n",
      "Iteration 1508, loss = 0.00665607\n",
      "Iteration 1509, loss = 0.00664671\n",
      "Iteration 1510, loss = 0.00663756\n",
      "Iteration 1511, loss = 0.00662834\n",
      "Iteration 1512, loss = 0.00661911\n",
      "Iteration 1513, loss = 0.00660990\n",
      "Iteration 1514, loss = 0.00660077\n",
      "Iteration 1515, loss = 0.00659166\n",
      "Iteration 1516, loss = 0.00658243\n",
      "Iteration 1517, loss = 0.00657341\n",
      "Iteration 1518, loss = 0.00656434\n",
      "Iteration 1519, loss = 0.00655520\n",
      "Iteration 1520, loss = 0.00654618\n",
      "Iteration 1521, loss = 0.00653720\n",
      "Iteration 1522, loss = 0.00652811\n",
      "Iteration 1523, loss = 0.00651917\n",
      "Iteration 1524, loss = 0.00651019\n",
      "Iteration 1525, loss = 0.00650126\n",
      "Iteration 1526, loss = 0.00649210\n",
      "Iteration 1527, loss = 0.00648326\n",
      "Iteration 1528, loss = 0.00647437\n",
      "Iteration 1529, loss = 0.00646544\n",
      "Iteration 1530, loss = 0.00645656\n",
      "Iteration 1531, loss = 0.00644777\n",
      "Iteration 1532, loss = 0.00643886\n",
      "Iteration 1533, loss = 0.00643022\n",
      "Iteration 1534, loss = 0.00642129\n",
      "Iteration 1535, loss = 0.00641269\n",
      "Iteration 1536, loss = 0.00640379\n",
      "Iteration 1537, loss = 0.00639514\n",
      "Iteration 1538, loss = 0.00638640\n",
      "Iteration 1539, loss = 0.00637769\n",
      "Iteration 1540, loss = 0.00636905\n",
      "Iteration 1541, loss = 0.00636030\n",
      "Iteration 1542, loss = 0.00635167\n",
      "Iteration 1543, loss = 0.00634306\n",
      "Iteration 1544, loss = 0.00633443\n",
      "Iteration 1545, loss = 0.00632578\n",
      "Iteration 1546, loss = 0.00631724\n",
      "Iteration 1547, loss = 0.00630871\n",
      "Iteration 1548, loss = 0.00630020\n",
      "Iteration 1549, loss = 0.00629164\n",
      "Iteration 1550, loss = 0.00628305\n",
      "Iteration 1551, loss = 0.00627462\n",
      "Iteration 1552, loss = 0.00626620\n",
      "Iteration 1553, loss = 0.00625776\n",
      "Iteration 1554, loss = 0.00624942\n",
      "Iteration 1555, loss = 0.00624096\n",
      "Iteration 1556, loss = 0.00623254\n",
      "Iteration 1557, loss = 0.00622421\n",
      "Iteration 1558, loss = 0.00621577\n",
      "Iteration 1559, loss = 0.00620753\n",
      "Iteration 1560, loss = 0.00619910\n",
      "Iteration 1561, loss = 0.00619080\n",
      "Iteration 1562, loss = 0.00618239\n",
      "Iteration 1563, loss = 0.00617418\n",
      "Iteration 1564, loss = 0.00616591\n",
      "Iteration 1565, loss = 0.00615771\n",
      "Iteration 1566, loss = 0.00614957\n",
      "Iteration 1567, loss = 0.00614123\n",
      "Iteration 1568, loss = 0.00613317\n",
      "Iteration 1569, loss = 0.00612489\n",
      "Iteration 1570, loss = 0.00611685\n",
      "Iteration 1571, loss = 0.00610865\n",
      "Iteration 1572, loss = 0.00610053\n",
      "Iteration 1573, loss = 0.00609250\n",
      "Iteration 1574, loss = 0.00608427\n",
      "Iteration 1575, loss = 0.00607621\n",
      "Iteration 1576, loss = 0.00606819\n",
      "Iteration 1577, loss = 0.00606021\n",
      "Iteration 1578, loss = 0.00605203\n",
      "Iteration 1579, loss = 0.00604393\n",
      "Iteration 1580, loss = 0.00603610\n",
      "Iteration 1581, loss = 0.00602800\n",
      "Iteration 1582, loss = 0.00601998\n",
      "Iteration 1583, loss = 0.00601212\n",
      "Iteration 1584, loss = 0.00600418\n",
      "Iteration 1585, loss = 0.00599630\n",
      "Iteration 1586, loss = 0.00598838\n",
      "Iteration 1587, loss = 0.00598036\n",
      "Iteration 1588, loss = 0.00597253\n",
      "Iteration 1589, loss = 0.00596466\n",
      "Iteration 1590, loss = 0.00595678\n",
      "Iteration 1591, loss = 0.00594896\n",
      "Iteration 1592, loss = 0.00594110\n",
      "Iteration 1593, loss = 0.00593328\n",
      "Iteration 1594, loss = 0.00592541\n",
      "Iteration 1595, loss = 0.00591767\n",
      "Iteration 1596, loss = 0.00591004\n",
      "Iteration 1597, loss = 0.00590215\n",
      "Iteration 1598, loss = 0.00589457\n",
      "Iteration 1599, loss = 0.00588677\n",
      "Iteration 1600, loss = 0.00587912\n",
      "Iteration 1601, loss = 0.00587142\n",
      "Iteration 1602, loss = 0.00586385\n",
      "Iteration 1603, loss = 0.00585611\n",
      "Iteration 1604, loss = 0.00584848\n",
      "Iteration 1605, loss = 0.00584088\n",
      "Iteration 1606, loss = 0.00583336\n",
      "Iteration 1607, loss = 0.00582577\n",
      "Iteration 1608, loss = 0.00581832\n",
      "Iteration 1609, loss = 0.00581066\n",
      "Iteration 1610, loss = 0.00580318\n",
      "Iteration 1611, loss = 0.00579557\n",
      "Iteration 1612, loss = 0.00578815\n",
      "Iteration 1613, loss = 0.00578055\n",
      "Iteration 1614, loss = 0.00577300\n",
      "Iteration 1615, loss = 0.00576548\n",
      "Iteration 1616, loss = 0.00575802\n",
      "Iteration 1617, loss = 0.00575068\n",
      "Iteration 1618, loss = 0.00574311\n",
      "Iteration 1619, loss = 0.00573576\n",
      "Iteration 1620, loss = 0.00572829\n",
      "Iteration 1621, loss = 0.00572095\n",
      "Iteration 1622, loss = 0.00571356\n",
      "Iteration 1623, loss = 0.00570628\n",
      "Iteration 1624, loss = 0.00569883\n",
      "Iteration 1625, loss = 0.00569161\n",
      "Iteration 1626, loss = 0.00568431\n",
      "Iteration 1627, loss = 0.00567707\n",
      "Iteration 1628, loss = 0.00566973\n",
      "Iteration 1629, loss = 0.00566236\n",
      "Iteration 1630, loss = 0.00565507\n",
      "Iteration 1631, loss = 0.00564782\n",
      "Iteration 1632, loss = 0.00564060\n",
      "Iteration 1633, loss = 0.00563332\n",
      "Iteration 1634, loss = 0.00562610\n",
      "Iteration 1635, loss = 0.00561896\n",
      "Iteration 1636, loss = 0.00561172\n",
      "Iteration 1637, loss = 0.00560455\n",
      "Iteration 1638, loss = 0.00559739\n",
      "Iteration 1639, loss = 0.00559025\n",
      "Iteration 1640, loss = 0.00558319\n",
      "Iteration 1641, loss = 0.00557609\n",
      "Iteration 1642, loss = 0.00556900\n",
      "Iteration 1643, loss = 0.00556197\n",
      "Iteration 1644, loss = 0.00555495\n",
      "Iteration 1645, loss = 0.00554788\n",
      "Iteration 1646, loss = 0.00554087\n",
      "Iteration 1647, loss = 0.00553385\n",
      "Iteration 1648, loss = 0.00552689\n",
      "Iteration 1649, loss = 0.00551993\n",
      "Iteration 1650, loss = 0.00551296\n",
      "Iteration 1651, loss = 0.00550593\n",
      "Iteration 1652, loss = 0.00549901\n",
      "Iteration 1653, loss = 0.00549218\n",
      "Iteration 1654, loss = 0.00548532\n",
      "Iteration 1655, loss = 0.00547841\n",
      "Iteration 1656, loss = 0.00547152\n",
      "Iteration 1657, loss = 0.00546472\n",
      "Iteration 1658, loss = 0.00545785\n",
      "Iteration 1659, loss = 0.00545112\n",
      "Iteration 1660, loss = 0.00544430\n",
      "Iteration 1661, loss = 0.00543749\n",
      "Iteration 1662, loss = 0.00543077\n",
      "Iteration 1663, loss = 0.00542406\n",
      "Iteration 1664, loss = 0.00541731\n",
      "Iteration 1665, loss = 0.00541056\n",
      "Iteration 1666, loss = 0.00540386\n",
      "Iteration 1667, loss = 0.00539710\n",
      "Iteration 1668, loss = 0.00539053\n",
      "Iteration 1669, loss = 0.00538386\n",
      "Iteration 1670, loss = 0.00537715\n",
      "Iteration 1671, loss = 0.00537063\n",
      "Iteration 1672, loss = 0.00536395\n",
      "Iteration 1673, loss = 0.00535732\n",
      "Iteration 1674, loss = 0.00535073\n",
      "Iteration 1675, loss = 0.00534418\n",
      "Iteration 1676, loss = 0.00533751\n",
      "Iteration 1677, loss = 0.00533100\n",
      "Iteration 1678, loss = 0.00532438\n",
      "Iteration 1679, loss = 0.00531787\n",
      "Iteration 1680, loss = 0.00531130\n",
      "Iteration 1681, loss = 0.00530478\n",
      "Iteration 1682, loss = 0.00529827\n",
      "Iteration 1683, loss = 0.00529181\n",
      "Iteration 1684, loss = 0.00528528\n",
      "Iteration 1685, loss = 0.00527876\n",
      "Iteration 1686, loss = 0.00527228\n",
      "Iteration 1687, loss = 0.00526590\n",
      "Iteration 1688, loss = 0.00525938\n",
      "Iteration 1689, loss = 0.00525292\n",
      "Iteration 1690, loss = 0.00524661\n",
      "Iteration 1691, loss = 0.00524009\n",
      "Iteration 1692, loss = 0.00523370\n",
      "Iteration 1693, loss = 0.00522737\n",
      "Iteration 1694, loss = 0.00522093\n",
      "Iteration 1695, loss = 0.00521459\n",
      "Iteration 1696, loss = 0.00520824\n",
      "Iteration 1697, loss = 0.00520197\n",
      "Iteration 1698, loss = 0.00519552\n",
      "Iteration 1699, loss = 0.00518924\n",
      "Iteration 1700, loss = 0.00518301\n",
      "Iteration 1701, loss = 0.00517670\n",
      "Iteration 1702, loss = 0.00517036\n",
      "Iteration 1703, loss = 0.00516413\n",
      "Iteration 1704, loss = 0.00515788\n",
      "Iteration 1705, loss = 0.00515169\n",
      "Iteration 1706, loss = 0.00514546\n",
      "Iteration 1707, loss = 0.00513926\n",
      "Iteration 1708, loss = 0.00513309\n",
      "Iteration 1709, loss = 0.00512694\n",
      "Iteration 1710, loss = 0.00512075\n",
      "Iteration 1711, loss = 0.00511460\n",
      "Iteration 1712, loss = 0.00510852\n",
      "Iteration 1713, loss = 0.00510231\n",
      "Iteration 1714, loss = 0.00509619\n",
      "Iteration 1715, loss = 0.00509019\n",
      "Iteration 1716, loss = 0.00508407\n",
      "Iteration 1717, loss = 0.00507800\n",
      "Iteration 1718, loss = 0.00507190\n",
      "Iteration 1719, loss = 0.00506579\n",
      "Iteration 1720, loss = 0.00505980\n",
      "Iteration 1721, loss = 0.00505376\n",
      "Iteration 1722, loss = 0.00504778\n",
      "Iteration 1723, loss = 0.00504173\n",
      "Iteration 1724, loss = 0.00503573\n",
      "Iteration 1725, loss = 0.00502967\n",
      "Iteration 1726, loss = 0.00502371\n",
      "Iteration 1727, loss = 0.00501781\n",
      "Iteration 1728, loss = 0.00501182\n",
      "Iteration 1729, loss = 0.00500594\n",
      "Iteration 1730, loss = 0.00499998\n",
      "Iteration 1731, loss = 0.00499416\n",
      "Iteration 1732, loss = 0.00498820\n",
      "Iteration 1733, loss = 0.00498227\n",
      "Iteration 1734, loss = 0.00497636\n",
      "Iteration 1735, loss = 0.00497054\n",
      "Iteration 1736, loss = 0.00496463\n",
      "Iteration 1737, loss = 0.00495883\n",
      "Iteration 1738, loss = 0.00495297\n",
      "Iteration 1739, loss = 0.00494718\n",
      "Iteration 1740, loss = 0.00494128\n",
      "Iteration 1741, loss = 0.00493547\n",
      "Iteration 1742, loss = 0.00492968\n",
      "Iteration 1743, loss = 0.00492386\n",
      "Iteration 1744, loss = 0.00491807\n",
      "Iteration 1745, loss = 0.00491233\n",
      "Iteration 1746, loss = 0.00490648\n",
      "Iteration 1747, loss = 0.00490077\n",
      "Iteration 1748, loss = 0.00489509\n",
      "Iteration 1749, loss = 0.00488932\n",
      "Iteration 1750, loss = 0.00488358\n",
      "Iteration 1751, loss = 0.00487790\n",
      "Iteration 1752, loss = 0.00487223\n",
      "Iteration 1753, loss = 0.00486648\n",
      "Iteration 1754, loss = 0.00486079\n",
      "Iteration 1755, loss = 0.00485523\n",
      "Iteration 1756, loss = 0.00484950\n",
      "Iteration 1757, loss = 0.00484382\n",
      "Iteration 1758, loss = 0.00483822\n",
      "Iteration 1759, loss = 0.00483267\n",
      "Iteration 1760, loss = 0.00482696\n",
      "Iteration 1761, loss = 0.00482134\n",
      "Iteration 1762, loss = 0.00481572\n",
      "Iteration 1763, loss = 0.00481016\n",
      "Iteration 1764, loss = 0.00480454\n",
      "Iteration 1765, loss = 0.00479901\n",
      "Iteration 1766, loss = 0.00479344\n",
      "Iteration 1767, loss = 0.00478784\n",
      "Iteration 1768, loss = 0.00478229\n",
      "Iteration 1769, loss = 0.00477679\n",
      "Iteration 1770, loss = 0.00477130\n",
      "Iteration 1771, loss = 0.00476573\n",
      "Iteration 1772, loss = 0.00476023\n",
      "Iteration 1773, loss = 0.00475479\n",
      "Iteration 1774, loss = 0.00474930\n",
      "Iteration 1775, loss = 0.00474383\n",
      "Iteration 1776, loss = 0.00473841\n",
      "Iteration 1777, loss = 0.00473297\n",
      "Iteration 1778, loss = 0.00472753\n",
      "Iteration 1779, loss = 0.00472209\n",
      "Iteration 1780, loss = 0.00471676\n",
      "Iteration 1781, loss = 0.00471136\n",
      "Iteration 1782, loss = 0.00470598\n",
      "Iteration 1783, loss = 0.00470059\n",
      "Iteration 1784, loss = 0.00469517\n",
      "Iteration 1785, loss = 0.00468983\n",
      "Iteration 1786, loss = 0.00468457\n",
      "Iteration 1787, loss = 0.00467914\n",
      "Iteration 1788, loss = 0.00467379\n",
      "Iteration 1789, loss = 0.00466845\n",
      "Iteration 1790, loss = 0.00466320\n",
      "Iteration 1791, loss = 0.00465784\n",
      "Iteration 1792, loss = 0.00465257\n",
      "Iteration 1793, loss = 0.00464729\n",
      "Iteration 1794, loss = 0.00464208\n",
      "Iteration 1795, loss = 0.00463675\n",
      "Iteration 1796, loss = 0.00463161\n",
      "Iteration 1797, loss = 0.00462629\n",
      "Iteration 1798, loss = 0.00462106\n",
      "Iteration 1799, loss = 0.00461582\n",
      "Iteration 1800, loss = 0.00461063\n",
      "Iteration 1801, loss = 0.00460543\n",
      "Iteration 1802, loss = 0.00460016\n",
      "Iteration 1803, loss = 0.00459500\n",
      "Iteration 1804, loss = 0.00458983\n",
      "Iteration 1805, loss = 0.00458457\n",
      "Iteration 1806, loss = 0.00457943\n",
      "Iteration 1807, loss = 0.00457436\n",
      "Iteration 1808, loss = 0.00456914\n",
      "Iteration 1809, loss = 0.00456405\n",
      "Iteration 1810, loss = 0.00455889\n",
      "Iteration 1811, loss = 0.00455389\n",
      "Iteration 1812, loss = 0.00454875\n",
      "Iteration 1813, loss = 0.00454363\n",
      "Iteration 1814, loss = 0.00453851\n",
      "Iteration 1815, loss = 0.00453347\n",
      "Iteration 1816, loss = 0.00452840\n",
      "Iteration 1817, loss = 0.00452332\n",
      "Iteration 1818, loss = 0.00451830\n",
      "Iteration 1819, loss = 0.00451326\n",
      "Iteration 1820, loss = 0.00450816\n",
      "Iteration 1821, loss = 0.00450323\n",
      "Iteration 1822, loss = 0.00449825\n",
      "Iteration 1823, loss = 0.00449322\n",
      "Iteration 1824, loss = 0.00448818\n",
      "Iteration 1825, loss = 0.00448331\n",
      "Iteration 1826, loss = 0.00447823\n",
      "Iteration 1827, loss = 0.00447330\n",
      "Iteration 1828, loss = 0.00446835\n",
      "Iteration 1829, loss = 0.00446343\n",
      "Iteration 1830, loss = 0.00445848\n",
      "Iteration 1831, loss = 0.00445351\n",
      "Iteration 1832, loss = 0.00444870\n",
      "Iteration 1833, loss = 0.00444380\n",
      "Iteration 1834, loss = 0.00443889\n",
      "Iteration 1835, loss = 0.00443397\n",
      "Iteration 1836, loss = 0.00442911\n",
      "Iteration 1837, loss = 0.00442425\n",
      "Iteration 1838, loss = 0.00441937\n",
      "Iteration 1839, loss = 0.00441457\n",
      "Iteration 1840, loss = 0.00440969\n",
      "Iteration 1841, loss = 0.00440478\n",
      "Iteration 1842, loss = 0.00439999\n",
      "Iteration 1843, loss = 0.00439515\n",
      "Iteration 1844, loss = 0.00439035\n",
      "Iteration 1845, loss = 0.00438553\n",
      "Iteration 1846, loss = 0.00438077\n",
      "Iteration 1847, loss = 0.00437598\n",
      "Iteration 1848, loss = 0.00437123\n",
      "Iteration 1849, loss = 0.00436644\n",
      "Iteration 1850, loss = 0.00436172\n",
      "Iteration 1851, loss = 0.00435692\n",
      "Iteration 1852, loss = 0.00435219\n",
      "Iteration 1853, loss = 0.00434750\n",
      "Iteration 1854, loss = 0.00434271\n",
      "Iteration 1855, loss = 0.00433801\n",
      "Iteration 1856, loss = 0.00433326\n",
      "Iteration 1857, loss = 0.00432861\n",
      "Iteration 1858, loss = 0.00432383\n",
      "Iteration 1859, loss = 0.00431920\n",
      "Iteration 1860, loss = 0.00431447\n",
      "Iteration 1861, loss = 0.00430976\n",
      "Iteration 1862, loss = 0.00430521\n",
      "Iteration 1863, loss = 0.00430046\n",
      "Iteration 1864, loss = 0.00429584\n",
      "Iteration 1865, loss = 0.00429121\n",
      "Iteration 1866, loss = 0.00428652\n",
      "Iteration 1867, loss = 0.00428199\n",
      "Iteration 1868, loss = 0.00427730\n",
      "Iteration 1869, loss = 0.00427269\n",
      "Iteration 1870, loss = 0.00426807\n",
      "Iteration 1871, loss = 0.00426353\n",
      "Iteration 1872, loss = 0.00425887\n",
      "Iteration 1873, loss = 0.00425437\n",
      "Iteration 1874, loss = 0.00424979\n",
      "Iteration 1875, loss = 0.00424515\n",
      "Iteration 1876, loss = 0.00424067\n",
      "Iteration 1877, loss = 0.00423614\n",
      "Iteration 1878, loss = 0.00423155\n",
      "Iteration 1879, loss = 0.00422706\n",
      "Iteration 1880, loss = 0.00422246\n",
      "Iteration 1881, loss = 0.00421800\n",
      "Iteration 1882, loss = 0.00421346\n",
      "Iteration 1883, loss = 0.00420899\n",
      "Iteration 1884, loss = 0.00420454\n",
      "Iteration 1885, loss = 0.00420000\n",
      "Iteration 1886, loss = 0.00419554\n",
      "Iteration 1887, loss = 0.00419109\n",
      "Iteration 1888, loss = 0.00418658\n",
      "Iteration 1889, loss = 0.00418217\n",
      "Iteration 1890, loss = 0.00417773\n",
      "Iteration 1891, loss = 0.00417327\n",
      "Iteration 1892, loss = 0.00416886\n",
      "Iteration 1893, loss = 0.00416448\n",
      "Iteration 1894, loss = 0.00415999\n",
      "Iteration 1895, loss = 0.00415560\n",
      "Iteration 1896, loss = 0.00415121\n",
      "Iteration 1897, loss = 0.00414681\n",
      "Iteration 1898, loss = 0.00414244\n",
      "Iteration 1899, loss = 0.00413805\n",
      "Iteration 1900, loss = 0.00413371\n",
      "Iteration 1901, loss = 0.00412931\n",
      "Iteration 1902, loss = 0.00412497\n",
      "Iteration 1903, loss = 0.00412063\n",
      "Iteration 1904, loss = 0.00411631\n",
      "Iteration 1905, loss = 0.00411196\n",
      "Iteration 1906, loss = 0.00410760\n",
      "Iteration 1907, loss = 0.00410334\n",
      "Iteration 1908, loss = 0.00409899\n",
      "Iteration 1909, loss = 0.00409471\n",
      "Iteration 1910, loss = 0.00409047\n",
      "Iteration 1911, loss = 0.00408614\n",
      "Iteration 1912, loss = 0.00408183\n",
      "Iteration 1913, loss = 0.00407752\n",
      "Iteration 1914, loss = 0.00407329\n",
      "Iteration 1915, loss = 0.00406902\n",
      "Iteration 1916, loss = 0.00406472\n",
      "Iteration 1917, loss = 0.00406053\n",
      "Iteration 1918, loss = 0.00405621\n",
      "Iteration 1919, loss = 0.00405200\n",
      "Iteration 1920, loss = 0.00404781\n",
      "Iteration 1921, loss = 0.00404358\n",
      "Iteration 1922, loss = 0.00403930\n",
      "Iteration 1923, loss = 0.00403514\n",
      "Iteration 1924, loss = 0.00403093\n",
      "Iteration 1925, loss = 0.00402672\n",
      "Iteration 1926, loss = 0.00402247\n",
      "Iteration 1927, loss = 0.00401831\n",
      "Iteration 1928, loss = 0.00401411\n",
      "Iteration 1929, loss = 0.00400996\n",
      "Iteration 1930, loss = 0.00400579\n",
      "Iteration 1931, loss = 0.00400160\n",
      "Iteration 1932, loss = 0.00399749\n",
      "Iteration 1933, loss = 0.00399332\n",
      "Iteration 1934, loss = 0.00398924\n",
      "Iteration 1935, loss = 0.00398518\n",
      "Iteration 1936, loss = 0.00398104\n",
      "Iteration 1937, loss = 0.00397690\n",
      "Iteration 1938, loss = 0.00397283\n",
      "Iteration 1939, loss = 0.00396875\n",
      "Iteration 1940, loss = 0.00396466\n",
      "Iteration 1941, loss = 0.00396059\n",
      "Iteration 1942, loss = 0.00395652\n",
      "Iteration 1943, loss = 0.00395245\n",
      "Iteration 1944, loss = 0.00394841\n",
      "Iteration 1945, loss = 0.00394435\n",
      "Iteration 1946, loss = 0.00394035\n",
      "Iteration 1947, loss = 0.00393630\n",
      "Iteration 1948, loss = 0.00393224\n",
      "Iteration 1949, loss = 0.00392825\n",
      "Iteration 1950, loss = 0.00392425\n",
      "Iteration 1951, loss = 0.00392032\n",
      "Iteration 1952, loss = 0.00391627\n",
      "Iteration 1953, loss = 0.00391232\n",
      "Iteration 1954, loss = 0.00390830\n",
      "Iteration 1955, loss = 0.00390436\n",
      "Iteration 1956, loss = 0.00390038\n",
      "Iteration 1957, loss = 0.00389643\n",
      "Iteration 1958, loss = 0.00389242\n",
      "Iteration 1959, loss = 0.00388848\n",
      "Iteration 1960, loss = 0.00388451\n",
      "Iteration 1961, loss = 0.00388061\n",
      "Iteration 1962, loss = 0.00387662\n",
      "Iteration 1963, loss = 0.00387269\n",
      "Iteration 1964, loss = 0.00386889\n",
      "Iteration 1965, loss = 0.00386484\n",
      "Iteration 1966, loss = 0.00386093\n",
      "Iteration 1967, loss = 0.00385702\n",
      "Iteration 1968, loss = 0.00385311\n",
      "Iteration 1969, loss = 0.00384922\n",
      "Iteration 1970, loss = 0.00384530\n",
      "Iteration 1971, loss = 0.00384138\n",
      "Iteration 1972, loss = 0.00383752\n",
      "Iteration 1973, loss = 0.00383360\n",
      "Iteration 1974, loss = 0.00382971\n",
      "Iteration 1975, loss = 0.00382582\n",
      "Iteration 1976, loss = 0.00382191\n",
      "Iteration 1977, loss = 0.00381805\n",
      "Iteration 1978, loss = 0.00381422\n",
      "Iteration 1979, loss = 0.00381031\n",
      "Iteration 1980, loss = 0.00380646\n",
      "Iteration 1981, loss = 0.00380255\n",
      "Iteration 1982, loss = 0.00379868\n",
      "Iteration 1983, loss = 0.00379482\n",
      "Iteration 1984, loss = 0.00379097\n",
      "Iteration 1985, loss = 0.00378714\n",
      "Iteration 1986, loss = 0.00378328\n",
      "Iteration 1987, loss = 0.00377945\n",
      "Iteration 1988, loss = 0.00377566\n",
      "Iteration 1989, loss = 0.00377193\n",
      "Iteration 1990, loss = 0.00376808\n",
      "Iteration 1991, loss = 0.00376430\n",
      "Iteration 1992, loss = 0.00376061\n",
      "Iteration 1993, loss = 0.00375680\n",
      "Iteration 1994, loss = 0.00375304\n",
      "Iteration 1995, loss = 0.00374930\n",
      "Iteration 1996, loss = 0.00374555\n",
      "Iteration 1997, loss = 0.00374188\n",
      "Iteration 1998, loss = 0.00373816\n",
      "Iteration 1999, loss = 0.00373443\n",
      "Iteration 2000, loss = 0.00373079\n",
      "Iteration 2001, loss = 0.00372707\n",
      "Iteration 2002, loss = 0.00372337\n",
      "Iteration 2003, loss = 0.00371973\n",
      "Iteration 2004, loss = 0.00371605\n",
      "Iteration 2005, loss = 0.00371239\n",
      "Iteration 2006, loss = 0.00370872\n",
      "Iteration 2007, loss = 0.00370507\n",
      "Iteration 2008, loss = 0.00370143\n",
      "Iteration 2009, loss = 0.00369783\n",
      "Iteration 2010, loss = 0.00369417\n",
      "Iteration 2011, loss = 0.00369055\n",
      "Iteration 2012, loss = 0.00368693\n",
      "Iteration 2013, loss = 0.00368330\n",
      "Iteration 2014, loss = 0.00367964\n",
      "Iteration 2015, loss = 0.00367606\n",
      "Iteration 2016, loss = 0.00367249\n",
      "Iteration 2017, loss = 0.00366892\n",
      "Iteration 2018, loss = 0.00366529\n",
      "Iteration 2019, loss = 0.00366175\n",
      "Iteration 2020, loss = 0.00365818\n",
      "Iteration 2021, loss = 0.00365461\n",
      "Iteration 2022, loss = 0.00365106\n",
      "Iteration 2023, loss = 0.00364751\n",
      "Iteration 2024, loss = 0.00364394\n",
      "Iteration 2025, loss = 0.00364042\n",
      "Iteration 2026, loss = 0.00363689\n",
      "Iteration 2027, loss = 0.00363336\n",
      "Iteration 2028, loss = 0.00362985\n",
      "Iteration 2029, loss = 0.00362634\n",
      "Iteration 2030, loss = 0.00362286\n",
      "Iteration 2031, loss = 0.00361930\n",
      "Iteration 2032, loss = 0.00361587\n",
      "Iteration 2033, loss = 0.00361232\n",
      "Iteration 2034, loss = 0.00360888\n",
      "Iteration 2035, loss = 0.00360540\n",
      "Iteration 2036, loss = 0.00360192\n",
      "Iteration 2037, loss = 0.00359844\n",
      "Iteration 2038, loss = 0.00359497\n",
      "Iteration 2039, loss = 0.00359149\n",
      "Iteration 2040, loss = 0.00358810\n",
      "Iteration 2041, loss = 0.00358461\n",
      "Iteration 2042, loss = 0.00358115\n",
      "Iteration 2043, loss = 0.00357778\n",
      "Iteration 2044, loss = 0.00357434\n",
      "Iteration 2045, loss = 0.00357094\n",
      "Iteration 2046, loss = 0.00356755\n",
      "Iteration 2047, loss = 0.00356415\n",
      "Iteration 2048, loss = 0.00356075\n",
      "Iteration 2049, loss = 0.00355734\n",
      "Iteration 2050, loss = 0.00355395\n",
      "Iteration 2051, loss = 0.00355057\n",
      "Iteration 2052, loss = 0.00354719\n",
      "Iteration 2053, loss = 0.00354384\n",
      "Iteration 2054, loss = 0.00354045\n",
      "Iteration 2055, loss = 0.00353712\n",
      "Iteration 2056, loss = 0.00353379\n",
      "Iteration 2057, loss = 0.00353045\n",
      "Iteration 2058, loss = 0.00352714\n",
      "Iteration 2059, loss = 0.00352377\n",
      "Iteration 2060, loss = 0.00352049\n",
      "Iteration 2061, loss = 0.00351716\n",
      "Iteration 2062, loss = 0.00351380\n",
      "Iteration 2063, loss = 0.00351054\n",
      "Iteration 2064, loss = 0.00350722\n",
      "Iteration 2065, loss = 0.00350393\n",
      "Iteration 2066, loss = 0.00350061\n",
      "Iteration 2067, loss = 0.00349734\n",
      "Iteration 2068, loss = 0.00349408\n",
      "Iteration 2069, loss = 0.00349081\n",
      "Iteration 2070, loss = 0.00348754\n",
      "Iteration 2071, loss = 0.00348431\n",
      "Iteration 2072, loss = 0.00348105\n",
      "Iteration 2073, loss = 0.00347777\n",
      "Iteration 2074, loss = 0.00347454\n",
      "Iteration 2075, loss = 0.00347126\n",
      "Iteration 2076, loss = 0.00346810\n",
      "Iteration 2077, loss = 0.00346485\n",
      "Iteration 2078, loss = 0.00346160\n",
      "Iteration 2079, loss = 0.00345842\n",
      "Iteration 2080, loss = 0.00345523\n",
      "Iteration 2081, loss = 0.00345198\n",
      "Iteration 2082, loss = 0.00344876\n",
      "Iteration 2083, loss = 0.00344557\n",
      "Iteration 2084, loss = 0.00344238\n",
      "Iteration 2085, loss = 0.00343920\n",
      "Iteration 2086, loss = 0.00343603\n",
      "Iteration 2087, loss = 0.00343283\n",
      "Iteration 2088, loss = 0.00342970\n",
      "Iteration 2089, loss = 0.00342649\n",
      "Iteration 2090, loss = 0.00342334\n",
      "Iteration 2091, loss = 0.00342023\n",
      "Iteration 2092, loss = 0.00341710\n",
      "Iteration 2093, loss = 0.00341397\n",
      "Iteration 2094, loss = 0.00341081\n",
      "Iteration 2095, loss = 0.00340770\n",
      "Iteration 2096, loss = 0.00340458\n",
      "Iteration 2097, loss = 0.00340149\n",
      "Iteration 2098, loss = 0.00339836\n",
      "Iteration 2099, loss = 0.00339524\n",
      "Iteration 2100, loss = 0.00339212\n",
      "Iteration 2101, loss = 0.00338900\n",
      "Iteration 2102, loss = 0.00338591\n",
      "Iteration 2103, loss = 0.00338281\n",
      "Iteration 2104, loss = 0.00337977\n",
      "Iteration 2105, loss = 0.00337662\n",
      "Iteration 2106, loss = 0.00337361\n",
      "Iteration 2107, loss = 0.00337051\n",
      "Iteration 2108, loss = 0.00336744\n",
      "Iteration 2109, loss = 0.00336434\n",
      "Iteration 2110, loss = 0.00336131\n",
      "Iteration 2111, loss = 0.00335822\n",
      "Iteration 2112, loss = 0.00335520\n",
      "Iteration 2113, loss = 0.00335212\n",
      "Iteration 2114, loss = 0.00334913\n",
      "Iteration 2115, loss = 0.00334608\n",
      "Iteration 2116, loss = 0.00334302\n",
      "Iteration 2117, loss = 0.00334001\n",
      "Iteration 2118, loss = 0.00333696\n",
      "Iteration 2119, loss = 0.00333396\n",
      "Iteration 2120, loss = 0.00333098\n",
      "Iteration 2121, loss = 0.00332798\n",
      "Iteration 2122, loss = 0.00332495\n",
      "Iteration 2123, loss = 0.00332195\n",
      "Iteration 2124, loss = 0.00331900\n",
      "Iteration 2125, loss = 0.00331599\n",
      "Iteration 2126, loss = 0.00331300\n",
      "Iteration 2127, loss = 0.00331002\n",
      "Iteration 2128, loss = 0.00330706\n",
      "Iteration 2129, loss = 0.00330406\n",
      "Iteration 2130, loss = 0.00330109\n",
      "Iteration 2131, loss = 0.00329813\n",
      "Iteration 2132, loss = 0.00329519\n",
      "Iteration 2133, loss = 0.00329221\n",
      "Iteration 2134, loss = 0.00328928\n",
      "Iteration 2135, loss = 0.00328631\n",
      "Iteration 2136, loss = 0.00328343\n",
      "Iteration 2137, loss = 0.00328042\n",
      "Iteration 2138, loss = 0.00327751\n",
      "Iteration 2139, loss = 0.00327457\n",
      "Iteration 2140, loss = 0.00327164\n",
      "Iteration 2141, loss = 0.00326872\n",
      "Iteration 2142, loss = 0.00326581\n",
      "Iteration 2143, loss = 0.00326293\n",
      "Iteration 2144, loss = 0.00325998\n",
      "Iteration 2145, loss = 0.00325707\n",
      "Iteration 2146, loss = 0.00325416\n",
      "Iteration 2147, loss = 0.00325128\n",
      "Iteration 2148, loss = 0.00324837\n",
      "Iteration 2149, loss = 0.00324546\n",
      "Iteration 2150, loss = 0.00324251\n",
      "Iteration 2151, loss = 0.00323966\n",
      "Iteration 2152, loss = 0.00323664\n",
      "Iteration 2153, loss = 0.00323371\n",
      "Iteration 2154, loss = 0.00323080\n",
      "Iteration 2155, loss = 0.00322794\n",
      "Iteration 2156, loss = 0.00322505\n",
      "Iteration 2157, loss = 0.00322219\n",
      "Iteration 2158, loss = 0.00321933\n",
      "Iteration 2159, loss = 0.00321647\n",
      "Iteration 2160, loss = 0.00321357\n",
      "Iteration 2161, loss = 0.00321076\n",
      "Iteration 2162, loss = 0.00320790\n",
      "Iteration 2163, loss = 0.00320506\n",
      "Iteration 2164, loss = 0.00320227\n",
      "Iteration 2165, loss = 0.00319946\n",
      "Iteration 2166, loss = 0.00319670\n",
      "Iteration 2167, loss = 0.00319386\n",
      "Iteration 2168, loss = 0.00319105\n",
      "Iteration 2169, loss = 0.00318827\n",
      "Iteration 2170, loss = 0.00318550\n",
      "Iteration 2171, loss = 0.00318275\n",
      "Iteration 2172, loss = 0.00317993\n",
      "Iteration 2173, loss = 0.00317715\n",
      "Iteration 2174, loss = 0.00317438\n",
      "Iteration 2175, loss = 0.00317167\n",
      "Iteration 2176, loss = 0.00316888\n",
      "Iteration 2177, loss = 0.00316618\n",
      "Iteration 2178, loss = 0.00316340\n",
      "Iteration 2179, loss = 0.00316065\n",
      "Iteration 2180, loss = 0.00315799\n",
      "Iteration 2181, loss = 0.00315533\n",
      "Iteration 2182, loss = 0.00315255\n",
      "Iteration 2183, loss = 0.00315003\n",
      "Iteration 2184, loss = 0.00314727\n",
      "Iteration 2185, loss = 0.00314454\n",
      "Iteration 2186, loss = 0.00314176\n",
      "Iteration 2187, loss = 0.00313921\n",
      "Iteration 2188, loss = 0.00313633\n",
      "Iteration 2189, loss = 0.00313376\n",
      "Iteration 2190, loss = 0.00313104\n",
      "Iteration 2191, loss = 0.00312828\n",
      "Iteration 2192, loss = 0.00312566\n",
      "Iteration 2193, loss = 0.00312294\n",
      "Iteration 2194, loss = 0.00312032\n",
      "Iteration 2195, loss = 0.00311763\n",
      "Iteration 2196, loss = 0.00311494\n",
      "Iteration 2197, loss = 0.00311227\n",
      "Iteration 2198, loss = 0.00310963\n",
      "Iteration 2199, loss = 0.00310698\n",
      "Iteration 2200, loss = 0.00310435\n",
      "Iteration 2201, loss = 0.00310166\n",
      "Iteration 2202, loss = 0.00309905\n",
      "Iteration 2203, loss = 0.00309640\n",
      "Iteration 2204, loss = 0.00309375\n",
      "Iteration 2205, loss = 0.00309113\n",
      "Iteration 2206, loss = 0.00308849\n",
      "Iteration 2207, loss = 0.00308587\n",
      "Iteration 2208, loss = 0.00308323\n",
      "Iteration 2209, loss = 0.00308064\n",
      "Iteration 2210, loss = 0.00307803\n",
      "Iteration 2211, loss = 0.00307539\n",
      "Iteration 2212, loss = 0.00307279\n",
      "Iteration 2213, loss = 0.00307015\n",
      "Iteration 2214, loss = 0.00306757\n",
      "Iteration 2215, loss = 0.00306498\n",
      "Iteration 2216, loss = 0.00306241\n",
      "Iteration 2217, loss = 0.00305984\n",
      "Iteration 2218, loss = 0.00305721\n",
      "Iteration 2219, loss = 0.00305465\n",
      "Iteration 2220, loss = 0.00305206\n",
      "Iteration 2221, loss = 0.00304946\n",
      "Iteration 2222, loss = 0.00304691\n",
      "Iteration 2223, loss = 0.00304435\n",
      "Iteration 2224, loss = 0.00304178\n",
      "Iteration 2225, loss = 0.00303926\n",
      "Iteration 2226, loss = 0.00303667\n",
      "Iteration 2227, loss = 0.00303412\n",
      "Iteration 2228, loss = 0.00303163\n",
      "Iteration 2229, loss = 0.00302907\n",
      "Iteration 2230, loss = 0.00302655\n",
      "Iteration 2231, loss = 0.00302402\n",
      "Iteration 2232, loss = 0.00302151\n",
      "Iteration 2233, loss = 0.00301898\n",
      "Iteration 2234, loss = 0.00301651\n",
      "Iteration 2235, loss = 0.00301394\n",
      "Iteration 2236, loss = 0.00301146\n",
      "Iteration 2237, loss = 0.00300895\n",
      "Iteration 2238, loss = 0.00300645\n",
      "Iteration 2239, loss = 0.00300399\n",
      "Iteration 2240, loss = 0.00300147\n",
      "Iteration 2241, loss = 0.00299897\n",
      "Iteration 2242, loss = 0.00299651\n",
      "Iteration 2243, loss = 0.00299404\n",
      "Iteration 2244, loss = 0.00299157\n",
      "Iteration 2245, loss = 0.00298913\n",
      "Iteration 2246, loss = 0.00298664\n",
      "Iteration 2247, loss = 0.00298418\n",
      "Iteration 2248, loss = 0.00298174\n",
      "Iteration 2249, loss = 0.00297924\n",
      "Iteration 2250, loss = 0.00297680\n",
      "Iteration 2251, loss = 0.00297437\n",
      "Iteration 2252, loss = 0.00297192\n",
      "Iteration 2253, loss = 0.00296946\n",
      "Iteration 2254, loss = 0.00296702\n",
      "Iteration 2255, loss = 0.00296457\n",
      "Iteration 2256, loss = 0.00296214\n",
      "Iteration 2257, loss = 0.00295968\n",
      "Iteration 2258, loss = 0.00295727\n",
      "Iteration 2259, loss = 0.00295481\n",
      "Iteration 2260, loss = 0.00295243\n",
      "Iteration 2261, loss = 0.00294996\n",
      "Iteration 2262, loss = 0.00294752\n",
      "Iteration 2263, loss = 0.00294510\n",
      "Iteration 2264, loss = 0.00294268\n",
      "Iteration 2265, loss = 0.00294029\n",
      "Iteration 2266, loss = 0.00293784\n",
      "Iteration 2267, loss = 0.00293544\n",
      "Iteration 2268, loss = 0.00293304\n",
      "Iteration 2269, loss = 0.00293059\n",
      "Iteration 2270, loss = 0.00292829\n",
      "Iteration 2271, loss = 0.00292582\n",
      "Iteration 2272, loss = 0.00292346\n",
      "Iteration 2273, loss = 0.00292108\n",
      "Iteration 2274, loss = 0.00291869\n",
      "Iteration 2275, loss = 0.00291632\n",
      "Iteration 2276, loss = 0.00291394\n",
      "Iteration 2277, loss = 0.00291157\n",
      "Iteration 2278, loss = 0.00290927\n",
      "Iteration 2279, loss = 0.00290689\n",
      "Iteration 2280, loss = 0.00290453\n",
      "Iteration 2281, loss = 0.00290222\n",
      "Iteration 2282, loss = 0.00289987\n",
      "Iteration 2283, loss = 0.00289752\n",
      "Iteration 2284, loss = 0.00289520\n",
      "Iteration 2285, loss = 0.00289289\n",
      "Iteration 2286, loss = 0.00289055\n",
      "Iteration 2287, loss = 0.00288827\n",
      "Iteration 2288, loss = 0.00288595\n",
      "Iteration 2289, loss = 0.00288362\n",
      "Iteration 2290, loss = 0.00288133\n",
      "Iteration 2291, loss = 0.00287904\n",
      "Iteration 2292, loss = 0.00287675\n",
      "Iteration 2293, loss = 0.00287443\n",
      "Iteration 2294, loss = 0.00287214\n",
      "Iteration 2295, loss = 0.00286984\n",
      "Iteration 2296, loss = 0.00286757\n",
      "Iteration 2297, loss = 0.00286529\n",
      "Iteration 2298, loss = 0.00286303\n",
      "Iteration 2299, loss = 0.00286075\n",
      "Iteration 2300, loss = 0.00285845\n",
      "Iteration 2301, loss = 0.00285622\n",
      "Iteration 2302, loss = 0.00285391\n",
      "Iteration 2303, loss = 0.00285167\n",
      "Iteration 2304, loss = 0.00284943\n",
      "Iteration 2305, loss = 0.00284717\n",
      "Iteration 2306, loss = 0.00284491\n",
      "Iteration 2307, loss = 0.00284265\n",
      "Iteration 2308, loss = 0.00284043\n",
      "Iteration 2309, loss = 0.00283818\n",
      "Iteration 2310, loss = 0.00283594\n",
      "Iteration 2311, loss = 0.00283371\n",
      "Iteration 2312, loss = 0.00283147\n",
      "Iteration 2313, loss = 0.00282926\n",
      "Iteration 2314, loss = 0.00282702\n",
      "Iteration 2315, loss = 0.00282478\n",
      "Iteration 2316, loss = 0.00282256\n",
      "Iteration 2317, loss = 0.00282036\n",
      "Iteration 2318, loss = 0.00281815\n",
      "Iteration 2319, loss = 0.00281592\n",
      "Iteration 2320, loss = 0.00281370\n",
      "Iteration 2321, loss = 0.00281149\n",
      "Iteration 2322, loss = 0.00280928\n",
      "Iteration 2323, loss = 0.00280707\n",
      "Iteration 2324, loss = 0.00280485\n",
      "Iteration 2325, loss = 0.00280266\n",
      "Iteration 2326, loss = 0.00280048\n",
      "Iteration 2327, loss = 0.00279827\n",
      "Iteration 2328, loss = 0.00279610\n",
      "Iteration 2329, loss = 0.00279390\n",
      "Iteration 2330, loss = 0.00279172\n",
      "Iteration 2331, loss = 0.00278953\n",
      "Iteration 2332, loss = 0.00278738\n",
      "Iteration 2333, loss = 0.00278521\n",
      "Iteration 2334, loss = 0.00278304\n",
      "Iteration 2335, loss = 0.00278087\n",
      "Iteration 2336, loss = 0.00277874\n",
      "Iteration 2337, loss = 0.00277656\n",
      "Iteration 2338, loss = 0.00277445\n",
      "Iteration 2339, loss = 0.00277230\n",
      "Iteration 2340, loss = 0.00277017\n",
      "Iteration 2341, loss = 0.00276805\n",
      "Iteration 2342, loss = 0.00276589\n",
      "Iteration 2343, loss = 0.00276378\n",
      "Iteration 2344, loss = 0.00276169\n",
      "Iteration 2345, loss = 0.00275955\n",
      "Iteration 2346, loss = 0.00275746\n",
      "Iteration 2347, loss = 0.00275532\n",
      "Iteration 2348, loss = 0.00275319\n",
      "Iteration 2349, loss = 0.00275111\n",
      "Iteration 2350, loss = 0.00274902\n",
      "Iteration 2351, loss = 0.00274688\n",
      "Iteration 2352, loss = 0.00274480\n",
      "Iteration 2353, loss = 0.00274269\n",
      "Iteration 2354, loss = 0.00274060\n",
      "Iteration 2355, loss = 0.00273851\n",
      "Iteration 2356, loss = 0.00273643\n",
      "Iteration 2357, loss = 0.00273433\n",
      "Iteration 2358, loss = 0.00273222\n",
      "Iteration 2359, loss = 0.00273016\n",
      "Iteration 2360, loss = 0.00272810\n",
      "Iteration 2361, loss = 0.00272603\n",
      "Iteration 2362, loss = 0.00272394\n",
      "Iteration 2363, loss = 0.00272189\n",
      "Iteration 2364, loss = 0.00271979\n",
      "Iteration 2365, loss = 0.00271773\n",
      "Iteration 2366, loss = 0.00271568\n",
      "Iteration 2367, loss = 0.00271362\n",
      "Iteration 2368, loss = 0.00271156\n",
      "Iteration 2369, loss = 0.00270952\n",
      "Iteration 2370, loss = 0.00270743\n",
      "Iteration 2371, loss = 0.00270542\n",
      "Iteration 2372, loss = 0.00270337\n",
      "Iteration 2373, loss = 0.00270133\n",
      "Iteration 2374, loss = 0.00269928\n",
      "Iteration 2375, loss = 0.00269725\n",
      "Iteration 2376, loss = 0.00269524\n",
      "Iteration 2377, loss = 0.00269318\n",
      "Iteration 2378, loss = 0.00269119\n",
      "Iteration 2379, loss = 0.00268918\n",
      "Iteration 2380, loss = 0.00268714\n",
      "Iteration 2381, loss = 0.00268513\n",
      "Iteration 2382, loss = 0.00268311\n",
      "Iteration 2383, loss = 0.00268113\n",
      "Iteration 2384, loss = 0.00267910\n",
      "Iteration 2385, loss = 0.00267714\n",
      "Iteration 2386, loss = 0.00267510\n",
      "Iteration 2387, loss = 0.00267314\n",
      "Iteration 2388, loss = 0.00267114\n",
      "Iteration 2389, loss = 0.00266913\n",
      "Iteration 2390, loss = 0.00266713\n",
      "Iteration 2391, loss = 0.00266515\n",
      "Iteration 2392, loss = 0.00266317\n",
      "Iteration 2393, loss = 0.00266118\n",
      "Iteration 2394, loss = 0.00265919\n",
      "Iteration 2395, loss = 0.00265721\n",
      "Iteration 2396, loss = 0.00265525\n",
      "Iteration 2397, loss = 0.00265328\n",
      "Iteration 2398, loss = 0.00265132\n",
      "Iteration 2399, loss = 0.00264933\n",
      "Iteration 2400, loss = 0.00264739\n",
      "Iteration 2401, loss = 0.00264542\n",
      "Iteration 2402, loss = 0.00264347\n",
      "Iteration 2403, loss = 0.00264154\n",
      "Iteration 2404, loss = 0.00263958\n",
      "Iteration 2405, loss = 0.00263763\n",
      "Iteration 2406, loss = 0.00263569\n",
      "Iteration 2407, loss = 0.00263376\n",
      "Iteration 2408, loss = 0.00263181\n",
      "Iteration 2409, loss = 0.00262989\n",
      "Iteration 2410, loss = 0.00262797\n",
      "Iteration 2411, loss = 0.00262602\n",
      "Iteration 2412, loss = 0.00262408\n",
      "Iteration 2413, loss = 0.00262217\n",
      "Iteration 2414, loss = 0.00262025\n",
      "Iteration 2415, loss = 0.00261832\n",
      "Iteration 2416, loss = 0.00261645\n",
      "Iteration 2417, loss = 0.00261450\n",
      "Iteration 2418, loss = 0.00261262\n",
      "Iteration 2419, loss = 0.00261069\n",
      "Iteration 2420, loss = 0.00260878\n",
      "Iteration 2421, loss = 0.00260689\n",
      "Iteration 2422, loss = 0.00260500\n",
      "Iteration 2423, loss = 0.00260311\n",
      "Iteration 2424, loss = 0.00260120\n",
      "Iteration 2425, loss = 0.00259931\n",
      "Iteration 2426, loss = 0.00259742\n",
      "Iteration 2427, loss = 0.00259552\n",
      "Iteration 2428, loss = 0.00259364\n",
      "Iteration 2429, loss = 0.00259175\n",
      "Iteration 2430, loss = 0.00258988\n",
      "Iteration 2431, loss = 0.00258801\n",
      "Iteration 2432, loss = 0.00258614\n",
      "Iteration 2433, loss = 0.00258426\n",
      "Iteration 2434, loss = 0.00258237\n",
      "Iteration 2435, loss = 0.00258054\n",
      "Iteration 2436, loss = 0.00257867\n",
      "Iteration 2437, loss = 0.00257682\n",
      "Iteration 2438, loss = 0.00257495\n",
      "Iteration 2439, loss = 0.00257309\n",
      "Iteration 2440, loss = 0.00257125\n",
      "Iteration 2441, loss = 0.00256942\n",
      "Iteration 2442, loss = 0.00256756\n",
      "Iteration 2443, loss = 0.00256571\n",
      "Iteration 2444, loss = 0.00256387\n",
      "Iteration 2445, loss = 0.00256203\n",
      "Iteration 2446, loss = 0.00256019\n",
      "Iteration 2447, loss = 0.00255834\n",
      "Iteration 2448, loss = 0.00255654\n",
      "Iteration 2449, loss = 0.00255469\n",
      "Iteration 2450, loss = 0.00255285\n",
      "Iteration 2451, loss = 0.00255104\n",
      "Iteration 2452, loss = 0.00254920\n",
      "Iteration 2453, loss = 0.00254736\n",
      "Iteration 2454, loss = 0.00254558\n",
      "Iteration 2455, loss = 0.00254372\n",
      "Iteration 2456, loss = 0.00254190\n",
      "Iteration 2457, loss = 0.00254009\n",
      "Iteration 2458, loss = 0.00253829\n",
      "Iteration 2459, loss = 0.00253646\n",
      "Iteration 2460, loss = 0.00253467\n",
      "Iteration 2461, loss = 0.00253284\n",
      "Iteration 2462, loss = 0.00253105\n",
      "Iteration 2463, loss = 0.00252923\n",
      "Iteration 2464, loss = 0.00252744\n",
      "Iteration 2465, loss = 0.00252565\n",
      "Iteration 2466, loss = 0.00252382\n",
      "Iteration 2467, loss = 0.00252206\n",
      "Iteration 2468, loss = 0.00252030\n",
      "Iteration 2469, loss = 0.00251851\n",
      "Iteration 2470, loss = 0.00251673\n",
      "Iteration 2471, loss = 0.00251494\n",
      "Iteration 2472, loss = 0.00251320\n",
      "Iteration 2473, loss = 0.00251140\n",
      "Iteration 2474, loss = 0.00250965\n",
      "Iteration 2475, loss = 0.00250788\n",
      "Iteration 2476, loss = 0.00250612\n",
      "Iteration 2477, loss = 0.00250437\n",
      "Iteration 2478, loss = 0.00250261\n",
      "Iteration 2479, loss = 0.00250086\n",
      "Iteration 2480, loss = 0.00249912\n",
      "Iteration 2481, loss = 0.00249737\n",
      "Iteration 2482, loss = 0.00249565\n",
      "Iteration 2483, loss = 0.00249388\n",
      "Iteration 2484, loss = 0.00249217\n",
      "Iteration 2485, loss = 0.00249041\n",
      "Iteration 2486, loss = 0.00248867\n",
      "Iteration 2487, loss = 0.00248695\n",
      "Iteration 2488, loss = 0.00248523\n",
      "Iteration 2489, loss = 0.00248353\n",
      "Iteration 2490, loss = 0.00248178\n",
      "Iteration 2491, loss = 0.00248007\n",
      "Iteration 2492, loss = 0.00247834\n",
      "Iteration 2493, loss = 0.00247666\n",
      "Iteration 2494, loss = 0.00247494\n",
      "Iteration 2495, loss = 0.00247325\n",
      "Iteration 2496, loss = 0.00247156\n",
      "Iteration 2497, loss = 0.00246979\n",
      "Iteration 2498, loss = 0.00246821\n",
      "Iteration 2499, loss = 0.00246642\n",
      "Iteration 2500, loss = 0.00246474\n",
      "Iteration 2501, loss = 0.00246299\n",
      "Iteration 2502, loss = 0.00246132\n",
      "Iteration 2503, loss = 0.00245962\n",
      "Iteration 2504, loss = 0.00245795\n",
      "Iteration 2505, loss = 0.00245625\n",
      "Iteration 2506, loss = 0.00245456\n",
      "Iteration 2507, loss = 0.00245287\n",
      "Iteration 2508, loss = 0.00245118\n",
      "Iteration 2509, loss = 0.00244949\n",
      "Iteration 2510, loss = 0.00244782\n",
      "Iteration 2511, loss = 0.00244612\n",
      "Iteration 2512, loss = 0.00244447\n",
      "Iteration 2513, loss = 0.00244279\n",
      "Iteration 2514, loss = 0.00244113\n",
      "Iteration 2515, loss = 0.00243944\n",
      "Iteration 2516, loss = 0.00243779\n",
      "Iteration 2517, loss = 0.00243612\n",
      "Iteration 2518, loss = 0.00243447\n",
      "Iteration 2519, loss = 0.00243282\n",
      "Iteration 2520, loss = 0.00243118\n",
      "Iteration 2521, loss = 0.00242952\n",
      "Iteration 2522, loss = 0.00242789\n",
      "Iteration 2523, loss = 0.00242622\n",
      "Iteration 2524, loss = 0.00242461\n",
      "Iteration 2525, loss = 0.00242295\n",
      "Iteration 2526, loss = 0.00242136\n",
      "Iteration 2527, loss = 0.00241970\n",
      "Iteration 2528, loss = 0.00241806\n",
      "Iteration 2529, loss = 0.00241642\n",
      "Iteration 2530, loss = 0.00241479\n",
      "Iteration 2531, loss = 0.00241315\n",
      "Iteration 2532, loss = 0.00241152\n",
      "Iteration 2533, loss = 0.00240991\n",
      "Iteration 2534, loss = 0.00240828\n",
      "Iteration 2535, loss = 0.00240666\n",
      "Iteration 2536, loss = 0.00240503\n",
      "Iteration 2537, loss = 0.00240341\n",
      "Iteration 2538, loss = 0.00240180\n",
      "Iteration 2539, loss = 0.00240019\n",
      "Iteration 2540, loss = 0.00239857\n",
      "Iteration 2541, loss = 0.00239697\n",
      "Iteration 2542, loss = 0.00239537\n",
      "Iteration 2543, loss = 0.00239379\n",
      "Iteration 2544, loss = 0.00239219\n",
      "Iteration 2545, loss = 0.00239058\n",
      "Iteration 2546, loss = 0.00238900\n",
      "Iteration 2547, loss = 0.00238741\n",
      "Iteration 2548, loss = 0.00238583\n",
      "Iteration 2549, loss = 0.00238422\n",
      "Iteration 2550, loss = 0.00238264\n",
      "Iteration 2551, loss = 0.00238110\n",
      "Iteration 2552, loss = 0.00237948\n",
      "Iteration 2553, loss = 0.00237791\n",
      "Iteration 2554, loss = 0.00237632\n",
      "Iteration 2555, loss = 0.00237474\n",
      "Iteration 2556, loss = 0.00237320\n",
      "Iteration 2557, loss = 0.00237160\n",
      "Iteration 2558, loss = 0.00237006\n",
      "Iteration 2559, loss = 0.00236847\n",
      "Iteration 2560, loss = 0.00236688\n",
      "Iteration 2561, loss = 0.00236533\n",
      "Iteration 2562, loss = 0.00236377\n",
      "Iteration 2563, loss = 0.00236225\n",
      "Iteration 2564, loss = 0.00236067\n",
      "Iteration 2565, loss = 0.00235911\n",
      "Iteration 2566, loss = 0.00235757\n",
      "Iteration 2567, loss = 0.00235604\n",
      "Iteration 2568, loss = 0.00235445\n",
      "Iteration 2569, loss = 0.00235290\n",
      "Iteration 2570, loss = 0.00235134\n",
      "Iteration 2571, loss = 0.00234979\n",
      "Iteration 2572, loss = 0.00234825\n",
      "Iteration 2573, loss = 0.00234674\n",
      "Iteration 2574, loss = 0.00234518\n",
      "Iteration 2575, loss = 0.00234365\n",
      "Iteration 2576, loss = 0.00234213\n",
      "Iteration 2577, loss = 0.00234057\n",
      "Iteration 2578, loss = 0.00233905\n",
      "Iteration 2579, loss = 0.00233752\n",
      "Iteration 2580, loss = 0.00233601\n",
      "Iteration 2581, loss = 0.00233448\n",
      "Iteration 2582, loss = 0.00233298\n",
      "Iteration 2583, loss = 0.00233145\n",
      "Iteration 2584, loss = 0.00232993\n",
      "Iteration 2585, loss = 0.00232842\n",
      "Iteration 2586, loss = 0.00232692\n",
      "Iteration 2587, loss = 0.00232543\n",
      "Iteration 2588, loss = 0.00232392\n",
      "Iteration 2589, loss = 0.00232242\n",
      "Iteration 2590, loss = 0.00232090\n",
      "Iteration 2591, loss = 0.00231942\n",
      "Iteration 2592, loss = 0.00231792\n",
      "Iteration 2593, loss = 0.00231642\n",
      "Iteration 2594, loss = 0.00231493\n",
      "Iteration 2595, loss = 0.00231346\n",
      "Iteration 2596, loss = 0.00231196\n",
      "Iteration 2597, loss = 0.00231049\n",
      "Iteration 2598, loss = 0.00230899\n",
      "Iteration 2599, loss = 0.00230750\n",
      "Iteration 2600, loss = 0.00230603\n",
      "Iteration 2601, loss = 0.00230455\n",
      "Iteration 2602, loss = 0.00230309\n",
      "Iteration 2603, loss = 0.00230159\n",
      "Iteration 2604, loss = 0.00230012\n",
      "Iteration 2605, loss = 0.00229868\n",
      "Iteration 2606, loss = 0.00229717\n",
      "Iteration 2607, loss = 0.00229572\n",
      "Iteration 2608, loss = 0.00229428\n",
      "Iteration 2609, loss = 0.00229280\n",
      "Iteration 2610, loss = 0.00229135\n",
      "Iteration 2611, loss = 0.00228988\n",
      "Iteration 2612, loss = 0.00228845\n",
      "Iteration 2613, loss = 0.00228698\n",
      "Iteration 2614, loss = 0.00228554\n",
      "Iteration 2615, loss = 0.00228406\n",
      "Iteration 2616, loss = 0.00228262\n",
      "Iteration 2617, loss = 0.00228118\n",
      "Iteration 2618, loss = 0.00227977\n",
      "Iteration 2619, loss = 0.00227832\n",
      "Iteration 2620, loss = 0.00227688\n",
      "Iteration 2621, loss = 0.00227544\n",
      "Iteration 2622, loss = 0.00227400\n",
      "Iteration 2623, loss = 0.00227256\n",
      "Iteration 2624, loss = 0.00227115\n",
      "Iteration 2625, loss = 0.00226971\n",
      "Iteration 2626, loss = 0.00226829\n",
      "Iteration 2627, loss = 0.00226686\n",
      "Iteration 2628, loss = 0.00226546\n",
      "Iteration 2629, loss = 0.00226401\n",
      "Iteration 2630, loss = 0.00226262\n",
      "Iteration 2631, loss = 0.00226118\n",
      "Iteration 2632, loss = 0.00225979\n",
      "Iteration 2633, loss = 0.00225836\n",
      "Iteration 2634, loss = 0.00225697\n",
      "Iteration 2635, loss = 0.00225554\n",
      "Iteration 2636, loss = 0.00225412\n",
      "Iteration 2637, loss = 0.00225273\n",
      "Iteration 2638, loss = 0.00225133\n",
      "Iteration 2639, loss = 0.00224992\n",
      "Iteration 2640, loss = 0.00224851\n",
      "Iteration 2641, loss = 0.00224714\n",
      "Iteration 2642, loss = 0.00224575\n",
      "Iteration 2643, loss = 0.00224438\n",
      "Iteration 2644, loss = 0.00224294\n",
      "Iteration 2645, loss = 0.00224158\n",
      "Iteration 2646, loss = 0.00224019\n",
      "Iteration 2647, loss = 0.00223879\n",
      "Iteration 2648, loss = 0.00223740\n",
      "Iteration 2649, loss = 0.00223604\n",
      "Iteration 2650, loss = 0.00223464\n",
      "Iteration 2651, loss = 0.00223326\n",
      "Iteration 2652, loss = 0.00223189\n",
      "Iteration 2653, loss = 0.00223051\n",
      "Iteration 2654, loss = 0.00222914\n",
      "Iteration 2655, loss = 0.00222778\n",
      "Iteration 2656, loss = 0.00222641\n",
      "Iteration 2657, loss = 0.00222504\n",
      "Iteration 2658, loss = 0.00222367\n",
      "Iteration 2659, loss = 0.00222231\n",
      "Iteration 2660, loss = 0.00222096\n",
      "Iteration 2661, loss = 0.00221961\n",
      "Iteration 2662, loss = 0.00221825\n",
      "Iteration 2663, loss = 0.00221690\n",
      "Iteration 2664, loss = 0.00221556\n",
      "Iteration 2665, loss = 0.00221420\n",
      "Iteration 2666, loss = 0.00221287\n",
      "Iteration 2667, loss = 0.00221152\n",
      "Iteration 2668, loss = 0.00221018\n",
      "Iteration 2669, loss = 0.00220883\n",
      "Iteration 2670, loss = 0.00220750\n",
      "Iteration 2671, loss = 0.00220616\n",
      "Iteration 2672, loss = 0.00220482\n",
      "Iteration 2673, loss = 0.00220351\n",
      "Iteration 2674, loss = 0.00220215\n",
      "Iteration 2675, loss = 0.00220085\n",
      "Iteration 2676, loss = 0.00219951\n",
      "Iteration 2677, loss = 0.00219817\n",
      "Iteration 2678, loss = 0.00219683\n",
      "Iteration 2679, loss = 0.00219552\n",
      "Iteration 2680, loss = 0.00219422\n",
      "Iteration 2681, loss = 0.00219287\n",
      "Iteration 2682, loss = 0.00219155\n",
      "Iteration 2683, loss = 0.00219026\n",
      "Iteration 2684, loss = 0.00218890\n",
      "Iteration 2685, loss = 0.00218762\n",
      "Iteration 2686, loss = 0.00218629\n",
      "Iteration 2687, loss = 0.00218497\n",
      "Iteration 2688, loss = 0.00218368\n",
      "Iteration 2689, loss = 0.00218238\n",
      "Iteration 2690, loss = 0.00218106\n",
      "Iteration 2691, loss = 0.00217976\n",
      "Iteration 2692, loss = 0.00217846\n",
      "Iteration 2693, loss = 0.00217717\n",
      "Iteration 2694, loss = 0.00217587\n",
      "Iteration 2695, loss = 0.00217459\n",
      "Iteration 2696, loss = 0.00217329\n",
      "Iteration 2697, loss = 0.00217201\n",
      "Iteration 2698, loss = 0.00217070\n",
      "Iteration 2699, loss = 0.00216941\n",
      "Iteration 2700, loss = 0.00216812\n",
      "Iteration 2701, loss = 0.00216681\n",
      "Iteration 2702, loss = 0.00216553\n",
      "Iteration 2703, loss = 0.00216424\n",
      "Iteration 2704, loss = 0.00216297\n",
      "Iteration 2705, loss = 0.00216168\n",
      "Iteration 2706, loss = 0.00216042\n",
      "Iteration 2707, loss = 0.00215914\n",
      "Iteration 2708, loss = 0.00215787\n",
      "Iteration 2709, loss = 0.00215659\n",
      "Iteration 2710, loss = 0.00215533\n",
      "Iteration 2711, loss = 0.00215407\n",
      "Iteration 2712, loss = 0.00215279\n",
      "Iteration 2713, loss = 0.00215153\n",
      "Iteration 2714, loss = 0.00215028\n",
      "Iteration 2715, loss = 0.00214901\n",
      "Iteration 2716, loss = 0.00214777\n",
      "Iteration 2717, loss = 0.00214647\n",
      "Iteration 2718, loss = 0.00214527\n",
      "Iteration 2719, loss = 0.00214398\n",
      "Iteration 2720, loss = 0.00214274\n",
      "Iteration 2721, loss = 0.00214150\n",
      "Iteration 2722, loss = 0.00214025\n",
      "Iteration 2723, loss = 0.00213898\n",
      "Iteration 2724, loss = 0.00213775\n",
      "Iteration 2725, loss = 0.00213649\n",
      "Iteration 2726, loss = 0.00213524\n",
      "Iteration 2727, loss = 0.00213403\n",
      "Iteration 2728, loss = 0.00213276\n",
      "Iteration 2729, loss = 0.00213152\n",
      "Iteration 2730, loss = 0.00213029\n",
      "Iteration 2731, loss = 0.00212908\n",
      "Iteration 2732, loss = 0.00212783\n",
      "Iteration 2733, loss = 0.00212658\n",
      "Iteration 2734, loss = 0.00212537\n",
      "Iteration 2735, loss = 0.00212412\n",
      "Iteration 2736, loss = 0.00212290\n",
      "Iteration 2737, loss = 0.00212166\n",
      "Iteration 2738, loss = 0.00212046\n",
      "Iteration 2739, loss = 0.00211924\n",
      "Iteration 2740, loss = 0.00211801\n",
      "Iteration 2741, loss = 0.00211679\n",
      "Iteration 2742, loss = 0.00211558\n",
      "Iteration 2743, loss = 0.00211435\n",
      "Iteration 2744, loss = 0.00211315\n",
      "Iteration 2745, loss = 0.00211195\n",
      "Iteration 2746, loss = 0.00211072\n",
      "Iteration 2747, loss = 0.00210954\n",
      "Iteration 2748, loss = 0.00210833\n",
      "Iteration 2749, loss = 0.00210712\n",
      "Iteration 2750, loss = 0.00210591\n",
      "Iteration 2751, loss = 0.00210470\n",
      "Iteration 2752, loss = 0.00210352\n",
      "Iteration 2753, loss = 0.00210232\n",
      "Iteration 2754, loss = 0.00210111\n",
      "Iteration 2755, loss = 0.00209993\n",
      "Iteration 2756, loss = 0.00209872\n",
      "Iteration 2757, loss = 0.00209754\n",
      "Iteration 2758, loss = 0.00209634\n",
      "Iteration 2759, loss = 0.00209515\n",
      "Iteration 2760, loss = 0.00209396\n",
      "Iteration 2761, loss = 0.00209277\n",
      "Iteration 2762, loss = 0.00209160\n",
      "Iteration 2763, loss = 0.00209040\n",
      "Iteration 2764, loss = 0.00208923\n",
      "Iteration 2765, loss = 0.00208803\n",
      "Iteration 2766, loss = 0.00208685\n",
      "Iteration 2767, loss = 0.00208567\n",
      "Iteration 2768, loss = 0.00208449\n",
      "Iteration 2769, loss = 0.00208334\n",
      "Iteration 2770, loss = 0.00208216\n",
      "Iteration 2771, loss = 0.00208099\n",
      "Iteration 2772, loss = 0.00207984\n",
      "Iteration 2773, loss = 0.00207866\n",
      "Iteration 2774, loss = 0.00207750\n",
      "Iteration 2775, loss = 0.00207635\n",
      "Iteration 2776, loss = 0.00207516\n",
      "Iteration 2777, loss = 0.00207400\n",
      "Iteration 2778, loss = 0.00207285\n",
      "Iteration 2779, loss = 0.00207169\n",
      "Iteration 2780, loss = 0.00207055\n",
      "Iteration 2781, loss = 0.00206937\n",
      "Iteration 2782, loss = 0.00206820\n",
      "Iteration 2783, loss = 0.00206707\n",
      "Iteration 2784, loss = 0.00206589\n",
      "Iteration 2785, loss = 0.00206476\n",
      "Iteration 2786, loss = 0.00206360\n",
      "Iteration 2787, loss = 0.00206242\n",
      "Iteration 2788, loss = 0.00206134\n",
      "Iteration 2789, loss = 0.00206014\n",
      "Iteration 2790, loss = 0.00205900\n",
      "Iteration 2791, loss = 0.00205786\n",
      "Iteration 2792, loss = 0.00205672\n",
      "Iteration 2793, loss = 0.00205557\n",
      "Iteration 2794, loss = 0.00205445\n",
      "Iteration 2795, loss = 0.00205333\n",
      "Iteration 2796, loss = 0.00205218\n",
      "Iteration 2797, loss = 0.00205105\n",
      "Iteration 2798, loss = 0.00204991\n",
      "Iteration 2799, loss = 0.00204881\n",
      "Iteration 2800, loss = 0.00204770\n",
      "Iteration 2801, loss = 0.00204656\n",
      "Iteration 2802, loss = 0.00204544\n",
      "Iteration 2803, loss = 0.00204434\n",
      "Iteration 2804, loss = 0.00204318\n",
      "Iteration 2805, loss = 0.00204211\n",
      "Iteration 2806, loss = 0.00204094\n",
      "Iteration 2807, loss = 0.00203984\n",
      "Iteration 2808, loss = 0.00203871\n",
      "Iteration 2809, loss = 0.00203759\n",
      "Iteration 2810, loss = 0.00203647\n",
      "Iteration 2811, loss = 0.00203537\n",
      "Iteration 2812, loss = 0.00203428\n",
      "Iteration 2813, loss = 0.00203315\n",
      "Iteration 2814, loss = 0.00203208\n",
      "Iteration 2815, loss = 0.00203094\n",
      "Iteration 2816, loss = 0.00202986\n",
      "Iteration 2817, loss = 0.00202876\n",
      "Iteration 2818, loss = 0.00202765\n",
      "Iteration 2819, loss = 0.00202657\n",
      "Iteration 2820, loss = 0.00202548\n",
      "Iteration 2821, loss = 0.00202436\n",
      "Iteration 2822, loss = 0.00202329\n",
      "Iteration 2823, loss = 0.00202218\n",
      "Iteration 2824, loss = 0.00202112\n",
      "Iteration 2825, loss = 0.00202001\n",
      "Iteration 2826, loss = 0.00201893\n",
      "Iteration 2827, loss = 0.00201782\n",
      "Iteration 2828, loss = 0.00201673\n",
      "Iteration 2829, loss = 0.00201564\n",
      "Iteration 2830, loss = 0.00201455\n",
      "Iteration 2831, loss = 0.00201348\n",
      "Iteration 2832, loss = 0.00201238\n",
      "Iteration 2833, loss = 0.00201130\n",
      "Iteration 2834, loss = 0.00201023\n",
      "Iteration 2835, loss = 0.00200915\n",
      "Iteration 2836, loss = 0.00200811\n",
      "Iteration 2837, loss = 0.00200701\n",
      "Iteration 2838, loss = 0.00200593\n",
      "Iteration 2839, loss = 0.00200485\n",
      "Iteration 2840, loss = 0.00200377\n",
      "Iteration 2841, loss = 0.00200270\n",
      "Iteration 2842, loss = 0.00200164\n",
      "Iteration 2843, loss = 0.00200056\n",
      "Iteration 2844, loss = 0.00199948\n",
      "Iteration 2845, loss = 0.00199842\n",
      "Iteration 2846, loss = 0.00199731\n",
      "Iteration 2847, loss = 0.00199627\n",
      "Iteration 2848, loss = 0.00199519\n",
      "Iteration 2849, loss = 0.00199412\n",
      "Iteration 2850, loss = 0.00199308\n",
      "Iteration 2851, loss = 0.00199202\n",
      "Iteration 2852, loss = 0.00199096\n",
      "Iteration 2853, loss = 0.00198990\n",
      "Iteration 2854, loss = 0.00198884\n",
      "Iteration 2855, loss = 0.00198778\n",
      "Iteration 2856, loss = 0.00198674\n",
      "Iteration 2857, loss = 0.00198567\n",
      "Iteration 2858, loss = 0.00198463\n",
      "Iteration 2859, loss = 0.00198357\n",
      "Iteration 2860, loss = 0.00198254\n",
      "Iteration 2861, loss = 0.00198149\n",
      "Iteration 2862, loss = 0.00198045\n",
      "Iteration 2863, loss = 0.00197941\n",
      "Iteration 2864, loss = 0.00197839\n",
      "Iteration 2865, loss = 0.00197735\n",
      "Iteration 2866, loss = 0.00197629\n",
      "Iteration 2867, loss = 0.00197526\n",
      "Iteration 2868, loss = 0.00197426\n",
      "Iteration 2869, loss = 0.00197321\n",
      "Iteration 2870, loss = 0.00197218\n",
      "Iteration 2871, loss = 0.00197114\n",
      "Iteration 2872, loss = 0.00197012\n",
      "Iteration 2873, loss = 0.00196908\n",
      "Iteration 2874, loss = 0.00196805\n",
      "Iteration 2875, loss = 0.00196704\n",
      "Iteration 2876, loss = 0.00196601\n",
      "Iteration 2877, loss = 0.00196499\n",
      "Iteration 2878, loss = 0.00196397\n",
      "Iteration 2879, loss = 0.00196294\n",
      "Iteration 2880, loss = 0.00196194\n",
      "Iteration 2881, loss = 0.00196092\n",
      "Iteration 2882, loss = 0.00195990\n",
      "Iteration 2883, loss = 0.00195890\n",
      "Iteration 2884, loss = 0.00195788\n",
      "Iteration 2885, loss = 0.00195688\n",
      "Iteration 2886, loss = 0.00195586\n",
      "Iteration 2887, loss = 0.00195485\n",
      "Iteration 2888, loss = 0.00195386\n",
      "Iteration 2889, loss = 0.00195285\n",
      "Iteration 2890, loss = 0.00195184\n",
      "Iteration 2891, loss = 0.00195084\n",
      "Iteration 2892, loss = 0.00194982\n",
      "Iteration 2893, loss = 0.00194882\n",
      "Iteration 2894, loss = 0.00194783\n",
      "Iteration 2895, loss = 0.00194684\n",
      "Iteration 2896, loss = 0.00194582\n",
      "Iteration 2897, loss = 0.00194484\n",
      "Iteration 2898, loss = 0.00194384\n",
      "Iteration 2899, loss = 0.00194287\n",
      "Iteration 2900, loss = 0.00194186\n",
      "Iteration 2901, loss = 0.00194088\n",
      "Iteration 2902, loss = 0.00193991\n",
      "Iteration 2903, loss = 0.00193890\n",
      "Iteration 2904, loss = 0.00193791\n",
      "Iteration 2905, loss = 0.00193693\n",
      "Iteration 2906, loss = 0.00193594\n",
      "Iteration 2907, loss = 0.00193494\n",
      "Iteration 2908, loss = 0.00193399\n",
      "Iteration 2909, loss = 0.00193298\n",
      "Iteration 2910, loss = 0.00193201\n",
      "Iteration 2911, loss = 0.00193100\n",
      "Iteration 2912, loss = 0.00193004\n",
      "Iteration 2913, loss = 0.00192906\n",
      "Iteration 2914, loss = 0.00192808\n",
      "Iteration 2915, loss = 0.00192711\n",
      "Iteration 2916, loss = 0.00192613\n",
      "Iteration 2917, loss = 0.00192517\n",
      "Iteration 2918, loss = 0.00192421\n",
      "Iteration 2919, loss = 0.00192324\n",
      "Iteration 2920, loss = 0.00192229\n",
      "Iteration 2921, loss = 0.00192132\n",
      "Iteration 2922, loss = 0.00192035\n",
      "Training loss did not improve more than tol=0.000001 for 10 consecutive epochs. Stopping.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.67      0.80         3\n",
      "           1       1.00      0.50      0.67         2\n",
      "           2       1.00      1.00      1.00         3\n",
      "           3       1.00      0.75      0.86         4\n",
      "           4       1.00      1.00      1.00         2\n",
      "           5       1.00      1.00      1.00         4\n",
      "           6       1.00      1.00      1.00         3\n",
      "           7       1.00      0.60      0.75         5\n",
      "           8       0.67      1.00      0.80         2\n",
      "           9       1.00      1.00      1.00         2\n",
      "          10       1.00      1.00      1.00         4\n",
      "          11       1.00      1.00      1.00         3\n",
      "          12       0.00      0.00      0.00         1\n",
      "          13       1.00      1.00      1.00         2\n",
      "          14       1.00      1.00      1.00         3\n",
      "          15       0.60      1.00      0.75         3\n",
      "          16       1.00      0.60      0.75         5\n",
      "          17       1.00      1.00      1.00         5\n",
      "          18       0.33      1.00      0.50         1\n",
      "          19       1.00      1.00      1.00         4\n",
      "          20       1.00      1.00      1.00         3\n",
      "          21       1.00      1.00      1.00         3\n",
      "          22       0.67      1.00      0.80         4\n",
      "          23       1.00      0.80      0.89         5\n",
      "          25       0.67      1.00      0.80         2\n",
      "          26       1.00      1.00      1.00         3\n",
      "          27       1.00      1.00      1.00         2\n",
      "          28       1.00      1.00      1.00         5\n",
      "          29       0.60      1.00      0.75         3\n",
      "          30       0.50      1.00      0.67         1\n",
      "          31       1.00      1.00      1.00         2\n",
      "          32       1.00      1.00      1.00         3\n",
      "          33       1.00      1.00      1.00         2\n",
      "          34       1.00      1.00      1.00         3\n",
      "          35       1.00      1.00      1.00         2\n",
      "          36       1.00      1.00      1.00         5\n",
      "          37       1.00      1.00      1.00         3\n",
      "          38       1.00      1.00      1.00         4\n",
      "          39       1.00      0.50      0.67         4\n",
      "\n",
      "    accuracy                           0.91       120\n",
      "   macro avg       0.90      0.91      0.88       120\n",
      "weighted avg       0.94      0.91      0.91       120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "hidden_layers = (60,60,60)\n",
    "activation = 'logistic'\n",
    "opts = dict(hidden_layer_sizes = hidden_layers,verbose = True,activation = activation,tol = 1e-6,max_iter = int(1e6))\n",
    "solver = 'adam'\n",
    "clf_MLP = MLPClassifier(solver = solver, **opts)\n",
    "clf_MLP.fit(X_train_,y_train)\n",
    "predictions = clf_MLP.predict(X_test_)\n",
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- hidden_layers = (512, 512, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 3.75579787\n",
      "Iteration 2, loss = 3.66156903\n",
      "Iteration 3, loss = 3.63444604\n",
      "Iteration 4, loss = 3.59642082\n",
      "Iteration 5, loss = 3.54088638\n",
      "Iteration 6, loss = 3.47528850\n",
      "Iteration 7, loss = 3.41007488\n",
      "Iteration 8, loss = 3.33491817\n",
      "Iteration 9, loss = 3.24625292\n",
      "Iteration 10, loss = 3.13987158\n",
      "Iteration 11, loss = 3.03355789\n",
      "Iteration 12, loss = 2.92350691\n",
      "Iteration 13, loss = 2.80637735\n",
      "Iteration 14, loss = 2.68491349\n",
      "Iteration 15, loss = 2.56299280\n",
      "Iteration 16, loss = 2.43585464\n",
      "Iteration 17, loss = 2.30935641\n",
      "Iteration 18, loss = 2.18977938\n",
      "Iteration 19, loss = 2.06583782\n",
      "Iteration 20, loss = 1.93991985\n",
      "Iteration 21, loss = 1.81868251\n",
      "Iteration 22, loss = 1.70053212\n",
      "Iteration 23, loss = 1.58641755\n",
      "Iteration 24, loss = 1.47427676\n",
      "Iteration 25, loss = 1.36885817\n",
      "Iteration 26, loss = 1.26850831\n",
      "Iteration 27, loss = 1.17248843\n",
      "Iteration 28, loss = 1.08108804\n",
      "Iteration 29, loss = 0.99112100\n",
      "Iteration 30, loss = 0.90939707\n",
      "Iteration 31, loss = 0.83183677\n",
      "Iteration 32, loss = 0.76157407\n",
      "Iteration 33, loss = 0.69692355\n",
      "Iteration 34, loss = 0.63483862\n",
      "Iteration 35, loss = 0.58066685\n",
      "Iteration 36, loss = 0.52729350\n",
      "Iteration 37, loss = 0.47795642\n",
      "Iteration 38, loss = 0.43475122\n",
      "Iteration 39, loss = 0.39779958\n",
      "Iteration 40, loss = 0.36123795\n",
      "Iteration 41, loss = 0.32908873\n",
      "Iteration 42, loss = 0.29990077\n",
      "Iteration 43, loss = 0.27376279\n",
      "Iteration 44, loss = 0.25075005\n",
      "Iteration 45, loss = 0.23047783\n",
      "Iteration 46, loss = 0.21177722\n",
      "Iteration 47, loss = 0.19438384\n",
      "Iteration 48, loss = 0.17877868\n",
      "Iteration 49, loss = 0.16559154\n",
      "Iteration 50, loss = 0.15360292\n",
      "Iteration 51, loss = 0.14321198\n",
      "Iteration 52, loss = 0.13363993\n",
      "Iteration 53, loss = 0.12507186\n",
      "Iteration 54, loss = 0.11725542\n",
      "Iteration 55, loss = 0.11020092\n",
      "Iteration 56, loss = 0.10373554\n",
      "Iteration 57, loss = 0.09778946\n",
      "Iteration 58, loss = 0.09257581\n",
      "Iteration 59, loss = 0.08778023\n",
      "Iteration 60, loss = 0.08352815\n",
      "Iteration 61, loss = 0.07968071\n",
      "Iteration 62, loss = 0.07597660\n",
      "Iteration 63, loss = 0.07254858\n",
      "Iteration 64, loss = 0.06953280\n",
      "Iteration 65, loss = 0.06659057\n",
      "Iteration 66, loss = 0.06394138\n",
      "Iteration 67, loss = 0.06146867\n",
      "Iteration 68, loss = 0.05927031\n",
      "Iteration 69, loss = 0.05711474\n",
      "Iteration 70, loss = 0.05503957\n",
      "Iteration 71, loss = 0.05308179\n",
      "Iteration 72, loss = 0.05130656\n",
      "Iteration 73, loss = 0.04962321\n",
      "Iteration 74, loss = 0.04806325\n",
      "Iteration 75, loss = 0.04657937\n",
      "Iteration 76, loss = 0.04518013\n",
      "Iteration 77, loss = 0.04389165\n",
      "Iteration 78, loss = 0.04264442\n",
      "Iteration 79, loss = 0.04144028\n",
      "Iteration 80, loss = 0.04032341\n",
      "Iteration 81, loss = 0.03922701\n",
      "Iteration 82, loss = 0.03822608\n",
      "Iteration 83, loss = 0.03724601\n",
      "Iteration 84, loss = 0.03631618\n",
      "Iteration 85, loss = 0.03542998\n",
      "Iteration 86, loss = 0.03458144\n",
      "Iteration 87, loss = 0.03377164\n",
      "Iteration 88, loss = 0.03299733\n",
      "Iteration 89, loss = 0.03223519\n",
      "Iteration 90, loss = 0.03152538\n",
      "Iteration 91, loss = 0.03082922\n",
      "Iteration 92, loss = 0.03016709\n",
      "Iteration 93, loss = 0.02952390\n",
      "Iteration 94, loss = 0.02890810\n",
      "Iteration 95, loss = 0.02831558\n",
      "Iteration 96, loss = 0.02774388\n",
      "Iteration 97, loss = 0.02718864\n",
      "Iteration 98, loss = 0.02665776\n",
      "Iteration 99, loss = 0.02613961\n",
      "Iteration 100, loss = 0.02564020\n",
      "Iteration 101, loss = 0.02516262\n",
      "Iteration 102, loss = 0.02470437\n",
      "Iteration 103, loss = 0.02424488\n",
      "Iteration 104, loss = 0.02381666\n",
      "Iteration 105, loss = 0.02339479\n",
      "Iteration 106, loss = 0.02299083\n",
      "Iteration 107, loss = 0.02259502\n",
      "Iteration 108, loss = 0.02221440\n",
      "Iteration 109, loss = 0.02184051\n",
      "Iteration 110, loss = 0.02148045\n",
      "Iteration 111, loss = 0.02112507\n",
      "Iteration 112, loss = 0.02077906\n",
      "Iteration 113, loss = 0.02044459\n",
      "Iteration 114, loss = 0.02011616\n",
      "Iteration 115, loss = 0.01980735\n",
      "Iteration 116, loss = 0.01949459\n",
      "Iteration 117, loss = 0.01919588\n",
      "Iteration 118, loss = 0.01890262\n",
      "Iteration 119, loss = 0.01861818\n",
      "Iteration 120, loss = 0.01834275\n",
      "Iteration 121, loss = 0.01808054\n",
      "Iteration 122, loss = 0.01781614\n",
      "Iteration 123, loss = 0.01756074\n",
      "Iteration 124, loss = 0.01731430\n",
      "Iteration 125, loss = 0.01707121\n",
      "Iteration 126, loss = 0.01683725\n",
      "Iteration 127, loss = 0.01660683\n",
      "Iteration 128, loss = 0.01638006\n",
      "Iteration 129, loss = 0.01616156\n",
      "Iteration 130, loss = 0.01594920\n",
      "Iteration 131, loss = 0.01574183\n",
      "Iteration 132, loss = 0.01553636\n",
      "Iteration 133, loss = 0.01533380\n",
      "Iteration 134, loss = 0.01513948\n",
      "Iteration 135, loss = 0.01494336\n",
      "Iteration 136, loss = 0.01475938\n",
      "Iteration 137, loss = 0.01457504\n",
      "Iteration 138, loss = 0.01439624\n",
      "Iteration 139, loss = 0.01422166\n",
      "Iteration 140, loss = 0.01405245\n",
      "Iteration 141, loss = 0.01388682\n",
      "Iteration 142, loss = 0.01371930\n",
      "Iteration 143, loss = 0.01355672\n",
      "Iteration 144, loss = 0.01339881\n",
      "Iteration 145, loss = 0.01324367\n",
      "Iteration 146, loss = 0.01308909\n",
      "Iteration 147, loss = 0.01294222\n",
      "Iteration 148, loss = 0.01279685\n",
      "Iteration 149, loss = 0.01265486\n",
      "Iteration 150, loss = 0.01251743\n",
      "Iteration 151, loss = 0.01238222\n",
      "Iteration 152, loss = 0.01224775\n",
      "Iteration 153, loss = 0.01211828\n",
      "Iteration 154, loss = 0.01198936\n",
      "Iteration 155, loss = 0.01186344\n",
      "Iteration 156, loss = 0.01173922\n",
      "Iteration 157, loss = 0.01161685\n",
      "Iteration 158, loss = 0.01149791\n",
      "Iteration 159, loss = 0.01138134\n",
      "Iteration 160, loss = 0.01126527\n",
      "Iteration 161, loss = 0.01115298\n",
      "Iteration 162, loss = 0.01104142\n",
      "Iteration 163, loss = 0.01093271\n",
      "Iteration 164, loss = 0.01082645\n",
      "Iteration 165, loss = 0.01072219\n",
      "Iteration 166, loss = 0.01061696\n",
      "Iteration 167, loss = 0.01051610\n",
      "Iteration 168, loss = 0.01041682\n",
      "Iteration 169, loss = 0.01031719\n",
      "Iteration 170, loss = 0.01022197\n",
      "Iteration 171, loss = 0.01012705\n",
      "Iteration 172, loss = 0.01003464\n",
      "Iteration 173, loss = 0.00994162\n",
      "Iteration 174, loss = 0.00985101\n",
      "Iteration 175, loss = 0.00976281\n",
      "Iteration 176, loss = 0.00967379\n",
      "Iteration 177, loss = 0.00958794\n",
      "Iteration 178, loss = 0.00950304\n",
      "Iteration 179, loss = 0.00941982\n",
      "Iteration 180, loss = 0.00933792\n",
      "Iteration 181, loss = 0.00925752\n",
      "Iteration 182, loss = 0.00917907\n",
      "Iteration 183, loss = 0.00909991\n",
      "Iteration 184, loss = 0.00902186\n",
      "Iteration 185, loss = 0.00894710\n",
      "Iteration 186, loss = 0.00887269\n",
      "Iteration 187, loss = 0.00879850\n",
      "Iteration 188, loss = 0.00872631\n",
      "Iteration 189, loss = 0.00865446\n",
      "Iteration 190, loss = 0.00858439\n",
      "Iteration 191, loss = 0.00851344\n",
      "Iteration 192, loss = 0.00844682\n",
      "Iteration 193, loss = 0.00837862\n",
      "Iteration 194, loss = 0.00831281\n",
      "Iteration 195, loss = 0.00824671\n",
      "Iteration 196, loss = 0.00818237\n",
      "Iteration 197, loss = 0.00811994\n",
      "Iteration 198, loss = 0.00805672\n",
      "Iteration 199, loss = 0.00799434\n",
      "Iteration 200, loss = 0.00793446\n",
      "Iteration 201, loss = 0.00787380\n",
      "Iteration 202, loss = 0.00781392\n",
      "Iteration 203, loss = 0.00775572\n",
      "Iteration 204, loss = 0.00769833\n",
      "Iteration 205, loss = 0.00764207\n",
      "Iteration 206, loss = 0.00758525\n",
      "Iteration 207, loss = 0.00752981\n",
      "Iteration 208, loss = 0.00747569\n",
      "Iteration 209, loss = 0.00742187\n",
      "Iteration 210, loss = 0.00736890\n",
      "Iteration 211, loss = 0.00731755\n",
      "Iteration 212, loss = 0.00726600\n",
      "Iteration 213, loss = 0.00721453\n",
      "Iteration 214, loss = 0.00716453\n",
      "Iteration 215, loss = 0.00711507\n",
      "Iteration 216, loss = 0.00706554\n",
      "Iteration 217, loss = 0.00701831\n",
      "Iteration 218, loss = 0.00696995\n",
      "Iteration 219, loss = 0.00692244\n",
      "Iteration 220, loss = 0.00687680\n",
      "Iteration 221, loss = 0.00683061\n",
      "Iteration 222, loss = 0.00678600\n",
      "Iteration 223, loss = 0.00674115\n",
      "Iteration 224, loss = 0.00669722\n",
      "Iteration 225, loss = 0.00665431\n",
      "Iteration 226, loss = 0.00661127\n",
      "Iteration 227, loss = 0.00656931\n",
      "Iteration 228, loss = 0.00652736\n",
      "Iteration 229, loss = 0.00648597\n",
      "Iteration 230, loss = 0.00644534\n",
      "Iteration 231, loss = 0.00640489\n",
      "Iteration 232, loss = 0.00636561\n",
      "Iteration 233, loss = 0.00632597\n",
      "Iteration 234, loss = 0.00628657\n",
      "Iteration 235, loss = 0.00624850\n",
      "Iteration 236, loss = 0.00621051\n",
      "Iteration 237, loss = 0.00617264\n",
      "Iteration 238, loss = 0.00613531\n",
      "Iteration 239, loss = 0.00609815\n",
      "Iteration 240, loss = 0.00606198\n",
      "Iteration 241, loss = 0.00602585\n",
      "Iteration 242, loss = 0.00599012\n",
      "Iteration 243, loss = 0.00595469\n",
      "Iteration 244, loss = 0.00591993\n",
      "Iteration 245, loss = 0.00588537\n",
      "Iteration 246, loss = 0.00585128\n",
      "Iteration 247, loss = 0.00581760\n",
      "Iteration 248, loss = 0.00578500\n",
      "Iteration 249, loss = 0.00575166\n",
      "Iteration 250, loss = 0.00571956\n",
      "Iteration 251, loss = 0.00568723\n",
      "Iteration 252, loss = 0.00565614\n",
      "Iteration 253, loss = 0.00562474\n",
      "Iteration 254, loss = 0.00559401\n",
      "Iteration 255, loss = 0.00556335\n",
      "Iteration 256, loss = 0.00553260\n",
      "Iteration 257, loss = 0.00550259\n",
      "Iteration 258, loss = 0.00547299\n",
      "Iteration 259, loss = 0.00544355\n",
      "Iteration 260, loss = 0.00541469\n",
      "Iteration 261, loss = 0.00538593\n",
      "Iteration 262, loss = 0.00535713\n",
      "Iteration 263, loss = 0.00532946\n",
      "Iteration 264, loss = 0.00530153\n",
      "Iteration 265, loss = 0.00527399\n",
      "Iteration 266, loss = 0.00524670\n",
      "Iteration 267, loss = 0.00521964\n",
      "Iteration 268, loss = 0.00519293\n",
      "Iteration 269, loss = 0.00516632\n",
      "Iteration 270, loss = 0.00514007\n",
      "Iteration 271, loss = 0.00511409\n",
      "Iteration 272, loss = 0.00508791\n",
      "Iteration 273, loss = 0.00506233\n",
      "Iteration 274, loss = 0.00503712\n",
      "Iteration 275, loss = 0.00501184\n",
      "Iteration 276, loss = 0.00498715\n",
      "Iteration 277, loss = 0.00496242\n",
      "Iteration 278, loss = 0.00493788\n",
      "Iteration 279, loss = 0.00491393\n",
      "Iteration 280, loss = 0.00489001\n",
      "Iteration 281, loss = 0.00486631\n",
      "Iteration 282, loss = 0.00484274\n",
      "Iteration 283, loss = 0.00481997\n",
      "Iteration 284, loss = 0.00479677\n",
      "Iteration 285, loss = 0.00477356\n",
      "Iteration 286, loss = 0.00475125\n",
      "Iteration 287, loss = 0.00472874\n",
      "Iteration 288, loss = 0.00470659\n",
      "Iteration 289, loss = 0.00468471\n",
      "Iteration 290, loss = 0.00466321\n",
      "Iteration 291, loss = 0.00464133\n",
      "Iteration 292, loss = 0.00462013\n",
      "Iteration 293, loss = 0.00459871\n",
      "Iteration 294, loss = 0.00457806\n",
      "Iteration 295, loss = 0.00455702\n",
      "Iteration 296, loss = 0.00453638\n",
      "Iteration 297, loss = 0.00451609\n",
      "Iteration 298, loss = 0.00449612\n",
      "Iteration 299, loss = 0.00447650\n",
      "Iteration 300, loss = 0.00445603\n",
      "Iteration 301, loss = 0.00443653\n",
      "Iteration 302, loss = 0.00441707\n",
      "Iteration 303, loss = 0.00439757\n",
      "Iteration 304, loss = 0.00437843\n",
      "Iteration 305, loss = 0.00435952\n",
      "Iteration 306, loss = 0.00434057\n",
      "Iteration 307, loss = 0.00432192\n",
      "Iteration 308, loss = 0.00430351\n",
      "Iteration 309, loss = 0.00428552\n",
      "Iteration 310, loss = 0.00426731\n",
      "Iteration 311, loss = 0.00424940\n",
      "Iteration 312, loss = 0.00423154\n",
      "Iteration 313, loss = 0.00421385\n",
      "Iteration 314, loss = 0.00419633\n",
      "Iteration 315, loss = 0.00417937\n",
      "Iteration 316, loss = 0.00416201\n",
      "Iteration 317, loss = 0.00414485\n",
      "Iteration 318, loss = 0.00412794\n",
      "Iteration 319, loss = 0.00411123\n",
      "Iteration 320, loss = 0.00409436\n",
      "Iteration 321, loss = 0.00407783\n",
      "Iteration 322, loss = 0.00406143\n",
      "Iteration 323, loss = 0.00404551\n",
      "Iteration 324, loss = 0.00402927\n",
      "Iteration 325, loss = 0.00401333\n",
      "Iteration 326, loss = 0.00399740\n",
      "Iteration 327, loss = 0.00398153\n",
      "Iteration 328, loss = 0.00396608\n",
      "Iteration 329, loss = 0.00395069\n",
      "Iteration 330, loss = 0.00393537\n",
      "Iteration 331, loss = 0.00392017\n",
      "Iteration 332, loss = 0.00390525\n",
      "Iteration 333, loss = 0.00389046\n",
      "Iteration 334, loss = 0.00387562\n",
      "Iteration 335, loss = 0.00386104\n",
      "Iteration 336, loss = 0.00384664\n",
      "Iteration 337, loss = 0.00383221\n",
      "Iteration 338, loss = 0.00381781\n",
      "Iteration 339, loss = 0.00380365\n",
      "Iteration 340, loss = 0.00378970\n",
      "Iteration 341, loss = 0.00377552\n",
      "Iteration 342, loss = 0.00376158\n",
      "Iteration 343, loss = 0.00374771\n",
      "Iteration 344, loss = 0.00373404\n",
      "Iteration 345, loss = 0.00372038\n",
      "Iteration 346, loss = 0.00370702\n",
      "Iteration 347, loss = 0.00369363\n",
      "Iteration 348, loss = 0.00368038\n",
      "Iteration 349, loss = 0.00366719\n",
      "Iteration 350, loss = 0.00365407\n",
      "Iteration 351, loss = 0.00364111\n",
      "Iteration 352, loss = 0.00362824\n",
      "Iteration 353, loss = 0.00361551\n",
      "Iteration 354, loss = 0.00360285\n",
      "Iteration 355, loss = 0.00359012\n",
      "Iteration 356, loss = 0.00357762\n",
      "Iteration 357, loss = 0.00356508\n",
      "Iteration 358, loss = 0.00355298\n",
      "Iteration 359, loss = 0.00354061\n",
      "Iteration 360, loss = 0.00352847\n",
      "Iteration 361, loss = 0.00351661\n",
      "Iteration 362, loss = 0.00350469\n",
      "Iteration 363, loss = 0.00349283\n",
      "Iteration 364, loss = 0.00348104\n",
      "Iteration 365, loss = 0.00346915\n",
      "Iteration 366, loss = 0.00345772\n",
      "Iteration 367, loss = 0.00344616\n",
      "Iteration 368, loss = 0.00343459\n",
      "Iteration 369, loss = 0.00342324\n",
      "Iteration 370, loss = 0.00341204\n",
      "Iteration 371, loss = 0.00340067\n",
      "Iteration 372, loss = 0.00338937\n",
      "Iteration 373, loss = 0.00337842\n",
      "Iteration 374, loss = 0.00336739\n",
      "Iteration 375, loss = 0.00335655\n",
      "Iteration 376, loss = 0.00334569\n",
      "Iteration 377, loss = 0.00333484\n",
      "Iteration 378, loss = 0.00332429\n",
      "Iteration 379, loss = 0.00331360\n",
      "Iteration 380, loss = 0.00330316\n",
      "Iteration 381, loss = 0.00329266\n",
      "Iteration 382, loss = 0.00328228\n",
      "Iteration 383, loss = 0.00327189\n",
      "Iteration 384, loss = 0.00326196\n",
      "Iteration 385, loss = 0.00325167\n",
      "Iteration 386, loss = 0.00324170\n",
      "Iteration 387, loss = 0.00323175\n",
      "Iteration 388, loss = 0.00322170\n",
      "Iteration 389, loss = 0.00321183\n",
      "Iteration 390, loss = 0.00320209\n",
      "Iteration 391, loss = 0.00319232\n",
      "Iteration 392, loss = 0.00318260\n",
      "Iteration 393, loss = 0.00317301\n",
      "Iteration 394, loss = 0.00316343\n",
      "Iteration 395, loss = 0.00315399\n",
      "Iteration 396, loss = 0.00314451\n",
      "Iteration 397, loss = 0.00313508\n",
      "Iteration 398, loss = 0.00312589\n",
      "Iteration 399, loss = 0.00311651\n",
      "Iteration 400, loss = 0.00310733\n",
      "Iteration 401, loss = 0.00309827\n",
      "Iteration 402, loss = 0.00308909\n",
      "Iteration 403, loss = 0.00308009\n",
      "Iteration 404, loss = 0.00307105\n",
      "Iteration 405, loss = 0.00306222\n",
      "Iteration 406, loss = 0.00305329\n",
      "Iteration 407, loss = 0.00304451\n",
      "Iteration 408, loss = 0.00303580\n",
      "Iteration 409, loss = 0.00302706\n",
      "Iteration 410, loss = 0.00301842\n",
      "Iteration 411, loss = 0.00300988\n",
      "Iteration 412, loss = 0.00300131\n",
      "Iteration 413, loss = 0.00299291\n",
      "Iteration 414, loss = 0.00298446\n",
      "Iteration 415, loss = 0.00297612\n",
      "Iteration 416, loss = 0.00296782\n",
      "Iteration 417, loss = 0.00295950\n",
      "Iteration 418, loss = 0.00295136\n",
      "Iteration 419, loss = 0.00294323\n",
      "Iteration 420, loss = 0.00293506\n",
      "Iteration 421, loss = 0.00292702\n",
      "Iteration 422, loss = 0.00291905\n",
      "Iteration 423, loss = 0.00291106\n",
      "Iteration 424, loss = 0.00290317\n",
      "Iteration 425, loss = 0.00289541\n",
      "Iteration 426, loss = 0.00288766\n",
      "Iteration 427, loss = 0.00288001\n",
      "Iteration 428, loss = 0.00287225\n",
      "Iteration 429, loss = 0.00286457\n",
      "Iteration 430, loss = 0.00285703\n",
      "Iteration 431, loss = 0.00284942\n",
      "Iteration 432, loss = 0.00284207\n",
      "Iteration 433, loss = 0.00283447\n",
      "Iteration 434, loss = 0.00282701\n",
      "Iteration 435, loss = 0.00281968\n",
      "Iteration 436, loss = 0.00281223\n",
      "Iteration 437, loss = 0.00280501\n",
      "Iteration 438, loss = 0.00279764\n",
      "Iteration 439, loss = 0.00279051\n",
      "Iteration 440, loss = 0.00278331\n",
      "Iteration 441, loss = 0.00277618\n",
      "Iteration 442, loss = 0.00276907\n",
      "Iteration 443, loss = 0.00276209\n",
      "Iteration 444, loss = 0.00275512\n",
      "Iteration 445, loss = 0.00274800\n",
      "Iteration 446, loss = 0.00274118\n",
      "Iteration 447, loss = 0.00273426\n",
      "Iteration 448, loss = 0.00272747\n",
      "Iteration 449, loss = 0.00272070\n",
      "Iteration 450, loss = 0.00271400\n",
      "Iteration 451, loss = 0.00270722\n",
      "Iteration 452, loss = 0.00270056\n",
      "Iteration 453, loss = 0.00269379\n",
      "Iteration 454, loss = 0.00268718\n",
      "Iteration 455, loss = 0.00268061\n",
      "Iteration 456, loss = 0.00267412\n",
      "Iteration 457, loss = 0.00266767\n",
      "Iteration 458, loss = 0.00266116\n",
      "Iteration 459, loss = 0.00265467\n",
      "Iteration 460, loss = 0.00264827\n",
      "Iteration 461, loss = 0.00264195\n",
      "Iteration 462, loss = 0.00263555\n",
      "Iteration 463, loss = 0.00262925\n",
      "Iteration 464, loss = 0.00262296\n",
      "Iteration 465, loss = 0.00261675\n",
      "Iteration 466, loss = 0.00261054\n",
      "Iteration 467, loss = 0.00260441\n",
      "Iteration 468, loss = 0.00259823\n",
      "Iteration 469, loss = 0.00259223\n",
      "Iteration 470, loss = 0.00258612\n",
      "Iteration 471, loss = 0.00258013\n",
      "Iteration 472, loss = 0.00257415\n",
      "Iteration 473, loss = 0.00256822\n",
      "Iteration 474, loss = 0.00256228\n",
      "Iteration 475, loss = 0.00255637\n",
      "Iteration 476, loss = 0.00255060\n",
      "Iteration 477, loss = 0.00254475\n",
      "Iteration 478, loss = 0.00253896\n",
      "Iteration 479, loss = 0.00253315\n",
      "Iteration 480, loss = 0.00252737\n",
      "Iteration 481, loss = 0.00252174\n",
      "Iteration 482, loss = 0.00251602\n",
      "Iteration 483, loss = 0.00251041\n",
      "Iteration 484, loss = 0.00250483\n",
      "Iteration 485, loss = 0.00249919\n",
      "Iteration 486, loss = 0.00249371\n",
      "Iteration 487, loss = 0.00248816\n",
      "Iteration 488, loss = 0.00248267\n",
      "Iteration 489, loss = 0.00247717\n",
      "Iteration 490, loss = 0.00247169\n",
      "Iteration 491, loss = 0.00246633\n",
      "Iteration 492, loss = 0.00246091\n",
      "Iteration 493, loss = 0.00245551\n",
      "Iteration 494, loss = 0.00245022\n",
      "Iteration 495, loss = 0.00244481\n",
      "Iteration 496, loss = 0.00243956\n",
      "Iteration 497, loss = 0.00243427\n",
      "Iteration 498, loss = 0.00242907\n",
      "Iteration 499, loss = 0.00242378\n",
      "Iteration 500, loss = 0.00241859\n",
      "Iteration 501, loss = 0.00241347\n",
      "Iteration 502, loss = 0.00240828\n",
      "Iteration 503, loss = 0.00240320\n",
      "Iteration 504, loss = 0.00239807\n",
      "Iteration 505, loss = 0.00239293\n",
      "Iteration 506, loss = 0.00238794\n",
      "Iteration 507, loss = 0.00238295\n",
      "Iteration 508, loss = 0.00237794\n",
      "Iteration 509, loss = 0.00237297\n",
      "Iteration 510, loss = 0.00236797\n",
      "Iteration 511, loss = 0.00236311\n",
      "Iteration 512, loss = 0.00235820\n",
      "Iteration 513, loss = 0.00235337\n",
      "Iteration 514, loss = 0.00234853\n",
      "Iteration 515, loss = 0.00234365\n",
      "Iteration 516, loss = 0.00233893\n",
      "Iteration 517, loss = 0.00233417\n",
      "Iteration 518, loss = 0.00232939\n",
      "Iteration 519, loss = 0.00232473\n",
      "Iteration 520, loss = 0.00232001\n",
      "Iteration 521, loss = 0.00231530\n",
      "Iteration 522, loss = 0.00231068\n",
      "Iteration 523, loss = 0.00230604\n",
      "Iteration 524, loss = 0.00230147\n",
      "Iteration 525, loss = 0.00229687\n",
      "Iteration 526, loss = 0.00229228\n",
      "Iteration 527, loss = 0.00228774\n",
      "Iteration 528, loss = 0.00228320\n",
      "Iteration 529, loss = 0.00227873\n",
      "Iteration 530, loss = 0.00227423\n",
      "Iteration 531, loss = 0.00226974\n",
      "Iteration 532, loss = 0.00226527\n",
      "Iteration 533, loss = 0.00226089\n",
      "Iteration 534, loss = 0.00225639\n",
      "Iteration 535, loss = 0.00225207\n",
      "Iteration 536, loss = 0.00224763\n",
      "Iteration 537, loss = 0.00224333\n",
      "Iteration 538, loss = 0.00223898\n",
      "Iteration 539, loss = 0.00223467\n",
      "Iteration 540, loss = 0.00223036\n",
      "Iteration 541, loss = 0.00222609\n",
      "Iteration 542, loss = 0.00222191\n",
      "Iteration 543, loss = 0.00221768\n",
      "Iteration 544, loss = 0.00221345\n",
      "Iteration 545, loss = 0.00220928\n",
      "Iteration 546, loss = 0.00220514\n",
      "Iteration 547, loss = 0.00220103\n",
      "Iteration 548, loss = 0.00219687\n",
      "Iteration 549, loss = 0.00219276\n",
      "Iteration 550, loss = 0.00218867\n",
      "Iteration 551, loss = 0.00218458\n",
      "Iteration 552, loss = 0.00218055\n",
      "Iteration 553, loss = 0.00217650\n",
      "Iteration 554, loss = 0.00217240\n",
      "Iteration 555, loss = 0.00216843\n",
      "Iteration 556, loss = 0.00216445\n",
      "Iteration 557, loss = 0.00216043\n",
      "Iteration 558, loss = 0.00215651\n",
      "Iteration 559, loss = 0.00215254\n",
      "Iteration 560, loss = 0.00214868\n",
      "Iteration 561, loss = 0.00214476\n",
      "Iteration 562, loss = 0.00214082\n",
      "Iteration 563, loss = 0.00213699\n",
      "Iteration 564, loss = 0.00213311\n",
      "Iteration 565, loss = 0.00212927\n",
      "Iteration 566, loss = 0.00212550\n",
      "Iteration 567, loss = 0.00212167\n",
      "Iteration 568, loss = 0.00211789\n",
      "Iteration 569, loss = 0.00211412\n",
      "Iteration 570, loss = 0.00211034\n",
      "Iteration 571, loss = 0.00210667\n",
      "Iteration 572, loss = 0.00210290\n",
      "Iteration 573, loss = 0.00209919\n",
      "Iteration 574, loss = 0.00209555\n",
      "Iteration 575, loss = 0.00209186\n",
      "Iteration 576, loss = 0.00208818\n",
      "Iteration 577, loss = 0.00208454\n",
      "Iteration 578, loss = 0.00208088\n",
      "Iteration 579, loss = 0.00207730\n",
      "Iteration 580, loss = 0.00207362\n",
      "Iteration 581, loss = 0.00207003\n",
      "Iteration 582, loss = 0.00206648\n",
      "Iteration 583, loss = 0.00206293\n",
      "Iteration 584, loss = 0.00205937\n",
      "Iteration 585, loss = 0.00205586\n",
      "Iteration 586, loss = 0.00205235\n",
      "Iteration 587, loss = 0.00204888\n",
      "Iteration 588, loss = 0.00204542\n",
      "Iteration 589, loss = 0.00204193\n",
      "Iteration 590, loss = 0.00203849\n",
      "Iteration 591, loss = 0.00203508\n",
      "Iteration 592, loss = 0.00203165\n",
      "Iteration 593, loss = 0.00202830\n",
      "Iteration 594, loss = 0.00202487\n",
      "Iteration 595, loss = 0.00202151\n",
      "Iteration 596, loss = 0.00201813\n",
      "Iteration 597, loss = 0.00201476\n",
      "Iteration 598, loss = 0.00201146\n",
      "Iteration 599, loss = 0.00200807\n",
      "Iteration 600, loss = 0.00200480\n",
      "Iteration 601, loss = 0.00200149\n",
      "Iteration 602, loss = 0.00199818\n",
      "Iteration 603, loss = 0.00199490\n",
      "Iteration 604, loss = 0.00199162\n",
      "Iteration 605, loss = 0.00198837\n",
      "Iteration 606, loss = 0.00198518\n",
      "Iteration 607, loss = 0.00198187\n",
      "Iteration 608, loss = 0.00197869\n",
      "Iteration 609, loss = 0.00197547\n",
      "Iteration 610, loss = 0.00197227\n",
      "Iteration 611, loss = 0.00196911\n",
      "Iteration 612, loss = 0.00196595\n",
      "Iteration 613, loss = 0.00196276\n",
      "Iteration 614, loss = 0.00195962\n",
      "Iteration 615, loss = 0.00195649\n",
      "Iteration 616, loss = 0.00195340\n",
      "Iteration 617, loss = 0.00195029\n",
      "Iteration 618, loss = 0.00194720\n",
      "Iteration 619, loss = 0.00194413\n",
      "Iteration 620, loss = 0.00194106\n",
      "Iteration 621, loss = 0.00193801\n",
      "Iteration 622, loss = 0.00193501\n",
      "Iteration 623, loss = 0.00193195\n",
      "Iteration 624, loss = 0.00192894\n",
      "Iteration 625, loss = 0.00192591\n",
      "Iteration 626, loss = 0.00192294\n",
      "Iteration 627, loss = 0.00191996\n",
      "Iteration 628, loss = 0.00191699\n",
      "Iteration 629, loss = 0.00191402\n",
      "Iteration 630, loss = 0.00191107\n",
      "Iteration 631, loss = 0.00190814\n",
      "Iteration 632, loss = 0.00190520\n",
      "Iteration 633, loss = 0.00190228\n",
      "Iteration 634, loss = 0.00189937\n",
      "Iteration 635, loss = 0.00189646\n",
      "Iteration 636, loss = 0.00189359\n",
      "Iteration 637, loss = 0.00189069\n",
      "Iteration 638, loss = 0.00188785\n",
      "Iteration 639, loss = 0.00188496\n",
      "Iteration 640, loss = 0.00188217\n",
      "Iteration 641, loss = 0.00187932\n",
      "Iteration 642, loss = 0.00187653\n",
      "Iteration 643, loss = 0.00187369\n",
      "Iteration 644, loss = 0.00187090\n",
      "Iteration 645, loss = 0.00186815\n",
      "Iteration 646, loss = 0.00186532\n",
      "Iteration 647, loss = 0.00186258\n",
      "Iteration 648, loss = 0.00185981\n",
      "Iteration 649, loss = 0.00185709\n",
      "Iteration 650, loss = 0.00185433\n",
      "Iteration 651, loss = 0.00185161\n",
      "Iteration 652, loss = 0.00184889\n",
      "Iteration 653, loss = 0.00184621\n",
      "Iteration 654, loss = 0.00184349\n",
      "Iteration 655, loss = 0.00184083\n",
      "Iteration 656, loss = 0.00183814\n",
      "Iteration 657, loss = 0.00183546\n",
      "Iteration 658, loss = 0.00183281\n",
      "Iteration 659, loss = 0.00183017\n",
      "Iteration 660, loss = 0.00182754\n",
      "Iteration 661, loss = 0.00182489\n",
      "Iteration 662, loss = 0.00182226\n",
      "Iteration 663, loss = 0.00181963\n",
      "Iteration 664, loss = 0.00181704\n",
      "Iteration 665, loss = 0.00181444\n",
      "Iteration 666, loss = 0.00181179\n",
      "Iteration 667, loss = 0.00180923\n",
      "Iteration 668, loss = 0.00180666\n",
      "Iteration 669, loss = 0.00180409\n",
      "Iteration 670, loss = 0.00180152\n",
      "Iteration 671, loss = 0.00179900\n",
      "Iteration 672, loss = 0.00179645\n",
      "Iteration 673, loss = 0.00179392\n",
      "Iteration 674, loss = 0.00179141\n",
      "Iteration 675, loss = 0.00178890\n",
      "Iteration 676, loss = 0.00178639\n",
      "Iteration 677, loss = 0.00178392\n",
      "Iteration 678, loss = 0.00178144\n",
      "Iteration 679, loss = 0.00177897\n",
      "Iteration 680, loss = 0.00177651\n",
      "Iteration 681, loss = 0.00177407\n",
      "Iteration 682, loss = 0.00177161\n",
      "Iteration 683, loss = 0.00176918\n",
      "Iteration 684, loss = 0.00176675\n",
      "Iteration 685, loss = 0.00176433\n",
      "Iteration 686, loss = 0.00176196\n",
      "Iteration 687, loss = 0.00175956\n",
      "Iteration 688, loss = 0.00175719\n",
      "Iteration 689, loss = 0.00175482\n",
      "Iteration 690, loss = 0.00175241\n",
      "Iteration 691, loss = 0.00175005\n",
      "Iteration 692, loss = 0.00174768\n",
      "Iteration 693, loss = 0.00174536\n",
      "Iteration 694, loss = 0.00174297\n",
      "Iteration 695, loss = 0.00174063\n",
      "Iteration 696, loss = 0.00173832\n",
      "Iteration 697, loss = 0.00173599\n",
      "Iteration 698, loss = 0.00173368\n",
      "Iteration 699, loss = 0.00173136\n",
      "Iteration 700, loss = 0.00172909\n",
      "Iteration 701, loss = 0.00172679\n",
      "Iteration 702, loss = 0.00172450\n",
      "Iteration 703, loss = 0.00172223\n",
      "Iteration 704, loss = 0.00171995\n",
      "Iteration 705, loss = 0.00171769\n",
      "Iteration 706, loss = 0.00171543\n",
      "Iteration 707, loss = 0.00171319\n",
      "Iteration 708, loss = 0.00171096\n",
      "Iteration 709, loss = 0.00170873\n",
      "Iteration 710, loss = 0.00170652\n",
      "Iteration 711, loss = 0.00170429\n",
      "Iteration 712, loss = 0.00170208\n",
      "Iteration 713, loss = 0.00169989\n",
      "Iteration 714, loss = 0.00169768\n",
      "Iteration 715, loss = 0.00169548\n",
      "Iteration 716, loss = 0.00169332\n",
      "Iteration 717, loss = 0.00169113\n",
      "Iteration 718, loss = 0.00168897\n",
      "Iteration 719, loss = 0.00168681\n",
      "Iteration 720, loss = 0.00168465\n",
      "Iteration 721, loss = 0.00168249\n",
      "Iteration 722, loss = 0.00168038\n",
      "Iteration 723, loss = 0.00167823\n",
      "Iteration 724, loss = 0.00167611\n",
      "Iteration 725, loss = 0.00167399\n",
      "Iteration 726, loss = 0.00167188\n",
      "Iteration 727, loss = 0.00166975\n",
      "Iteration 728, loss = 0.00166767\n",
      "Iteration 729, loss = 0.00166556\n",
      "Iteration 730, loss = 0.00166348\n",
      "Iteration 731, loss = 0.00166140\n",
      "Iteration 732, loss = 0.00165932\n",
      "Iteration 733, loss = 0.00165726\n",
      "Iteration 734, loss = 0.00165521\n",
      "Iteration 735, loss = 0.00165315\n",
      "Iteration 736, loss = 0.00165109\n",
      "Iteration 737, loss = 0.00164904\n",
      "Iteration 738, loss = 0.00164704\n",
      "Iteration 739, loss = 0.00164498\n",
      "Iteration 740, loss = 0.00164297\n",
      "Iteration 741, loss = 0.00164095\n",
      "Iteration 742, loss = 0.00163894\n",
      "Iteration 743, loss = 0.00163692\n",
      "Iteration 744, loss = 0.00163493\n",
      "Iteration 745, loss = 0.00163295\n",
      "Iteration 746, loss = 0.00163099\n",
      "Iteration 747, loss = 0.00162899\n",
      "Iteration 748, loss = 0.00162701\n",
      "Iteration 749, loss = 0.00162506\n",
      "Iteration 750, loss = 0.00162309\n",
      "Iteration 751, loss = 0.00162115\n",
      "Iteration 752, loss = 0.00161918\n",
      "Iteration 753, loss = 0.00161725\n",
      "Iteration 754, loss = 0.00161532\n",
      "Iteration 755, loss = 0.00161337\n",
      "Iteration 756, loss = 0.00161146\n",
      "Iteration 757, loss = 0.00160953\n",
      "Iteration 758, loss = 0.00160761\n",
      "Iteration 759, loss = 0.00160571\n",
      "Iteration 760, loss = 0.00160380\n",
      "Iteration 761, loss = 0.00160191\n",
      "Iteration 762, loss = 0.00160000\n",
      "Iteration 763, loss = 0.00159812\n",
      "Iteration 764, loss = 0.00159627\n",
      "Iteration 765, loss = 0.00159436\n",
      "Iteration 766, loss = 0.00159251\n",
      "Iteration 767, loss = 0.00159064\n",
      "Iteration 768, loss = 0.00158878\n",
      "Iteration 769, loss = 0.00158692\n",
      "Iteration 770, loss = 0.00158508\n",
      "Iteration 771, loss = 0.00158325\n",
      "Iteration 772, loss = 0.00158142\n",
      "Iteration 773, loss = 0.00157960\n",
      "Iteration 774, loss = 0.00157778\n",
      "Iteration 775, loss = 0.00157598\n",
      "Iteration 776, loss = 0.00157416\n",
      "Iteration 777, loss = 0.00157236\n",
      "Iteration 778, loss = 0.00157056\n",
      "Iteration 779, loss = 0.00156877\n",
      "Iteration 780, loss = 0.00156700\n",
      "Iteration 781, loss = 0.00156520\n",
      "Iteration 782, loss = 0.00156344\n",
      "Iteration 783, loss = 0.00156168\n",
      "Iteration 784, loss = 0.00155991\n",
      "Iteration 785, loss = 0.00155813\n",
      "Iteration 786, loss = 0.00155638\n",
      "Iteration 787, loss = 0.00155463\n",
      "Iteration 788, loss = 0.00155291\n",
      "Iteration 789, loss = 0.00155115\n",
      "Iteration 790, loss = 0.00154943\n",
      "Iteration 791, loss = 0.00154770\n",
      "Iteration 792, loss = 0.00154596\n",
      "Iteration 793, loss = 0.00154423\n",
      "Iteration 794, loss = 0.00154252\n",
      "Iteration 795, loss = 0.00154080\n",
      "Iteration 796, loss = 0.00153910\n",
      "Iteration 797, loss = 0.00153739\n",
      "Iteration 798, loss = 0.00153569\n",
      "Iteration 799, loss = 0.00153402\n",
      "Iteration 800, loss = 0.00153230\n",
      "Iteration 801, loss = 0.00153063\n",
      "Iteration 802, loss = 0.00152895\n",
      "Iteration 803, loss = 0.00152728\n",
      "Iteration 804, loss = 0.00152562\n",
      "Iteration 805, loss = 0.00152396\n",
      "Iteration 806, loss = 0.00152231\n",
      "Iteration 807, loss = 0.00152065\n",
      "Iteration 808, loss = 0.00151902\n",
      "Iteration 809, loss = 0.00151737\n",
      "Iteration 810, loss = 0.00151573\n",
      "Iteration 811, loss = 0.00151413\n",
      "Iteration 812, loss = 0.00151249\n",
      "Iteration 813, loss = 0.00151088\n",
      "Iteration 814, loss = 0.00150926\n",
      "Iteration 815, loss = 0.00150763\n",
      "Iteration 816, loss = 0.00150602\n",
      "Iteration 817, loss = 0.00150441\n",
      "Iteration 818, loss = 0.00150280\n",
      "Iteration 819, loss = 0.00150120\n",
      "Iteration 820, loss = 0.00149960\n",
      "Iteration 821, loss = 0.00149801\n",
      "Iteration 822, loss = 0.00149644\n",
      "Iteration 823, loss = 0.00149485\n",
      "Iteration 824, loss = 0.00149327\n",
      "Iteration 825, loss = 0.00149171\n",
      "Iteration 826, loss = 0.00149013\n",
      "Iteration 827, loss = 0.00148858\n",
      "Iteration 828, loss = 0.00148700\n",
      "Iteration 829, loss = 0.00148547\n",
      "Iteration 830, loss = 0.00148392\n",
      "Iteration 831, loss = 0.00148236\n",
      "Iteration 832, loss = 0.00148082\n",
      "Iteration 833, loss = 0.00147929\n",
      "Iteration 834, loss = 0.00147776\n",
      "Iteration 835, loss = 0.00147622\n",
      "Iteration 836, loss = 0.00147472\n",
      "Iteration 837, loss = 0.00147318\n",
      "Iteration 838, loss = 0.00147168\n",
      "Iteration 839, loss = 0.00147015\n",
      "Iteration 840, loss = 0.00146868\n",
      "Iteration 841, loss = 0.00146716\n",
      "Iteration 842, loss = 0.00146569\n",
      "Iteration 843, loss = 0.00146418\n",
      "Iteration 844, loss = 0.00146270\n",
      "Iteration 845, loss = 0.00146121\n",
      "Iteration 846, loss = 0.00145974\n",
      "Iteration 847, loss = 0.00145825\n",
      "Iteration 848, loss = 0.00145678\n",
      "Iteration 849, loss = 0.00145532\n",
      "Iteration 850, loss = 0.00145383\n",
      "Iteration 851, loss = 0.00145237\n",
      "Iteration 852, loss = 0.00145090\n",
      "Iteration 853, loss = 0.00144944\n",
      "Iteration 854, loss = 0.00144799\n",
      "Iteration 855, loss = 0.00144654\n",
      "Iteration 856, loss = 0.00144507\n",
      "Iteration 857, loss = 0.00144363\n",
      "Iteration 858, loss = 0.00144220\n",
      "Iteration 859, loss = 0.00144075\n",
      "Iteration 860, loss = 0.00143932\n",
      "Iteration 861, loss = 0.00143789\n",
      "Iteration 862, loss = 0.00143645\n",
      "Iteration 863, loss = 0.00143504\n",
      "Iteration 864, loss = 0.00143361\n",
      "Iteration 865, loss = 0.00143220\n",
      "Iteration 866, loss = 0.00143081\n",
      "Iteration 867, loss = 0.00142939\n",
      "Iteration 868, loss = 0.00142799\n",
      "Iteration 869, loss = 0.00142660\n",
      "Iteration 870, loss = 0.00142519\n",
      "Iteration 871, loss = 0.00142381\n",
      "Iteration 872, loss = 0.00142239\n",
      "Iteration 873, loss = 0.00142101\n",
      "Iteration 874, loss = 0.00141963\n",
      "Iteration 875, loss = 0.00141824\n",
      "Iteration 876, loss = 0.00141686\n",
      "Iteration 877, loss = 0.00141550\n",
      "Iteration 878, loss = 0.00141410\n",
      "Iteration 879, loss = 0.00141276\n",
      "Iteration 880, loss = 0.00141140\n",
      "Iteration 881, loss = 0.00141004\n",
      "Iteration 882, loss = 0.00140867\n",
      "Iteration 883, loss = 0.00140733\n",
      "Iteration 884, loss = 0.00140597\n",
      "Iteration 885, loss = 0.00140463\n",
      "Iteration 886, loss = 0.00140328\n",
      "Iteration 887, loss = 0.00140194\n",
      "Iteration 888, loss = 0.00140061\n",
      "Iteration 889, loss = 0.00139928\n",
      "Iteration 890, loss = 0.00139794\n",
      "Iteration 891, loss = 0.00139665\n",
      "Iteration 892, loss = 0.00139530\n",
      "Iteration 893, loss = 0.00139399\n",
      "Iteration 894, loss = 0.00139267\n",
      "Iteration 895, loss = 0.00139136\n",
      "Iteration 896, loss = 0.00139007\n",
      "Iteration 897, loss = 0.00138877\n",
      "Iteration 898, loss = 0.00138746\n",
      "Iteration 899, loss = 0.00138618\n",
      "Iteration 900, loss = 0.00138489\n",
      "Iteration 901, loss = 0.00138359\n",
      "Iteration 902, loss = 0.00138231\n",
      "Iteration 903, loss = 0.00138104\n",
      "Iteration 904, loss = 0.00137977\n",
      "Iteration 905, loss = 0.00137849\n",
      "Iteration 906, loss = 0.00137722\n",
      "Iteration 907, loss = 0.00137596\n",
      "Iteration 908, loss = 0.00137470\n",
      "Iteration 909, loss = 0.00137343\n",
      "Iteration 910, loss = 0.00137217\n",
      "Iteration 911, loss = 0.00137091\n",
      "Iteration 912, loss = 0.00136966\n",
      "Iteration 913, loss = 0.00136841\n",
      "Iteration 914, loss = 0.00136717\n",
      "Iteration 915, loss = 0.00136592\n",
      "Iteration 916, loss = 0.00136467\n",
      "Iteration 917, loss = 0.00136344\n",
      "Iteration 918, loss = 0.00136220\n",
      "Iteration 919, loss = 0.00136097\n",
      "Iteration 920, loss = 0.00135975\n",
      "Iteration 921, loss = 0.00135852\n",
      "Iteration 922, loss = 0.00135730\n",
      "Iteration 923, loss = 0.00135607\n",
      "Iteration 924, loss = 0.00135485\n",
      "Iteration 925, loss = 0.00135364\n",
      "Iteration 926, loss = 0.00135242\n",
      "Iteration 927, loss = 0.00135122\n",
      "Iteration 928, loss = 0.00135001\n",
      "Iteration 929, loss = 0.00134882\n",
      "Iteration 930, loss = 0.00134763\n",
      "Iteration 931, loss = 0.00134641\n",
      "Iteration 932, loss = 0.00134523\n",
      "Iteration 933, loss = 0.00134403\n",
      "Iteration 934, loss = 0.00134285\n",
      "Iteration 935, loss = 0.00134166\n",
      "Iteration 936, loss = 0.00134048\n",
      "Iteration 937, loss = 0.00133931\n",
      "Iteration 938, loss = 0.00133813\n",
      "Iteration 939, loss = 0.00133694\n",
      "Iteration 940, loss = 0.00133579\n",
      "Iteration 941, loss = 0.00133462\n",
      "Iteration 942, loss = 0.00133348\n",
      "Iteration 943, loss = 0.00133232\n",
      "Iteration 944, loss = 0.00133117\n",
      "Iteration 945, loss = 0.00133001\n",
      "Iteration 946, loss = 0.00132886\n",
      "Iteration 947, loss = 0.00132773\n",
      "Iteration 948, loss = 0.00132657\n",
      "Iteration 949, loss = 0.00132543\n",
      "Iteration 950, loss = 0.00132430\n",
      "Iteration 951, loss = 0.00132316\n",
      "Iteration 952, loss = 0.00132202\n",
      "Iteration 953, loss = 0.00132088\n",
      "Iteration 954, loss = 0.00131976\n",
      "Iteration 955, loss = 0.00131862\n",
      "Iteration 956, loss = 0.00131751\n",
      "Iteration 957, loss = 0.00131639\n",
      "Iteration 958, loss = 0.00131527\n",
      "Iteration 959, loss = 0.00131416\n",
      "Iteration 960, loss = 0.00131305\n",
      "Iteration 961, loss = 0.00131195\n",
      "Iteration 962, loss = 0.00131083\n",
      "Iteration 963, loss = 0.00130973\n",
      "Iteration 964, loss = 0.00130863\n",
      "Iteration 965, loss = 0.00130754\n",
      "Iteration 966, loss = 0.00130643\n",
      "Iteration 967, loss = 0.00130535\n",
      "Iteration 968, loss = 0.00130424\n",
      "Iteration 969, loss = 0.00130316\n",
      "Iteration 970, loss = 0.00130206\n",
      "Iteration 971, loss = 0.00130098\n",
      "Iteration 972, loss = 0.00129989\n",
      "Iteration 973, loss = 0.00129881\n",
      "Iteration 974, loss = 0.00129773\n",
      "Iteration 975, loss = 0.00129666\n",
      "Iteration 976, loss = 0.00129558\n",
      "Iteration 977, loss = 0.00129452\n",
      "Iteration 978, loss = 0.00129345\n",
      "Iteration 979, loss = 0.00129239\n",
      "Iteration 980, loss = 0.00129133\n",
      "Iteration 981, loss = 0.00129027\n",
      "Iteration 982, loss = 0.00128921\n",
      "Iteration 983, loss = 0.00128816\n",
      "Iteration 984, loss = 0.00128712\n",
      "Iteration 985, loss = 0.00128607\n",
      "Iteration 986, loss = 0.00128502\n",
      "Iteration 987, loss = 0.00128398\n",
      "Iteration 988, loss = 0.00128296\n",
      "Iteration 989, loss = 0.00128191\n",
      "Iteration 990, loss = 0.00128086\n",
      "Iteration 991, loss = 0.00127984\n",
      "Iteration 992, loss = 0.00127881\n",
      "Iteration 993, loss = 0.00127778\n",
      "Iteration 994, loss = 0.00127676\n",
      "Iteration 995, loss = 0.00127573\n",
      "Iteration 996, loss = 0.00127472\n",
      "Iteration 997, loss = 0.00127371\n",
      "Iteration 998, loss = 0.00127268\n",
      "Iteration 999, loss = 0.00127167\n",
      "Iteration 1000, loss = 0.00127067\n",
      "Iteration 1001, loss = 0.00126966\n",
      "Iteration 1002, loss = 0.00126865\n",
      "Iteration 1003, loss = 0.00126764\n",
      "Iteration 1004, loss = 0.00126665\n",
      "Iteration 1005, loss = 0.00126565\n",
      "Iteration 1006, loss = 0.00126465\n",
      "Iteration 1007, loss = 0.00126367\n",
      "Iteration 1008, loss = 0.00126268\n",
      "Iteration 1009, loss = 0.00126169\n",
      "Iteration 1010, loss = 0.00126070\n",
      "Iteration 1011, loss = 0.00125972\n",
      "Iteration 1012, loss = 0.00125874\n",
      "Iteration 1013, loss = 0.00125777\n",
      "Iteration 1014, loss = 0.00125678\n",
      "Iteration 1015, loss = 0.00125580\n",
      "Iteration 1016, loss = 0.00125483\n",
      "Iteration 1017, loss = 0.00125386\n",
      "Training loss did not improve more than tol=0.000001 for 10 consecutive epochs. Stopping.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.67      0.80         3\n",
      "           1       1.00      1.00      1.00         2\n",
      "           2       1.00      1.00      1.00         3\n",
      "           3       1.00      1.00      1.00         4\n",
      "           4       1.00      1.00      1.00         2\n",
      "           5       1.00      1.00      1.00         4\n",
      "           6       1.00      1.00      1.00         3\n",
      "           7       1.00      1.00      1.00         5\n",
      "           8       1.00      1.00      1.00         2\n",
      "           9       1.00      1.00      1.00         2\n",
      "          10       1.00      1.00      1.00         4\n",
      "          11       1.00      1.00      1.00         3\n",
      "          12       1.00      1.00      1.00         1\n",
      "          13       1.00      1.00      1.00         2\n",
      "          14       1.00      1.00      1.00         3\n",
      "          15       1.00      1.00      1.00         3\n",
      "          16       1.00      0.60      0.75         5\n",
      "          17       1.00      1.00      1.00         5\n",
      "          18       1.00      1.00      1.00         1\n",
      "          19       1.00      1.00      1.00         4\n",
      "          20       0.75      1.00      0.86         3\n",
      "          21       1.00      1.00      1.00         3\n",
      "          22       1.00      1.00      1.00         4\n",
      "          23       1.00      1.00      1.00         5\n",
      "          25       0.50      1.00      0.67         2\n",
      "          26       1.00      1.00      1.00         3\n",
      "          27       1.00      1.00      1.00         2\n",
      "          28       1.00      1.00      1.00         5\n",
      "          29       1.00      1.00      1.00         3\n",
      "          30       1.00      1.00      1.00         1\n",
      "          31       1.00      1.00      1.00         2\n",
      "          32       1.00      1.00      1.00         3\n",
      "          33       1.00      1.00      1.00         2\n",
      "          34       1.00      1.00      1.00         3\n",
      "          35       1.00      1.00      1.00         2\n",
      "          36       1.00      1.00      1.00         5\n",
      "          37       1.00      1.00      1.00         3\n",
      "          38       1.00      1.00      1.00         4\n",
      "          39       1.00      1.00      1.00         4\n",
      "\n",
      "    accuracy                           0.97       120\n",
      "   macro avg       0.98      0.98      0.98       120\n",
      "weighted avg       0.99      0.97      0.98       120\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "hidden_layers = (512,512,512)\n",
    "activation = 'logistic'\n",
    "opts = dict(hidden_layer_sizes = hidden_layers,verbose = True,activation = activation,tol = 1e-6,max_iter = int(1e6))\n",
    "solver = 'adam'\n",
    "clf_MLP = MLPClassifier(solver = solver, **opts)\n",
    "clf_MLP.fit(X_train_,y_train)\n",
    "predictions = clf_MLP.predict(X_test_)\n",
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "訓練資料的classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         7\n",
      "           1       1.00      1.00      1.00         8\n",
      "           2       1.00      1.00      1.00         7\n",
      "           3       1.00      1.00      1.00         6\n",
      "           4       1.00      1.00      1.00         8\n",
      "           5       1.00      1.00      1.00         6\n",
      "           6       1.00      1.00      1.00         7\n",
      "           7       1.00      1.00      1.00         5\n",
      "           8       1.00      1.00      1.00         8\n",
      "           9       1.00      1.00      1.00         8\n",
      "          10       1.00      1.00      1.00         6\n",
      "          11       1.00      1.00      1.00         7\n",
      "          12       1.00      1.00      1.00         9\n",
      "          13       1.00      1.00      1.00         8\n",
      "          14       1.00      1.00      1.00         7\n",
      "          15       1.00      1.00      1.00         7\n",
      "          16       1.00      1.00      1.00         5\n",
      "          17       1.00      1.00      1.00         5\n",
      "          18       1.00      1.00      1.00         9\n",
      "          19       1.00      1.00      1.00         6\n",
      "          20       1.00      1.00      1.00         7\n",
      "          21       1.00      1.00      1.00         7\n",
      "          22       1.00      1.00      1.00         6\n",
      "          23       1.00      1.00      1.00         5\n",
      "          24       1.00      1.00      1.00        10\n",
      "          25       1.00      1.00      1.00         8\n",
      "          26       1.00      1.00      1.00         7\n",
      "          27       1.00      1.00      1.00         8\n",
      "          28       1.00      1.00      1.00         5\n",
      "          29       1.00      1.00      1.00         7\n",
      "          30       1.00      1.00      1.00         9\n",
      "          31       1.00      1.00      1.00         8\n",
      "          32       1.00      1.00      1.00         7\n",
      "          33       1.00      1.00      1.00         8\n",
      "          34       1.00      1.00      1.00         7\n",
      "          35       1.00      1.00      1.00         8\n",
      "          36       1.00      1.00      1.00         5\n",
      "          37       1.00      1.00      1.00         7\n",
      "          38       1.00      1.00      1.00         6\n",
      "          39       1.00      1.00      1.00         6\n",
      "\n",
      "    accuracy                           1.00       280\n",
      "   macro avg       1.00      1.00      1.00       280\n",
      "weighted avg       1.00      1.00      1.00       280\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred = clf_MLP.predict(X_train_)\n",
    "print(classification_report(y_train,pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 結論:\n",
    "### 1. hidden layers = (30,)，選擇不同的activation 跟 solver ，效果並沒有差太多，準確率皆在90%以上\n",
    "### 2. 當hidden layers = (512,) 跟 hidden layers = (60,60,60) 或 hidden layers = (512,512,512)，差別並沒有太大，只用一層512個神經元就可以達到跟三層60個神經元或是三層512個神經元的效果，所以下面採hidden layers = (512,)\n",
    "### 3. 由2.可以知道，低維度的資料如果擴展到過多的神經元，其實資料無法提供足夠多的細節，也因此造成了許多無用、重複的數據。有時候不是模型大、模型深就是會比較好，要根據不同的狀況做適當的設計才是最好的選擇\n",
    "### 4. 訓練準確率提升後，實際上測試的準確度沒下降多少，可能因為模型過度訓練到適合訓練資料集，反而不適合其他資料"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA主成分分析(成分比例採0.8) + 神經網路"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 3.79965357\n",
      "Iteration 2, loss = 3.46697809\n",
      "Iteration 3, loss = 3.19959574\n",
      "Iteration 4, loss = 2.95940895\n",
      "Iteration 5, loss = 2.73788417\n",
      "Iteration 6, loss = 2.52542154\n",
      "Iteration 7, loss = 2.32427314\n",
      "Iteration 8, loss = 2.13590139\n",
      "Iteration 9, loss = 1.96169883\n",
      "Iteration 10, loss = 1.80158187\n",
      "Iteration 11, loss = 1.65560148\n",
      "Iteration 12, loss = 1.51771600\n",
      "Iteration 13, loss = 1.39213266\n",
      "Iteration 14, loss = 1.27952599\n",
      "Iteration 15, loss = 1.17495957\n",
      "Iteration 16, loss = 1.08226297\n",
      "Iteration 17, loss = 0.99465287\n",
      "Iteration 18, loss = 0.91619497\n",
      "Iteration 19, loss = 0.84367556\n",
      "Iteration 20, loss = 0.77786020\n",
      "Iteration 21, loss = 0.71790559\n",
      "Iteration 22, loss = 0.66244893\n",
      "Iteration 23, loss = 0.61389715\n",
      "Iteration 24, loss = 0.56813914\n",
      "Iteration 25, loss = 0.52813199\n",
      "Iteration 26, loss = 0.49118107\n",
      "Iteration 27, loss = 0.45783128\n",
      "Iteration 28, loss = 0.42687692\n",
      "Iteration 29, loss = 0.39877348\n",
      "Iteration 30, loss = 0.37288859\n",
      "Iteration 31, loss = 0.34913107\n",
      "Iteration 32, loss = 0.32809592\n",
      "Iteration 33, loss = 0.30808182\n",
      "Iteration 34, loss = 0.29004790\n",
      "Iteration 35, loss = 0.27308272\n",
      "Iteration 36, loss = 0.25812339\n",
      "Iteration 37, loss = 0.24432448\n",
      "Iteration 38, loss = 0.23151048\n",
      "Iteration 39, loss = 0.21952146\n",
      "Iteration 40, loss = 0.20855642\n",
      "Iteration 41, loss = 0.19853033\n",
      "Iteration 42, loss = 0.18883228\n",
      "Iteration 43, loss = 0.18011257\n",
      "Iteration 44, loss = 0.17207750\n",
      "Iteration 45, loss = 0.16463550\n",
      "Iteration 46, loss = 0.15729688\n",
      "Iteration 47, loss = 0.15084182\n",
      "Iteration 48, loss = 0.14479536\n",
      "Iteration 49, loss = 0.13928375\n",
      "Iteration 50, loss = 0.13405261\n",
      "Iteration 51, loss = 0.12910437\n",
      "Iteration 52, loss = 0.12424215\n",
      "Iteration 53, loss = 0.11968079\n",
      "Iteration 54, loss = 0.11537838\n",
      "Iteration 55, loss = 0.11130653\n",
      "Iteration 56, loss = 0.10755530\n",
      "Iteration 57, loss = 0.10392889\n",
      "Iteration 58, loss = 0.10058561\n",
      "Iteration 59, loss = 0.09740727\n",
      "Iteration 60, loss = 0.09445565\n",
      "Iteration 61, loss = 0.09138564\n",
      "Iteration 62, loss = 0.08864256\n",
      "Iteration 63, loss = 0.08590543\n",
      "Iteration 64, loss = 0.08345522\n",
      "Iteration 65, loss = 0.08101882\n",
      "Iteration 66, loss = 0.07872436\n",
      "Iteration 67, loss = 0.07653544\n",
      "Iteration 68, loss = 0.07458584\n",
      "Iteration 69, loss = 0.07264982\n",
      "Iteration 70, loss = 0.07081589\n",
      "Iteration 71, loss = 0.06893071\n",
      "Iteration 72, loss = 0.06718347\n",
      "Iteration 73, loss = 0.06549548\n",
      "Iteration 74, loss = 0.06386304\n",
      "Iteration 75, loss = 0.06232175\n",
      "Iteration 76, loss = 0.06086151\n",
      "Iteration 77, loss = 0.05945212\n",
      "Iteration 78, loss = 0.05807202\n",
      "Iteration 79, loss = 0.05674055\n",
      "Iteration 80, loss = 0.05547340\n",
      "Iteration 81, loss = 0.05425247\n",
      "Iteration 82, loss = 0.05307924\n",
      "Iteration 83, loss = 0.05195007\n",
      "Iteration 84, loss = 0.05081632\n",
      "Iteration 85, loss = 0.04981217\n",
      "Iteration 86, loss = 0.04883231\n",
      "Iteration 87, loss = 0.04782756\n",
      "Iteration 88, loss = 0.04690385\n",
      "Iteration 89, loss = 0.04596778\n",
      "Iteration 90, loss = 0.04507096\n",
      "Iteration 91, loss = 0.04417063\n",
      "Iteration 92, loss = 0.04334197\n",
      "Iteration 93, loss = 0.04252274\n",
      "Iteration 94, loss = 0.04174479\n",
      "Iteration 95, loss = 0.04096363\n",
      "Iteration 96, loss = 0.04020280\n",
      "Iteration 97, loss = 0.03948012\n",
      "Iteration 98, loss = 0.03880218\n",
      "Iteration 99, loss = 0.03807369\n",
      "Iteration 100, loss = 0.03741678\n",
      "Iteration 101, loss = 0.03679829\n",
      "Iteration 102, loss = 0.03615213\n",
      "Iteration 103, loss = 0.03555673\n",
      "Iteration 104, loss = 0.03495798\n",
      "Iteration 105, loss = 0.03435179\n",
      "Iteration 106, loss = 0.03377153\n",
      "Iteration 107, loss = 0.03322112\n",
      "Iteration 108, loss = 0.03270664\n",
      "Iteration 109, loss = 0.03219711\n",
      "Iteration 110, loss = 0.03168527\n",
      "Iteration 111, loss = 0.03119720\n",
      "Iteration 112, loss = 0.03072628\n",
      "Iteration 113, loss = 0.03026224\n",
      "Iteration 114, loss = 0.02981573\n",
      "Iteration 115, loss = 0.02938105\n",
      "Iteration 116, loss = 0.02894235\n",
      "Iteration 117, loss = 0.02850260\n",
      "Iteration 118, loss = 0.02809560\n",
      "Iteration 119, loss = 0.02766945\n",
      "Iteration 120, loss = 0.02728144\n",
      "Iteration 121, loss = 0.02689144\n",
      "Iteration 122, loss = 0.02651606\n",
      "Iteration 123, loss = 0.02615939\n",
      "Iteration 124, loss = 0.02580155\n",
      "Iteration 125, loss = 0.02543993\n",
      "Iteration 126, loss = 0.02511768\n",
      "Iteration 127, loss = 0.02476675\n",
      "Iteration 128, loss = 0.02442790\n",
      "Iteration 129, loss = 0.02409847\n",
      "Iteration 130, loss = 0.02378530\n",
      "Iteration 131, loss = 0.02345635\n",
      "Iteration 132, loss = 0.02315154\n",
      "Iteration 133, loss = 0.02284055\n",
      "Iteration 134, loss = 0.02254618\n",
      "Iteration 135, loss = 0.02226464\n",
      "Iteration 136, loss = 0.02198985\n",
      "Iteration 137, loss = 0.02170989\n",
      "Iteration 138, loss = 0.02143725\n",
      "Iteration 139, loss = 0.02117122\n",
      "Iteration 140, loss = 0.02090292\n",
      "Iteration 141, loss = 0.02065247\n",
      "Iteration 142, loss = 0.02039578\n",
      "Iteration 143, loss = 0.02015538\n",
      "Iteration 144, loss = 0.01990901\n",
      "Iteration 145, loss = 0.01966878\n",
      "Iteration 146, loss = 0.01943813\n",
      "Iteration 147, loss = 0.01921584\n",
      "Iteration 148, loss = 0.01899432\n",
      "Iteration 149, loss = 0.01876910\n",
      "Iteration 150, loss = 0.01856109\n",
      "Iteration 151, loss = 0.01835874\n",
      "Iteration 152, loss = 0.01815128\n",
      "Iteration 153, loss = 0.01795082\n",
      "Iteration 154, loss = 0.01775024\n",
      "Iteration 155, loss = 0.01756120\n",
      "Iteration 156, loss = 0.01736213\n",
      "Iteration 157, loss = 0.01717409\n",
      "Iteration 158, loss = 0.01698564\n",
      "Iteration 159, loss = 0.01681088\n",
      "Iteration 160, loss = 0.01662805\n",
      "Iteration 161, loss = 0.01645815\n",
      "Iteration 162, loss = 0.01627267\n",
      "Iteration 163, loss = 0.01610621\n",
      "Iteration 164, loss = 0.01592797\n",
      "Iteration 165, loss = 0.01576332\n",
      "Iteration 166, loss = 0.01560006\n",
      "Iteration 167, loss = 0.01543119\n",
      "Iteration 168, loss = 0.01527883\n",
      "Iteration 169, loss = 0.01512680\n",
      "Iteration 170, loss = 0.01497414\n",
      "Iteration 171, loss = 0.01482465\n",
      "Iteration 172, loss = 0.01467978\n",
      "Iteration 173, loss = 0.01453703\n",
      "Iteration 174, loss = 0.01439878\n",
      "Iteration 175, loss = 0.01425865\n",
      "Iteration 176, loss = 0.01412052\n",
      "Iteration 177, loss = 0.01399000\n",
      "Iteration 178, loss = 0.01385533\n",
      "Iteration 179, loss = 0.01372381\n",
      "Iteration 180, loss = 0.01359909\n",
      "Iteration 181, loss = 0.01346924\n",
      "Iteration 182, loss = 0.01334480\n",
      "Iteration 183, loss = 0.01322410\n",
      "Iteration 184, loss = 0.01310245\n",
      "Iteration 185, loss = 0.01298592\n",
      "Iteration 186, loss = 0.01287134\n",
      "Iteration 187, loss = 0.01275430\n",
      "Iteration 188, loss = 0.01264203\n",
      "Iteration 189, loss = 0.01253119\n",
      "Iteration 190, loss = 0.01241648\n",
      "Iteration 191, loss = 0.01230856\n",
      "Iteration 192, loss = 0.01220225\n",
      "Iteration 193, loss = 0.01209628\n",
      "Iteration 194, loss = 0.01198878\n",
      "Iteration 195, loss = 0.01188367\n",
      "Iteration 196, loss = 0.01177790\n",
      "Iteration 197, loss = 0.01167658\n",
      "Iteration 198, loss = 0.01157661\n",
      "Iteration 199, loss = 0.01147781\n",
      "Iteration 200, loss = 0.01137941\n",
      "Iteration 201, loss = 0.01128414\n",
      "Iteration 202, loss = 0.01118962\n",
      "Iteration 203, loss = 0.01109535\n",
      "Iteration 204, loss = 0.01100669\n",
      "Iteration 205, loss = 0.01091095\n",
      "Iteration 206, loss = 0.01082466\n",
      "Iteration 207, loss = 0.01073630\n",
      "Iteration 208, loss = 0.01064681\n",
      "Iteration 209, loss = 0.01056013\n",
      "Iteration 210, loss = 0.01047494\n",
      "Iteration 211, loss = 0.01038878\n",
      "Iteration 212, loss = 0.01030530\n",
      "Iteration 213, loss = 0.01022688\n",
      "Iteration 214, loss = 0.01014535\n",
      "Iteration 215, loss = 0.01006586\n",
      "Iteration 216, loss = 0.00998932\n",
      "Iteration 217, loss = 0.00991162\n",
      "Iteration 218, loss = 0.00983801\n",
      "Iteration 219, loss = 0.00976226\n",
      "Iteration 220, loss = 0.00968738\n",
      "Iteration 221, loss = 0.00961584\n",
      "Iteration 222, loss = 0.00954022\n",
      "Iteration 223, loss = 0.00947249\n",
      "Iteration 224, loss = 0.00939950\n",
      "Iteration 225, loss = 0.00932828\n",
      "Iteration 226, loss = 0.00925955\n",
      "Iteration 227, loss = 0.00919034\n",
      "Iteration 228, loss = 0.00912090\n",
      "Iteration 229, loss = 0.00905330\n",
      "Iteration 230, loss = 0.00899109\n",
      "Iteration 231, loss = 0.00892126\n",
      "Iteration 232, loss = 0.00885818\n",
      "Iteration 233, loss = 0.00879249\n",
      "Iteration 234, loss = 0.00872937\n",
      "Iteration 235, loss = 0.00866774\n",
      "Iteration 236, loss = 0.00860592\n",
      "Iteration 237, loss = 0.00854219\n",
      "Iteration 238, loss = 0.00848304\n",
      "Iteration 239, loss = 0.00842196\n",
      "Iteration 240, loss = 0.00836153\n",
      "Iteration 241, loss = 0.00830486\n",
      "Iteration 242, loss = 0.00824729\n",
      "Iteration 243, loss = 0.00819000\n",
      "Iteration 244, loss = 0.00813170\n",
      "Iteration 245, loss = 0.00807702\n",
      "Iteration 246, loss = 0.00802271\n",
      "Iteration 247, loss = 0.00796611\n",
      "Iteration 248, loss = 0.00791301\n",
      "Iteration 249, loss = 0.00785954\n",
      "Iteration 250, loss = 0.00780684\n",
      "Iteration 251, loss = 0.00775435\n",
      "Iteration 252, loss = 0.00770159\n",
      "Iteration 253, loss = 0.00764994\n",
      "Iteration 254, loss = 0.00759751\n",
      "Iteration 255, loss = 0.00754809\n",
      "Iteration 256, loss = 0.00749772\n",
      "Iteration 257, loss = 0.00744787\n",
      "Iteration 258, loss = 0.00739936\n",
      "Iteration 259, loss = 0.00734972\n",
      "Iteration 260, loss = 0.00730338\n",
      "Iteration 261, loss = 0.00725439\n",
      "Iteration 262, loss = 0.00720700\n",
      "Iteration 263, loss = 0.00716184\n",
      "Iteration 264, loss = 0.00711471\n",
      "Iteration 265, loss = 0.00707029\n",
      "Iteration 266, loss = 0.00702453\n",
      "Iteration 267, loss = 0.00698110\n",
      "Iteration 268, loss = 0.00693604\n",
      "Iteration 269, loss = 0.00689231\n",
      "Iteration 270, loss = 0.00684841\n",
      "Iteration 271, loss = 0.00680624\n",
      "Iteration 272, loss = 0.00676297\n",
      "Iteration 273, loss = 0.00672144\n",
      "Iteration 274, loss = 0.00668044\n",
      "Iteration 275, loss = 0.00663778\n",
      "Iteration 276, loss = 0.00659745\n",
      "Iteration 277, loss = 0.00655710\n",
      "Iteration 278, loss = 0.00651724\n",
      "Iteration 279, loss = 0.00647852\n",
      "Iteration 280, loss = 0.00643922\n",
      "Iteration 281, loss = 0.00640026\n",
      "Iteration 282, loss = 0.00636192\n",
      "Iteration 283, loss = 0.00632346\n",
      "Iteration 284, loss = 0.00628671\n",
      "Iteration 285, loss = 0.00624846\n",
      "Iteration 286, loss = 0.00621180\n",
      "Iteration 287, loss = 0.00617552\n",
      "Iteration 288, loss = 0.00614003\n",
      "Iteration 289, loss = 0.00610374\n",
      "Iteration 290, loss = 0.00606917\n",
      "Iteration 291, loss = 0.00603399\n",
      "Iteration 292, loss = 0.00599957\n",
      "Iteration 293, loss = 0.00596427\n",
      "Iteration 294, loss = 0.00592972\n",
      "Iteration 295, loss = 0.00589622\n",
      "Iteration 296, loss = 0.00586171\n",
      "Iteration 297, loss = 0.00582890\n",
      "Iteration 298, loss = 0.00579494\n",
      "Iteration 299, loss = 0.00576256\n",
      "Iteration 300, loss = 0.00572940\n",
      "Iteration 301, loss = 0.00569834\n",
      "Iteration 302, loss = 0.00566642\n",
      "Iteration 303, loss = 0.00563479\n",
      "Iteration 304, loss = 0.00560395\n",
      "Iteration 305, loss = 0.00557342\n",
      "Iteration 306, loss = 0.00554250\n",
      "Iteration 307, loss = 0.00551216\n",
      "Iteration 308, loss = 0.00548077\n",
      "Iteration 309, loss = 0.00545095\n",
      "Iteration 310, loss = 0.00542241\n",
      "Iteration 311, loss = 0.00539216\n",
      "Iteration 312, loss = 0.00536219\n",
      "Iteration 313, loss = 0.00533329\n",
      "Iteration 314, loss = 0.00530434\n",
      "Iteration 315, loss = 0.00527681\n",
      "Iteration 316, loss = 0.00524791\n",
      "Iteration 317, loss = 0.00521988\n",
      "Iteration 318, loss = 0.00519258\n",
      "Iteration 319, loss = 0.00516425\n",
      "Iteration 320, loss = 0.00513743\n",
      "Iteration 321, loss = 0.00511046\n",
      "Iteration 322, loss = 0.00508365\n",
      "Iteration 323, loss = 0.00505696\n",
      "Iteration 324, loss = 0.00503123\n",
      "Iteration 325, loss = 0.00500432\n",
      "Iteration 326, loss = 0.00497924\n",
      "Iteration 327, loss = 0.00495370\n",
      "Iteration 328, loss = 0.00492862\n",
      "Iteration 329, loss = 0.00490309\n",
      "Iteration 330, loss = 0.00487752\n",
      "Iteration 331, loss = 0.00485417\n",
      "Iteration 332, loss = 0.00482855\n",
      "Iteration 333, loss = 0.00480408\n",
      "Iteration 334, loss = 0.00477929\n",
      "Iteration 335, loss = 0.00475615\n",
      "Iteration 336, loss = 0.00473232\n",
      "Iteration 337, loss = 0.00470813\n",
      "Iteration 338, loss = 0.00468446\n",
      "Iteration 339, loss = 0.00466103\n",
      "Iteration 340, loss = 0.00463742\n",
      "Iteration 341, loss = 0.00461451\n",
      "Iteration 342, loss = 0.00459136\n",
      "Iteration 343, loss = 0.00456876\n",
      "Iteration 344, loss = 0.00454607\n",
      "Iteration 345, loss = 0.00452375\n",
      "Iteration 346, loss = 0.00450231\n",
      "Iteration 347, loss = 0.00448044\n",
      "Iteration 348, loss = 0.00445824\n",
      "Iteration 349, loss = 0.00443672\n",
      "Iteration 350, loss = 0.00441476\n",
      "Iteration 351, loss = 0.00439417\n",
      "Iteration 352, loss = 0.00437311\n",
      "Iteration 353, loss = 0.00435140\n",
      "Iteration 354, loss = 0.00433126\n",
      "Iteration 355, loss = 0.00431014\n",
      "Iteration 356, loss = 0.00428998\n",
      "Iteration 357, loss = 0.00426967\n",
      "Iteration 358, loss = 0.00424927\n",
      "Iteration 359, loss = 0.00422928\n",
      "Iteration 360, loss = 0.00420949\n",
      "Iteration 361, loss = 0.00419058\n",
      "Iteration 362, loss = 0.00417096\n",
      "Iteration 363, loss = 0.00415187\n",
      "Iteration 364, loss = 0.00413244\n",
      "Iteration 365, loss = 0.00411363\n",
      "Iteration 366, loss = 0.00409417\n",
      "Iteration 367, loss = 0.00407613\n",
      "Iteration 368, loss = 0.00405614\n",
      "Iteration 369, loss = 0.00403809\n",
      "Iteration 370, loss = 0.00402027\n",
      "Iteration 371, loss = 0.00400145\n",
      "Iteration 372, loss = 0.00398286\n",
      "Iteration 373, loss = 0.00396534\n",
      "Iteration 374, loss = 0.00394798\n",
      "Iteration 375, loss = 0.00393016\n",
      "Iteration 376, loss = 0.00391273\n",
      "Iteration 377, loss = 0.00389525\n",
      "Iteration 378, loss = 0.00387735\n",
      "Iteration 379, loss = 0.00385997\n",
      "Iteration 380, loss = 0.00384310\n",
      "Iteration 381, loss = 0.00382654\n",
      "Iteration 382, loss = 0.00380948\n",
      "Iteration 383, loss = 0.00379349\n",
      "Iteration 384, loss = 0.00377655\n",
      "Iteration 385, loss = 0.00376013\n",
      "Iteration 386, loss = 0.00374331\n",
      "Iteration 387, loss = 0.00372669\n",
      "Iteration 388, loss = 0.00371028\n",
      "Iteration 389, loss = 0.00369448\n",
      "Iteration 390, loss = 0.00367849\n",
      "Iteration 391, loss = 0.00366204\n",
      "Iteration 392, loss = 0.00364607\n",
      "Iteration 393, loss = 0.00362989\n",
      "Iteration 394, loss = 0.00361391\n",
      "Iteration 395, loss = 0.00359835\n",
      "Iteration 396, loss = 0.00358228\n",
      "Iteration 397, loss = 0.00356747\n",
      "Iteration 398, loss = 0.00355172\n",
      "Iteration 399, loss = 0.00353638\n",
      "Iteration 400, loss = 0.00352124\n",
      "Iteration 401, loss = 0.00350597\n",
      "Iteration 402, loss = 0.00349194\n",
      "Iteration 403, loss = 0.00347645\n",
      "Iteration 404, loss = 0.00346214\n",
      "Iteration 405, loss = 0.00344710\n",
      "Iteration 406, loss = 0.00343279\n",
      "Iteration 407, loss = 0.00341929\n",
      "Iteration 408, loss = 0.00340454\n",
      "Iteration 409, loss = 0.00339026\n",
      "Iteration 410, loss = 0.00337618\n",
      "Iteration 411, loss = 0.00336241\n",
      "Iteration 412, loss = 0.00334843\n",
      "Iteration 413, loss = 0.00333471\n",
      "Iteration 414, loss = 0.00332097\n",
      "Iteration 415, loss = 0.00330770\n",
      "Iteration 416, loss = 0.00329401\n",
      "Iteration 417, loss = 0.00328061\n",
      "Iteration 418, loss = 0.00326707\n",
      "Iteration 419, loss = 0.00325363\n",
      "Iteration 420, loss = 0.00324036\n",
      "Iteration 421, loss = 0.00322720\n",
      "Iteration 422, loss = 0.00321421\n",
      "Iteration 423, loss = 0.00320147\n",
      "Iteration 424, loss = 0.00318858\n",
      "Iteration 425, loss = 0.00317574\n",
      "Iteration 426, loss = 0.00316300\n",
      "Iteration 427, loss = 0.00315050\n",
      "Iteration 428, loss = 0.00313776\n",
      "Iteration 429, loss = 0.00312546\n",
      "Iteration 430, loss = 0.00311314\n",
      "Iteration 431, loss = 0.00310081\n",
      "Iteration 432, loss = 0.00308854\n",
      "Iteration 433, loss = 0.00307629\n",
      "Iteration 434, loss = 0.00306466\n",
      "Iteration 435, loss = 0.00305251\n",
      "Iteration 436, loss = 0.00304091\n",
      "Iteration 437, loss = 0.00302883\n",
      "Iteration 438, loss = 0.00301713\n",
      "Iteration 439, loss = 0.00300530\n",
      "Iteration 440, loss = 0.00299386\n",
      "Iteration 441, loss = 0.00298222\n",
      "Iteration 442, loss = 0.00297048\n",
      "Iteration 443, loss = 0.00295923\n",
      "Iteration 444, loss = 0.00294798\n",
      "Iteration 445, loss = 0.00293672\n",
      "Iteration 446, loss = 0.00292518\n",
      "Iteration 447, loss = 0.00291444\n",
      "Iteration 448, loss = 0.00290332\n",
      "Iteration 449, loss = 0.00289215\n",
      "Iteration 450, loss = 0.00288138\n",
      "Iteration 451, loss = 0.00287037\n",
      "Iteration 452, loss = 0.00285949\n",
      "Iteration 453, loss = 0.00284851\n",
      "Iteration 454, loss = 0.00283805\n",
      "Iteration 455, loss = 0.00282725\n",
      "Iteration 456, loss = 0.00281634\n",
      "Iteration 457, loss = 0.00280611\n",
      "Iteration 458, loss = 0.00279574\n",
      "Iteration 459, loss = 0.00278480\n",
      "Iteration 460, loss = 0.00277494\n",
      "Iteration 461, loss = 0.00276447\n",
      "Iteration 462, loss = 0.00275402\n",
      "Iteration 463, loss = 0.00274394\n",
      "Iteration 464, loss = 0.00273367\n",
      "Iteration 465, loss = 0.00272386\n",
      "Iteration 466, loss = 0.00271399\n",
      "Iteration 467, loss = 0.00270401\n",
      "Iteration 468, loss = 0.00269432\n",
      "Iteration 469, loss = 0.00268465\n",
      "Iteration 470, loss = 0.00267515\n",
      "Iteration 471, loss = 0.00266557\n",
      "Iteration 472, loss = 0.00265600\n",
      "Iteration 473, loss = 0.00264637\n",
      "Iteration 474, loss = 0.00263678\n",
      "Iteration 475, loss = 0.00262736\n",
      "Iteration 476, loss = 0.00261832\n",
      "Iteration 477, loss = 0.00260877\n",
      "Iteration 478, loss = 0.00259929\n",
      "Iteration 479, loss = 0.00259025\n",
      "Iteration 480, loss = 0.00258061\n",
      "Iteration 481, loss = 0.00257144\n",
      "Iteration 482, loss = 0.00256234\n",
      "Iteration 483, loss = 0.00255310\n",
      "Iteration 484, loss = 0.00254425\n",
      "Iteration 485, loss = 0.00253533\n",
      "Iteration 486, loss = 0.00252651\n",
      "Iteration 487, loss = 0.00251767\n",
      "Iteration 488, loss = 0.00250863\n",
      "Iteration 489, loss = 0.00250015\n",
      "Iteration 490, loss = 0.00249141\n",
      "Iteration 491, loss = 0.00248277\n",
      "Iteration 492, loss = 0.00247413\n",
      "Iteration 493, loss = 0.00246580\n",
      "Iteration 494, loss = 0.00245719\n",
      "Iteration 495, loss = 0.00244886\n",
      "Iteration 496, loss = 0.00244037\n",
      "Iteration 497, loss = 0.00243166\n",
      "Iteration 498, loss = 0.00242347\n",
      "Iteration 499, loss = 0.00241523\n",
      "Iteration 500, loss = 0.00240696\n",
      "Iteration 501, loss = 0.00239861\n",
      "Iteration 502, loss = 0.00239022\n",
      "Iteration 503, loss = 0.00238226\n",
      "Iteration 504, loss = 0.00237388\n",
      "Iteration 505, loss = 0.00236581\n",
      "Iteration 506, loss = 0.00235798\n",
      "Iteration 507, loss = 0.00234969\n",
      "Iteration 508, loss = 0.00234188\n",
      "Iteration 509, loss = 0.00233407\n",
      "Iteration 510, loss = 0.00232588\n",
      "Iteration 511, loss = 0.00231852\n",
      "Iteration 512, loss = 0.00231070\n",
      "Iteration 513, loss = 0.00230288\n",
      "Iteration 514, loss = 0.00229513\n",
      "Iteration 515, loss = 0.00228742\n",
      "Iteration 516, loss = 0.00227985\n",
      "Iteration 517, loss = 0.00227231\n",
      "Iteration 518, loss = 0.00226455\n",
      "Iteration 519, loss = 0.00225728\n",
      "Iteration 520, loss = 0.00224986\n",
      "Iteration 521, loss = 0.00224271\n",
      "Iteration 522, loss = 0.00223541\n",
      "Iteration 523, loss = 0.00222788\n",
      "Iteration 524, loss = 0.00222059\n",
      "Iteration 525, loss = 0.00221334\n",
      "Iteration 526, loss = 0.00220624\n",
      "Iteration 527, loss = 0.00219929\n",
      "Iteration 528, loss = 0.00219217\n",
      "Iteration 529, loss = 0.00218503\n",
      "Iteration 530, loss = 0.00217805\n",
      "Iteration 531, loss = 0.00217096\n",
      "Iteration 532, loss = 0.00216401\n",
      "Iteration 533, loss = 0.00215717\n",
      "Iteration 534, loss = 0.00215021\n",
      "Iteration 535, loss = 0.00214312\n",
      "Iteration 536, loss = 0.00213648\n",
      "Iteration 537, loss = 0.00212955\n",
      "Iteration 538, loss = 0.00212280\n",
      "Iteration 539, loss = 0.00211593\n",
      "Iteration 540, loss = 0.00210921\n",
      "Iteration 541, loss = 0.00210239\n",
      "Iteration 542, loss = 0.00209560\n",
      "Iteration 543, loss = 0.00208935\n",
      "Iteration 544, loss = 0.00208233\n",
      "Iteration 545, loss = 0.00207578\n",
      "Iteration 546, loss = 0.00206941\n",
      "Iteration 547, loss = 0.00206266\n",
      "Iteration 548, loss = 0.00205607\n",
      "Iteration 549, loss = 0.00204991\n",
      "Iteration 550, loss = 0.00204347\n",
      "Iteration 551, loss = 0.00203701\n",
      "Iteration 552, loss = 0.00203074\n",
      "Iteration 553, loss = 0.00202431\n",
      "Iteration 554, loss = 0.00201786\n",
      "Iteration 555, loss = 0.00201161\n",
      "Iteration 556, loss = 0.00200543\n",
      "Iteration 557, loss = 0.00199948\n",
      "Iteration 558, loss = 0.00199318\n",
      "Iteration 559, loss = 0.00198718\n",
      "Iteration 560, loss = 0.00198111\n",
      "Iteration 561, loss = 0.00197516\n",
      "Iteration 562, loss = 0.00196904\n",
      "Iteration 563, loss = 0.00196325\n",
      "Iteration 564, loss = 0.00195744\n",
      "Iteration 565, loss = 0.00195136\n",
      "Iteration 566, loss = 0.00194561\n",
      "Iteration 567, loss = 0.00193999\n",
      "Iteration 568, loss = 0.00193414\n",
      "Iteration 569, loss = 0.00192818\n",
      "Iteration 570, loss = 0.00192227\n",
      "Iteration 571, loss = 0.00191660\n",
      "Iteration 572, loss = 0.00191090\n",
      "Iteration 573, loss = 0.00190504\n",
      "Iteration 574, loss = 0.00189942\n",
      "Iteration 575, loss = 0.00189363\n",
      "Iteration 576, loss = 0.00188824\n",
      "Iteration 577, loss = 0.00188266\n",
      "Iteration 578, loss = 0.00187699\n",
      "Iteration 579, loss = 0.00187141\n",
      "Iteration 580, loss = 0.00186595\n",
      "Iteration 581, loss = 0.00186040\n",
      "Iteration 582, loss = 0.00185476\n",
      "Iteration 583, loss = 0.00184947\n",
      "Iteration 584, loss = 0.00184396\n",
      "Iteration 585, loss = 0.00183868\n",
      "Iteration 586, loss = 0.00183321\n",
      "Iteration 587, loss = 0.00182800\n",
      "Iteration 588, loss = 0.00182272\n",
      "Iteration 589, loss = 0.00181734\n",
      "Iteration 590, loss = 0.00181220\n",
      "Iteration 591, loss = 0.00180684\n",
      "Iteration 592, loss = 0.00180162\n",
      "Iteration 593, loss = 0.00179646\n",
      "Iteration 594, loss = 0.00179131\n",
      "Iteration 595, loss = 0.00178616\n",
      "Iteration 596, loss = 0.00178092\n",
      "Iteration 597, loss = 0.00177580\n",
      "Iteration 598, loss = 0.00177075\n",
      "Iteration 599, loss = 0.00176573\n",
      "Iteration 600, loss = 0.00176062\n",
      "Iteration 601, loss = 0.00175559\n",
      "Iteration 602, loss = 0.00175052\n",
      "Iteration 603, loss = 0.00174543\n",
      "Iteration 604, loss = 0.00174035\n",
      "Iteration 605, loss = 0.00173551\n",
      "Iteration 606, loss = 0.00173052\n",
      "Iteration 607, loss = 0.00172560\n",
      "Iteration 608, loss = 0.00172057\n",
      "Iteration 609, loss = 0.00171567\n",
      "Iteration 610, loss = 0.00171088\n",
      "Iteration 611, loss = 0.00170618\n",
      "Iteration 612, loss = 0.00170134\n",
      "Iteration 613, loss = 0.00169654\n",
      "Iteration 614, loss = 0.00169177\n",
      "Iteration 615, loss = 0.00168719\n",
      "Iteration 616, loss = 0.00168225\n",
      "Iteration 617, loss = 0.00167758\n",
      "Iteration 618, loss = 0.00167308\n",
      "Iteration 619, loss = 0.00166848\n",
      "Iteration 620, loss = 0.00166382\n",
      "Iteration 621, loss = 0.00165927\n",
      "Iteration 622, loss = 0.00165467\n",
      "Iteration 623, loss = 0.00165000\n",
      "Iteration 624, loss = 0.00164569\n",
      "Iteration 625, loss = 0.00164105\n",
      "Iteration 626, loss = 0.00163665\n",
      "Iteration 627, loss = 0.00163209\n",
      "Iteration 628, loss = 0.00162754\n",
      "Iteration 629, loss = 0.00162328\n",
      "Iteration 630, loss = 0.00161886\n",
      "Iteration 631, loss = 0.00161443\n",
      "Iteration 632, loss = 0.00161007\n",
      "Iteration 633, loss = 0.00160570\n",
      "Iteration 634, loss = 0.00160151\n",
      "Iteration 635, loss = 0.00159712\n",
      "Iteration 636, loss = 0.00159286\n",
      "Iteration 637, loss = 0.00158855\n",
      "Iteration 638, loss = 0.00158442\n",
      "Iteration 639, loss = 0.00158024\n",
      "Iteration 640, loss = 0.00157591\n",
      "Iteration 641, loss = 0.00157184\n",
      "Iteration 642, loss = 0.00156764\n",
      "Iteration 643, loss = 0.00156343\n",
      "Iteration 644, loss = 0.00155925\n",
      "Iteration 645, loss = 0.00155524\n",
      "Iteration 646, loss = 0.00155119\n",
      "Iteration 647, loss = 0.00154696\n",
      "Iteration 648, loss = 0.00154288\n",
      "Iteration 649, loss = 0.00153880\n",
      "Iteration 650, loss = 0.00153460\n",
      "Iteration 651, loss = 0.00153062\n",
      "Iteration 652, loss = 0.00152657\n",
      "Iteration 653, loss = 0.00152259\n",
      "Iteration 654, loss = 0.00151862\n",
      "Iteration 655, loss = 0.00151464\n",
      "Iteration 656, loss = 0.00151078\n",
      "Iteration 657, loss = 0.00150684\n",
      "Iteration 658, loss = 0.00150311\n",
      "Iteration 659, loss = 0.00149909\n",
      "Iteration 660, loss = 0.00149530\n",
      "Iteration 661, loss = 0.00149142\n",
      "Iteration 662, loss = 0.00148749\n",
      "Iteration 663, loss = 0.00148371\n",
      "Iteration 664, loss = 0.00148005\n",
      "Iteration 665, loss = 0.00147612\n",
      "Iteration 666, loss = 0.00147243\n",
      "Iteration 667, loss = 0.00146858\n",
      "Iteration 668, loss = 0.00146482\n",
      "Iteration 669, loss = 0.00146096\n",
      "Iteration 670, loss = 0.00145732\n",
      "Iteration 671, loss = 0.00145343\n",
      "Iteration 672, loss = 0.00144975\n",
      "Iteration 673, loss = 0.00144604\n",
      "Iteration 674, loss = 0.00144230\n",
      "Iteration 675, loss = 0.00143868\n",
      "Iteration 676, loss = 0.00143508\n",
      "Iteration 677, loss = 0.00143139\n",
      "Iteration 678, loss = 0.00142780\n",
      "Iteration 679, loss = 0.00142416\n",
      "Iteration 680, loss = 0.00142048\n",
      "Iteration 681, loss = 0.00141694\n",
      "Iteration 682, loss = 0.00141335\n",
      "Iteration 683, loss = 0.00140984\n",
      "Iteration 684, loss = 0.00140613\n",
      "Iteration 685, loss = 0.00140255\n",
      "Iteration 686, loss = 0.00139896\n",
      "Iteration 687, loss = 0.00139536\n",
      "Iteration 688, loss = 0.00139180\n",
      "Iteration 689, loss = 0.00138830\n",
      "Iteration 690, loss = 0.00138468\n",
      "Iteration 691, loss = 0.00138142\n",
      "Iteration 692, loss = 0.00137786\n",
      "Iteration 693, loss = 0.00137433\n",
      "Iteration 694, loss = 0.00137096\n",
      "Iteration 695, loss = 0.00136764\n",
      "Iteration 696, loss = 0.00136420\n",
      "Iteration 697, loss = 0.00136084\n",
      "Iteration 698, loss = 0.00135745\n",
      "Iteration 699, loss = 0.00135394\n",
      "Iteration 700, loss = 0.00135072\n",
      "Iteration 701, loss = 0.00134734\n",
      "Iteration 702, loss = 0.00134418\n",
      "Iteration 703, loss = 0.00134078\n",
      "Iteration 704, loss = 0.00133754\n",
      "Iteration 705, loss = 0.00133422\n",
      "Iteration 706, loss = 0.00133099\n",
      "Iteration 707, loss = 0.00132785\n",
      "Iteration 708, loss = 0.00132454\n",
      "Iteration 709, loss = 0.00132127\n",
      "Iteration 710, loss = 0.00131806\n",
      "Iteration 711, loss = 0.00131493\n",
      "Iteration 712, loss = 0.00131170\n",
      "Iteration 713, loss = 0.00130849\n",
      "Iteration 714, loss = 0.00130525\n",
      "Iteration 715, loss = 0.00130221\n",
      "Iteration 716, loss = 0.00129913\n",
      "Iteration 717, loss = 0.00129605\n",
      "Iteration 718, loss = 0.00129284\n",
      "Iteration 719, loss = 0.00128975\n",
      "Iteration 720, loss = 0.00128670\n",
      "Iteration 721, loss = 0.00128365\n",
      "Iteration 722, loss = 0.00128055\n",
      "Iteration 723, loss = 0.00127754\n",
      "Iteration 724, loss = 0.00127446\n",
      "Iteration 725, loss = 0.00127154\n",
      "Iteration 726, loss = 0.00126849\n",
      "Iteration 727, loss = 0.00126544\n",
      "Iteration 728, loss = 0.00126252\n",
      "Iteration 729, loss = 0.00125950\n",
      "Iteration 730, loss = 0.00125663\n",
      "Iteration 731, loss = 0.00125361\n",
      "Iteration 732, loss = 0.00125075\n",
      "Iteration 733, loss = 0.00124779\n",
      "Iteration 734, loss = 0.00124488\n",
      "Iteration 735, loss = 0.00124199\n",
      "Iteration 736, loss = 0.00123913\n",
      "Iteration 737, loss = 0.00123619\n",
      "Iteration 738, loss = 0.00123329\n",
      "Iteration 739, loss = 0.00123050\n",
      "Iteration 740, loss = 0.00122768\n",
      "Iteration 741, loss = 0.00122495\n",
      "Iteration 742, loss = 0.00122202\n",
      "Iteration 743, loss = 0.00121917\n",
      "Iteration 744, loss = 0.00121648\n",
      "Iteration 745, loss = 0.00121370\n",
      "Iteration 746, loss = 0.00121093\n",
      "Iteration 747, loss = 0.00120817\n",
      "Iteration 748, loss = 0.00120542\n",
      "Iteration 749, loss = 0.00120264\n",
      "Iteration 750, loss = 0.00119991\n",
      "Iteration 751, loss = 0.00119716\n",
      "Iteration 752, loss = 0.00119448\n",
      "Iteration 753, loss = 0.00119178\n",
      "Iteration 754, loss = 0.00118904\n",
      "Iteration 755, loss = 0.00118641\n",
      "Iteration 756, loss = 0.00118361\n",
      "Iteration 757, loss = 0.00118104\n",
      "Iteration 758, loss = 0.00117840\n",
      "Iteration 759, loss = 0.00117573\n",
      "Iteration 760, loss = 0.00117310\n",
      "Iteration 761, loss = 0.00117052\n",
      "Iteration 762, loss = 0.00116774\n",
      "Iteration 763, loss = 0.00116513\n",
      "Iteration 764, loss = 0.00116254\n",
      "Iteration 765, loss = 0.00115993\n",
      "Iteration 766, loss = 0.00115728\n",
      "Iteration 767, loss = 0.00115468\n",
      "Iteration 768, loss = 0.00115219\n",
      "Iteration 769, loss = 0.00114966\n",
      "Iteration 770, loss = 0.00114707\n",
      "Iteration 771, loss = 0.00114461\n",
      "Iteration 772, loss = 0.00114197\n",
      "Iteration 773, loss = 0.00113939\n",
      "Iteration 774, loss = 0.00113689\n",
      "Iteration 775, loss = 0.00113435\n",
      "Iteration 776, loss = 0.00113175\n",
      "Iteration 777, loss = 0.00112934\n",
      "Iteration 778, loss = 0.00112686\n",
      "Iteration 779, loss = 0.00112438\n",
      "Iteration 780, loss = 0.00112192\n",
      "Iteration 781, loss = 0.00111944\n",
      "Iteration 782, loss = 0.00111695\n",
      "Iteration 783, loss = 0.00111452\n",
      "Iteration 784, loss = 0.00111202\n",
      "Iteration 785, loss = 0.00110961\n",
      "Iteration 786, loss = 0.00110718\n",
      "Iteration 787, loss = 0.00110480\n",
      "Iteration 788, loss = 0.00110229\n",
      "Iteration 789, loss = 0.00109991\n",
      "Iteration 790, loss = 0.00109744\n",
      "Iteration 791, loss = 0.00109499\n",
      "Iteration 792, loss = 0.00109258\n",
      "Iteration 793, loss = 0.00109026\n",
      "Iteration 794, loss = 0.00108793\n",
      "Iteration 795, loss = 0.00108545\n",
      "Iteration 796, loss = 0.00108311\n",
      "Iteration 797, loss = 0.00108079\n",
      "Iteration 798, loss = 0.00107859\n",
      "Iteration 799, loss = 0.00107625\n",
      "Iteration 800, loss = 0.00107391\n",
      "Iteration 801, loss = 0.00107162\n",
      "Iteration 802, loss = 0.00106931\n",
      "Iteration 803, loss = 0.00106700\n",
      "Iteration 804, loss = 0.00106479\n",
      "Iteration 805, loss = 0.00106248\n",
      "Iteration 806, loss = 0.00106021\n",
      "Iteration 807, loss = 0.00105795\n",
      "Iteration 808, loss = 0.00105566\n",
      "Iteration 809, loss = 0.00105339\n",
      "Iteration 810, loss = 0.00105121\n",
      "Iteration 811, loss = 0.00104903\n",
      "Iteration 812, loss = 0.00104677\n",
      "Iteration 813, loss = 0.00104451\n",
      "Iteration 814, loss = 0.00104233\n",
      "Iteration 815, loss = 0.00104018\n",
      "Iteration 816, loss = 0.00103798\n",
      "Iteration 817, loss = 0.00103579\n",
      "Iteration 818, loss = 0.00103365\n",
      "Iteration 819, loss = 0.00103141\n",
      "Iteration 820, loss = 0.00102927\n",
      "Iteration 821, loss = 0.00102718\n",
      "Iteration 822, loss = 0.00102503\n",
      "Iteration 823, loss = 0.00102291\n",
      "Iteration 824, loss = 0.00102076\n",
      "Iteration 825, loss = 0.00101870\n",
      "Iteration 826, loss = 0.00101657\n",
      "Iteration 827, loss = 0.00101449\n",
      "Iteration 828, loss = 0.00101245\n",
      "Iteration 829, loss = 0.00101029\n",
      "Iteration 830, loss = 0.00100825\n",
      "Iteration 831, loss = 0.00100614\n",
      "Iteration 832, loss = 0.00100405\n",
      "Iteration 833, loss = 0.00100198\n",
      "Iteration 834, loss = 0.00099996\n",
      "Iteration 835, loss = 0.00099783\n",
      "Iteration 836, loss = 0.00099582\n",
      "Iteration 837, loss = 0.00099376\n",
      "Iteration 838, loss = 0.00099176\n",
      "Iteration 839, loss = 0.00098971\n",
      "Iteration 840, loss = 0.00098771\n",
      "Iteration 841, loss = 0.00098572\n",
      "Iteration 842, loss = 0.00098371\n",
      "Iteration 843, loss = 0.00098166\n",
      "Iteration 844, loss = 0.00097970\n",
      "Iteration 845, loss = 0.00097771\n",
      "Iteration 846, loss = 0.00097571\n",
      "Iteration 847, loss = 0.00097374\n",
      "Iteration 848, loss = 0.00097184\n",
      "Iteration 849, loss = 0.00096983\n",
      "Iteration 850, loss = 0.00096788\n",
      "Iteration 851, loss = 0.00096594\n",
      "Iteration 852, loss = 0.00096404\n",
      "Iteration 853, loss = 0.00096215\n",
      "Iteration 854, loss = 0.00096018\n",
      "Iteration 855, loss = 0.00095831\n",
      "Iteration 856, loss = 0.00095640\n",
      "Iteration 857, loss = 0.00095450\n",
      "Iteration 858, loss = 0.00095257\n",
      "Iteration 859, loss = 0.00095069\n",
      "Iteration 860, loss = 0.00094879\n",
      "Iteration 861, loss = 0.00094688\n",
      "Iteration 862, loss = 0.00094496\n",
      "Iteration 863, loss = 0.00094308\n",
      "Iteration 864, loss = 0.00094116\n",
      "Iteration 865, loss = 0.00093931\n",
      "Iteration 866, loss = 0.00093744\n",
      "Iteration 867, loss = 0.00093553\n",
      "Iteration 868, loss = 0.00093369\n",
      "Iteration 869, loss = 0.00093186\n",
      "Iteration 870, loss = 0.00092998\n",
      "Iteration 871, loss = 0.00092818\n",
      "Iteration 872, loss = 0.00092636\n",
      "Iteration 873, loss = 0.00092454\n",
      "Iteration 874, loss = 0.00092267\n",
      "Iteration 875, loss = 0.00092092\n",
      "Iteration 876, loss = 0.00091906\n",
      "Iteration 877, loss = 0.00091729\n",
      "Iteration 878, loss = 0.00091550\n",
      "Iteration 879, loss = 0.00091372\n",
      "Iteration 880, loss = 0.00091193\n",
      "Iteration 881, loss = 0.00091019\n",
      "Iteration 882, loss = 0.00090839\n",
      "Iteration 883, loss = 0.00090669\n",
      "Iteration 884, loss = 0.00090490\n",
      "Iteration 885, loss = 0.00090312\n",
      "Iteration 886, loss = 0.00090138\n",
      "Iteration 887, loss = 0.00089966\n",
      "Iteration 888, loss = 0.00089788\n",
      "Iteration 889, loss = 0.00089616\n",
      "Iteration 890, loss = 0.00089443\n",
      "Iteration 891, loss = 0.00089265\n",
      "Iteration 892, loss = 0.00089089\n",
      "Iteration 893, loss = 0.00088926\n",
      "Iteration 894, loss = 0.00088755\n",
      "Iteration 895, loss = 0.00088582\n",
      "Iteration 896, loss = 0.00088409\n",
      "Iteration 897, loss = 0.00088242\n",
      "Iteration 898, loss = 0.00088068\n",
      "Iteration 899, loss = 0.00087901\n",
      "Iteration 900, loss = 0.00087733\n",
      "Iteration 901, loss = 0.00087572\n",
      "Iteration 902, loss = 0.00087402\n",
      "Iteration 903, loss = 0.00087236\n",
      "Iteration 904, loss = 0.00087072\n",
      "Iteration 905, loss = 0.00086903\n",
      "Iteration 906, loss = 0.00086742\n",
      "Iteration 907, loss = 0.00086578\n",
      "Iteration 908, loss = 0.00086412\n",
      "Iteration 909, loss = 0.00086250\n",
      "Iteration 910, loss = 0.00086089\n",
      "Iteration 911, loss = 0.00085920\n",
      "Iteration 912, loss = 0.00085767\n",
      "Iteration 913, loss = 0.00085604\n",
      "Iteration 914, loss = 0.00085444\n",
      "Iteration 915, loss = 0.00085284\n",
      "Iteration 916, loss = 0.00085123\n",
      "Iteration 917, loss = 0.00084963\n",
      "Iteration 918, loss = 0.00084802\n",
      "Iteration 919, loss = 0.00084644\n",
      "Iteration 920, loss = 0.00084486\n",
      "Iteration 921, loss = 0.00084332\n",
      "Iteration 922, loss = 0.00084176\n",
      "Iteration 923, loss = 0.00084019\n",
      "Iteration 924, loss = 0.00083869\n",
      "Iteration 925, loss = 0.00083713\n",
      "Iteration 926, loss = 0.00083561\n",
      "Iteration 927, loss = 0.00083412\n",
      "Iteration 928, loss = 0.00083259\n",
      "Iteration 929, loss = 0.00083107\n",
      "Iteration 930, loss = 0.00082956\n",
      "Iteration 931, loss = 0.00082805\n",
      "Iteration 932, loss = 0.00082649\n",
      "Iteration 933, loss = 0.00082499\n",
      "Iteration 934, loss = 0.00082348\n",
      "Iteration 935, loss = 0.00082195\n",
      "Iteration 936, loss = 0.00082048\n",
      "Iteration 937, loss = 0.00081900\n",
      "Iteration 938, loss = 0.00081745\n",
      "Iteration 939, loss = 0.00081601\n",
      "Iteration 940, loss = 0.00081456\n",
      "Iteration 941, loss = 0.00081314\n",
      "Iteration 942, loss = 0.00081163\n",
      "Iteration 943, loss = 0.00081014\n",
      "Iteration 944, loss = 0.00080867\n",
      "Iteration 945, loss = 0.00080721\n",
      "Iteration 946, loss = 0.00080573\n",
      "Iteration 947, loss = 0.00080435\n",
      "Iteration 948, loss = 0.00080287\n",
      "Iteration 949, loss = 0.00080141\n",
      "Iteration 950, loss = 0.00079995\n",
      "Iteration 951, loss = 0.00079850\n",
      "Iteration 952, loss = 0.00079714\n",
      "Iteration 953, loss = 0.00079566\n",
      "Iteration 954, loss = 0.00079424\n",
      "Iteration 955, loss = 0.00079282\n",
      "Iteration 956, loss = 0.00079137\n",
      "Iteration 957, loss = 0.00078997\n",
      "Iteration 958, loss = 0.00078857\n",
      "Iteration 959, loss = 0.00078714\n",
      "Iteration 960, loss = 0.00078575\n",
      "Iteration 961, loss = 0.00078435\n",
      "Iteration 962, loss = 0.00078291\n",
      "Iteration 963, loss = 0.00078155\n",
      "Iteration 964, loss = 0.00078014\n",
      "Iteration 965, loss = 0.00077876\n",
      "Iteration 966, loss = 0.00077738\n",
      "Iteration 967, loss = 0.00077595\n",
      "Iteration 968, loss = 0.00077463\n",
      "Iteration 969, loss = 0.00077327\n",
      "Iteration 970, loss = 0.00077192\n",
      "Iteration 971, loss = 0.00077059\n",
      "Iteration 972, loss = 0.00076920\n",
      "Iteration 973, loss = 0.00076789\n",
      "Iteration 974, loss = 0.00076654\n",
      "Iteration 975, loss = 0.00076519\n",
      "Iteration 976, loss = 0.00076386\n",
      "Iteration 977, loss = 0.00076252\n",
      "Iteration 978, loss = 0.00076121\n",
      "Iteration 979, loss = 0.00075988\n",
      "Iteration 980, loss = 0.00075858\n",
      "Iteration 981, loss = 0.00075730\n",
      "Iteration 982, loss = 0.00075594\n",
      "Iteration 983, loss = 0.00075460\n",
      "Iteration 984, loss = 0.00075334\n",
      "Iteration 985, loss = 0.00075199\n",
      "Iteration 986, loss = 0.00075069\n",
      "Iteration 987, loss = 0.00074935\n",
      "Iteration 988, loss = 0.00074809\n",
      "Iteration 989, loss = 0.00074681\n",
      "Iteration 990, loss = 0.00074554\n",
      "Iteration 991, loss = 0.00074422\n",
      "Iteration 992, loss = 0.00074292\n",
      "Iteration 993, loss = 0.00074169\n",
      "Iteration 994, loss = 0.00074036\n",
      "Iteration 995, loss = 0.00073913\n",
      "Iteration 996, loss = 0.00073783\n",
      "Iteration 997, loss = 0.00073656\n",
      "Iteration 998, loss = 0.00073528\n",
      "Iteration 999, loss = 0.00073406\n",
      "Iteration 1000, loss = 0.00073280\n",
      "Iteration 1001, loss = 0.00073157\n",
      "Iteration 1002, loss = 0.00073030\n",
      "Iteration 1003, loss = 0.00072907\n",
      "Iteration 1004, loss = 0.00072787\n",
      "Iteration 1005, loss = 0.00072666\n",
      "Iteration 1006, loss = 0.00072542\n",
      "Iteration 1007, loss = 0.00072418\n",
      "Iteration 1008, loss = 0.00072298\n",
      "Iteration 1009, loss = 0.00072179\n",
      "Iteration 1010, loss = 0.00072058\n",
      "Iteration 1011, loss = 0.00071939\n",
      "Iteration 1012, loss = 0.00071820\n",
      "Iteration 1013, loss = 0.00071703\n",
      "Iteration 1014, loss = 0.00071580\n",
      "Iteration 1015, loss = 0.00071464\n",
      "Iteration 1016, loss = 0.00071341\n",
      "Iteration 1017, loss = 0.00071226\n",
      "Iteration 1018, loss = 0.00071105\n",
      "Iteration 1019, loss = 0.00070986\n",
      "Iteration 1020, loss = 0.00070865\n",
      "Iteration 1021, loss = 0.00070754\n",
      "Iteration 1022, loss = 0.00070632\n",
      "Iteration 1023, loss = 0.00070517\n",
      "Iteration 1024, loss = 0.00070401\n",
      "Iteration 1025, loss = 0.00070287\n",
      "Iteration 1026, loss = 0.00070170\n",
      "Iteration 1027, loss = 0.00070055\n",
      "Iteration 1028, loss = 0.00069942\n",
      "Iteration 1029, loss = 0.00069831\n",
      "Iteration 1030, loss = 0.00069716\n",
      "Iteration 1031, loss = 0.00069602\n",
      "Iteration 1032, loss = 0.00069488\n",
      "Iteration 1033, loss = 0.00069375\n",
      "Iteration 1034, loss = 0.00069257\n",
      "Iteration 1035, loss = 0.00069148\n",
      "Iteration 1036, loss = 0.00069035\n",
      "Iteration 1037, loss = 0.00068921\n",
      "Iteration 1038, loss = 0.00068809\n",
      "Iteration 1039, loss = 0.00068697\n",
      "Iteration 1040, loss = 0.00068585\n",
      "Iteration 1041, loss = 0.00068473\n",
      "Iteration 1042, loss = 0.00068363\n",
      "Iteration 1043, loss = 0.00068250\n",
      "Iteration 1044, loss = 0.00068138\n",
      "Iteration 1045, loss = 0.00068028\n",
      "Iteration 1046, loss = 0.00067917\n",
      "Iteration 1047, loss = 0.00067809\n",
      "Iteration 1048, loss = 0.00067694\n",
      "Iteration 1049, loss = 0.00067583\n",
      "Iteration 1050, loss = 0.00067473\n",
      "Iteration 1051, loss = 0.00067365\n",
      "Iteration 1052, loss = 0.00067255\n",
      "Iteration 1053, loss = 0.00067150\n",
      "Iteration 1054, loss = 0.00067038\n",
      "Iteration 1055, loss = 0.00066934\n",
      "Iteration 1056, loss = 0.00066829\n",
      "Iteration 1057, loss = 0.00066719\n",
      "Iteration 1058, loss = 0.00066611\n",
      "Iteration 1059, loss = 0.00066507\n",
      "Iteration 1060, loss = 0.00066399\n",
      "Iteration 1061, loss = 0.00066294\n",
      "Iteration 1062, loss = 0.00066187\n",
      "Iteration 1063, loss = 0.00066081\n",
      "Iteration 1064, loss = 0.00065976\n",
      "Iteration 1065, loss = 0.00065867\n",
      "Iteration 1066, loss = 0.00065765\n",
      "Iteration 1067, loss = 0.00065660\n",
      "Iteration 1068, loss = 0.00065553\n",
      "Iteration 1069, loss = 0.00065446\n",
      "Iteration 1070, loss = 0.00065343\n",
      "Iteration 1071, loss = 0.00065238\n",
      "Iteration 1072, loss = 0.00065138\n",
      "Iteration 1073, loss = 0.00065033\n",
      "Iteration 1074, loss = 0.00064930\n",
      "Iteration 1075, loss = 0.00064825\n",
      "Iteration 1076, loss = 0.00064723\n",
      "Iteration 1077, loss = 0.00064619\n",
      "Iteration 1078, loss = 0.00064515\n",
      "Iteration 1079, loss = 0.00064414\n",
      "Iteration 1080, loss = 0.00064312\n",
      "Iteration 1081, loss = 0.00064209\n",
      "Iteration 1082, loss = 0.00064109\n",
      "Iteration 1083, loss = 0.00064016\n",
      "Iteration 1084, loss = 0.00063909\n",
      "Iteration 1085, loss = 0.00063811\n",
      "Iteration 1086, loss = 0.00063709\n",
      "Iteration 1087, loss = 0.00063612\n",
      "Iteration 1088, loss = 0.00063512\n",
      "Iteration 1089, loss = 0.00063414\n",
      "Iteration 1090, loss = 0.00063311\n",
      "Iteration 1091, loss = 0.00063213\n",
      "Iteration 1092, loss = 0.00063118\n",
      "Iteration 1093, loss = 0.00063018\n",
      "Iteration 1094, loss = 0.00062921\n",
      "Iteration 1095, loss = 0.00062819\n",
      "Iteration 1096, loss = 0.00062721\n",
      "Iteration 1097, loss = 0.00062625\n",
      "Iteration 1098, loss = 0.00062527\n",
      "Iteration 1099, loss = 0.00062431\n",
      "Iteration 1100, loss = 0.00062335\n",
      "Iteration 1101, loss = 0.00062240\n",
      "Iteration 1102, loss = 0.00062147\n",
      "Iteration 1103, loss = 0.00062054\n",
      "Iteration 1104, loss = 0.00061960\n",
      "Iteration 1105, loss = 0.00061865\n",
      "Iteration 1106, loss = 0.00061771\n",
      "Training loss did not improve more than tol=0.000001 for 10 consecutive epochs. Stopping.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.67      0.80         3\n",
      "           1       1.00      1.00      1.00         2\n",
      "           2       0.75      1.00      0.86         3\n",
      "           3       1.00      1.00      1.00         4\n",
      "           4       1.00      1.00      1.00         2\n",
      "           5       1.00      1.00      1.00         4\n",
      "           6       1.00      1.00      1.00         3\n",
      "           7       1.00      0.60      0.75         5\n",
      "           8       1.00      1.00      1.00         2\n",
      "           9       0.50      1.00      0.67         2\n",
      "          10       1.00      1.00      1.00         4\n",
      "          11       1.00      1.00      1.00         3\n",
      "          12       1.00      1.00      1.00         1\n",
      "          13       1.00      1.00      1.00         2\n",
      "          14       1.00      1.00      1.00         3\n",
      "          15       1.00      1.00      1.00         3\n",
      "          16       0.75      0.60      0.67         5\n",
      "          17       1.00      1.00      1.00         5\n",
      "          18       1.00      1.00      1.00         1\n",
      "          19       1.00      1.00      1.00         4\n",
      "          20       1.00      1.00      1.00         3\n",
      "          21       1.00      1.00      1.00         3\n",
      "          22       1.00      1.00      1.00         4\n",
      "          23       1.00      1.00      1.00         5\n",
      "          24       0.00      0.00      0.00         0\n",
      "          25       0.67      1.00      0.80         2\n",
      "          26       1.00      1.00      1.00         3\n",
      "          27       1.00      1.00      1.00         2\n",
      "          28       1.00      1.00      1.00         5\n",
      "          29       1.00      1.00      1.00         3\n",
      "          30       1.00      1.00      1.00         1\n",
      "          31       1.00      1.00      1.00         2\n",
      "          32       1.00      1.00      1.00         3\n",
      "          33       1.00      1.00      1.00         2\n",
      "          34       1.00      1.00      1.00         3\n",
      "          35       1.00      1.00      1.00         2\n",
      "          36       1.00      1.00      1.00         5\n",
      "          37       1.00      1.00      1.00         3\n",
      "          38       1.00      1.00      1.00         4\n",
      "          39       1.00      0.50      0.67         4\n",
      "\n",
      "    accuracy                           0.94       120\n",
      "   macro avg       0.94      0.93      0.93       120\n",
      "weighted avg       0.97      0.94      0.95       120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "hidden_layers = (512,)\n",
    "activation = 'logistic'\n",
    "opts = dict(hidden_layer_sizes = hidden_layers,verbose = True,activation = activation,tol = 1e-6,max_iter = int(1e6))\n",
    "solver = 'adam'\n",
    "clf_MLP = MLPClassifier(solver = solver, **opts)\n",
    "clf_MLP.fit(Z_train,y_train)\n",
    "predictions = clf_MLP.predict(Z_test)\n",
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         7\n",
      "           1       1.00      1.00      1.00         8\n",
      "           2       1.00      1.00      1.00         7\n",
      "           3       1.00      1.00      1.00         6\n",
      "           4       1.00      1.00      1.00         8\n",
      "           5       1.00      1.00      1.00         6\n",
      "           6       1.00      1.00      1.00         7\n",
      "           7       1.00      1.00      1.00         5\n",
      "           8       1.00      1.00      1.00         8\n",
      "           9       1.00      1.00      1.00         8\n",
      "          10       1.00      1.00      1.00         6\n",
      "          11       1.00      1.00      1.00         7\n",
      "          12       1.00      1.00      1.00         9\n",
      "          13       1.00      1.00      1.00         8\n",
      "          14       1.00      1.00      1.00         7\n",
      "          15       1.00      1.00      1.00         7\n",
      "          16       1.00      1.00      1.00         5\n",
      "          17       1.00      1.00      1.00         5\n",
      "          18       1.00      1.00      1.00         9\n",
      "          19       1.00      1.00      1.00         6\n",
      "          20       1.00      1.00      1.00         7\n",
      "          21       1.00      1.00      1.00         7\n",
      "          22       1.00      1.00      1.00         6\n",
      "          23       1.00      1.00      1.00         5\n",
      "          24       1.00      1.00      1.00        10\n",
      "          25       1.00      1.00      1.00         8\n",
      "          26       1.00      1.00      1.00         7\n",
      "          27       1.00      1.00      1.00         8\n",
      "          28       1.00      1.00      1.00         5\n",
      "          29       1.00      1.00      1.00         7\n",
      "          30       1.00      1.00      1.00         9\n",
      "          31       1.00      1.00      1.00         8\n",
      "          32       1.00      1.00      1.00         7\n",
      "          33       1.00      1.00      1.00         8\n",
      "          34       1.00      1.00      1.00         7\n",
      "          35       1.00      1.00      1.00         8\n",
      "          36       1.00      1.00      1.00         5\n",
      "          37       1.00      1.00      1.00         7\n",
      "          38       1.00      1.00      1.00         6\n",
      "          39       1.00      1.00      1.00         6\n",
      "\n",
      "    accuracy                           1.00       280\n",
      "   macro avg       1.00      1.00      1.00       280\n",
      "weighted avg       1.00      1.00      1.00       280\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred = clf_MLP.predict(Z_train)\n",
    "print(classification_report(y_train,pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 結論:\n",
    "### 當成分比例採0.8時，準確率還不錯，為91%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
