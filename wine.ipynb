{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 作業三: 分類器的原理與評比實驗\n",
    "## 資料來源: 來自 3 個產區，178 瓶葡萄酒，含 13 種葡萄酒成分\n",
    "## 目標: \n",
    "## 計畫執行這篇講義描述的分類器比較，即採用三種分類器分別對三組資料進行分類學習與測試。其中分類器包括： \n",
    "## 1.多元羅吉斯回歸 2.支援向量機 3.神經網路\n",
    "## 影像資料處理: \n",
    "## 1.原始資料 2.進行PCA主成分分析\n",
    "## 分類方法: \n",
    "- ## Logistic Regression\n",
    "- ## SVM\n",
    "- ## Neural Network\n",
    "### 姓名: 鄭欣莉\n",
    "### 學號: 410877039"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 標準化後原始資料+羅吉斯回歸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Read data\n",
    "df = pd.read_excel('wine.xlsx')\n",
    "X = np.array(df.iloc[:, :-1]) # 排 除 最 後 一 欄 標 籤\n",
    "y = np.array(df.iloc[:, -1])\n",
    "\n",
    "# Split data into training and testing data\n",
    "test_size = 0.30 #多少比較好\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size) #分完train,test資料之後再標準化比較好\n",
    "#標準化\n",
    "scaler = StandardScaler()\n",
    "X_train_ = scaler.fit_transform(X_train)\n",
    "X_test_ = scaler.fit_transform(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- solver = '’lbfgs’'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def clf_LR(solver):\n",
    "    opts = dict(tol = 1e-6, max_iter = int(1e6), verbose=1) #tol=tolerance\n",
    "    clf_LR = LogisticRegression(solver = solver, **opts)\n",
    "    clf_LR.fit(X_train_, y_train)\n",
    "    y_pred = clf_LR.predict(X_test_)\n",
    "    # 測 試 資 料 之 準 確 率 回 報\n",
    "    print(f\"{accuracy_score(y_test, y_pred):.2%}\\n\")\n",
    "    print(f\"{clf_LR.score(X_test_, y_test):.2%}\\n\")\n",
    "    print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96.30%\n",
      "\n",
      "96.30%\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.93      1.00      0.97        14\n",
      "           2       0.95      0.95      0.95        20\n",
      "           3       1.00      0.95      0.97        20\n",
      "\n",
      "    accuracy                           0.96        54\n",
      "   macro avg       0.96      0.97      0.96        54\n",
      "weighted avg       0.96      0.96      0.96        54\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "solver = 'lbfgs' # ’lbfgs’ is the default \n",
    "clf_LR(solver)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- solver = 'liblinear'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]98.15%\n",
      "\n",
      "98.15%\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00        14\n",
      "           2       0.95      1.00      0.98        20\n",
      "           3       1.00      0.95      0.97        20\n",
      "\n",
      "    accuracy                           0.98        54\n",
      "   macro avg       0.98      0.98      0.98        54\n",
      "weighted avg       0.98      0.98      0.98        54\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf_LR(solver = 'liblinear')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- solver = 'newton-cg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96.30%\n",
      "\n",
      "96.30%\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.93      1.00      0.97        14\n",
      "           2       0.95      0.95      0.95        20\n",
      "           3       1.00      0.95      0.97        20\n",
      "\n",
      "    accuracy                           0.96        54\n",
      "   macro avg       0.96      0.97      0.96        54\n",
      "weighted avg       0.96      0.96      0.96        54\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "clf_LR(solver = 'newton-cg')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 結論:\n",
    "### 1. Classification Report 的衡量指標個別代表的意思:\n",
    "- ### 精確率(Precision) 為預測為真的樣本有幾個預測正確\n",
    "- ### 召回率(Recall) 為事實為真的樣本中有幾個是預測正確的\n",
    "- ### F1 score 為精確率和召回率的調和平均數\n",
    "### 2. 以不同solver的第一筆資料為例:\n",
    "- ### solver = 'lbfgs' 跟 'newton-cg'預測為真且預測正確的比例為93%，solver = 'liblinear'為100%\n",
    "- ### 每個 solver 在事實為真的樣本中預測正確率為百分之百\n",
    "- ### solver = 'lbfgs' 跟 'newton-cg'精確率和召回率的調和平均為97%，solver = 'liblinear'為100%\n",
    "### 3. 標準化過後的AT&T資料，solver = 'lbfgs' 跟 'newton-cg' 表現均差不多，準確率均在90%以上，solver = 'liblinear' 準確率最高98%，各個 solver 表現不錯"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA主成分分析(成分比例採0.8)+羅吉斯回歸"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- solver = 'lbfgs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def PCA_LR(n_components,solver):\n",
    "    pca = PCA(n_components = n_components).fit(X_train_) \n",
    "    Z_train = pca.transform(X_train_)\n",
    "    Z_test = pca.transform(X_test_)\n",
    "    opts = dict(tol = 1e-4, max_iter = int(1e6), verbose=1)\n",
    "    clf_PCA = LogisticRegression(solver = solver, **opts)\n",
    "    clf_PCA.fit(Z_train, y_train)\n",
    "    y_pred = clf_PCA.predict(Z_test)\n",
    "    print(f\"{clf_PCA.score(Z_test, y_test):.2%}\\n\")\n",
    "    print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92.59%\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.88      1.00      0.93        14\n",
      "           2       0.95      0.90      0.92        20\n",
      "           3       0.95      0.90      0.92        20\n",
      "\n",
      "    accuracy                           0.93        54\n",
      "   macro avg       0.92      0.93      0.93        54\n",
      "weighted avg       0.93      0.93      0.93        54\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "solver = 'lbfgs'\n",
    "PCA_LR(n_components = 0.8, solver = solver)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- solver = 'liblinear'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]92.59%\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.88      1.00      0.93        14\n",
      "           2       0.95      0.90      0.92        20\n",
      "           3       0.95      0.90      0.92        20\n",
      "\n",
      "    accuracy                           0.93        54\n",
      "   macro avg       0.92      0.93      0.93        54\n",
      "weighted avg       0.93      0.93      0.93        54\n",
      "\n"
     ]
    }
   ],
   "source": [
    "solver = 'liblinear'\n",
    "PCA_LR(n_components = 0.8, solver = solver)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- solver = 'newton−cg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92.59%\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.88      1.00      0.93        14\n",
      "           2       0.95      0.90      0.92        20\n",
      "           3       0.95      0.90      0.92        20\n",
      "\n",
      "    accuracy                           0.93        54\n",
      "   macro avg       0.92      0.93      0.93        54\n",
      "weighted avg       0.93      0.93      0.93        54\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "solver = 'newton-cg'\n",
    "PCA_LR(n_components = 0.8, solver = solver)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 結論:\n",
    "### PCA的AT&T資料，各個solver('lbfgs','liblinear','newton-cg')表現均差不多，準確率均在90%以上"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 若成分比例採0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92.59%\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.82      1.00      0.90        14\n",
      "           2       0.95      0.90      0.92        20\n",
      "           3       1.00      0.90      0.95        20\n",
      "\n",
      "    accuracy                           0.93        54\n",
      "   macro avg       0.92      0.93      0.92        54\n",
      "weighted avg       0.93      0.93      0.93        54\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "solver = 'lbfgs'\n",
    "PCA_LR(n_components = 0.6, solver = solver)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 若成分比例採0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66.67%\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.68      0.93      0.79        14\n",
      "           2       0.54      0.65      0.59        20\n",
      "           3       0.91      0.50      0.65        20\n",
      "\n",
      "    accuracy                           0.67        54\n",
      "   macro avg       0.71      0.69      0.67        54\n",
      "weighted avg       0.71      0.67      0.66        54\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "solver = 'lbfgs'\n",
    "PCA_LR(n_components = 0.3, solver = solver)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 結論:\n",
    "### 可以發現當成分比例採0.8時，準確率都還維持在90%以上，當成分比例採0.4以下時，準確率已經下滑至67%，所以可以推測出當成分比例在0.4左右時，在降低維度的同時也保留原本資料的重要資訊，所以接下來的成分比例都將採0.8跟0.4做比較"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 標準化後原始資料+SVC"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## one vs one "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- kernel = 'linear'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC,LinearSVC\n",
    "\n",
    "def clf_SVM(C, opts, clf_svm):\n",
    "    clf_svm.fit(X_train_,y_train)\n",
    "    predictions = clf_svm.predict(X_test_)\n",
    "    print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.88      1.00      0.93        14\n",
      "           2       0.95      0.90      0.92        20\n",
      "           3       1.00      0.95      0.97        20\n",
      "\n",
      "    accuracy                           0.94        54\n",
      "   macro avg       0.94      0.95      0.94        54\n",
      "weighted avg       0.95      0.94      0.94        54\n",
      "\n"
     ]
    }
   ],
   "source": [
    "C = 1\n",
    "opts = dict(C = C, decision_function_shape = 'ovo', \\\n",
    "             tol = 1e-6, max_iter = int(1e6))\n",
    "clf_svm = SVC(kernel = 'linear', **opts)\n",
    "clf_SVM(C = C, opts = opts, clf_svm = clf_svm)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- kernel = 'rbf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00        14\n",
      "           2       0.91      1.00      0.95        20\n",
      "           3       1.00      0.90      0.95        20\n",
      "\n",
      "    accuracy                           0.96        54\n",
      "   macro avg       0.97      0.97      0.97        54\n",
      "weighted avg       0.97      0.96      0.96        54\n",
      "\n"
     ]
    }
   ],
   "source": [
    "C = 1\n",
    "opts = dict(C = C, decision_function_shape = 'ovo', \\\n",
    "             tol = 1e-6, max_iter = int(1e6))\n",
    "clf_svm = SVC(kernel = 'rbf', gamma = 0.2, **opts)\n",
    "clf_SVM(C = C, opts = opts, clf_svm = clf_svm)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- kernel = 'poly'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.87      0.93      0.90        14\n",
      "           2       0.76      0.95      0.84        20\n",
      "           3       1.00      0.70      0.82        20\n",
      "\n",
      "    accuracy                           0.85        54\n",
      "   macro avg       0.88      0.86      0.85        54\n",
      "weighted avg       0.88      0.85      0.85        54\n",
      "\n"
     ]
    }
   ],
   "source": [
    "C = 1\n",
    "opts = dict(C = C, decision_function_shape = 'ovo', \\\n",
    "             tol = 1e-6, max_iter = int(1e6))\n",
    "clf_svm = SVC(kernel = 'poly', **opts)\n",
    "clf_SVM(C = C, opts = opts, clf_svm = clf_svm)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- kernel = 'sigmoid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.95      1.00      0.97        18\n",
      "           2       1.00      0.95      0.98        21\n",
      "           3       1.00      1.00      1.00        15\n",
      "\n",
      "    accuracy                           0.98        54\n",
      "   macro avg       0.98      0.98      0.98        54\n",
      "weighted avg       0.98      0.98      0.98        54\n",
      "\n"
     ]
    }
   ],
   "source": [
    "C = 1\n",
    "opts = dict(C=C,tol = 1e-6,max_iter = int(1e6))\n",
    "clf_svm = SVC(kernel = 'sigmoid', **opts)\n",
    "clf_svm.fit(X_train_,y_train)\n",
    "predictions = clf_svm.predict(X_test_)\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 結論:\n",
    "### 在決策函數中選擇一對一(one vs one)的模式，可以看到當 kernel 選擇 'sigmoid' 準確率有最高98%，kernel 選擇 'linear' 或 'rbf' 準確率有90%以上，而 kernel 選擇 'poly' 準確率只有85%，表現得不好"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## one vs the rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00        14\n",
      "           2       0.95      1.00      0.98        20\n",
      "           3       1.00      0.95      0.97        20\n",
      "\n",
      "    accuracy                           0.98        54\n",
      "   macro avg       0.98      0.98      0.98        54\n",
      "weighted avg       0.98      0.98      0.98        54\n",
      "\n"
     ]
    }
   ],
   "source": [
    "C = 1\n",
    "opts = dict(C = C,tol = 1e-6,max_iter = int(1e6))\n",
    "clf_svm = LinearSVC(**opts)\n",
    "clf_SVM(C = C, opts = opts, clf_svm = clf_svm)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 結論:\n",
    "### 1. 當模式為一對其他（one vs the rest），準確率為98%\n",
    "### 2. 模式為 一對一 跟 一對其他，可以看到準確率皆在90%以上，兩個模式的準確率差不多"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA主成分分析(成分比例採0.8) + SVM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## one vs one "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- kernel = 'sigmoid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00        14\n",
      "           2       0.90      0.95      0.93        20\n",
      "           3       0.95      0.90      0.92        20\n",
      "\n",
      "    accuracy                           0.94        54\n",
      "   macro avg       0.95      0.95      0.95        54\n",
      "weighted avg       0.95      0.94      0.94        54\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=0.8).fit(X_train_)\n",
    "Z_train = pca.transform(X_train_) #降維\n",
    "Z_test = pca.transform(X_test_)\n",
    "C = 1\n",
    "opts = dict(C = C, decision_function_shape = 'ovo', \\\n",
    "             tol = 1e-6, max_iter = int(1e6))\n",
    "clf_svm = SVC(kernel = 'linear',**opts)\n",
    "clf_svm.fit(Z_train,y_train)\n",
    "predictions = clf_svm.predict(Z_test)\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## one vs the rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00        14\n",
      "           2       0.90      0.95      0.93        20\n",
      "           3       0.95      0.90      0.92        20\n",
      "\n",
      "    accuracy                           0.94        54\n",
      "   macro avg       0.95      0.95      0.95        54\n",
      "weighted avg       0.95      0.94      0.94        54\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=0.8).fit(X_train_)\n",
    "Z_train = pca.transform(X_train_) #降維\n",
    "Z_test = pca.transform(X_test_)\n",
    "C = 1\n",
    "opts = dict(C=C,tol = 1e-6,max_iter = int(1e6))\n",
    "clf_svm = SVC(kernel = 'linear',**opts)\n",
    "clf_svm.fit(Z_train,y_train)\n",
    "predictions = clf_svm.predict(Z_test)\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 結論:\n",
    "### 當成分比例採0.8時，不管選擇一對一或一對其他都能維持準確率在90%"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA主成分分析(成分比例採0.4) + SVM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## one vs one "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- kernel = 'sigmoid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.88      1.00      0.93        14\n",
      "           2       0.95      0.90      0.92        20\n",
      "           3       1.00      0.95      0.97        20\n",
      "\n",
      "    accuracy                           0.94        54\n",
      "   macro avg       0.94      0.95      0.94        54\n",
      "weighted avg       0.95      0.94      0.94        54\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=0.4).fit(X_train_)\n",
    "Z_train = pca.transform(X_train_) #降維\n",
    "Z_test = pca.transform(X_test_)\n",
    "C = 1\n",
    "opts = dict(C = C, decision_function_shape = 'ovo', \\\n",
    "             tol = 1e-6, max_iter = int(1e6))\n",
    "clf_svm = SVC(kernel = 'linear',**opts)\n",
    "clf_svm.fit(Z_train,y_train)\n",
    "predictions = clf_svm.predict(Z_test)\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## one vs the rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.88      1.00      0.93        14\n",
      "           2       0.95      0.90      0.92        20\n",
      "           3       1.00      0.95      0.97        20\n",
      "\n",
      "    accuracy                           0.94        54\n",
      "   macro avg       0.94      0.95      0.94        54\n",
      "weighted avg       0.95      0.94      0.94        54\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=0.4).fit(X_train_)\n",
    "Z_train = pca.transform(X_train_) #降維\n",
    "Z_test = pca.transform(X_test_)\n",
    "C = 1\n",
    "opts = dict(C = C, decision_function_shape = 'ovo', \\\n",
    "             tol = 1e-6, max_iter = int(1e6))\n",
    "clf_svm = SVC(kernel = 'linear',**opts)\n",
    "clf_svm.fit(Z_train,y_train)\n",
    "predictions = clf_svm.predict(Z_test)\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 結論:\n",
    "### 當成分比例採0.8或0.4時，不管選擇一對一或一對其他都能維持準確率在90%，因此下面主成分分析的成分比例將採0.4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 標準化後原始資料+神經網路NN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- hidden_layers = (30,) , activation = 'logistic', solver = 'adam'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.15989271\n",
      "Iteration 2, loss = 1.14968396\n",
      "Iteration 3, loss = 1.13975042\n",
      "Iteration 4, loss = 1.13009480\n",
      "Iteration 5, loss = 1.12071827\n",
      "Iteration 6, loss = 1.11162018\n",
      "Iteration 7, loss = 1.10279781\n",
      "Iteration 8, loss = 1.09424665\n",
      "Iteration 9, loss = 1.08596089\n",
      "Iteration 10, loss = 1.07793380\n",
      "Iteration 11, loss = 1.07015783\n",
      "Iteration 12, loss = 1.06262477\n",
      "Iteration 13, loss = 1.05532580\n",
      "Iteration 14, loss = 1.04825163\n",
      "Iteration 15, loss = 1.04139259\n",
      "Iteration 16, loss = 1.03473868\n",
      "Iteration 17, loss = 1.02827968\n",
      "Iteration 18, loss = 1.02200515\n",
      "Iteration 19, loss = 1.01590456\n",
      "Iteration 20, loss = 1.00996727\n",
      "Iteration 21, loss = 1.00418271\n",
      "Iteration 22, loss = 0.99854034\n",
      "Iteration 23, loss = 0.99302983\n",
      "Iteration 24, loss = 0.98764109\n",
      "Iteration 25, loss = 0.98236436\n",
      "Iteration 26, loss = 0.97719024\n",
      "Iteration 27, loss = 0.97210977\n",
      "Iteration 28, loss = 0.96711446\n",
      "Iteration 29, loss = 0.96219628\n",
      "Iteration 30, loss = 0.95734774\n",
      "Iteration 31, loss = 0.95256181\n",
      "Iteration 32, loss = 0.94783198\n",
      "Iteration 33, loss = 0.94315224\n",
      "Iteration 34, loss = 0.93851706\n",
      "Iteration 35, loss = 0.93392135\n",
      "Iteration 36, loss = 0.92936047\n",
      "Iteration 37, loss = 0.92483019\n",
      "Iteration 38, loss = 0.92032668\n",
      "Iteration 39, loss = 0.91584642\n",
      "Iteration 40, loss = 0.91138627\n",
      "Iteration 41, loss = 0.90694337\n",
      "Iteration 42, loss = 0.90251513\n",
      "Iteration 43, loss = 0.89809923\n",
      "Iteration 44, loss = 0.89369356\n",
      "Iteration 45, loss = 0.88929622\n",
      "Iteration 46, loss = 0.88490552\n",
      "Iteration 47, loss = 0.88051990\n",
      "Iteration 48, loss = 0.87613798\n",
      "Iteration 49, loss = 0.87175851\n",
      "Iteration 50, loss = 0.86738035\n",
      "Iteration 51, loss = 0.86300249\n",
      "Iteration 52, loss = 0.85862400\n",
      "Iteration 53, loss = 0.85424404\n",
      "Iteration 54, loss = 0.84986186\n",
      "Iteration 55, loss = 0.84547678\n",
      "Iteration 56, loss = 0.84108818\n",
      "Iteration 57, loss = 0.83669552\n",
      "Iteration 58, loss = 0.83229830\n",
      "Iteration 59, loss = 0.82789609\n",
      "Iteration 60, loss = 0.82348850\n",
      "Iteration 61, loss = 0.81907520\n",
      "Iteration 62, loss = 0.81465590\n",
      "Iteration 63, loss = 0.81023037\n",
      "Iteration 64, loss = 0.80579840\n",
      "Iteration 65, loss = 0.80135985\n",
      "Iteration 66, loss = 0.79691462\n",
      "Iteration 67, loss = 0.79246264\n",
      "Iteration 68, loss = 0.78800388\n",
      "Iteration 69, loss = 0.78353835\n",
      "Iteration 70, loss = 0.77906613\n",
      "Iteration 71, loss = 0.77458728\n",
      "Iteration 72, loss = 0.77010195\n",
      "Iteration 73, loss = 0.76561029\n",
      "Iteration 74, loss = 0.76111251\n",
      "Iteration 75, loss = 0.75660884\n",
      "Iteration 76, loss = 0.75209954\n",
      "Iteration 77, loss = 0.74758491\n",
      "Iteration 78, loss = 0.74306530\n",
      "Iteration 79, loss = 0.73854104\n",
      "Iteration 80, loss = 0.73401255\n",
      "Iteration 81, loss = 0.72948023\n",
      "Iteration 82, loss = 0.72494452\n",
      "Iteration 83, loss = 0.72040592\n",
      "Iteration 84, loss = 0.71586490\n",
      "Iteration 85, loss = 0.71132200\n",
      "Iteration 86, loss = 0.70677774\n",
      "Iteration 87, loss = 0.70223271\n",
      "Iteration 88, loss = 0.69768748\n",
      "Iteration 89, loss = 0.69314266\n",
      "Iteration 90, loss = 0.68859886\n",
      "Iteration 91, loss = 0.68405672\n",
      "Iteration 92, loss = 0.67951689\n",
      "Iteration 93, loss = 0.67498002\n",
      "Iteration 94, loss = 0.67044679\n",
      "Iteration 95, loss = 0.66591788\n",
      "Iteration 96, loss = 0.66139399\n",
      "Iteration 97, loss = 0.65687579\n",
      "Iteration 98, loss = 0.65236400\n",
      "Iteration 99, loss = 0.64785932\n",
      "Iteration 100, loss = 0.64336245\n",
      "Iteration 101, loss = 0.63887411\n",
      "Iteration 102, loss = 0.63439500\n",
      "Iteration 103, loss = 0.62992583\n",
      "Iteration 104, loss = 0.62546730\n",
      "Iteration 105, loss = 0.62102011\n",
      "Iteration 106, loss = 0.61658496\n",
      "Iteration 107, loss = 0.61216254\n",
      "Iteration 108, loss = 0.60775351\n",
      "Iteration 109, loss = 0.60335858\n",
      "Iteration 110, loss = 0.59897838\n",
      "Iteration 111, loss = 0.59461359\n",
      "Iteration 112, loss = 0.59026484\n",
      "Iteration 113, loss = 0.58593276\n",
      "Iteration 114, loss = 0.58161799\n",
      "Iteration 115, loss = 0.57732112\n",
      "Iteration 116, loss = 0.57304275\n",
      "Iteration 117, loss = 0.56878346\n",
      "Iteration 118, loss = 0.56454381\n",
      "Iteration 119, loss = 0.56032435\n",
      "Iteration 120, loss = 0.55612563\n",
      "Iteration 121, loss = 0.55194814\n",
      "Iteration 122, loss = 0.54779241\n",
      "Iteration 123, loss = 0.54365891\n",
      "Iteration 124, loss = 0.53954810\n",
      "Iteration 125, loss = 0.53546045\n",
      "Iteration 126, loss = 0.53139638\n",
      "Iteration 127, loss = 0.52735630\n",
      "Iteration 128, loss = 0.52334062\n",
      "Iteration 129, loss = 0.51934971\n",
      "Iteration 130, loss = 0.51538393\n",
      "Iteration 131, loss = 0.51144364\n",
      "Iteration 132, loss = 0.50752915\n",
      "Iteration 133, loss = 0.50364076\n",
      "Iteration 134, loss = 0.49977878\n",
      "Iteration 135, loss = 0.49594347\n",
      "Iteration 136, loss = 0.49213509\n",
      "Iteration 137, loss = 0.48835388\n",
      "Iteration 138, loss = 0.48460005\n",
      "Iteration 139, loss = 0.48087381\n",
      "Iteration 140, loss = 0.47717534\n",
      "Iteration 141, loss = 0.47350483\n",
      "Iteration 142, loss = 0.46986241\n",
      "Iteration 143, loss = 0.46624823\n",
      "Iteration 144, loss = 0.46266241\n",
      "Iteration 145, loss = 0.45910506\n",
      "Iteration 146, loss = 0.45557627\n",
      "Iteration 147, loss = 0.45207611\n",
      "Iteration 148, loss = 0.44860467\n",
      "Iteration 149, loss = 0.44516197\n",
      "Iteration 150, loss = 0.44174806\n",
      "Iteration 151, loss = 0.43836297\n",
      "Iteration 152, loss = 0.43500670\n",
      "Iteration 153, loss = 0.43167924\n",
      "Iteration 154, loss = 0.42838060\n",
      "Iteration 155, loss = 0.42511074\n",
      "Iteration 156, loss = 0.42186963\n",
      "Iteration 157, loss = 0.41865721\n",
      "Iteration 158, loss = 0.41547343\n",
      "Iteration 159, loss = 0.41231823\n",
      "Iteration 160, loss = 0.40919152\n",
      "Iteration 161, loss = 0.40609322\n",
      "Iteration 162, loss = 0.40302323\n",
      "Iteration 163, loss = 0.39998145\n",
      "Iteration 164, loss = 0.39696777\n",
      "Iteration 165, loss = 0.39398206\n",
      "Iteration 166, loss = 0.39102421\n",
      "Iteration 167, loss = 0.38809406\n",
      "Iteration 168, loss = 0.38519149\n",
      "Iteration 169, loss = 0.38231634\n",
      "Iteration 170, loss = 0.37946846\n",
      "Iteration 171, loss = 0.37664769\n",
      "Iteration 172, loss = 0.37385387\n",
      "Iteration 173, loss = 0.37108682\n",
      "Iteration 174, loss = 0.36834637\n",
      "Iteration 175, loss = 0.36563234\n",
      "Iteration 176, loss = 0.36294454\n",
      "Iteration 177, loss = 0.36028280\n",
      "Iteration 178, loss = 0.35764691\n",
      "Iteration 179, loss = 0.35503668\n",
      "Iteration 180, loss = 0.35245191\n",
      "Iteration 181, loss = 0.34989240\n",
      "Iteration 182, loss = 0.34735795\n",
      "Iteration 183, loss = 0.34484836\n",
      "Iteration 184, loss = 0.34236340\n",
      "Iteration 185, loss = 0.33990288\n",
      "Iteration 186, loss = 0.33746658\n",
      "Iteration 187, loss = 0.33505428\n",
      "Iteration 188, loss = 0.33266577\n",
      "Iteration 189, loss = 0.33030084\n",
      "Iteration 190, loss = 0.32795926\n",
      "Iteration 191, loss = 0.32564082\n",
      "Iteration 192, loss = 0.32334530\n",
      "Iteration 193, loss = 0.32107247\n",
      "Iteration 194, loss = 0.31882213\n",
      "Iteration 195, loss = 0.31659404\n",
      "Iteration 196, loss = 0.31438799\n",
      "Iteration 197, loss = 0.31220375\n",
      "Iteration 198, loss = 0.31004111\n",
      "Iteration 199, loss = 0.30789985\n",
      "Iteration 200, loss = 0.30577974\n",
      "Iteration 201, loss = 0.30368057\n",
      "Iteration 202, loss = 0.30160212\n",
      "Iteration 203, loss = 0.29954417\n",
      "Iteration 204, loss = 0.29750650\n",
      "Iteration 205, loss = 0.29548890\n",
      "Iteration 206, loss = 0.29349115\n",
      "Iteration 207, loss = 0.29151305\n",
      "Iteration 208, loss = 0.28955436\n",
      "Iteration 209, loss = 0.28761489\n",
      "Iteration 210, loss = 0.28569442\n",
      "Iteration 211, loss = 0.28379274\n",
      "Iteration 212, loss = 0.28190965\n",
      "Iteration 213, loss = 0.28004494\n",
      "Iteration 214, loss = 0.27819841\n",
      "Iteration 215, loss = 0.27636984\n",
      "Iteration 216, loss = 0.27455904\n",
      "Iteration 217, loss = 0.27276581\n",
      "Iteration 218, loss = 0.27098995\n",
      "Iteration 219, loss = 0.26923126\n",
      "Iteration 220, loss = 0.26748955\n",
      "Iteration 221, loss = 0.26576462\n",
      "Iteration 222, loss = 0.26405628\n",
      "Iteration 223, loss = 0.26236435\n",
      "Iteration 224, loss = 0.26068863\n",
      "Iteration 225, loss = 0.25902894\n",
      "Iteration 226, loss = 0.25738509\n",
      "Iteration 227, loss = 0.25575690\n",
      "Iteration 228, loss = 0.25414419\n",
      "Iteration 229, loss = 0.25254678\n",
      "Iteration 230, loss = 0.25096449\n",
      "Iteration 231, loss = 0.24939715\n",
      "Iteration 232, loss = 0.24784459\n",
      "Iteration 233, loss = 0.24630663\n",
      "Iteration 234, loss = 0.24478310\n",
      "Iteration 235, loss = 0.24327384\n",
      "Iteration 236, loss = 0.24177868\n",
      "Iteration 237, loss = 0.24029746\n",
      "Iteration 238, loss = 0.23883001\n",
      "Iteration 239, loss = 0.23737617\n",
      "Iteration 240, loss = 0.23593579\n",
      "Iteration 241, loss = 0.23450871\n",
      "Iteration 242, loss = 0.23309478\n",
      "Iteration 243, loss = 0.23169383\n",
      "Iteration 244, loss = 0.23030573\n",
      "Iteration 245, loss = 0.22893032\n",
      "Iteration 246, loss = 0.22756746\n",
      "Iteration 247, loss = 0.22621699\n",
      "Iteration 248, loss = 0.22487878\n",
      "Iteration 249, loss = 0.22355269\n",
      "Iteration 250, loss = 0.22223856\n",
      "Iteration 251, loss = 0.22093628\n",
      "Iteration 252, loss = 0.21964569\n",
      "Iteration 253, loss = 0.21836666\n",
      "Iteration 254, loss = 0.21709906\n",
      "Iteration 255, loss = 0.21584276\n",
      "Iteration 256, loss = 0.21459763\n",
      "Iteration 257, loss = 0.21336354\n",
      "Iteration 258, loss = 0.21214037\n",
      "Iteration 259, loss = 0.21092798\n",
      "Iteration 260, loss = 0.20972625\n",
      "Iteration 261, loss = 0.20853507\n",
      "Iteration 262, loss = 0.20735431\n",
      "Iteration 263, loss = 0.20618385\n",
      "Iteration 264, loss = 0.20502358\n",
      "Iteration 265, loss = 0.20387338\n",
      "Iteration 266, loss = 0.20273313\n",
      "Iteration 267, loss = 0.20160273\n",
      "Iteration 268, loss = 0.20048206\n",
      "Iteration 269, loss = 0.19937101\n",
      "Iteration 270, loss = 0.19826947\n",
      "Iteration 271, loss = 0.19717734\n",
      "Iteration 272, loss = 0.19609451\n",
      "Iteration 273, loss = 0.19502087\n",
      "Iteration 274, loss = 0.19395633\n",
      "Iteration 275, loss = 0.19290078\n",
      "Iteration 276, loss = 0.19185412\n",
      "Iteration 277, loss = 0.19081625\n",
      "Iteration 278, loss = 0.18978708\n",
      "Iteration 279, loss = 0.18876650\n",
      "Iteration 280, loss = 0.18775443\n",
      "Iteration 281, loss = 0.18675076\n",
      "Iteration 282, loss = 0.18575541\n",
      "Iteration 283, loss = 0.18476828\n",
      "Iteration 284, loss = 0.18378929\n",
      "Iteration 285, loss = 0.18281834\n",
      "Iteration 286, loss = 0.18185535\n",
      "Iteration 287, loss = 0.18090022\n",
      "Iteration 288, loss = 0.17995288\n",
      "Iteration 289, loss = 0.17901323\n",
      "Iteration 290, loss = 0.17808121\n",
      "Iteration 291, loss = 0.17715671\n",
      "Iteration 292, loss = 0.17623966\n",
      "Iteration 293, loss = 0.17532999\n",
      "Iteration 294, loss = 0.17442760\n",
      "Iteration 295, loss = 0.17353243\n",
      "Iteration 296, loss = 0.17264439\n",
      "Iteration 297, loss = 0.17176341\n",
      "Iteration 298, loss = 0.17088941\n",
      "Iteration 299, loss = 0.17002233\n",
      "Iteration 300, loss = 0.16916208\n",
      "Iteration 301, loss = 0.16830859\n",
      "Iteration 302, loss = 0.16746179\n",
      "Iteration 303, loss = 0.16662161\n",
      "Iteration 304, loss = 0.16578798\n",
      "Iteration 305, loss = 0.16496084\n",
      "Iteration 306, loss = 0.16414010\n",
      "Iteration 307, loss = 0.16332572\n",
      "Iteration 308, loss = 0.16251761\n",
      "Iteration 309, loss = 0.16171572\n",
      "Iteration 310, loss = 0.16091998\n",
      "Iteration 311, loss = 0.16013032\n",
      "Iteration 312, loss = 0.15934669\n",
      "Iteration 313, loss = 0.15856902\n",
      "Iteration 314, loss = 0.15779724\n",
      "Iteration 315, loss = 0.15703131\n",
      "Iteration 316, loss = 0.15627115\n",
      "Iteration 317, loss = 0.15551671\n",
      "Iteration 318, loss = 0.15476793\n",
      "Iteration 319, loss = 0.15402476\n",
      "Iteration 320, loss = 0.15328713\n",
      "Iteration 321, loss = 0.15255499\n",
      "Iteration 322, loss = 0.15182829\n",
      "Iteration 323, loss = 0.15110696\n",
      "Iteration 324, loss = 0.15039096\n",
      "Iteration 325, loss = 0.14968023\n",
      "Iteration 326, loss = 0.14897472\n",
      "Iteration 327, loss = 0.14827437\n",
      "Iteration 328, loss = 0.14757914\n",
      "Iteration 329, loss = 0.14688897\n",
      "Iteration 330, loss = 0.14620381\n",
      "Iteration 331, loss = 0.14552362\n",
      "Iteration 332, loss = 0.14484835\n",
      "Iteration 333, loss = 0.14417793\n",
      "Iteration 334, loss = 0.14351234\n",
      "Iteration 335, loss = 0.14285151\n",
      "Iteration 336, loss = 0.14219541\n",
      "Iteration 337, loss = 0.14154399\n",
      "Iteration 338, loss = 0.14089720\n",
      "Iteration 339, loss = 0.14025499\n",
      "Iteration 340, loss = 0.13961733\n",
      "Iteration 341, loss = 0.13898416\n",
      "Iteration 342, loss = 0.13835545\n",
      "Iteration 343, loss = 0.13773115\n",
      "Iteration 344, loss = 0.13711122\n",
      "Iteration 345, loss = 0.13649561\n",
      "Iteration 346, loss = 0.13588430\n",
      "Iteration 347, loss = 0.13527722\n",
      "Iteration 348, loss = 0.13467435\n",
      "Iteration 349, loss = 0.13407564\n",
      "Iteration 350, loss = 0.13348106\n",
      "Iteration 351, loss = 0.13289056\n",
      "Iteration 352, loss = 0.13230411\n",
      "Iteration 353, loss = 0.13172167\n",
      "Iteration 354, loss = 0.13114320\n",
      "Iteration 355, loss = 0.13056867\n",
      "Iteration 356, loss = 0.12999803\n",
      "Iteration 357, loss = 0.12943125\n",
      "Iteration 358, loss = 0.12886829\n",
      "Iteration 359, loss = 0.12830913\n",
      "Iteration 360, loss = 0.12775371\n",
      "Iteration 361, loss = 0.12720202\n",
      "Iteration 362, loss = 0.12665401\n",
      "Iteration 363, loss = 0.12610965\n",
      "Iteration 364, loss = 0.12556891\n",
      "Iteration 365, loss = 0.12503176\n",
      "Iteration 366, loss = 0.12449815\n",
      "Iteration 367, loss = 0.12396807\n",
      "Iteration 368, loss = 0.12344147\n",
      "Iteration 369, loss = 0.12291832\n",
      "Iteration 370, loss = 0.12239860\n",
      "Iteration 371, loss = 0.12188228\n",
      "Iteration 372, loss = 0.12136931\n",
      "Iteration 373, loss = 0.12085968\n",
      "Iteration 374, loss = 0.12035335\n",
      "Iteration 375, loss = 0.11985029\n",
      "Iteration 376, loss = 0.11935048\n",
      "Iteration 377, loss = 0.11885388\n",
      "Iteration 378, loss = 0.11836047\n",
      "Iteration 379, loss = 0.11787021\n",
      "Iteration 380, loss = 0.11738309\n",
      "Iteration 381, loss = 0.11689907\n",
      "Iteration 382, loss = 0.11641812\n",
      "Iteration 383, loss = 0.11594022\n",
      "Iteration 384, loss = 0.11546534\n",
      "Iteration 385, loss = 0.11499345\n",
      "Iteration 386, loss = 0.11452454\n",
      "Iteration 387, loss = 0.11405856\n",
      "Iteration 388, loss = 0.11359551\n",
      "Iteration 389, loss = 0.11313534\n",
      "Iteration 390, loss = 0.11267805\n",
      "Iteration 391, loss = 0.11222359\n",
      "Iteration 392, loss = 0.11177195\n",
      "Iteration 393, loss = 0.11132311\n",
      "Iteration 394, loss = 0.11087704\n",
      "Iteration 395, loss = 0.11043371\n",
      "Iteration 396, loss = 0.10999310\n",
      "Iteration 397, loss = 0.10955519\n",
      "Iteration 398, loss = 0.10911996\n",
      "Iteration 399, loss = 0.10868739\n",
      "Iteration 400, loss = 0.10825744\n",
      "Iteration 401, loss = 0.10783010\n",
      "Iteration 402, loss = 0.10740536\n",
      "Iteration 403, loss = 0.10698317\n",
      "Iteration 404, loss = 0.10656353\n",
      "Iteration 405, loss = 0.10614641\n",
      "Iteration 406, loss = 0.10573180\n",
      "Iteration 407, loss = 0.10531967\n",
      "Iteration 408, loss = 0.10490999\n",
      "Iteration 409, loss = 0.10450276\n",
      "Iteration 410, loss = 0.10409794\n",
      "Iteration 411, loss = 0.10369553\n",
      "Iteration 412, loss = 0.10329549\n",
      "Iteration 413, loss = 0.10289782\n",
      "Iteration 414, loss = 0.10250249\n",
      "Iteration 415, loss = 0.10210947\n",
      "Iteration 416, loss = 0.10171877\n",
      "Iteration 417, loss = 0.10133034\n",
      "Iteration 418, loss = 0.10094419\n",
      "Iteration 419, loss = 0.10056028\n",
      "Iteration 420, loss = 0.10017860\n",
      "Iteration 421, loss = 0.09979913\n",
      "Iteration 422, loss = 0.09942185\n",
      "Iteration 423, loss = 0.09904675\n",
      "Iteration 424, loss = 0.09867381\n",
      "Iteration 425, loss = 0.09830301\n",
      "Iteration 426, loss = 0.09793434\n",
      "Iteration 427, loss = 0.09756777\n",
      "Iteration 428, loss = 0.09720329\n",
      "Iteration 429, loss = 0.09684089\n",
      "Iteration 430, loss = 0.09648054\n",
      "Iteration 431, loss = 0.09612224\n",
      "Iteration 432, loss = 0.09576596\n",
      "Iteration 433, loss = 0.09541169\n",
      "Iteration 434, loss = 0.09505942\n",
      "Iteration 435, loss = 0.09470912\n",
      "Iteration 436, loss = 0.09436079\n",
      "Iteration 437, loss = 0.09401440\n",
      "Iteration 438, loss = 0.09366994\n",
      "Iteration 439, loss = 0.09332741\n",
      "Iteration 440, loss = 0.09298677\n",
      "Iteration 441, loss = 0.09264803\n",
      "Iteration 442, loss = 0.09231115\n",
      "Iteration 443, loss = 0.09197614\n",
      "Iteration 444, loss = 0.09164297\n",
      "Iteration 445, loss = 0.09131163\n",
      "Iteration 446, loss = 0.09098210\n",
      "Iteration 447, loss = 0.09065438\n",
      "Iteration 448, loss = 0.09032845\n",
      "Iteration 449, loss = 0.09000430\n",
      "Iteration 450, loss = 0.08968190\n",
      "Iteration 451, loss = 0.08936126\n",
      "Iteration 452, loss = 0.08904235\n",
      "Iteration 453, loss = 0.08872516\n",
      "Iteration 454, loss = 0.08840969\n",
      "Iteration 455, loss = 0.08809591\n",
      "Iteration 456, loss = 0.08778381\n",
      "Iteration 457, loss = 0.08747338\n",
      "Iteration 458, loss = 0.08716462\n",
      "Iteration 459, loss = 0.08685750\n",
      "Iteration 460, loss = 0.08655201\n",
      "Iteration 461, loss = 0.08624815\n",
      "Iteration 462, loss = 0.08594590\n",
      "Iteration 463, loss = 0.08564525\n",
      "Iteration 464, loss = 0.08534618\n",
      "Iteration 465, loss = 0.08504869\n",
      "Iteration 466, loss = 0.08475276\n",
      "Iteration 467, loss = 0.08445838\n",
      "Iteration 468, loss = 0.08416555\n",
      "Iteration 469, loss = 0.08387425\n",
      "Iteration 470, loss = 0.08358446\n",
      "Iteration 471, loss = 0.08329618\n",
      "Iteration 472, loss = 0.08300940\n",
      "Iteration 473, loss = 0.08272410\n",
      "Iteration 474, loss = 0.08244028\n",
      "Iteration 475, loss = 0.08215793\n",
      "Iteration 476, loss = 0.08187702\n",
      "Iteration 477, loss = 0.08159757\n",
      "Iteration 478, loss = 0.08131954\n",
      "Iteration 479, loss = 0.08104294\n",
      "Iteration 480, loss = 0.08076775\n",
      "Iteration 481, loss = 0.08049396\n",
      "Iteration 482, loss = 0.08022157\n",
      "Iteration 483, loss = 0.07995056\n",
      "Iteration 484, loss = 0.07968093\n",
      "Iteration 485, loss = 0.07941265\n",
      "Iteration 486, loss = 0.07914574\n",
      "Iteration 487, loss = 0.07888016\n",
      "Iteration 488, loss = 0.07861593\n",
      "Iteration 489, loss = 0.07835301\n",
      "Iteration 490, loss = 0.07809142\n",
      "Iteration 491, loss = 0.07783113\n",
      "Iteration 492, loss = 0.07757214\n",
      "Iteration 493, loss = 0.07731444\n",
      "Iteration 494, loss = 0.07705803\n",
      "Iteration 495, loss = 0.07680288\n",
      "Iteration 496, loss = 0.07654900\n",
      "Iteration 497, loss = 0.07629637\n",
      "Iteration 498, loss = 0.07604498\n",
      "Iteration 499, loss = 0.07579484\n",
      "Iteration 500, loss = 0.07554592\n",
      "Iteration 501, loss = 0.07529822\n",
      "Iteration 502, loss = 0.07505174\n",
      "Iteration 503, loss = 0.07480646\n",
      "Iteration 504, loss = 0.07456237\n",
      "Iteration 505, loss = 0.07431947\n",
      "Iteration 506, loss = 0.07407775\n",
      "Iteration 507, loss = 0.07383721\n",
      "Iteration 508, loss = 0.07359783\n",
      "Iteration 509, loss = 0.07335960\n",
      "Iteration 510, loss = 0.07312252\n",
      "Iteration 511, loss = 0.07288658\n",
      "Iteration 512, loss = 0.07265178\n",
      "Iteration 513, loss = 0.07241810\n",
      "Iteration 514, loss = 0.07218554\n",
      "Iteration 515, loss = 0.07195409\n",
      "Iteration 516, loss = 0.07172374\n",
      "Iteration 517, loss = 0.07149449\n",
      "Iteration 518, loss = 0.07126632\n",
      "Iteration 519, loss = 0.07103924\n",
      "Iteration 520, loss = 0.07081324\n",
      "Iteration 521, loss = 0.07058830\n",
      "Iteration 522, loss = 0.07036442\n",
      "Iteration 523, loss = 0.07014159\n",
      "Iteration 524, loss = 0.06991981\n",
      "Iteration 525, loss = 0.06969907\n",
      "Iteration 526, loss = 0.06947937\n",
      "Iteration 527, loss = 0.06926069\n",
      "Iteration 528, loss = 0.06904303\n",
      "Iteration 529, loss = 0.06882638\n",
      "Iteration 530, loss = 0.06861074\n",
      "Iteration 531, loss = 0.06839610\n",
      "Iteration 532, loss = 0.06818246\n",
      "Iteration 533, loss = 0.06796980\n",
      "Iteration 534, loss = 0.06775812\n",
      "Iteration 535, loss = 0.06754742\n",
      "Iteration 536, loss = 0.06733768\n",
      "Iteration 537, loss = 0.06712891\n",
      "Iteration 538, loss = 0.06692110\n",
      "Iteration 539, loss = 0.06671423\n",
      "Iteration 540, loss = 0.06650831\n",
      "Iteration 541, loss = 0.06630333\n",
      "Iteration 542, loss = 0.06609928\n",
      "Iteration 543, loss = 0.06589616\n",
      "Iteration 544, loss = 0.06569396\n",
      "Iteration 545, loss = 0.06549267\n",
      "Iteration 546, loss = 0.06529230\n",
      "Iteration 547, loss = 0.06509282\n",
      "Iteration 548, loss = 0.06489425\n",
      "Iteration 549, loss = 0.06469657\n",
      "Iteration 550, loss = 0.06449977\n",
      "Iteration 551, loss = 0.06430386\n",
      "Iteration 552, loss = 0.06410882\n",
      "Iteration 553, loss = 0.06391465\n",
      "Iteration 554, loss = 0.06372135\n",
      "Iteration 555, loss = 0.06352891\n",
      "Iteration 556, loss = 0.06333732\n",
      "Iteration 557, loss = 0.06314658\n",
      "Iteration 558, loss = 0.06295669\n",
      "Iteration 559, loss = 0.06276764\n",
      "Iteration 560, loss = 0.06257942\n",
      "Iteration 561, loss = 0.06239203\n",
      "Iteration 562, loss = 0.06220546\n",
      "Iteration 563, loss = 0.06201971\n",
      "Iteration 564, loss = 0.06183478\n",
      "Iteration 565, loss = 0.06165065\n",
      "Iteration 566, loss = 0.06146733\n",
      "Iteration 567, loss = 0.06128481\n",
      "Iteration 568, loss = 0.06110309\n",
      "Iteration 569, loss = 0.06092215\n",
      "Iteration 570, loss = 0.06074200\n",
      "Iteration 571, loss = 0.06056263\n",
      "Iteration 572, loss = 0.06038404\n",
      "Iteration 573, loss = 0.06020622\n",
      "Iteration 574, loss = 0.06002916\n",
      "Iteration 575, loss = 0.05985287\n",
      "Iteration 576, loss = 0.05967733\n",
      "Iteration 577, loss = 0.05950255\n",
      "Iteration 578, loss = 0.05932852\n",
      "Iteration 579, loss = 0.05915523\n",
      "Iteration 580, loss = 0.05898268\n",
      "Iteration 581, loss = 0.05881087\n",
      "Iteration 582, loss = 0.05863979\n",
      "Iteration 583, loss = 0.05846944\n",
      "Iteration 584, loss = 0.05829980\n",
      "Iteration 585, loss = 0.05813089\n",
      "Iteration 586, loss = 0.05796270\n",
      "Iteration 587, loss = 0.05779521\n",
      "Iteration 588, loss = 0.05762843\n",
      "Iteration 589, loss = 0.05746235\n",
      "Iteration 590, loss = 0.05729697\n",
      "Iteration 591, loss = 0.05713229\n",
      "Iteration 592, loss = 0.05696829\n",
      "Iteration 593, loss = 0.05680498\n",
      "Iteration 594, loss = 0.05664236\n",
      "Iteration 595, loss = 0.05648041\n",
      "Iteration 596, loss = 0.05631914\n",
      "Iteration 597, loss = 0.05615854\n",
      "Iteration 598, loss = 0.05599860\n",
      "Iteration 599, loss = 0.05583933\n",
      "Iteration 600, loss = 0.05568072\n",
      "Iteration 601, loss = 0.05552277\n",
      "Iteration 602, loss = 0.05536546\n",
      "Iteration 603, loss = 0.05520881\n",
      "Iteration 604, loss = 0.05505280\n",
      "Iteration 605, loss = 0.05489743\n",
      "Iteration 606, loss = 0.05474270\n",
      "Iteration 607, loss = 0.05458860\n",
      "Iteration 608, loss = 0.05443514\n",
      "Iteration 609, loss = 0.05428230\n",
      "Iteration 610, loss = 0.05413008\n",
      "Iteration 611, loss = 0.05397849\n",
      "Iteration 612, loss = 0.05382751\n",
      "Iteration 613, loss = 0.05367714\n",
      "Iteration 614, loss = 0.05352739\n",
      "Iteration 615, loss = 0.05337824\n",
      "Iteration 616, loss = 0.05322970\n",
      "Iteration 617, loss = 0.05308175\n",
      "Iteration 618, loss = 0.05293440\n",
      "Iteration 619, loss = 0.05278765\n",
      "Iteration 620, loss = 0.05264148\n",
      "Iteration 621, loss = 0.05249590\n",
      "Iteration 622, loss = 0.05235091\n",
      "Iteration 623, loss = 0.05220650\n",
      "Iteration 624, loss = 0.05206266\n",
      "Iteration 625, loss = 0.05191940\n",
      "Iteration 626, loss = 0.05177671\n",
      "Iteration 627, loss = 0.05163459\n",
      "Iteration 628, loss = 0.05149303\n",
      "Iteration 629, loss = 0.05135204\n",
      "Iteration 630, loss = 0.05121160\n",
      "Iteration 631, loss = 0.05107172\n",
      "Iteration 632, loss = 0.05093239\n",
      "Iteration 633, loss = 0.05079362\n",
      "Iteration 634, loss = 0.05065539\n",
      "Iteration 635, loss = 0.05051770\n",
      "Iteration 636, loss = 0.05038056\n",
      "Iteration 637, loss = 0.05024395\n",
      "Iteration 638, loss = 0.05010789\n",
      "Iteration 639, loss = 0.04997235\n",
      "Iteration 640, loss = 0.04983734\n",
      "Iteration 641, loss = 0.04970286\n",
      "Iteration 642, loss = 0.04956891\n",
      "Iteration 643, loss = 0.04943548\n",
      "Iteration 644, loss = 0.04930256\n",
      "Iteration 645, loss = 0.04917017\n",
      "Iteration 646, loss = 0.04903828\n",
      "Iteration 647, loss = 0.04890691\n",
      "Iteration 648, loss = 0.04877605\n",
      "Iteration 649, loss = 0.04864569\n",
      "Iteration 650, loss = 0.04851583\n",
      "Iteration 651, loss = 0.04838648\n",
      "Iteration 652, loss = 0.04825762\n",
      "Iteration 653, loss = 0.04812926\n",
      "Iteration 654, loss = 0.04800139\n",
      "Iteration 655, loss = 0.04787401\n",
      "Iteration 656, loss = 0.04774712\n",
      "Iteration 657, loss = 0.04762071\n",
      "Iteration 658, loss = 0.04749479\n",
      "Iteration 659, loss = 0.04736934\n",
      "Iteration 660, loss = 0.04724438\n",
      "Iteration 661, loss = 0.04711988\n",
      "Iteration 662, loss = 0.04699587\n",
      "Iteration 663, loss = 0.04687232\n",
      "Iteration 664, loss = 0.04674924\n",
      "Iteration 665, loss = 0.04662663\n",
      "Iteration 666, loss = 0.04650448\n",
      "Iteration 667, loss = 0.04638279\n",
      "Iteration 668, loss = 0.04626156\n",
      "Iteration 669, loss = 0.04614078\n",
      "Iteration 670, loss = 0.04602046\n",
      "Iteration 671, loss = 0.04590060\n",
      "Iteration 672, loss = 0.04578118\n",
      "Iteration 673, loss = 0.04566221\n",
      "Iteration 674, loss = 0.04554368\n",
      "Iteration 675, loss = 0.04542560\n",
      "Iteration 676, loss = 0.04530795\n",
      "Iteration 677, loss = 0.04519075\n",
      "Iteration 678, loss = 0.04507398\n",
      "Iteration 679, loss = 0.04495765\n",
      "Iteration 680, loss = 0.04484175\n",
      "Iteration 681, loss = 0.04472627\n",
      "Iteration 682, loss = 0.04461123\n",
      "Iteration 683, loss = 0.04449661\n",
      "Iteration 684, loss = 0.04438242\n",
      "Iteration 685, loss = 0.04426864\n",
      "Iteration 686, loss = 0.04415529\n",
      "Iteration 687, loss = 0.04404235\n",
      "Iteration 688, loss = 0.04392983\n",
      "Iteration 689, loss = 0.04381772\n",
      "Iteration 690, loss = 0.04370602\n",
      "Iteration 691, loss = 0.04359473\n",
      "Iteration 692, loss = 0.04348385\n",
      "Iteration 693, loss = 0.04337338\n",
      "Iteration 694, loss = 0.04326330\n",
      "Iteration 695, loss = 0.04315363\n",
      "Iteration 696, loss = 0.04304436\n",
      "Iteration 697, loss = 0.04293548\n",
      "Iteration 698, loss = 0.04282700\n",
      "Iteration 699, loss = 0.04271892\n",
      "Iteration 700, loss = 0.04261122\n",
      "Iteration 701, loss = 0.04250392\n",
      "Iteration 702, loss = 0.04239700\n",
      "Iteration 703, loss = 0.04229047\n",
      "Iteration 704, loss = 0.04218433\n",
      "Iteration 705, loss = 0.04207857\n",
      "Iteration 706, loss = 0.04197318\n",
      "Iteration 707, loss = 0.04186818\n",
      "Iteration 708, loss = 0.04176355\n",
      "Iteration 709, loss = 0.04165930\n",
      "Iteration 710, loss = 0.04155542\n",
      "Iteration 711, loss = 0.04145191\n",
      "Iteration 712, loss = 0.04134878\n",
      "Iteration 713, loss = 0.04124601\n",
      "Iteration 714, loss = 0.04114360\n",
      "Iteration 715, loss = 0.04104157\n",
      "Iteration 716, loss = 0.04093989\n",
      "Iteration 717, loss = 0.04083858\n",
      "Iteration 718, loss = 0.04073762\n",
      "Iteration 719, loss = 0.04063703\n",
      "Iteration 720, loss = 0.04053679\n",
      "Iteration 721, loss = 0.04043690\n",
      "Iteration 722, loss = 0.04033737\n",
      "Iteration 723, loss = 0.04023819\n",
      "Iteration 724, loss = 0.04013935\n",
      "Iteration 725, loss = 0.04004087\n",
      "Iteration 726, loss = 0.03994273\n",
      "Iteration 727, loss = 0.03984494\n",
      "Iteration 728, loss = 0.03974749\n",
      "Iteration 729, loss = 0.03965038\n",
      "Iteration 730, loss = 0.03955361\n",
      "Iteration 731, loss = 0.03945718\n",
      "Iteration 732, loss = 0.03936109\n",
      "Iteration 733, loss = 0.03926533\n",
      "Iteration 734, loss = 0.03916990\n",
      "Iteration 735, loss = 0.03907481\n",
      "Iteration 736, loss = 0.03898005\n",
      "Iteration 737, loss = 0.03888562\n",
      "Iteration 738, loss = 0.03879151\n",
      "Iteration 739, loss = 0.03869773\n",
      "Iteration 740, loss = 0.03860428\n",
      "Iteration 741, loss = 0.03851115\n",
      "Iteration 742, loss = 0.03841834\n",
      "Iteration 743, loss = 0.03832585\n",
      "Iteration 744, loss = 0.03823368\n",
      "Iteration 745, loss = 0.03814183\n",
      "Iteration 746, loss = 0.03805029\n",
      "Iteration 747, loss = 0.03795907\n",
      "Iteration 748, loss = 0.03786816\n",
      "Iteration 749, loss = 0.03777756\n",
      "Iteration 750, loss = 0.03768727\n",
      "Iteration 751, loss = 0.03759730\n",
      "Iteration 752, loss = 0.03750762\n",
      "Iteration 753, loss = 0.03741826\n",
      "Iteration 754, loss = 0.03732920\n",
      "Iteration 755, loss = 0.03724044\n",
      "Iteration 756, loss = 0.03715199\n",
      "Iteration 757, loss = 0.03706383\n",
      "Iteration 758, loss = 0.03697598\n",
      "Iteration 759, loss = 0.03688842\n",
      "Iteration 760, loss = 0.03680116\n",
      "Iteration 761, loss = 0.03671419\n",
      "Iteration 762, loss = 0.03662752\n",
      "Iteration 763, loss = 0.03654114\n",
      "Iteration 764, loss = 0.03645505\n",
      "Iteration 765, loss = 0.03636926\n",
      "Iteration 766, loss = 0.03628375\n",
      "Iteration 767, loss = 0.03619853\n",
      "Iteration 768, loss = 0.03611359\n",
      "Iteration 769, loss = 0.03602894\n",
      "Iteration 770, loss = 0.03594458\n",
      "Iteration 771, loss = 0.03586049\n",
      "Iteration 772, loss = 0.03577669\n",
      "Iteration 773, loss = 0.03569317\n",
      "Iteration 774, loss = 0.03560992\n",
      "Iteration 775, loss = 0.03552696\n",
      "Iteration 776, loss = 0.03544427\n",
      "Iteration 777, loss = 0.03536185\n",
      "Iteration 778, loss = 0.03527971\n",
      "Iteration 779, loss = 0.03519784\n",
      "Iteration 780, loss = 0.03511625\n",
      "Iteration 781, loss = 0.03503492\n",
      "Iteration 782, loss = 0.03495387\n",
      "Iteration 783, loss = 0.03487308\n",
      "Iteration 784, loss = 0.03479255\n",
      "Iteration 785, loss = 0.03471230\n",
      "Iteration 786, loss = 0.03463231\n",
      "Iteration 787, loss = 0.03455258\n",
      "Iteration 788, loss = 0.03447311\n",
      "Iteration 789, loss = 0.03439391\n",
      "Iteration 790, loss = 0.03431496\n",
      "Iteration 791, loss = 0.03423628\n",
      "Iteration 792, loss = 0.03415785\n",
      "Iteration 793, loss = 0.03407968\n",
      "Iteration 794, loss = 0.03400176\n",
      "Iteration 795, loss = 0.03392410\n",
      "Iteration 796, loss = 0.03384669\n",
      "Iteration 797, loss = 0.03376954\n",
      "Iteration 798, loss = 0.03369263\n",
      "Iteration 799, loss = 0.03361598\n",
      "Iteration 800, loss = 0.03353957\n",
      "Iteration 801, loss = 0.03346342\n",
      "Iteration 802, loss = 0.03338751\n",
      "Iteration 803, loss = 0.03331184\n",
      "Iteration 804, loss = 0.03323642\n",
      "Iteration 805, loss = 0.03316125\n",
      "Iteration 806, loss = 0.03308631\n",
      "Iteration 807, loss = 0.03301162\n",
      "Iteration 808, loss = 0.03293717\n",
      "Iteration 809, loss = 0.03286296\n",
      "Iteration 810, loss = 0.03278899\n",
      "Iteration 811, loss = 0.03271526\n",
      "Iteration 812, loss = 0.03264176\n",
      "Iteration 813, loss = 0.03256850\n",
      "Iteration 814, loss = 0.03249547\n",
      "Iteration 815, loss = 0.03242268\n",
      "Iteration 816, loss = 0.03235012\n",
      "Iteration 817, loss = 0.03227779\n",
      "Iteration 818, loss = 0.03220569\n",
      "Iteration 819, loss = 0.03213383\n",
      "Iteration 820, loss = 0.03206219\n",
      "Iteration 821, loss = 0.03199078\n",
      "Iteration 822, loss = 0.03191959\n",
      "Iteration 823, loss = 0.03184864\n",
      "Iteration 824, loss = 0.03177790\n",
      "Iteration 825, loss = 0.03170740\n",
      "Iteration 826, loss = 0.03163711\n",
      "Iteration 827, loss = 0.03156705\n",
      "Iteration 828, loss = 0.03149721\n",
      "Iteration 829, loss = 0.03142759\n",
      "Iteration 830, loss = 0.03135819\n",
      "Iteration 831, loss = 0.03128900\n",
      "Iteration 832, loss = 0.03122004\n",
      "Iteration 833, loss = 0.03115129\n",
      "Iteration 834, loss = 0.03108276\n",
      "Iteration 835, loss = 0.03101444\n",
      "Iteration 836, loss = 0.03094634\n",
      "Iteration 837, loss = 0.03087845\n",
      "Iteration 838, loss = 0.03081077\n",
      "Iteration 839, loss = 0.03074331\n",
      "Iteration 840, loss = 0.03067605\n",
      "Iteration 841, loss = 0.03060901\n",
      "Iteration 842, loss = 0.03054217\n",
      "Iteration 843, loss = 0.03047555\n",
      "Iteration 844, loss = 0.03040913\n",
      "Iteration 845, loss = 0.03034291\n",
      "Iteration 846, loss = 0.03027691\n",
      "Iteration 847, loss = 0.03021110\n",
      "Iteration 848, loss = 0.03014550\n",
      "Iteration 849, loss = 0.03008011\n",
      "Iteration 850, loss = 0.03001491\n",
      "Iteration 851, loss = 0.02994992\n",
      "Iteration 852, loss = 0.02988513\n",
      "Iteration 853, loss = 0.02982054\n",
      "Iteration 854, loss = 0.02975615\n",
      "Iteration 855, loss = 0.02969195\n",
      "Iteration 856, loss = 0.02962795\n",
      "Iteration 857, loss = 0.02956415\n",
      "Iteration 858, loss = 0.02950055\n",
      "Iteration 859, loss = 0.02943714\n",
      "Iteration 860, loss = 0.02937392\n",
      "Iteration 861, loss = 0.02931090\n",
      "Iteration 862, loss = 0.02924807\n",
      "Iteration 863, loss = 0.02918544\n",
      "Iteration 864, loss = 0.02912299\n",
      "Iteration 865, loss = 0.02906074\n",
      "Iteration 866, loss = 0.02899867\n",
      "Iteration 867, loss = 0.02893679\n",
      "Iteration 868, loss = 0.02887510\n",
      "Iteration 869, loss = 0.02881360\n",
      "Iteration 870, loss = 0.02875229\n",
      "Iteration 871, loss = 0.02869116\n",
      "Iteration 872, loss = 0.02863022\n",
      "Iteration 873, loss = 0.02856946\n",
      "Iteration 874, loss = 0.02850888\n",
      "Iteration 875, loss = 0.02844849\n",
      "Iteration 876, loss = 0.02838828\n",
      "Iteration 877, loss = 0.02832825\n",
      "Iteration 878, loss = 0.02826840\n",
      "Iteration 879, loss = 0.02820874\n",
      "Iteration 880, loss = 0.02814925\n",
      "Iteration 881, loss = 0.02808994\n",
      "Iteration 882, loss = 0.02803081\n",
      "Iteration 883, loss = 0.02797185\n",
      "Iteration 884, loss = 0.02791308\n",
      "Iteration 885, loss = 0.02785448\n",
      "Iteration 886, loss = 0.02779605\n",
      "Iteration 887, loss = 0.02773780\n",
      "Iteration 888, loss = 0.02767972\n",
      "Iteration 889, loss = 0.02762181\n",
      "Iteration 890, loss = 0.02756408\n",
      "Iteration 891, loss = 0.02750652\n",
      "Iteration 892, loss = 0.02744913\n",
      "Iteration 893, loss = 0.02739191\n",
      "Iteration 894, loss = 0.02733487\n",
      "Iteration 895, loss = 0.02727799\n",
      "Iteration 896, loss = 0.02722127\n",
      "Iteration 897, loss = 0.02716473\n",
      "Iteration 898, loss = 0.02710835\n",
      "Iteration 899, loss = 0.02705214\n",
      "Iteration 900, loss = 0.02699610\n",
      "Iteration 901, loss = 0.02694022\n",
      "Iteration 902, loss = 0.02688451\n",
      "Iteration 903, loss = 0.02682896\n",
      "Iteration 904, loss = 0.02677357\n",
      "Iteration 905, loss = 0.02671835\n",
      "Iteration 906, loss = 0.02666329\n",
      "Iteration 907, loss = 0.02660839\n",
      "Iteration 908, loss = 0.02655365\n",
      "Iteration 909, loss = 0.02649907\n",
      "Iteration 910, loss = 0.02644465\n",
      "Iteration 911, loss = 0.02639039\n",
      "Iteration 912, loss = 0.02633628\n",
      "Iteration 913, loss = 0.02628234\n",
      "Iteration 914, loss = 0.02622855\n",
      "Iteration 915, loss = 0.02617492\n",
      "Iteration 916, loss = 0.02612145\n",
      "Iteration 917, loss = 0.02606813\n",
      "Iteration 918, loss = 0.02601496\n",
      "Iteration 919, loss = 0.02596195\n",
      "Iteration 920, loss = 0.02590910\n",
      "Iteration 921, loss = 0.02585639\n",
      "Iteration 922, loss = 0.02580384\n",
      "Iteration 923, loss = 0.02575144\n",
      "Iteration 924, loss = 0.02569920\n",
      "Iteration 925, loss = 0.02564710\n",
      "Iteration 926, loss = 0.02559515\n",
      "Iteration 927, loss = 0.02554335\n",
      "Iteration 928, loss = 0.02549171\n",
      "Iteration 929, loss = 0.02544021\n",
      "Iteration 930, loss = 0.02538885\n",
      "Iteration 931, loss = 0.02533765\n",
      "Iteration 932, loss = 0.02528659\n",
      "Iteration 933, loss = 0.02523568\n",
      "Iteration 934, loss = 0.02518491\n",
      "Iteration 935, loss = 0.02513429\n",
      "Iteration 936, loss = 0.02508382\n",
      "Iteration 937, loss = 0.02503348\n",
      "Iteration 938, loss = 0.02498330\n",
      "Iteration 939, loss = 0.02493325\n",
      "Iteration 940, loss = 0.02488335\n",
      "Iteration 941, loss = 0.02483359\n",
      "Iteration 942, loss = 0.02478397\n",
      "Iteration 943, loss = 0.02473449\n",
      "Iteration 944, loss = 0.02468515\n",
      "Iteration 945, loss = 0.02463595\n",
      "Iteration 946, loss = 0.02458689\n",
      "Iteration 947, loss = 0.02453797\n",
      "Iteration 948, loss = 0.02448919\n",
      "Iteration 949, loss = 0.02444054\n",
      "Iteration 950, loss = 0.02439204\n",
      "Iteration 951, loss = 0.02434366\n",
      "Iteration 952, loss = 0.02429543\n",
      "Iteration 953, loss = 0.02424733\n",
      "Iteration 954, loss = 0.02419937\n",
      "Iteration 955, loss = 0.02415154\n",
      "Iteration 956, loss = 0.02410384\n",
      "Iteration 957, loss = 0.02405628\n",
      "Iteration 958, loss = 0.02400886\n",
      "Iteration 959, loss = 0.02396156\n",
      "Iteration 960, loss = 0.02391440\n",
      "Iteration 961, loss = 0.02386737\n",
      "Iteration 962, loss = 0.02382047\n",
      "Iteration 963, loss = 0.02377370\n",
      "Iteration 964, loss = 0.02372706\n",
      "Iteration 965, loss = 0.02368055\n",
      "Iteration 966, loss = 0.02363417\n",
      "Iteration 967, loss = 0.02358792\n",
      "Iteration 968, loss = 0.02354180\n",
      "Iteration 969, loss = 0.02349580\n",
      "Iteration 970, loss = 0.02344994\n",
      "Iteration 971, loss = 0.02340420\n",
      "Iteration 972, loss = 0.02335859\n",
      "Iteration 973, loss = 0.02331310\n",
      "Iteration 974, loss = 0.02326774\n",
      "Iteration 975, loss = 0.02322250\n",
      "Iteration 976, loss = 0.02317739\n",
      "Iteration 977, loss = 0.02313240\n",
      "Iteration 978, loss = 0.02308754\n",
      "Iteration 979, loss = 0.02304280\n",
      "Iteration 980, loss = 0.02299818\n",
      "Iteration 981, loss = 0.02295369\n",
      "Iteration 982, loss = 0.02290932\n",
      "Iteration 983, loss = 0.02286507\n",
      "Iteration 984, loss = 0.02282094\n",
      "Iteration 985, loss = 0.02277693\n",
      "Iteration 986, loss = 0.02273304\n",
      "Iteration 987, loss = 0.02268927\n",
      "Iteration 988, loss = 0.02264562\n",
      "Iteration 989, loss = 0.02260209\n",
      "Iteration 990, loss = 0.02255868\n",
      "Iteration 991, loss = 0.02251539\n",
      "Iteration 992, loss = 0.02247221\n",
      "Iteration 993, loss = 0.02242915\n",
      "Iteration 994, loss = 0.02238621\n",
      "Iteration 995, loss = 0.02234339\n",
      "Iteration 996, loss = 0.02230068\n",
      "Iteration 997, loss = 0.02225809\n",
      "Iteration 998, loss = 0.02221561\n",
      "Iteration 999, loss = 0.02217324\n",
      "Iteration 1000, loss = 0.02213100\n",
      "Iteration 1001, loss = 0.02208886\n",
      "Iteration 1002, loss = 0.02204684\n",
      "Iteration 1003, loss = 0.02200493\n",
      "Iteration 1004, loss = 0.02196314\n",
      "Iteration 1005, loss = 0.02192145\n",
      "Iteration 1006, loss = 0.02187988\n",
      "Iteration 1007, loss = 0.02183843\n",
      "Iteration 1008, loss = 0.02179708\n",
      "Iteration 1009, loss = 0.02175584\n",
      "Iteration 1010, loss = 0.02171471\n",
      "Iteration 1011, loss = 0.02167370\n",
      "Iteration 1012, loss = 0.02163279\n",
      "Iteration 1013, loss = 0.02159199\n",
      "Iteration 1014, loss = 0.02155130\n",
      "Iteration 1015, loss = 0.02151072\n",
      "Iteration 1016, loss = 0.02147025\n",
      "Iteration 1017, loss = 0.02142989\n",
      "Iteration 1018, loss = 0.02138963\n",
      "Iteration 1019, loss = 0.02134948\n",
      "Iteration 1020, loss = 0.02130944\n",
      "Iteration 1021, loss = 0.02126950\n",
      "Iteration 1022, loss = 0.02122967\n",
      "Iteration 1023, loss = 0.02118994\n",
      "Iteration 1024, loss = 0.02115032\n",
      "Iteration 1025, loss = 0.02111081\n",
      "Iteration 1026, loss = 0.02107140\n",
      "Iteration 1027, loss = 0.02103209\n",
      "Iteration 1028, loss = 0.02099289\n",
      "Iteration 1029, loss = 0.02095379\n",
      "Iteration 1030, loss = 0.02091479\n",
      "Iteration 1031, loss = 0.02087589\n",
      "Iteration 1032, loss = 0.02083710\n",
      "Iteration 1033, loss = 0.02079841\n",
      "Iteration 1034, loss = 0.02075982\n",
      "Iteration 1035, loss = 0.02072133\n",
      "Iteration 1036, loss = 0.02068295\n",
      "Iteration 1037, loss = 0.02064466\n",
      "Iteration 1038, loss = 0.02060647\n",
      "Iteration 1039, loss = 0.02056839\n",
      "Iteration 1040, loss = 0.02053040\n",
      "Iteration 1041, loss = 0.02049251\n",
      "Iteration 1042, loss = 0.02045472\n",
      "Iteration 1043, loss = 0.02041703\n",
      "Iteration 1044, loss = 0.02037944\n",
      "Iteration 1045, loss = 0.02034194\n",
      "Iteration 1046, loss = 0.02030455\n",
      "Iteration 1047, loss = 0.02026725\n",
      "Iteration 1048, loss = 0.02023004\n",
      "Iteration 1049, loss = 0.02019294\n",
      "Iteration 1050, loss = 0.02015592\n",
      "Iteration 1051, loss = 0.02011901\n",
      "Iteration 1052, loss = 0.02008219\n",
      "Iteration 1053, loss = 0.02004547\n",
      "Iteration 1054, loss = 0.02000884\n",
      "Iteration 1055, loss = 0.01997230\n",
      "Iteration 1056, loss = 0.01993586\n",
      "Iteration 1057, loss = 0.01989951\n",
      "Iteration 1058, loss = 0.01986326\n",
      "Iteration 1059, loss = 0.01982710\n",
      "Iteration 1060, loss = 0.01979103\n",
      "Iteration 1061, loss = 0.01975505\n",
      "Iteration 1062, loss = 0.01971917\n",
      "Iteration 1063, loss = 0.01968338\n",
      "Iteration 1064, loss = 0.01964768\n",
      "Iteration 1065, loss = 0.01961207\n",
      "Iteration 1066, loss = 0.01957656\n",
      "Iteration 1067, loss = 0.01954113\n",
      "Iteration 1068, loss = 0.01950579\n",
      "Iteration 1069, loss = 0.01947055\n",
      "Iteration 1070, loss = 0.01943539\n",
      "Iteration 1071, loss = 0.01940032\n",
      "Iteration 1072, loss = 0.01936535\n",
      "Iteration 1073, loss = 0.01933046\n",
      "Iteration 1074, loss = 0.01929566\n",
      "Iteration 1075, loss = 0.01926095\n",
      "Iteration 1076, loss = 0.01922632\n",
      "Iteration 1077, loss = 0.01919179\n",
      "Iteration 1078, loss = 0.01915734\n",
      "Iteration 1079, loss = 0.01912298\n",
      "Iteration 1080, loss = 0.01908870\n",
      "Iteration 1081, loss = 0.01905452\n",
      "Iteration 1082, loss = 0.01902042\n",
      "Iteration 1083, loss = 0.01898640\n",
      "Iteration 1084, loss = 0.01895247\n",
      "Iteration 1085, loss = 0.01891863\n",
      "Iteration 1086, loss = 0.01888487\n",
      "Iteration 1087, loss = 0.01885119\n",
      "Iteration 1088, loss = 0.01881760\n",
      "Iteration 1089, loss = 0.01878410\n",
      "Iteration 1090, loss = 0.01875067\n",
      "Iteration 1091, loss = 0.01871734\n",
      "Iteration 1092, loss = 0.01868408\n",
      "Iteration 1093, loss = 0.01865091\n",
      "Iteration 1094, loss = 0.01861782\n",
      "Iteration 1095, loss = 0.01858482\n",
      "Iteration 1096, loss = 0.01855189\n",
      "Iteration 1097, loss = 0.01851905\n",
      "Iteration 1098, loss = 0.01848629\n",
      "Iteration 1099, loss = 0.01845361\n",
      "Iteration 1100, loss = 0.01842102\n",
      "Iteration 1101, loss = 0.01838850\n",
      "Iteration 1102, loss = 0.01835607\n",
      "Iteration 1103, loss = 0.01832371\n",
      "Iteration 1104, loss = 0.01829144\n",
      "Iteration 1105, loss = 0.01825924\n",
      "Iteration 1106, loss = 0.01822713\n",
      "Iteration 1107, loss = 0.01819509\n",
      "Iteration 1108, loss = 0.01816314\n",
      "Iteration 1109, loss = 0.01813126\n",
      "Iteration 1110, loss = 0.01809946\n",
      "Iteration 1111, loss = 0.01806774\n",
      "Iteration 1112, loss = 0.01803610\n",
      "Iteration 1113, loss = 0.01800454\n",
      "Iteration 1114, loss = 0.01797305\n",
      "Iteration 1115, loss = 0.01794164\n",
      "Iteration 1116, loss = 0.01791031\n",
      "Iteration 1117, loss = 0.01787906\n",
      "Iteration 1118, loss = 0.01784788\n",
      "Iteration 1119, loss = 0.01781678\n",
      "Iteration 1120, loss = 0.01778575\n",
      "Iteration 1121, loss = 0.01775480\n",
      "Iteration 1122, loss = 0.01772393\n",
      "Iteration 1123, loss = 0.01769313\n",
      "Iteration 1124, loss = 0.01766240\n",
      "Iteration 1125, loss = 0.01763176\n",
      "Iteration 1126, loss = 0.01760118\n",
      "Iteration 1127, loss = 0.01757068\n",
      "Iteration 1128, loss = 0.01754026\n",
      "Iteration 1129, loss = 0.01750991\n",
      "Iteration 1130, loss = 0.01747963\n",
      "Iteration 1131, loss = 0.01744942\n",
      "Iteration 1132, loss = 0.01741929\n",
      "Iteration 1133, loss = 0.01738924\n",
      "Iteration 1134, loss = 0.01735925\n",
      "Iteration 1135, loss = 0.01732934\n",
      "Iteration 1136, loss = 0.01729950\n",
      "Iteration 1137, loss = 0.01726973\n",
      "Iteration 1138, loss = 0.01724003\n",
      "Iteration 1139, loss = 0.01721041\n",
      "Iteration 1140, loss = 0.01718086\n",
      "Iteration 1141, loss = 0.01715138\n",
      "Iteration 1142, loss = 0.01712196\n",
      "Iteration 1143, loss = 0.01709262\n",
      "Iteration 1144, loss = 0.01706335\n",
      "Iteration 1145, loss = 0.01703416\n",
      "Iteration 1146, loss = 0.01700503\n",
      "Iteration 1147, loss = 0.01697597\n",
      "Iteration 1148, loss = 0.01694698\n",
      "Iteration 1149, loss = 0.01691806\n",
      "Iteration 1150, loss = 0.01688921\n",
      "Iteration 1151, loss = 0.01686042\n",
      "Iteration 1152, loss = 0.01683171\n",
      "Iteration 1153, loss = 0.01680306\n",
      "Iteration 1154, loss = 0.01677449\n",
      "Iteration 1155, loss = 0.01674598\n",
      "Iteration 1156, loss = 0.01671754\n",
      "Iteration 1157, loss = 0.01668917\n",
      "Iteration 1158, loss = 0.01666086\n",
      "Iteration 1159, loss = 0.01663262\n",
      "Iteration 1160, loss = 0.01660445\n",
      "Iteration 1161, loss = 0.01657635\n",
      "Iteration 1162, loss = 0.01654831\n",
      "Iteration 1163, loss = 0.01652034\n",
      "Iteration 1164, loss = 0.01649243\n",
      "Iteration 1165, loss = 0.01646459\n",
      "Iteration 1166, loss = 0.01643682\n",
      "Iteration 1167, loss = 0.01640911\n",
      "Iteration 1168, loss = 0.01638147\n",
      "Iteration 1169, loss = 0.01635389\n",
      "Iteration 1170, loss = 0.01632638\n",
      "Iteration 1171, loss = 0.01629893\n",
      "Iteration 1172, loss = 0.01627155\n",
      "Iteration 1173, loss = 0.01624423\n",
      "Iteration 1174, loss = 0.01621698\n",
      "Iteration 1175, loss = 0.01618979\n",
      "Iteration 1176, loss = 0.01616266\n",
      "Iteration 1177, loss = 0.01613560\n",
      "Iteration 1178, loss = 0.01610860\n",
      "Iteration 1179, loss = 0.01608166\n",
      "Iteration 1180, loss = 0.01605479\n",
      "Iteration 1181, loss = 0.01602798\n",
      "Iteration 1182, loss = 0.01600123\n",
      "Iteration 1183, loss = 0.01597455\n",
      "Iteration 1184, loss = 0.01594792\n",
      "Iteration 1185, loss = 0.01592136\n",
      "Iteration 1186, loss = 0.01589486\n",
      "Iteration 1187, loss = 0.01586842\n",
      "Iteration 1188, loss = 0.01584205\n",
      "Iteration 1189, loss = 0.01581573\n",
      "Iteration 1190, loss = 0.01578948\n",
      "Iteration 1191, loss = 0.01576329\n",
      "Iteration 1192, loss = 0.01573715\n",
      "Iteration 1193, loss = 0.01571108\n",
      "Iteration 1194, loss = 0.01568507\n",
      "Iteration 1195, loss = 0.01565912\n",
      "Iteration 1196, loss = 0.01563323\n",
      "Iteration 1197, loss = 0.01560740\n",
      "Iteration 1198, loss = 0.01558162\n",
      "Iteration 1199, loss = 0.01555591\n",
      "Iteration 1200, loss = 0.01553026\n",
      "Iteration 1201, loss = 0.01550467\n",
      "Iteration 1202, loss = 0.01547913\n",
      "Iteration 1203, loss = 0.01545365\n",
      "Iteration 1204, loss = 0.01542824\n",
      "Iteration 1205, loss = 0.01540288\n",
      "Iteration 1206, loss = 0.01537757\n",
      "Iteration 1207, loss = 0.01535233\n",
      "Iteration 1208, loss = 0.01532715\n",
      "Iteration 1209, loss = 0.01530202\n",
      "Iteration 1210, loss = 0.01527695\n",
      "Iteration 1211, loss = 0.01525193\n",
      "Iteration 1212, loss = 0.01522698\n",
      "Iteration 1213, loss = 0.01520208\n",
      "Iteration 1214, loss = 0.01517724\n",
      "Iteration 1215, loss = 0.01515245\n",
      "Iteration 1216, loss = 0.01512772\n",
      "Iteration 1217, loss = 0.01510305\n",
      "Iteration 1218, loss = 0.01507843\n",
      "Iteration 1219, loss = 0.01505387\n",
      "Iteration 1220, loss = 0.01502937\n",
      "Iteration 1221, loss = 0.01500492\n",
      "Iteration 1222, loss = 0.01498052\n",
      "Iteration 1223, loss = 0.01495619\n",
      "Iteration 1224, loss = 0.01493190\n",
      "Iteration 1225, loss = 0.01490767\n",
      "Iteration 1226, loss = 0.01488350\n",
      "Iteration 1227, loss = 0.01485938\n",
      "Iteration 1228, loss = 0.01483532\n",
      "Iteration 1229, loss = 0.01481131\n",
      "Iteration 1230, loss = 0.01478735\n",
      "Iteration 1231, loss = 0.01476345\n",
      "Iteration 1232, loss = 0.01473960\n",
      "Iteration 1233, loss = 0.01471580\n",
      "Iteration 1234, loss = 0.01469206\n",
      "Iteration 1235, loss = 0.01466838\n",
      "Iteration 1236, loss = 0.01464474\n",
      "Iteration 1237, loss = 0.01462116\n",
      "Iteration 1238, loss = 0.01459763\n",
      "Iteration 1239, loss = 0.01457415\n",
      "Iteration 1240, loss = 0.01455073\n",
      "Iteration 1241, loss = 0.01452736\n",
      "Iteration 1242, loss = 0.01450404\n",
      "Iteration 1243, loss = 0.01448077\n",
      "Iteration 1244, loss = 0.01445756\n",
      "Iteration 1245, loss = 0.01443440\n",
      "Iteration 1246, loss = 0.01441128\n",
      "Iteration 1247, loss = 0.01438822\n",
      "Iteration 1248, loss = 0.01436522\n",
      "Iteration 1249, loss = 0.01434226\n",
      "Iteration 1250, loss = 0.01431935\n",
      "Iteration 1251, loss = 0.01429650\n",
      "Iteration 1252, loss = 0.01427369\n",
      "Iteration 1253, loss = 0.01425094\n",
      "Iteration 1254, loss = 0.01422823\n",
      "Iteration 1255, loss = 0.01420558\n",
      "Iteration 1256, loss = 0.01418298\n",
      "Iteration 1257, loss = 0.01416043\n",
      "Iteration 1258, loss = 0.01413792\n",
      "Iteration 1259, loss = 0.01411547\n",
      "Iteration 1260, loss = 0.01409306\n",
      "Iteration 1261, loss = 0.01407071\n",
      "Iteration 1262, loss = 0.01404840\n",
      "Iteration 1263, loss = 0.01402615\n",
      "Iteration 1264, loss = 0.01400394\n",
      "Iteration 1265, loss = 0.01398178\n",
      "Iteration 1266, loss = 0.01395967\n",
      "Iteration 1267, loss = 0.01393761\n",
      "Iteration 1268, loss = 0.01391560\n",
      "Iteration 1269, loss = 0.01389363\n",
      "Iteration 1270, loss = 0.01387172\n",
      "Iteration 1271, loss = 0.01384985\n",
      "Iteration 1272, loss = 0.01382803\n",
      "Iteration 1273, loss = 0.01380626\n",
      "Iteration 1274, loss = 0.01378453\n",
      "Iteration 1275, loss = 0.01376285\n",
      "Iteration 1276, loss = 0.01374122\n",
      "Iteration 1277, loss = 0.01371964\n",
      "Iteration 1278, loss = 0.01369811\n",
      "Iteration 1279, loss = 0.01367662\n",
      "Iteration 1280, loss = 0.01365517\n",
      "Iteration 1281, loss = 0.01363378\n",
      "Iteration 1282, loss = 0.01361243\n",
      "Iteration 1283, loss = 0.01359113\n",
      "Iteration 1284, loss = 0.01356987\n",
      "Iteration 1285, loss = 0.01354866\n",
      "Iteration 1286, loss = 0.01352750\n",
      "Iteration 1287, loss = 0.01350638\n",
      "Iteration 1288, loss = 0.01348531\n",
      "Iteration 1289, loss = 0.01346428\n",
      "Iteration 1290, loss = 0.01344330\n",
      "Iteration 1291, loss = 0.01342236\n",
      "Iteration 1292, loss = 0.01340147\n",
      "Iteration 1293, loss = 0.01338062\n",
      "Iteration 1294, loss = 0.01335982\n",
      "Iteration 1295, loss = 0.01333907\n",
      "Iteration 1296, loss = 0.01331835\n",
      "Iteration 1297, loss = 0.01329769\n",
      "Iteration 1298, loss = 0.01327707\n",
      "Iteration 1299, loss = 0.01325649\n",
      "Iteration 1300, loss = 0.01323595\n",
      "Iteration 1301, loss = 0.01321546\n",
      "Iteration 1302, loss = 0.01319502\n",
      "Iteration 1303, loss = 0.01317462\n",
      "Iteration 1304, loss = 0.01315426\n",
      "Iteration 1305, loss = 0.01313394\n",
      "Iteration 1306, loss = 0.01311367\n",
      "Iteration 1307, loss = 0.01309344\n",
      "Iteration 1308, loss = 0.01307326\n",
      "Iteration 1309, loss = 0.01305312\n",
      "Iteration 1310, loss = 0.01303302\n",
      "Iteration 1311, loss = 0.01301296\n",
      "Iteration 1312, loss = 0.01299295\n",
      "Iteration 1313, loss = 0.01297298\n",
      "Iteration 1314, loss = 0.01295305\n",
      "Iteration 1315, loss = 0.01293316\n",
      "Iteration 1316, loss = 0.01291332\n",
      "Iteration 1317, loss = 0.01289352\n",
      "Iteration 1318, loss = 0.01287376\n",
      "Iteration 1319, loss = 0.01285404\n",
      "Iteration 1320, loss = 0.01283437\n",
      "Iteration 1321, loss = 0.01281473\n",
      "Iteration 1322, loss = 0.01279514\n",
      "Iteration 1323, loss = 0.01277559\n",
      "Iteration 1324, loss = 0.01275608\n",
      "Iteration 1325, loss = 0.01273661\n",
      "Iteration 1326, loss = 0.01271719\n",
      "Iteration 1327, loss = 0.01269780\n",
      "Iteration 1328, loss = 0.01267845\n",
      "Iteration 1329, loss = 0.01265915\n",
      "Iteration 1330, loss = 0.01263988\n",
      "Iteration 1331, loss = 0.01262066\n",
      "Iteration 1332, loss = 0.01260148\n",
      "Iteration 1333, loss = 0.01258234\n",
      "Iteration 1334, loss = 0.01256323\n",
      "Iteration 1335, loss = 0.01254417\n",
      "Iteration 1336, loss = 0.01252515\n",
      "Iteration 1337, loss = 0.01250617\n",
      "Iteration 1338, loss = 0.01248722\n",
      "Iteration 1339, loss = 0.01246832\n",
      "Iteration 1340, loss = 0.01244946\n",
      "Iteration 1341, loss = 0.01243063\n",
      "Iteration 1342, loss = 0.01241185\n",
      "Iteration 1343, loss = 0.01239310\n",
      "Iteration 1344, loss = 0.01237439\n",
      "Iteration 1345, loss = 0.01235573\n",
      "Iteration 1346, loss = 0.01233710\n",
      "Iteration 1347, loss = 0.01231851\n",
      "Iteration 1348, loss = 0.01229996\n",
      "Iteration 1349, loss = 0.01228144\n",
      "Iteration 1350, loss = 0.01226297\n",
      "Iteration 1351, loss = 0.01224453\n",
      "Iteration 1352, loss = 0.01222614\n",
      "Iteration 1353, loss = 0.01220778\n",
      "Iteration 1354, loss = 0.01218945\n",
      "Iteration 1355, loss = 0.01217117\n",
      "Iteration 1356, loss = 0.01215293\n",
      "Iteration 1357, loss = 0.01213472\n",
      "Iteration 1358, loss = 0.01211655\n",
      "Iteration 1359, loss = 0.01209841\n",
      "Iteration 1360, loss = 0.01208032\n",
      "Iteration 1361, loss = 0.01206226\n",
      "Iteration 1362, loss = 0.01204424\n",
      "Iteration 1363, loss = 0.01202626\n",
      "Iteration 1364, loss = 0.01200831\n",
      "Iteration 1365, loss = 0.01199040\n",
      "Iteration 1366, loss = 0.01197253\n",
      "Iteration 1367, loss = 0.01195469\n",
      "Iteration 1368, loss = 0.01193689\n",
      "Iteration 1369, loss = 0.01191913\n",
      "Iteration 1370, loss = 0.01190140\n",
      "Iteration 1371, loss = 0.01188371\n",
      "Iteration 1372, loss = 0.01186606\n",
      "Iteration 1373, loss = 0.01184844\n",
      "Iteration 1374, loss = 0.01183086\n",
      "Iteration 1375, loss = 0.01181331\n",
      "Iteration 1376, loss = 0.01179580\n",
      "Iteration 1377, loss = 0.01177833\n",
      "Iteration 1378, loss = 0.01176089\n",
      "Iteration 1379, loss = 0.01174349\n",
      "Iteration 1380, loss = 0.01172612\n",
      "Iteration 1381, loss = 0.01170879\n",
      "Iteration 1382, loss = 0.01169149\n",
      "Iteration 1383, loss = 0.01167423\n",
      "Iteration 1384, loss = 0.01165701\n",
      "Iteration 1385, loss = 0.01163981\n",
      "Iteration 1386, loss = 0.01162266\n",
      "Iteration 1387, loss = 0.01160554\n",
      "Iteration 1388, loss = 0.01158845\n",
      "Iteration 1389, loss = 0.01157140\n",
      "Iteration 1390, loss = 0.01155438\n",
      "Iteration 1391, loss = 0.01153740\n",
      "Iteration 1392, loss = 0.01152045\n",
      "Iteration 1393, loss = 0.01150353\n",
      "Iteration 1394, loss = 0.01148665\n",
      "Iteration 1395, loss = 0.01146980\n",
      "Iteration 1396, loss = 0.01145299\n",
      "Iteration 1397, loss = 0.01143621\n",
      "Iteration 1398, loss = 0.01141947\n",
      "Iteration 1399, loss = 0.01140276\n",
      "Iteration 1400, loss = 0.01138608\n",
      "Iteration 1401, loss = 0.01136944\n",
      "Iteration 1402, loss = 0.01135283\n",
      "Iteration 1403, loss = 0.01133625\n",
      "Iteration 1404, loss = 0.01131971\n",
      "Iteration 1405, loss = 0.01130320\n",
      "Iteration 1406, loss = 0.01128672\n",
      "Iteration 1407, loss = 0.01127027\n",
      "Iteration 1408, loss = 0.01125386\n",
      "Iteration 1409, loss = 0.01123748\n",
      "Iteration 1410, loss = 0.01122114\n",
      "Iteration 1411, loss = 0.01120482\n",
      "Iteration 1412, loss = 0.01118854\n",
      "Iteration 1413, loss = 0.01117230\n",
      "Iteration 1414, loss = 0.01115608\n",
      "Iteration 1415, loss = 0.01113990\n",
      "Iteration 1416, loss = 0.01112375\n",
      "Iteration 1417, loss = 0.01110763\n",
      "Iteration 1418, loss = 0.01109154\n",
      "Iteration 1419, loss = 0.01107549\n",
      "Iteration 1420, loss = 0.01105946\n",
      "Iteration 1421, loss = 0.01104347\n",
      "Iteration 1422, loss = 0.01102751\n",
      "Iteration 1423, loss = 0.01101159\n",
      "Iteration 1424, loss = 0.01099569\n",
      "Iteration 1425, loss = 0.01097983\n",
      "Iteration 1426, loss = 0.01096399\n",
      "Iteration 1427, loss = 0.01094819\n",
      "Iteration 1428, loss = 0.01093242\n",
      "Iteration 1429, loss = 0.01091668\n",
      "Iteration 1430, loss = 0.01090097\n",
      "Iteration 1431, loss = 0.01088530\n",
      "Iteration 1432, loss = 0.01086965\n",
      "Iteration 1433, loss = 0.01085403\n",
      "Iteration 1434, loss = 0.01083845\n",
      "Iteration 1435, loss = 0.01082290\n",
      "Iteration 1436, loss = 0.01080737\n",
      "Iteration 1437, loss = 0.01079188\n",
      "Iteration 1438, loss = 0.01077642\n",
      "Iteration 1439, loss = 0.01076099\n",
      "Iteration 1440, loss = 0.01074558\n",
      "Iteration 1441, loss = 0.01073021\n",
      "Iteration 1442, loss = 0.01071487\n",
      "Iteration 1443, loss = 0.01069956\n",
      "Iteration 1444, loss = 0.01068428\n",
      "Iteration 1445, loss = 0.01066903\n",
      "Iteration 1446, loss = 0.01065381\n",
      "Iteration 1447, loss = 0.01063862\n",
      "Iteration 1448, loss = 0.01062346\n",
      "Iteration 1449, loss = 0.01060832\n",
      "Iteration 1450, loss = 0.01059322\n",
      "Iteration 1451, loss = 0.01057815\n",
      "Iteration 1452, loss = 0.01056310\n",
      "Iteration 1453, loss = 0.01054809\n",
      "Iteration 1454, loss = 0.01053310\n",
      "Iteration 1455, loss = 0.01051815\n",
      "Iteration 1456, loss = 0.01050322\n",
      "Iteration 1457, loss = 0.01048832\n",
      "Iteration 1458, loss = 0.01047345\n",
      "Iteration 1459, loss = 0.01045861\n",
      "Iteration 1460, loss = 0.01044380\n",
      "Iteration 1461, loss = 0.01042902\n",
      "Iteration 1462, loss = 0.01041427\n",
      "Iteration 1463, loss = 0.01039954\n",
      "Iteration 1464, loss = 0.01038485\n",
      "Iteration 1465, loss = 0.01037018\n",
      "Iteration 1466, loss = 0.01035554\n",
      "Iteration 1467, loss = 0.01034093\n",
      "Iteration 1468, loss = 0.01032634\n",
      "Iteration 1469, loss = 0.01031179\n",
      "Iteration 1470, loss = 0.01029726\n",
      "Iteration 1471, loss = 0.01028276\n",
      "Iteration 1472, loss = 0.01026829\n",
      "Iteration 1473, loss = 0.01025385\n",
      "Iteration 1474, loss = 0.01023943\n",
      "Iteration 1475, loss = 0.01022504\n",
      "Iteration 1476, loss = 0.01021068\n",
      "Iteration 1477, loss = 0.01019635\n",
      "Iteration 1478, loss = 0.01018204\n",
      "Iteration 1479, loss = 0.01016777\n",
      "Iteration 1480, loss = 0.01015352\n",
      "Iteration 1481, loss = 0.01013929\n",
      "Iteration 1482, loss = 0.01012510\n",
      "Iteration 1483, loss = 0.01011093\n",
      "Iteration 1484, loss = 0.01009679\n",
      "Iteration 1485, loss = 0.01008267\n",
      "Iteration 1486, loss = 0.01006859\n",
      "Iteration 1487, loss = 0.01005453\n",
      "Iteration 1488, loss = 0.01004049\n",
      "Iteration 1489, loss = 0.01002649\n",
      "Iteration 1490, loss = 0.01001251\n",
      "Iteration 1491, loss = 0.00999855\n",
      "Iteration 1492, loss = 0.00998463\n",
      "Iteration 1493, loss = 0.00997073\n",
      "Iteration 1494, loss = 0.00995685\n",
      "Iteration 1495, loss = 0.00994300\n",
      "Iteration 1496, loss = 0.00992918\n",
      "Iteration 1497, loss = 0.00991539\n",
      "Iteration 1498, loss = 0.00990162\n",
      "Iteration 1499, loss = 0.00988788\n",
      "Iteration 1500, loss = 0.00987416\n",
      "Iteration 1501, loss = 0.00986047\n",
      "Iteration 1502, loss = 0.00984681\n",
      "Iteration 1503, loss = 0.00983317\n",
      "Iteration 1504, loss = 0.00981955\n",
      "Iteration 1505, loss = 0.00980597\n",
      "Iteration 1506, loss = 0.00979240\n",
      "Iteration 1507, loss = 0.00977887\n",
      "Iteration 1508, loss = 0.00976536\n",
      "Iteration 1509, loss = 0.00975187\n",
      "Iteration 1510, loss = 0.00973841\n",
      "Iteration 1511, loss = 0.00972498\n",
      "Iteration 1512, loss = 0.00971157\n",
      "Iteration 1513, loss = 0.00969819\n",
      "Iteration 1514, loss = 0.00968483\n",
      "Iteration 1515, loss = 0.00967150\n",
      "Iteration 1516, loss = 0.00965819\n",
      "Iteration 1517, loss = 0.00964490\n",
      "Iteration 1518, loss = 0.00963165\n",
      "Iteration 1519, loss = 0.00961841\n",
      "Iteration 1520, loss = 0.00960520\n",
      "Iteration 1521, loss = 0.00959202\n",
      "Iteration 1522, loss = 0.00957886\n",
      "Iteration 1523, loss = 0.00956573\n",
      "Iteration 1524, loss = 0.00955262\n",
      "Iteration 1525, loss = 0.00953953\n",
      "Iteration 1526, loss = 0.00952647\n",
      "Iteration 1527, loss = 0.00951343\n",
      "Iteration 1528, loss = 0.00950042\n",
      "Iteration 1529, loss = 0.00948743\n",
      "Iteration 1530, loss = 0.00947447\n",
      "Iteration 1531, loss = 0.00946153\n",
      "Iteration 1532, loss = 0.00944861\n",
      "Iteration 1533, loss = 0.00943572\n",
      "Iteration 1534, loss = 0.00942285\n",
      "Iteration 1535, loss = 0.00941001\n",
      "Iteration 1536, loss = 0.00939719\n",
      "Iteration 1537, loss = 0.00938439\n",
      "Iteration 1538, loss = 0.00937162\n",
      "Iteration 1539, loss = 0.00935887\n",
      "Iteration 1540, loss = 0.00934615\n",
      "Iteration 1541, loss = 0.00933345\n",
      "Iteration 1542, loss = 0.00932077\n",
      "Iteration 1543, loss = 0.00930811\n",
      "Iteration 1544, loss = 0.00929548\n",
      "Iteration 1545, loss = 0.00928287\n",
      "Iteration 1546, loss = 0.00927029\n",
      "Iteration 1547, loss = 0.00925773\n",
      "Iteration 1548, loss = 0.00924519\n",
      "Iteration 1549, loss = 0.00923268\n",
      "Iteration 1550, loss = 0.00922018\n",
      "Iteration 1551, loss = 0.00920772\n",
      "Iteration 1552, loss = 0.00919527\n",
      "Iteration 1553, loss = 0.00918285\n",
      "Iteration 1554, loss = 0.00917045\n",
      "Iteration 1555, loss = 0.00915807\n",
      "Iteration 1556, loss = 0.00914571\n",
      "Iteration 1557, loss = 0.00913338\n",
      "Iteration 1558, loss = 0.00912107\n",
      "Iteration 1559, loss = 0.00910879\n",
      "Iteration 1560, loss = 0.00909652\n",
      "Iteration 1561, loss = 0.00908428\n",
      "Iteration 1562, loss = 0.00907206\n",
      "Iteration 1563, loss = 0.00905986\n",
      "Iteration 1564, loss = 0.00904769\n",
      "Iteration 1565, loss = 0.00903554\n",
      "Iteration 1566, loss = 0.00902341\n",
      "Iteration 1567, loss = 0.00901130\n",
      "Iteration 1568, loss = 0.00899921\n",
      "Iteration 1569, loss = 0.00898715\n",
      "Iteration 1570, loss = 0.00897511\n",
      "Iteration 1571, loss = 0.00896309\n",
      "Iteration 1572, loss = 0.00895109\n",
      "Iteration 1573, loss = 0.00893911\n",
      "Iteration 1574, loss = 0.00892716\n",
      "Iteration 1575, loss = 0.00891523\n",
      "Iteration 1576, loss = 0.00890331\n",
      "Iteration 1577, loss = 0.00889143\n",
      "Iteration 1578, loss = 0.00887956\n",
      "Iteration 1579, loss = 0.00886771\n",
      "Iteration 1580, loss = 0.00885589\n",
      "Iteration 1581, loss = 0.00884408\n",
      "Iteration 1582, loss = 0.00883230\n",
      "Iteration 1583, loss = 0.00882054\n",
      "Iteration 1584, loss = 0.00880880\n",
      "Iteration 1585, loss = 0.00879708\n",
      "Iteration 1586, loss = 0.00878539\n",
      "Iteration 1587, loss = 0.00877371\n",
      "Iteration 1588, loss = 0.00876206\n",
      "Iteration 1589, loss = 0.00875042\n",
      "Iteration 1590, loss = 0.00873881\n",
      "Iteration 1591, loss = 0.00872722\n",
      "Iteration 1592, loss = 0.00871565\n",
      "Iteration 1593, loss = 0.00870410\n",
      "Iteration 1594, loss = 0.00869257\n",
      "Iteration 1595, loss = 0.00868106\n",
      "Iteration 1596, loss = 0.00866957\n",
      "Iteration 1597, loss = 0.00865810\n",
      "Iteration 1598, loss = 0.00864666\n",
      "Iteration 1599, loss = 0.00863523\n",
      "Iteration 1600, loss = 0.00862382\n",
      "Iteration 1601, loss = 0.00861244\n",
      "Iteration 1602, loss = 0.00860107\n",
      "Iteration 1603, loss = 0.00858973\n",
      "Iteration 1604, loss = 0.00857841\n",
      "Iteration 1605, loss = 0.00856710\n",
      "Iteration 1606, loss = 0.00855582\n",
      "Iteration 1607, loss = 0.00854455\n",
      "Iteration 1608, loss = 0.00853331\n",
      "Iteration 1609, loss = 0.00852209\n",
      "Iteration 1610, loss = 0.00851088\n",
      "Iteration 1611, loss = 0.00849970\n",
      "Iteration 1612, loss = 0.00848854\n",
      "Iteration 1613, loss = 0.00847739\n",
      "Iteration 1614, loss = 0.00846627\n",
      "Iteration 1615, loss = 0.00845517\n",
      "Iteration 1616, loss = 0.00844408\n",
      "Iteration 1617, loss = 0.00843302\n",
      "Iteration 1618, loss = 0.00842197\n",
      "Iteration 1619, loss = 0.00841095\n",
      "Iteration 1620, loss = 0.00839994\n",
      "Iteration 1621, loss = 0.00838896\n",
      "Iteration 1622, loss = 0.00837799\n",
      "Iteration 1623, loss = 0.00836704\n",
      "Iteration 1624, loss = 0.00835612\n",
      "Iteration 1625, loss = 0.00834521\n",
      "Iteration 1626, loss = 0.00833432\n",
      "Iteration 1627, loss = 0.00832345\n",
      "Iteration 1628, loss = 0.00831260\n",
      "Iteration 1629, loss = 0.00830177\n",
      "Iteration 1630, loss = 0.00829095\n",
      "Iteration 1631, loss = 0.00828016\n",
      "Iteration 1632, loss = 0.00826939\n",
      "Iteration 1633, loss = 0.00825863\n",
      "Iteration 1634, loss = 0.00824789\n",
      "Iteration 1635, loss = 0.00823718\n",
      "Iteration 1636, loss = 0.00822648\n",
      "Iteration 1637, loss = 0.00821580\n",
      "Iteration 1638, loss = 0.00820514\n",
      "Iteration 1639, loss = 0.00819450\n",
      "Iteration 1640, loss = 0.00818387\n",
      "Iteration 1641, loss = 0.00817327\n",
      "Iteration 1642, loss = 0.00816268\n",
      "Iteration 1643, loss = 0.00815211\n",
      "Iteration 1644, loss = 0.00814156\n",
      "Iteration 1645, loss = 0.00813103\n",
      "Iteration 1646, loss = 0.00812052\n",
      "Iteration 1647, loss = 0.00811003\n",
      "Iteration 1648, loss = 0.00809955\n",
      "Iteration 1649, loss = 0.00808910\n",
      "Iteration 1650, loss = 0.00807866\n",
      "Iteration 1651, loss = 0.00806824\n",
      "Iteration 1652, loss = 0.00805783\n",
      "Iteration 1653, loss = 0.00804745\n",
      "Iteration 1654, loss = 0.00803708\n",
      "Iteration 1655, loss = 0.00802673\n",
      "Iteration 1656, loss = 0.00801640\n",
      "Iteration 1657, loss = 0.00800609\n",
      "Iteration 1658, loss = 0.00799580\n",
      "Iteration 1659, loss = 0.00798552\n",
      "Iteration 1660, loss = 0.00797526\n",
      "Iteration 1661, loss = 0.00796502\n",
      "Iteration 1662, loss = 0.00795480\n",
      "Iteration 1663, loss = 0.00794459\n",
      "Iteration 1664, loss = 0.00793441\n",
      "Iteration 1665, loss = 0.00792424\n",
      "Iteration 1666, loss = 0.00791409\n",
      "Iteration 1667, loss = 0.00790395\n",
      "Iteration 1668, loss = 0.00789383\n",
      "Iteration 1669, loss = 0.00788373\n",
      "Iteration 1670, loss = 0.00787365\n",
      "Iteration 1671, loss = 0.00786359\n",
      "Iteration 1672, loss = 0.00785354\n",
      "Iteration 1673, loss = 0.00784351\n",
      "Iteration 1674, loss = 0.00783350\n",
      "Iteration 1675, loss = 0.00782350\n",
      "Iteration 1676, loss = 0.00781352\n",
      "Iteration 1677, loss = 0.00780356\n",
      "Iteration 1678, loss = 0.00779362\n",
      "Iteration 1679, loss = 0.00778369\n",
      "Iteration 1680, loss = 0.00777378\n",
      "Iteration 1681, loss = 0.00776389\n",
      "Iteration 1682, loss = 0.00775402\n",
      "Iteration 1683, loss = 0.00774416\n",
      "Iteration 1684, loss = 0.00773432\n",
      "Iteration 1685, loss = 0.00772449\n",
      "Iteration 1686, loss = 0.00771468\n",
      "Iteration 1687, loss = 0.00770489\n",
      "Iteration 1688, loss = 0.00769512\n",
      "Iteration 1689, loss = 0.00768536\n",
      "Iteration 1690, loss = 0.00767562\n",
      "Iteration 1691, loss = 0.00766590\n",
      "Iteration 1692, loss = 0.00765619\n",
      "Iteration 1693, loss = 0.00764650\n",
      "Iteration 1694, loss = 0.00763682\n",
      "Iteration 1695, loss = 0.00762717\n",
      "Iteration 1696, loss = 0.00761752\n",
      "Iteration 1697, loss = 0.00760790\n",
      "Iteration 1698, loss = 0.00759829\n",
      "Iteration 1699, loss = 0.00758870\n",
      "Iteration 1700, loss = 0.00757912\n",
      "Iteration 1701, loss = 0.00756956\n",
      "Iteration 1702, loss = 0.00756002\n",
      "Iteration 1703, loss = 0.00755049\n",
      "Iteration 1704, loss = 0.00754098\n",
      "Iteration 1705, loss = 0.00753149\n",
      "Iteration 1706, loss = 0.00752201\n",
      "Iteration 1707, loss = 0.00751255\n",
      "Iteration 1708, loss = 0.00750310\n",
      "Iteration 1709, loss = 0.00749367\n",
      "Iteration 1710, loss = 0.00748426\n",
      "Iteration 1711, loss = 0.00747486\n",
      "Iteration 1712, loss = 0.00746548\n",
      "Iteration 1713, loss = 0.00745611\n",
      "Iteration 1714, loss = 0.00744676\n",
      "Iteration 1715, loss = 0.00743743\n",
      "Iteration 1716, loss = 0.00742811\n",
      "Iteration 1717, loss = 0.00741880\n",
      "Iteration 1718, loss = 0.00740952\n",
      "Iteration 1719, loss = 0.00740024\n",
      "Iteration 1720, loss = 0.00739099\n",
      "Iteration 1721, loss = 0.00738175\n",
      "Iteration 1722, loss = 0.00737252\n",
      "Iteration 1723, loss = 0.00736331\n",
      "Iteration 1724, loss = 0.00735412\n",
      "Iteration 1725, loss = 0.00734494\n",
      "Iteration 1726, loss = 0.00733578\n",
      "Iteration 1727, loss = 0.00732663\n",
      "Iteration 1728, loss = 0.00731750\n",
      "Iteration 1729, loss = 0.00730838\n",
      "Iteration 1730, loss = 0.00729928\n",
      "Iteration 1731, loss = 0.00729019\n",
      "Iteration 1732, loss = 0.00728112\n",
      "Iteration 1733, loss = 0.00727206\n",
      "Iteration 1734, loss = 0.00726302\n",
      "Iteration 1735, loss = 0.00725400\n",
      "Iteration 1736, loss = 0.00724499\n",
      "Iteration 1737, loss = 0.00723599\n",
      "Iteration 1738, loss = 0.00722701\n",
      "Iteration 1739, loss = 0.00721804\n",
      "Iteration 1740, loss = 0.00720909\n",
      "Iteration 1741, loss = 0.00720016\n",
      "Iteration 1742, loss = 0.00719124\n",
      "Iteration 1743, loss = 0.00718233\n",
      "Iteration 1744, loss = 0.00717344\n",
      "Iteration 1745, loss = 0.00716456\n",
      "Iteration 1746, loss = 0.00715570\n",
      "Iteration 1747, loss = 0.00714686\n",
      "Iteration 1748, loss = 0.00713802\n",
      "Iteration 1749, loss = 0.00712921\n",
      "Iteration 1750, loss = 0.00712040\n",
      "Iteration 1751, loss = 0.00711162\n",
      "Iteration 1752, loss = 0.00710284\n",
      "Iteration 1753, loss = 0.00709408\n",
      "Iteration 1754, loss = 0.00708534\n",
      "Iteration 1755, loss = 0.00707661\n",
      "Iteration 1756, loss = 0.00706789\n",
      "Iteration 1757, loss = 0.00705919\n",
      "Iteration 1758, loss = 0.00705051\n",
      "Iteration 1759, loss = 0.00704184\n",
      "Iteration 1760, loss = 0.00703318\n",
      "Iteration 1761, loss = 0.00702453\n",
      "Iteration 1762, loss = 0.00701591\n",
      "Iteration 1763, loss = 0.00700729\n",
      "Iteration 1764, loss = 0.00699869\n",
      "Iteration 1765, loss = 0.00699010\n",
      "Iteration 1766, loss = 0.00698153\n",
      "Iteration 1767, loss = 0.00697297\n",
      "Iteration 1768, loss = 0.00696443\n",
      "Iteration 1769, loss = 0.00695590\n",
      "Iteration 1770, loss = 0.00694738\n",
      "Iteration 1771, loss = 0.00693888\n",
      "Iteration 1772, loss = 0.00693039\n",
      "Iteration 1773, loss = 0.00692192\n",
      "Iteration 1774, loss = 0.00691346\n",
      "Iteration 1775, loss = 0.00690501\n",
      "Iteration 1776, loss = 0.00689658\n",
      "Iteration 1777, loss = 0.00688816\n",
      "Iteration 1778, loss = 0.00687976\n",
      "Iteration 1779, loss = 0.00687137\n",
      "Iteration 1780, loss = 0.00686299\n",
      "Iteration 1781, loss = 0.00685463\n",
      "Iteration 1782, loss = 0.00684628\n",
      "Iteration 1783, loss = 0.00683794\n",
      "Iteration 1784, loss = 0.00682962\n",
      "Iteration 1785, loss = 0.00682131\n",
      "Iteration 1786, loss = 0.00681302\n",
      "Iteration 1787, loss = 0.00680473\n",
      "Iteration 1788, loss = 0.00679647\n",
      "Iteration 1789, loss = 0.00678821\n",
      "Iteration 1790, loss = 0.00677997\n",
      "Iteration 1791, loss = 0.00677174\n",
      "Iteration 1792, loss = 0.00676353\n",
      "Iteration 1793, loss = 0.00675533\n",
      "Iteration 1794, loss = 0.00674714\n",
      "Iteration 1795, loss = 0.00673896\n",
      "Iteration 1796, loss = 0.00673080\n",
      "Iteration 1797, loss = 0.00672266\n",
      "Iteration 1798, loss = 0.00671452\n",
      "Iteration 1799, loss = 0.00670640\n",
      "Iteration 1800, loss = 0.00669829\n",
      "Iteration 1801, loss = 0.00669020\n",
      "Iteration 1802, loss = 0.00668212\n",
      "Iteration 1803, loss = 0.00667405\n",
      "Iteration 1804, loss = 0.00666599\n",
      "Iteration 1805, loss = 0.00665795\n",
      "Iteration 1806, loss = 0.00664992\n",
      "Iteration 1807, loss = 0.00664190\n",
      "Iteration 1808, loss = 0.00663390\n",
      "Iteration 1809, loss = 0.00662591\n",
      "Iteration 1810, loss = 0.00661793\n",
      "Iteration 1811, loss = 0.00660997\n",
      "Iteration 1812, loss = 0.00660201\n",
      "Iteration 1813, loss = 0.00659407\n",
      "Iteration 1814, loss = 0.00658615\n",
      "Iteration 1815, loss = 0.00657823\n",
      "Iteration 1816, loss = 0.00657033\n",
      "Iteration 1817, loss = 0.00656245\n",
      "Iteration 1818, loss = 0.00655457\n",
      "Iteration 1819, loss = 0.00654671\n",
      "Iteration 1820, loss = 0.00653886\n",
      "Iteration 1821, loss = 0.00653102\n",
      "Iteration 1822, loss = 0.00652320\n",
      "Iteration 1823, loss = 0.00651538\n",
      "Iteration 1824, loss = 0.00650758\n",
      "Iteration 1825, loss = 0.00649980\n",
      "Iteration 1826, loss = 0.00649202\n",
      "Iteration 1827, loss = 0.00648426\n",
      "Iteration 1828, loss = 0.00647651\n",
      "Iteration 1829, loss = 0.00646877\n",
      "Iteration 1830, loss = 0.00646105\n",
      "Iteration 1831, loss = 0.00645333\n",
      "Iteration 1832, loss = 0.00644563\n",
      "Iteration 1833, loss = 0.00643795\n",
      "Iteration 1834, loss = 0.00643027\n",
      "Iteration 1835, loss = 0.00642261\n",
      "Iteration 1836, loss = 0.00641496\n",
      "Iteration 1837, loss = 0.00640732\n",
      "Iteration 1838, loss = 0.00639969\n",
      "Iteration 1839, loss = 0.00639207\n",
      "Iteration 1840, loss = 0.00638447\n",
      "Iteration 1841, loss = 0.00637688\n",
      "Iteration 1842, loss = 0.00636930\n",
      "Iteration 1843, loss = 0.00636174\n",
      "Iteration 1844, loss = 0.00635418\n",
      "Iteration 1845, loss = 0.00634664\n",
      "Iteration 1846, loss = 0.00633911\n",
      "Iteration 1847, loss = 0.00633159\n",
      "Iteration 1848, loss = 0.00632409\n",
      "Iteration 1849, loss = 0.00631659\n",
      "Iteration 1850, loss = 0.00630911\n",
      "Iteration 1851, loss = 0.00630164\n",
      "Iteration 1852, loss = 0.00629418\n",
      "Iteration 1853, loss = 0.00628673\n",
      "Iteration 1854, loss = 0.00627930\n",
      "Iteration 1855, loss = 0.00627187\n",
      "Iteration 1856, loss = 0.00626446\n",
      "Iteration 1857, loss = 0.00625706\n",
      "Iteration 1858, loss = 0.00624967\n",
      "Iteration 1859, loss = 0.00624230\n",
      "Iteration 1860, loss = 0.00623493\n",
      "Iteration 1861, loss = 0.00622758\n",
      "Iteration 1862, loss = 0.00622024\n",
      "Iteration 1863, loss = 0.00621291\n",
      "Iteration 1864, loss = 0.00620559\n",
      "Iteration 1865, loss = 0.00619828\n",
      "Iteration 1866, loss = 0.00619099\n",
      "Iteration 1867, loss = 0.00618370\n",
      "Iteration 1868, loss = 0.00617643\n",
      "Iteration 1869, loss = 0.00616917\n",
      "Iteration 1870, loss = 0.00616192\n",
      "Iteration 1871, loss = 0.00615468\n",
      "Iteration 1872, loss = 0.00614745\n",
      "Iteration 1873, loss = 0.00614024\n",
      "Iteration 1874, loss = 0.00613303\n",
      "Iteration 1875, loss = 0.00612584\n",
      "Iteration 1876, loss = 0.00611866\n",
      "Iteration 1877, loss = 0.00611149\n",
      "Iteration 1878, loss = 0.00610433\n",
      "Iteration 1879, loss = 0.00609718\n",
      "Iteration 1880, loss = 0.00609004\n",
      "Iteration 1881, loss = 0.00608292\n",
      "Iteration 1882, loss = 0.00607580\n",
      "Iteration 1883, loss = 0.00606870\n",
      "Iteration 1884, loss = 0.00606161\n",
      "Iteration 1885, loss = 0.00605453\n",
      "Iteration 1886, loss = 0.00604746\n",
      "Iteration 1887, loss = 0.00604040\n",
      "Iteration 1888, loss = 0.00603335\n",
      "Iteration 1889, loss = 0.00602631\n",
      "Iteration 1890, loss = 0.00601929\n",
      "Iteration 1891, loss = 0.00601227\n",
      "Iteration 1892, loss = 0.00600527\n",
      "Iteration 1893, loss = 0.00599827\n",
      "Iteration 1894, loss = 0.00599129\n",
      "Iteration 1895, loss = 0.00598432\n",
      "Iteration 1896, loss = 0.00597736\n",
      "Iteration 1897, loss = 0.00597041\n",
      "Iteration 1898, loss = 0.00596347\n",
      "Iteration 1899, loss = 0.00595654\n",
      "Iteration 1900, loss = 0.00594962\n",
      "Iteration 1901, loss = 0.00594272\n",
      "Iteration 1902, loss = 0.00593582\n",
      "Iteration 1903, loss = 0.00592893\n",
      "Iteration 1904, loss = 0.00592206\n",
      "Iteration 1905, loss = 0.00591520\n",
      "Iteration 1906, loss = 0.00590834\n",
      "Iteration 1907, loss = 0.00590150\n",
      "Iteration 1908, loss = 0.00589467\n",
      "Iteration 1909, loss = 0.00588784\n",
      "Iteration 1910, loss = 0.00588103\n",
      "Iteration 1911, loss = 0.00587423\n",
      "Iteration 1912, loss = 0.00586744\n",
      "Iteration 1913, loss = 0.00586066\n",
      "Iteration 1914, loss = 0.00585389\n",
      "Iteration 1915, loss = 0.00584713\n",
      "Iteration 1916, loss = 0.00584039\n",
      "Iteration 1917, loss = 0.00583365\n",
      "Iteration 1918, loss = 0.00582692\n",
      "Iteration 1919, loss = 0.00582020\n",
      "Iteration 1920, loss = 0.00581349\n",
      "Iteration 1921, loss = 0.00580680\n",
      "Iteration 1922, loss = 0.00580011\n",
      "Iteration 1923, loss = 0.00579344\n",
      "Iteration 1924, loss = 0.00578677\n",
      "Iteration 1925, loss = 0.00578011\n",
      "Iteration 1926, loss = 0.00577347\n",
      "Iteration 1927, loss = 0.00576683\n",
      "Iteration 1928, loss = 0.00576021\n",
      "Iteration 1929, loss = 0.00575359\n",
      "Iteration 1930, loss = 0.00574699\n",
      "Iteration 1931, loss = 0.00574039\n",
      "Iteration 1932, loss = 0.00573381\n",
      "Iteration 1933, loss = 0.00572723\n",
      "Iteration 1934, loss = 0.00572067\n",
      "Iteration 1935, loss = 0.00571411\n",
      "Iteration 1936, loss = 0.00570757\n",
      "Iteration 1937, loss = 0.00570104\n",
      "Iteration 1938, loss = 0.00569451\n",
      "Iteration 1939, loss = 0.00568800\n",
      "Iteration 1940, loss = 0.00568149\n",
      "Iteration 1941, loss = 0.00567500\n",
      "Iteration 1942, loss = 0.00566851\n",
      "Iteration 1943, loss = 0.00566204\n",
      "Iteration 1944, loss = 0.00565557\n",
      "Iteration 1945, loss = 0.00564912\n",
      "Iteration 1946, loss = 0.00564267\n",
      "Iteration 1947, loss = 0.00563624\n",
      "Iteration 1948, loss = 0.00562981\n",
      "Iteration 1949, loss = 0.00562339\n",
      "Iteration 1950, loss = 0.00561699\n",
      "Iteration 1951, loss = 0.00561059\n",
      "Iteration 1952, loss = 0.00560420\n",
      "Iteration 1953, loss = 0.00559783\n",
      "Iteration 1954, loss = 0.00559146\n",
      "Iteration 1955, loss = 0.00558510\n",
      "Iteration 1956, loss = 0.00557875\n",
      "Iteration 1957, loss = 0.00557242\n",
      "Iteration 1958, loss = 0.00556609\n",
      "Iteration 1959, loss = 0.00555977\n",
      "Iteration 1960, loss = 0.00555346\n",
      "Iteration 1961, loss = 0.00554716\n",
      "Iteration 1962, loss = 0.00554087\n",
      "Iteration 1963, loss = 0.00553458\n",
      "Iteration 1964, loss = 0.00552831\n",
      "Iteration 1965, loss = 0.00552205\n",
      "Iteration 1966, loss = 0.00551580\n",
      "Iteration 1967, loss = 0.00550955\n",
      "Iteration 1968, loss = 0.00550332\n",
      "Iteration 1969, loss = 0.00549709\n",
      "Iteration 1970, loss = 0.00549088\n",
      "Iteration 1971, loss = 0.00548467\n",
      "Iteration 1972, loss = 0.00547848\n",
      "Iteration 1973, loss = 0.00547229\n",
      "Iteration 1974, loss = 0.00546611\n",
      "Iteration 1975, loss = 0.00545994\n",
      "Iteration 1976, loss = 0.00545378\n",
      "Iteration 1977, loss = 0.00544763\n",
      "Iteration 1978, loss = 0.00544149\n",
      "Iteration 1979, loss = 0.00543536\n",
      "Iteration 1980, loss = 0.00542924\n",
      "Iteration 1981, loss = 0.00542312\n",
      "Iteration 1982, loss = 0.00541702\n",
      "Iteration 1983, loss = 0.00541092\n",
      "Iteration 1984, loss = 0.00540484\n",
      "Iteration 1985, loss = 0.00539876\n",
      "Iteration 1986, loss = 0.00539269\n",
      "Iteration 1987, loss = 0.00538663\n",
      "Iteration 1988, loss = 0.00538058\n",
      "Iteration 1989, loss = 0.00537454\n",
      "Iteration 1990, loss = 0.00536851\n",
      "Iteration 1991, loss = 0.00536249\n",
      "Iteration 1992, loss = 0.00535648\n",
      "Iteration 1993, loss = 0.00535047\n",
      "Iteration 1994, loss = 0.00534448\n",
      "Iteration 1995, loss = 0.00533849\n",
      "Iteration 1996, loss = 0.00533251\n",
      "Iteration 1997, loss = 0.00532654\n",
      "Iteration 1998, loss = 0.00532058\n",
      "Iteration 1999, loss = 0.00531463\n",
      "Iteration 2000, loss = 0.00530869\n",
      "Iteration 2001, loss = 0.00530275\n",
      "Iteration 2002, loss = 0.00529683\n",
      "Iteration 2003, loss = 0.00529091\n",
      "Iteration 2004, loss = 0.00528501\n",
      "Iteration 2005, loss = 0.00527911\n",
      "Iteration 2006, loss = 0.00527322\n",
      "Iteration 2007, loss = 0.00526734\n",
      "Iteration 2008, loss = 0.00526147\n",
      "Iteration 2009, loss = 0.00525560\n",
      "Iteration 2010, loss = 0.00524975\n",
      "Iteration 2011, loss = 0.00524390\n",
      "Iteration 2012, loss = 0.00523806\n",
      "Iteration 2013, loss = 0.00523223\n",
      "Iteration 2014, loss = 0.00522641\n",
      "Iteration 2015, loss = 0.00522060\n",
      "Iteration 2016, loss = 0.00521480\n",
      "Iteration 2017, loss = 0.00520900\n",
      "Iteration 2018, loss = 0.00520322\n",
      "Iteration 2019, loss = 0.00519744\n",
      "Iteration 2020, loss = 0.00519167\n",
      "Iteration 2021, loss = 0.00518591\n",
      "Iteration 2022, loss = 0.00518016\n",
      "Iteration 2023, loss = 0.00517442\n",
      "Iteration 2024, loss = 0.00516868\n",
      "Iteration 2025, loss = 0.00516296\n",
      "Iteration 2026, loss = 0.00515724\n",
      "Iteration 2027, loss = 0.00515153\n",
      "Iteration 2028, loss = 0.00514583\n",
      "Iteration 2029, loss = 0.00514013\n",
      "Iteration 2030, loss = 0.00513445\n",
      "Iteration 2031, loss = 0.00512877\n",
      "Iteration 2032, loss = 0.00512311\n",
      "Iteration 2033, loss = 0.00511745\n",
      "Iteration 2034, loss = 0.00511179\n",
      "Iteration 2035, loss = 0.00510615\n",
      "Iteration 2036, loss = 0.00510052\n",
      "Iteration 2037, loss = 0.00509489\n",
      "Iteration 2038, loss = 0.00508927\n",
      "Iteration 2039, loss = 0.00508366\n",
      "Iteration 2040, loss = 0.00507806\n",
      "Iteration 2041, loss = 0.00507247\n",
      "Iteration 2042, loss = 0.00506688\n",
      "Iteration 2043, loss = 0.00506130\n",
      "Iteration 2044, loss = 0.00505574\n",
      "Iteration 2045, loss = 0.00505017\n",
      "Iteration 2046, loss = 0.00504462\n",
      "Iteration 2047, loss = 0.00503908\n",
      "Iteration 2048, loss = 0.00503354\n",
      "Iteration 2049, loss = 0.00502801\n",
      "Iteration 2050, loss = 0.00502249\n",
      "Iteration 2051, loss = 0.00501698\n",
      "Iteration 2052, loss = 0.00501147\n",
      "Iteration 2053, loss = 0.00500598\n",
      "Iteration 2054, loss = 0.00500049\n",
      "Iteration 2055, loss = 0.00499501\n",
      "Iteration 2056, loss = 0.00498954\n",
      "Iteration 2057, loss = 0.00498407\n",
      "Iteration 2058, loss = 0.00497861\n",
      "Iteration 2059, loss = 0.00497317\n",
      "Iteration 2060, loss = 0.00496773\n",
      "Iteration 2061, loss = 0.00496229\n",
      "Iteration 2062, loss = 0.00495687\n",
      "Iteration 2063, loss = 0.00495145\n",
      "Iteration 2064, loss = 0.00494604\n",
      "Iteration 2065, loss = 0.00494064\n",
      "Iteration 2066, loss = 0.00493525\n",
      "Iteration 2067, loss = 0.00492986\n",
      "Iteration 2068, loss = 0.00492448\n",
      "Iteration 2069, loss = 0.00491911\n",
      "Iteration 2070, loss = 0.00491375\n",
      "Iteration 2071, loss = 0.00490839\n",
      "Iteration 2072, loss = 0.00490305\n",
      "Iteration 2073, loss = 0.00489771\n",
      "Iteration 2074, loss = 0.00489238\n",
      "Iteration 2075, loss = 0.00488705\n",
      "Iteration 2076, loss = 0.00488174\n",
      "Iteration 2077, loss = 0.00487643\n",
      "Iteration 2078, loss = 0.00487113\n",
      "Iteration 2079, loss = 0.00486583\n",
      "Iteration 2080, loss = 0.00486055\n",
      "Iteration 2081, loss = 0.00485527\n",
      "Iteration 2082, loss = 0.00485000\n",
      "Iteration 2083, loss = 0.00484474\n",
      "Iteration 2084, loss = 0.00483948\n",
      "Iteration 2085, loss = 0.00483423\n",
      "Iteration 2086, loss = 0.00482899\n",
      "Iteration 2087, loss = 0.00482376\n",
      "Iteration 2088, loss = 0.00481854\n",
      "Iteration 2089, loss = 0.00481332\n",
      "Iteration 2090, loss = 0.00480811\n",
      "Iteration 2091, loss = 0.00480290\n",
      "Iteration 2092, loss = 0.00479771\n",
      "Iteration 2093, loss = 0.00479252\n",
      "Iteration 2094, loss = 0.00478734\n",
      "Iteration 2095, loss = 0.00478217\n",
      "Iteration 2096, loss = 0.00477700\n",
      "Iteration 2097, loss = 0.00477184\n",
      "Iteration 2098, loss = 0.00476669\n",
      "Iteration 2099, loss = 0.00476155\n",
      "Iteration 2100, loss = 0.00475641\n",
      "Iteration 2101, loss = 0.00475128\n",
      "Iteration 2102, loss = 0.00474616\n",
      "Iteration 2103, loss = 0.00474105\n",
      "Iteration 2104, loss = 0.00473594\n",
      "Iteration 2105, loss = 0.00473084\n",
      "Iteration 2106, loss = 0.00472575\n",
      "Iteration 2107, loss = 0.00472067\n",
      "Iteration 2108, loss = 0.00471559\n",
      "Iteration 2109, loss = 0.00471052\n",
      "Iteration 2110, loss = 0.00470545\n",
      "Iteration 2111, loss = 0.00470040\n",
      "Iteration 2112, loss = 0.00469535\n",
      "Iteration 2113, loss = 0.00469031\n",
      "Iteration 2114, loss = 0.00468527\n",
      "Iteration 2115, loss = 0.00468024\n",
      "Iteration 2116, loss = 0.00467522\n",
      "Iteration 2117, loss = 0.00467021\n",
      "Iteration 2118, loss = 0.00466520\n",
      "Iteration 2119, loss = 0.00466021\n",
      "Iteration 2120, loss = 0.00465521\n",
      "Iteration 2121, loss = 0.00465023\n",
      "Iteration 2122, loss = 0.00464525\n",
      "Iteration 2123, loss = 0.00464028\n",
      "Iteration 2124, loss = 0.00463532\n",
      "Iteration 2125, loss = 0.00463036\n",
      "Iteration 2126, loss = 0.00462541\n",
      "Iteration 2127, loss = 0.00462047\n",
      "Iteration 2128, loss = 0.00461553\n",
      "Iteration 2129, loss = 0.00461060\n",
      "Iteration 2130, loss = 0.00460568\n",
      "Iteration 2131, loss = 0.00460077\n",
      "Iteration 2132, loss = 0.00459586\n",
      "Iteration 2133, loss = 0.00459096\n",
      "Iteration 2134, loss = 0.00458606\n",
      "Iteration 2135, loss = 0.00458118\n",
      "Iteration 2136, loss = 0.00457630\n",
      "Iteration 2137, loss = 0.00457142\n",
      "Iteration 2138, loss = 0.00456656\n",
      "Iteration 2139, loss = 0.00456170\n",
      "Iteration 2140, loss = 0.00455684\n",
      "Iteration 2141, loss = 0.00455200\n",
      "Iteration 2142, loss = 0.00454716\n",
      "Iteration 2143, loss = 0.00454233\n",
      "Iteration 2144, loss = 0.00453750\n",
      "Iteration 2145, loss = 0.00453268\n",
      "Iteration 2146, loss = 0.00452787\n",
      "Iteration 2147, loss = 0.00452306\n",
      "Iteration 2148, loss = 0.00451826\n",
      "Iteration 2149, loss = 0.00451347\n",
      "Iteration 2150, loss = 0.00450869\n",
      "Iteration 2151, loss = 0.00450391\n",
      "Iteration 2152, loss = 0.00449914\n",
      "Iteration 2153, loss = 0.00449437\n",
      "Iteration 2154, loss = 0.00448961\n",
      "Iteration 2155, loss = 0.00448486\n",
      "Iteration 2156, loss = 0.00448012\n",
      "Iteration 2157, loss = 0.00447538\n",
      "Iteration 2158, loss = 0.00447065\n",
      "Iteration 2159, loss = 0.00446592\n",
      "Iteration 2160, loss = 0.00446120\n",
      "Iteration 2161, loss = 0.00445649\n",
      "Iteration 2162, loss = 0.00445179\n",
      "Iteration 2163, loss = 0.00444709\n",
      "Iteration 2164, loss = 0.00444239\n",
      "Iteration 2165, loss = 0.00443771\n",
      "Iteration 2166, loss = 0.00443303\n",
      "Iteration 2167, loss = 0.00442836\n",
      "Iteration 2168, loss = 0.00442369\n",
      "Iteration 2169, loss = 0.00441903\n",
      "Iteration 2170, loss = 0.00441438\n",
      "Iteration 2171, loss = 0.00440973\n",
      "Iteration 2172, loss = 0.00440509\n",
      "Iteration 2173, loss = 0.00440045\n",
      "Iteration 2174, loss = 0.00439583\n",
      "Iteration 2175, loss = 0.00439121\n",
      "Iteration 2176, loss = 0.00438659\n",
      "Iteration 2177, loss = 0.00438198\n",
      "Iteration 2178, loss = 0.00437738\n",
      "Iteration 2179, loss = 0.00437279\n",
      "Iteration 2180, loss = 0.00436820\n",
      "Iteration 2181, loss = 0.00436361\n",
      "Iteration 2182, loss = 0.00435904\n",
      "Iteration 2183, loss = 0.00435447\n",
      "Iteration 2184, loss = 0.00434990\n",
      "Iteration 2185, loss = 0.00434535\n",
      "Iteration 2186, loss = 0.00434079\n",
      "Iteration 2187, loss = 0.00433625\n",
      "Iteration 2188, loss = 0.00433171\n",
      "Iteration 2189, loss = 0.00432718\n",
      "Iteration 2190, loss = 0.00432265\n",
      "Iteration 2191, loss = 0.00431813\n",
      "Iteration 2192, loss = 0.00431362\n",
      "Iteration 2193, loss = 0.00430911\n",
      "Iteration 2194, loss = 0.00430461\n",
      "Iteration 2195, loss = 0.00430012\n",
      "Iteration 2196, loss = 0.00429563\n",
      "Iteration 2197, loss = 0.00429114\n",
      "Iteration 2198, loss = 0.00428667\n",
      "Iteration 2199, loss = 0.00428220\n",
      "Iteration 2200, loss = 0.00427773\n",
      "Iteration 2201, loss = 0.00427327\n",
      "Iteration 2202, loss = 0.00426882\n",
      "Iteration 2203, loss = 0.00426438\n",
      "Iteration 2204, loss = 0.00425994\n",
      "Iteration 2205, loss = 0.00425550\n",
      "Iteration 2206, loss = 0.00425108\n",
      "Iteration 2207, loss = 0.00424665\n",
      "Iteration 2208, loss = 0.00424224\n",
      "Iteration 2209, loss = 0.00423783\n",
      "Iteration 2210, loss = 0.00423343\n",
      "Iteration 2211, loss = 0.00422903\n",
      "Iteration 2212, loss = 0.00422464\n",
      "Iteration 2213, loss = 0.00422025\n",
      "Iteration 2214, loss = 0.00421587\n",
      "Iteration 2215, loss = 0.00421150\n",
      "Iteration 2216, loss = 0.00420713\n",
      "Iteration 2217, loss = 0.00420277\n",
      "Iteration 2218, loss = 0.00419842\n",
      "Iteration 2219, loss = 0.00419407\n",
      "Iteration 2220, loss = 0.00418972\n",
      "Iteration 2221, loss = 0.00418539\n",
      "Iteration 2222, loss = 0.00418106\n",
      "Iteration 2223, loss = 0.00417673\n",
      "Iteration 2224, loss = 0.00417241\n",
      "Iteration 2225, loss = 0.00416810\n",
      "Iteration 2226, loss = 0.00416379\n",
      "Iteration 2227, loss = 0.00415949\n",
      "Iteration 2228, loss = 0.00415519\n",
      "Iteration 2229, loss = 0.00415090\n",
      "Iteration 2230, loss = 0.00414661\n",
      "Iteration 2231, loss = 0.00414234\n",
      "Iteration 2232, loss = 0.00413806\n",
      "Iteration 2233, loss = 0.00413380\n",
      "Iteration 2234, loss = 0.00412953\n",
      "Iteration 2235, loss = 0.00412528\n",
      "Iteration 2236, loss = 0.00412103\n",
      "Iteration 2237, loss = 0.00411678\n",
      "Iteration 2238, loss = 0.00411255\n",
      "Iteration 2239, loss = 0.00410831\n",
      "Iteration 2240, loss = 0.00410409\n",
      "Iteration 2241, loss = 0.00409987\n",
      "Iteration 2242, loss = 0.00409565\n",
      "Iteration 2243, loss = 0.00409144\n",
      "Iteration 2244, loss = 0.00408724\n",
      "Iteration 2245, loss = 0.00408304\n",
      "Iteration 2246, loss = 0.00407885\n",
      "Iteration 2247, loss = 0.00407466\n",
      "Iteration 2248, loss = 0.00407048\n",
      "Iteration 2249, loss = 0.00406630\n",
      "Iteration 2250, loss = 0.00406213\n",
      "Iteration 2251, loss = 0.00405797\n",
      "Iteration 2252, loss = 0.00405381\n",
      "Iteration 2253, loss = 0.00404966\n",
      "Iteration 2254, loss = 0.00404551\n",
      "Iteration 2255, loss = 0.00404137\n",
      "Iteration 2256, loss = 0.00403723\n",
      "Iteration 2257, loss = 0.00403310\n",
      "Iteration 2258, loss = 0.00402897\n",
      "Iteration 2259, loss = 0.00402485\n",
      "Iteration 2260, loss = 0.00402074\n",
      "Iteration 2261, loss = 0.00401663\n",
      "Iteration 2262, loss = 0.00401253\n",
      "Iteration 2263, loss = 0.00400843\n",
      "Iteration 2264, loss = 0.00400434\n",
      "Iteration 2265, loss = 0.00400025\n",
      "Iteration 2266, loss = 0.00399617\n",
      "Iteration 2267, loss = 0.00399210\n",
      "Iteration 2268, loss = 0.00398803\n",
      "Iteration 2269, loss = 0.00398396\n",
      "Iteration 2270, loss = 0.00397990\n",
      "Iteration 2271, loss = 0.00397585\n",
      "Iteration 2272, loss = 0.00397180\n",
      "Iteration 2273, loss = 0.00396776\n",
      "Iteration 2274, loss = 0.00396372\n",
      "Iteration 2275, loss = 0.00395969\n",
      "Iteration 2276, loss = 0.00395566\n",
      "Iteration 2277, loss = 0.00395164\n",
      "Iteration 2278, loss = 0.00394762\n",
      "Iteration 2279, loss = 0.00394361\n",
      "Iteration 2280, loss = 0.00393961\n",
      "Iteration 2281, loss = 0.00393561\n",
      "Iteration 2282, loss = 0.00393161\n",
      "Iteration 2283, loss = 0.00392762\n",
      "Iteration 2284, loss = 0.00392364\n",
      "Iteration 2285, loss = 0.00391966\n",
      "Iteration 2286, loss = 0.00391569\n",
      "Iteration 2287, loss = 0.00391172\n",
      "Iteration 2288, loss = 0.00390776\n",
      "Iteration 2289, loss = 0.00390380\n",
      "Iteration 2290, loss = 0.00389985\n",
      "Iteration 2291, loss = 0.00389590\n",
      "Iteration 2292, loss = 0.00389196\n",
      "Iteration 2293, loss = 0.00388802\n",
      "Iteration 2294, loss = 0.00388409\n",
      "Iteration 2295, loss = 0.00388016\n",
      "Iteration 2296, loss = 0.00387624\n",
      "Iteration 2297, loss = 0.00387233\n",
      "Iteration 2298, loss = 0.00386842\n",
      "Iteration 2299, loss = 0.00386451\n",
      "Iteration 2300, loss = 0.00386061\n",
      "Iteration 2301, loss = 0.00385672\n",
      "Iteration 2302, loss = 0.00385283\n",
      "Iteration 2303, loss = 0.00384894\n",
      "Iteration 2304, loss = 0.00384506\n",
      "Iteration 2305, loss = 0.00384119\n",
      "Iteration 2306, loss = 0.00383732\n",
      "Iteration 2307, loss = 0.00383346\n",
      "Iteration 2308, loss = 0.00382960\n",
      "Iteration 2309, loss = 0.00382575\n",
      "Iteration 2310, loss = 0.00382190\n",
      "Iteration 2311, loss = 0.00381805\n",
      "Iteration 2312, loss = 0.00381421\n",
      "Iteration 2313, loss = 0.00381038\n",
      "Iteration 2314, loss = 0.00380655\n",
      "Iteration 2315, loss = 0.00380273\n",
      "Iteration 2316, loss = 0.00379891\n",
      "Iteration 2317, loss = 0.00379510\n",
      "Iteration 2318, loss = 0.00379129\n",
      "Iteration 2319, loss = 0.00378749\n",
      "Iteration 2320, loss = 0.00378369\n",
      "Iteration 2321, loss = 0.00377990\n",
      "Iteration 2322, loss = 0.00377611\n",
      "Iteration 2323, loss = 0.00377232\n",
      "Iteration 2324, loss = 0.00376855\n",
      "Iteration 2325, loss = 0.00376477\n",
      "Iteration 2326, loss = 0.00376100\n",
      "Iteration 2327, loss = 0.00375724\n",
      "Iteration 2328, loss = 0.00375348\n",
      "Iteration 2329, loss = 0.00374973\n",
      "Iteration 2330, loss = 0.00374598\n",
      "Iteration 2331, loss = 0.00374224\n",
      "Iteration 2332, loss = 0.00373850\n",
      "Iteration 2333, loss = 0.00373476\n",
      "Iteration 2334, loss = 0.00373103\n",
      "Iteration 2335, loss = 0.00372731\n",
      "Iteration 2336, loss = 0.00372359\n",
      "Iteration 2337, loss = 0.00371988\n",
      "Iteration 2338, loss = 0.00371617\n",
      "Iteration 2339, loss = 0.00371246\n",
      "Iteration 2340, loss = 0.00370876\n",
      "Iteration 2341, loss = 0.00370507\n",
      "Iteration 2342, loss = 0.00370138\n",
      "Iteration 2343, loss = 0.00369769\n",
      "Iteration 2344, loss = 0.00369401\n",
      "Iteration 2345, loss = 0.00369034\n",
      "Iteration 2346, loss = 0.00368667\n",
      "Iteration 2347, loss = 0.00368300\n",
      "Iteration 2348, loss = 0.00367934\n",
      "Iteration 2349, loss = 0.00367568\n",
      "Iteration 2350, loss = 0.00367203\n",
      "Iteration 2351, loss = 0.00366838\n",
      "Iteration 2352, loss = 0.00366474\n",
      "Iteration 2353, loss = 0.00366110\n",
      "Iteration 2354, loss = 0.00365747\n",
      "Iteration 2355, loss = 0.00365384\n",
      "Iteration 2356, loss = 0.00365022\n",
      "Iteration 2357, loss = 0.00364660\n",
      "Iteration 2358, loss = 0.00364299\n",
      "Iteration 2359, loss = 0.00363938\n",
      "Iteration 2360, loss = 0.00363577\n",
      "Iteration 2361, loss = 0.00363217\n",
      "Iteration 2362, loss = 0.00362858\n",
      "Iteration 2363, loss = 0.00362499\n",
      "Iteration 2364, loss = 0.00362140\n",
      "Iteration 2365, loss = 0.00361782\n",
      "Iteration 2366, loss = 0.00361425\n",
      "Iteration 2367, loss = 0.00361067\n",
      "Iteration 2368, loss = 0.00360711\n",
      "Iteration 2369, loss = 0.00360354\n",
      "Iteration 2370, loss = 0.00359999\n",
      "Iteration 2371, loss = 0.00359643\n",
      "Iteration 2372, loss = 0.00359288\n",
      "Iteration 2373, loss = 0.00358934\n",
      "Iteration 2374, loss = 0.00358580\n",
      "Iteration 2375, loss = 0.00358226\n",
      "Iteration 2376, loss = 0.00357873\n",
      "Iteration 2377, loss = 0.00357521\n",
      "Iteration 2378, loss = 0.00357169\n",
      "Iteration 2379, loss = 0.00356817\n",
      "Iteration 2380, loss = 0.00356466\n",
      "Iteration 2381, loss = 0.00356115\n",
      "Iteration 2382, loss = 0.00355765\n",
      "Iteration 2383, loss = 0.00355415\n",
      "Iteration 2384, loss = 0.00355065\n",
      "Iteration 2385, loss = 0.00354716\n",
      "Iteration 2386, loss = 0.00354368\n",
      "Iteration 2387, loss = 0.00354020\n",
      "Iteration 2388, loss = 0.00353672\n",
      "Iteration 2389, loss = 0.00353325\n",
      "Iteration 2390, loss = 0.00352978\n",
      "Iteration 2391, loss = 0.00352632\n",
      "Iteration 2392, loss = 0.00352286\n",
      "Iteration 2393, loss = 0.00351941\n",
      "Iteration 2394, loss = 0.00351596\n",
      "Iteration 2395, loss = 0.00351251\n",
      "Iteration 2396, loss = 0.00350907\n",
      "Iteration 2397, loss = 0.00350564\n",
      "Iteration 2398, loss = 0.00350220\n",
      "Iteration 2399, loss = 0.00349878\n",
      "Iteration 2400, loss = 0.00349535\n",
      "Iteration 2401, loss = 0.00349193\n",
      "Iteration 2402, loss = 0.00348852\n",
      "Iteration 2403, loss = 0.00348511\n",
      "Iteration 2404, loss = 0.00348170\n",
      "Iteration 2405, loss = 0.00347830\n",
      "Iteration 2406, loss = 0.00347491\n",
      "Iteration 2407, loss = 0.00347151\n",
      "Iteration 2408, loss = 0.00346813\n",
      "Iteration 2409, loss = 0.00346474\n",
      "Iteration 2410, loss = 0.00346136\n",
      "Iteration 2411, loss = 0.00345799\n",
      "Iteration 2412, loss = 0.00345462\n",
      "Iteration 2413, loss = 0.00345125\n",
      "Iteration 2414, loss = 0.00344789\n",
      "Iteration 2415, loss = 0.00344453\n",
      "Iteration 2416, loss = 0.00344118\n",
      "Iteration 2417, loss = 0.00343783\n",
      "Iteration 2418, loss = 0.00343448\n",
      "Iteration 2419, loss = 0.00343114\n",
      "Iteration 2420, loss = 0.00342780\n",
      "Iteration 2421, loss = 0.00342447\n",
      "Iteration 2422, loss = 0.00342114\n",
      "Iteration 2423, loss = 0.00341782\n",
      "Iteration 2424, loss = 0.00341450\n",
      "Iteration 2425, loss = 0.00341118\n",
      "Iteration 2426, loss = 0.00340787\n",
      "Iteration 2427, loss = 0.00340456\n",
      "Iteration 2428, loss = 0.00340126\n",
      "Iteration 2429, loss = 0.00339796\n",
      "Iteration 2430, loss = 0.00339467\n",
      "Iteration 2431, loss = 0.00339138\n",
      "Iteration 2432, loss = 0.00338809\n",
      "Iteration 2433, loss = 0.00338481\n",
      "Iteration 2434, loss = 0.00338153\n",
      "Iteration 2435, loss = 0.00337826\n",
      "Iteration 2436, loss = 0.00337499\n",
      "Iteration 2437, loss = 0.00337172\n",
      "Iteration 2438, loss = 0.00336846\n",
      "Iteration 2439, loss = 0.00336520\n",
      "Iteration 2440, loss = 0.00336195\n",
      "Iteration 2441, loss = 0.00335870\n",
      "Iteration 2442, loss = 0.00335546\n",
      "Iteration 2443, loss = 0.00335222\n",
      "Iteration 2444, loss = 0.00334898\n",
      "Iteration 2445, loss = 0.00334575\n",
      "Iteration 2446, loss = 0.00334252\n",
      "Iteration 2447, loss = 0.00333929\n",
      "Iteration 2448, loss = 0.00333607\n",
      "Iteration 2449, loss = 0.00333286\n",
      "Iteration 2450, loss = 0.00332965\n",
      "Iteration 2451, loss = 0.00332644\n",
      "Iteration 2452, loss = 0.00332323\n",
      "Iteration 2453, loss = 0.00332003\n",
      "Iteration 2454, loss = 0.00331684\n",
      "Iteration 2455, loss = 0.00331364\n",
      "Iteration 2456, loss = 0.00331046\n",
      "Iteration 2457, loss = 0.00330727\n",
      "Iteration 2458, loss = 0.00330409\n",
      "Iteration 2459, loss = 0.00330092\n",
      "Iteration 2460, loss = 0.00329774\n",
      "Iteration 2461, loss = 0.00329458\n",
      "Iteration 2462, loss = 0.00329141\n",
      "Iteration 2463, loss = 0.00328825\n",
      "Iteration 2464, loss = 0.00328510\n",
      "Iteration 2465, loss = 0.00328194\n",
      "Iteration 2466, loss = 0.00327880\n",
      "Iteration 2467, loss = 0.00327565\n",
      "Iteration 2468, loss = 0.00327251\n",
      "Iteration 2469, loss = 0.00326937\n",
      "Iteration 2470, loss = 0.00326624\n",
      "Iteration 2471, loss = 0.00326311\n",
      "Iteration 2472, loss = 0.00325999\n",
      "Iteration 2473, loss = 0.00325687\n",
      "Iteration 2474, loss = 0.00325375\n",
      "Iteration 2475, loss = 0.00325064\n",
      "Iteration 2476, loss = 0.00324753\n",
      "Iteration 2477, loss = 0.00324442\n",
      "Iteration 2478, loss = 0.00324132\n",
      "Iteration 2479, loss = 0.00323823\n",
      "Iteration 2480, loss = 0.00323513\n",
      "Iteration 2481, loss = 0.00323204\n",
      "Iteration 2482, loss = 0.00322896\n",
      "Iteration 2483, loss = 0.00322587\n",
      "Iteration 2484, loss = 0.00322280\n",
      "Iteration 2485, loss = 0.00321972\n",
      "Iteration 2486, loss = 0.00321665\n",
      "Iteration 2487, loss = 0.00321358\n",
      "Iteration 2488, loss = 0.00321052\n",
      "Iteration 2489, loss = 0.00320746\n",
      "Iteration 2490, loss = 0.00320441\n",
      "Iteration 2491, loss = 0.00320136\n",
      "Iteration 2492, loss = 0.00319831\n",
      "Iteration 2493, loss = 0.00319526\n",
      "Iteration 2494, loss = 0.00319222\n",
      "Iteration 2495, loss = 0.00318919\n",
      "Iteration 2496, loss = 0.00318616\n",
      "Iteration 2497, loss = 0.00318313\n",
      "Iteration 2498, loss = 0.00318010\n",
      "Iteration 2499, loss = 0.00317708\n",
      "Iteration 2500, loss = 0.00317406\n",
      "Iteration 2501, loss = 0.00317105\n",
      "Iteration 2502, loss = 0.00316804\n",
      "Iteration 2503, loss = 0.00316503\n",
      "Iteration 2504, loss = 0.00316203\n",
      "Iteration 2505, loss = 0.00315903\n",
      "Iteration 2506, loss = 0.00315604\n",
      "Iteration 2507, loss = 0.00315304\n",
      "Iteration 2508, loss = 0.00315006\n",
      "Iteration 2509, loss = 0.00314707\n",
      "Iteration 2510, loss = 0.00314409\n",
      "Iteration 2511, loss = 0.00314112\n",
      "Iteration 2512, loss = 0.00313814\n",
      "Iteration 2513, loss = 0.00313517\n",
      "Iteration 2514, loss = 0.00313221\n",
      "Iteration 2515, loss = 0.00312925\n",
      "Iteration 2516, loss = 0.00312629\n",
      "Iteration 2517, loss = 0.00312333\n",
      "Iteration 2518, loss = 0.00312038\n",
      "Iteration 2519, loss = 0.00311743\n",
      "Iteration 2520, loss = 0.00311449\n",
      "Iteration 2521, loss = 0.00311155\n",
      "Iteration 2522, loss = 0.00310861\n",
      "Iteration 2523, loss = 0.00310568\n",
      "Iteration 2524, loss = 0.00310275\n",
      "Iteration 2525, loss = 0.00309983\n",
      "Iteration 2526, loss = 0.00309690\n",
      "Iteration 2527, loss = 0.00309398\n",
      "Iteration 2528, loss = 0.00309107\n",
      "Iteration 2529, loss = 0.00308816\n",
      "Iteration 2530, loss = 0.00308525\n",
      "Iteration 2531, loss = 0.00308235\n",
      "Iteration 2532, loss = 0.00307945\n",
      "Iteration 2533, loss = 0.00307655\n",
      "Iteration 2534, loss = 0.00307365\n",
      "Iteration 2535, loss = 0.00307076\n",
      "Iteration 2536, loss = 0.00306788\n",
      "Iteration 2537, loss = 0.00306500\n",
      "Iteration 2538, loss = 0.00306212\n",
      "Iteration 2539, loss = 0.00305924\n",
      "Iteration 2540, loss = 0.00305637\n",
      "Iteration 2541, loss = 0.00305350\n",
      "Iteration 2542, loss = 0.00305063\n",
      "Iteration 2543, loss = 0.00304777\n",
      "Iteration 2544, loss = 0.00304491\n",
      "Iteration 2545, loss = 0.00304206\n",
      "Iteration 2546, loss = 0.00303921\n",
      "Iteration 2547, loss = 0.00303636\n",
      "Iteration 2548, loss = 0.00303352\n",
      "Iteration 2549, loss = 0.00303067\n",
      "Iteration 2550, loss = 0.00302784\n",
      "Iteration 2551, loss = 0.00302500\n",
      "Iteration 2552, loss = 0.00302217\n",
      "Iteration 2553, loss = 0.00301935\n",
      "Iteration 2554, loss = 0.00301652\n",
      "Iteration 2555, loss = 0.00301370\n",
      "Iteration 2556, loss = 0.00301088\n",
      "Iteration 2557, loss = 0.00300807\n",
      "Iteration 2558, loss = 0.00300526\n",
      "Iteration 2559, loss = 0.00300246\n",
      "Iteration 2560, loss = 0.00299965\n",
      "Iteration 2561, loss = 0.00299685\n",
      "Iteration 2562, loss = 0.00299406\n",
      "Iteration 2563, loss = 0.00299126\n",
      "Iteration 2564, loss = 0.00298847\n",
      "Iteration 2565, loss = 0.00298569\n",
      "Iteration 2566, loss = 0.00298291\n",
      "Iteration 2567, loss = 0.00298013\n",
      "Iteration 2568, loss = 0.00297735\n",
      "Iteration 2569, loss = 0.00297458\n",
      "Iteration 2570, loss = 0.00297181\n",
      "Iteration 2571, loss = 0.00296904\n",
      "Iteration 2572, loss = 0.00296628\n",
      "Iteration 2573, loss = 0.00296352\n",
      "Iteration 2574, loss = 0.00296077\n",
      "Iteration 2575, loss = 0.00295801\n",
      "Iteration 2576, loss = 0.00295526\n",
      "Iteration 2577, loss = 0.00295252\n",
      "Iteration 2578, loss = 0.00294978\n",
      "Iteration 2579, loss = 0.00294704\n",
      "Iteration 2580, loss = 0.00294430\n",
      "Iteration 2581, loss = 0.00294157\n",
      "Iteration 2582, loss = 0.00293884\n",
      "Iteration 2583, loss = 0.00293611\n",
      "Iteration 2584, loss = 0.00293339\n",
      "Iteration 2585, loss = 0.00293067\n",
      "Iteration 2586, loss = 0.00292795\n",
      "Iteration 2587, loss = 0.00292524\n",
      "Iteration 2588, loss = 0.00292253\n",
      "Iteration 2589, loss = 0.00291983\n",
      "Iteration 2590, loss = 0.00291712\n",
      "Iteration 2591, loss = 0.00291442\n",
      "Iteration 2592, loss = 0.00291173\n",
      "Iteration 2593, loss = 0.00290903\n",
      "Iteration 2594, loss = 0.00290634\n",
      "Iteration 2595, loss = 0.00290366\n",
      "Iteration 2596, loss = 0.00290097\n",
      "Iteration 2597, loss = 0.00289829\n",
      "Iteration 2598, loss = 0.00289562\n",
      "Iteration 2599, loss = 0.00289294\n",
      "Iteration 2600, loss = 0.00289027\n",
      "Iteration 2601, loss = 0.00288760\n",
      "Iteration 2602, loss = 0.00288494\n",
      "Iteration 2603, loss = 0.00288228\n",
      "Iteration 2604, loss = 0.00287962\n",
      "Iteration 2605, loss = 0.00287697\n",
      "Iteration 2606, loss = 0.00287431\n",
      "Iteration 2607, loss = 0.00287167\n",
      "Iteration 2608, loss = 0.00286902\n",
      "Iteration 2609, loss = 0.00286638\n",
      "Iteration 2610, loss = 0.00286374\n",
      "Iteration 2611, loss = 0.00286110\n",
      "Iteration 2612, loss = 0.00285847\n",
      "Iteration 2613, loss = 0.00285584\n",
      "Iteration 2614, loss = 0.00285322\n",
      "Iteration 2615, loss = 0.00285059\n",
      "Iteration 2616, loss = 0.00284797\n",
      "Iteration 2617, loss = 0.00284536\n",
      "Iteration 2618, loss = 0.00284274\n",
      "Iteration 2619, loss = 0.00284013\n",
      "Iteration 2620, loss = 0.00283753\n",
      "Iteration 2621, loss = 0.00283492\n",
      "Iteration 2622, loss = 0.00283232\n",
      "Iteration 2623, loss = 0.00282972\n",
      "Iteration 2624, loss = 0.00282713\n",
      "Iteration 2625, loss = 0.00282454\n",
      "Iteration 2626, loss = 0.00282195\n",
      "Iteration 2627, loss = 0.00281936\n",
      "Iteration 2628, loss = 0.00281678\n",
      "Iteration 2629, loss = 0.00281420\n",
      "Iteration 2630, loss = 0.00281162\n",
      "Iteration 2631, loss = 0.00280905\n",
      "Iteration 2632, loss = 0.00280648\n",
      "Iteration 2633, loss = 0.00280391\n",
      "Iteration 2634, loss = 0.00280135\n",
      "Iteration 2635, loss = 0.00279879\n",
      "Iteration 2636, loss = 0.00279623\n",
      "Iteration 2637, loss = 0.00279367\n",
      "Iteration 2638, loss = 0.00279112\n",
      "Iteration 2639, loss = 0.00278857\n",
      "Iteration 2640, loss = 0.00278603\n",
      "Iteration 2641, loss = 0.00278348\n",
      "Iteration 2642, loss = 0.00278094\n",
      "Iteration 2643, loss = 0.00277841\n",
      "Iteration 2644, loss = 0.00277587\n",
      "Iteration 2645, loss = 0.00277334\n",
      "Iteration 2646, loss = 0.00277081\n",
      "Iteration 2647, loss = 0.00276829\n",
      "Iteration 2648, loss = 0.00276577\n",
      "Iteration 2649, loss = 0.00276325\n",
      "Iteration 2650, loss = 0.00276073\n",
      "Iteration 2651, loss = 0.00275822\n",
      "Iteration 2652, loss = 0.00275571\n",
      "Iteration 2653, loss = 0.00275320\n",
      "Iteration 2654, loss = 0.00275070\n",
      "Iteration 2655, loss = 0.00274820\n",
      "Iteration 2656, loss = 0.00274570\n",
      "Iteration 2657, loss = 0.00274320\n",
      "Iteration 2658, loss = 0.00274071\n",
      "Iteration 2659, loss = 0.00273822\n",
      "Iteration 2660, loss = 0.00273573\n",
      "Iteration 2661, loss = 0.00273325\n",
      "Iteration 2662, loss = 0.00273077\n",
      "Iteration 2663, loss = 0.00272829\n",
      "Iteration 2664, loss = 0.00272582\n",
      "Iteration 2665, loss = 0.00272335\n",
      "Iteration 2666, loss = 0.00272088\n",
      "Iteration 2667, loss = 0.00271841\n",
      "Iteration 2668, loss = 0.00271595\n",
      "Iteration 2669, loss = 0.00271349\n",
      "Iteration 2670, loss = 0.00271103\n",
      "Iteration 2671, loss = 0.00270858\n",
      "Iteration 2672, loss = 0.00270612\n",
      "Iteration 2673, loss = 0.00270368\n",
      "Iteration 2674, loss = 0.00270123\n",
      "Iteration 2675, loss = 0.00269879\n",
      "Iteration 2676, loss = 0.00269635\n",
      "Iteration 2677, loss = 0.00269391\n",
      "Iteration 2678, loss = 0.00269148\n",
      "Iteration 2679, loss = 0.00268904\n",
      "Iteration 2680, loss = 0.00268662\n",
      "Iteration 2681, loss = 0.00268419\n",
      "Iteration 2682, loss = 0.00268177\n",
      "Iteration 2683, loss = 0.00267935\n",
      "Iteration 2684, loss = 0.00267693\n",
      "Iteration 2685, loss = 0.00267452\n",
      "Iteration 2686, loss = 0.00267210\n",
      "Iteration 2687, loss = 0.00266970\n",
      "Iteration 2688, loss = 0.00266729\n",
      "Iteration 2689, loss = 0.00266489\n",
      "Iteration 2690, loss = 0.00266249\n",
      "Iteration 2691, loss = 0.00266009\n",
      "Iteration 2692, loss = 0.00265769\n",
      "Iteration 2693, loss = 0.00265530\n",
      "Iteration 2694, loss = 0.00265291\n",
      "Iteration 2695, loss = 0.00265053\n",
      "Iteration 2696, loss = 0.00264814\n",
      "Iteration 2697, loss = 0.00264576\n",
      "Iteration 2698, loss = 0.00264339\n",
      "Iteration 2699, loss = 0.00264101\n",
      "Iteration 2700, loss = 0.00263864\n",
      "Iteration 2701, loss = 0.00263627\n",
      "Iteration 2702, loss = 0.00263390\n",
      "Iteration 2703, loss = 0.00263154\n",
      "Iteration 2704, loss = 0.00262918\n",
      "Iteration 2705, loss = 0.00262682\n",
      "Iteration 2706, loss = 0.00262446\n",
      "Iteration 2707, loss = 0.00262211\n",
      "Iteration 2708, loss = 0.00261976\n",
      "Iteration 2709, loss = 0.00261741\n",
      "Iteration 2710, loss = 0.00261506\n",
      "Iteration 2711, loss = 0.00261272\n",
      "Iteration 2712, loss = 0.00261038\n",
      "Iteration 2713, loss = 0.00260805\n",
      "Iteration 2714, loss = 0.00260571\n",
      "Iteration 2715, loss = 0.00260338\n",
      "Iteration 2716, loss = 0.00260105\n",
      "Iteration 2717, loss = 0.00259873\n",
      "Iteration 2718, loss = 0.00259640\n",
      "Iteration 2719, loss = 0.00259408\n",
      "Iteration 2720, loss = 0.00259176\n",
      "Iteration 2721, loss = 0.00258945\n",
      "Iteration 2722, loss = 0.00258714\n",
      "Iteration 2723, loss = 0.00258483\n",
      "Iteration 2724, loss = 0.00258252\n",
      "Iteration 2725, loss = 0.00258022\n",
      "Iteration 2726, loss = 0.00257791\n",
      "Iteration 2727, loss = 0.00257562\n",
      "Iteration 2728, loss = 0.00257332\n",
      "Iteration 2729, loss = 0.00257102\n",
      "Iteration 2730, loss = 0.00256873\n",
      "Iteration 2731, loss = 0.00256645\n",
      "Iteration 2732, loss = 0.00256416\n",
      "Iteration 2733, loss = 0.00256188\n",
      "Iteration 2734, loss = 0.00255960\n",
      "Iteration 2735, loss = 0.00255732\n",
      "Iteration 2736, loss = 0.00255504\n",
      "Iteration 2737, loss = 0.00255277\n",
      "Iteration 2738, loss = 0.00255050\n",
      "Iteration 2739, loss = 0.00254823\n",
      "Iteration 2740, loss = 0.00254597\n",
      "Iteration 2741, loss = 0.00254370\n",
      "Iteration 2742, loss = 0.00254145\n",
      "Iteration 2743, loss = 0.00253919\n",
      "Iteration 2744, loss = 0.00253693\n",
      "Iteration 2745, loss = 0.00253468\n",
      "Iteration 2746, loss = 0.00253243\n",
      "Iteration 2747, loss = 0.00253019\n",
      "Iteration 2748, loss = 0.00252794\n",
      "Iteration 2749, loss = 0.00252570\n",
      "Iteration 2750, loss = 0.00252346\n",
      "Iteration 2751, loss = 0.00252122\n",
      "Iteration 2752, loss = 0.00251899\n",
      "Iteration 2753, loss = 0.00251676\n",
      "Iteration 2754, loss = 0.00251453\n",
      "Iteration 2755, loss = 0.00251231\n",
      "Iteration 2756, loss = 0.00251008\n",
      "Iteration 2757, loss = 0.00250786\n",
      "Iteration 2758, loss = 0.00250564\n",
      "Iteration 2759, loss = 0.00250343\n",
      "Iteration 2760, loss = 0.00250121\n",
      "Iteration 2761, loss = 0.00249900\n",
      "Iteration 2762, loss = 0.00249679\n",
      "Iteration 2763, loss = 0.00249459\n",
      "Iteration 2764, loss = 0.00249238\n",
      "Iteration 2765, loss = 0.00249018\n",
      "Iteration 2766, loss = 0.00248799\n",
      "Iteration 2767, loss = 0.00248579\n",
      "Iteration 2768, loss = 0.00248360\n",
      "Iteration 2769, loss = 0.00248141\n",
      "Iteration 2770, loss = 0.00247922\n",
      "Iteration 2771, loss = 0.00247703\n",
      "Iteration 2772, loss = 0.00247485\n",
      "Iteration 2773, loss = 0.00247267\n",
      "Iteration 2774, loss = 0.00247049\n",
      "Iteration 2775, loss = 0.00246831\n",
      "Iteration 2776, loss = 0.00246614\n",
      "Iteration 2777, loss = 0.00246397\n",
      "Iteration 2778, loss = 0.00246180\n",
      "Iteration 2779, loss = 0.00245964\n",
      "Iteration 2780, loss = 0.00245747\n",
      "Iteration 2781, loss = 0.00245531\n",
      "Iteration 2782, loss = 0.00245315\n",
      "Iteration 2783, loss = 0.00245100\n",
      "Iteration 2784, loss = 0.00244884\n",
      "Iteration 2785, loss = 0.00244669\n",
      "Iteration 2786, loss = 0.00244454\n",
      "Iteration 2787, loss = 0.00244240\n",
      "Iteration 2788, loss = 0.00244025\n",
      "Iteration 2789, loss = 0.00243811\n",
      "Iteration 2790, loss = 0.00243597\n",
      "Iteration 2791, loss = 0.00243384\n",
      "Iteration 2792, loss = 0.00243170\n",
      "Iteration 2793, loss = 0.00242957\n",
      "Iteration 2794, loss = 0.00242744\n",
      "Iteration 2795, loss = 0.00242532\n",
      "Iteration 2796, loss = 0.00242319\n",
      "Iteration 2797, loss = 0.00242107\n",
      "Iteration 2798, loss = 0.00241895\n",
      "Iteration 2799, loss = 0.00241683\n",
      "Iteration 2800, loss = 0.00241472\n",
      "Iteration 2801, loss = 0.00241261\n",
      "Iteration 2802, loss = 0.00241050\n",
      "Iteration 2803, loss = 0.00240839\n",
      "Iteration 2804, loss = 0.00240628\n",
      "Iteration 2805, loss = 0.00240418\n",
      "Iteration 2806, loss = 0.00240208\n",
      "Iteration 2807, loss = 0.00239998\n",
      "Iteration 2808, loss = 0.00239789\n",
      "Iteration 2809, loss = 0.00239579\n",
      "Iteration 2810, loss = 0.00239370\n",
      "Iteration 2811, loss = 0.00239161\n",
      "Iteration 2812, loss = 0.00238953\n",
      "Iteration 2813, loss = 0.00238744\n",
      "Iteration 2814, loss = 0.00238536\n",
      "Iteration 2815, loss = 0.00238328\n",
      "Iteration 2816, loss = 0.00238121\n",
      "Iteration 2817, loss = 0.00237913\n",
      "Iteration 2818, loss = 0.00237706\n",
      "Iteration 2819, loss = 0.00237499\n",
      "Iteration 2820, loss = 0.00237292\n",
      "Iteration 2821, loss = 0.00237086\n",
      "Iteration 2822, loss = 0.00236880\n",
      "Iteration 2823, loss = 0.00236673\n",
      "Iteration 2824, loss = 0.00236468\n",
      "Iteration 2825, loss = 0.00236262\n",
      "Iteration 2826, loss = 0.00236057\n",
      "Iteration 2827, loss = 0.00235852\n",
      "Iteration 2828, loss = 0.00235647\n",
      "Iteration 2829, loss = 0.00235442\n",
      "Iteration 2830, loss = 0.00235238\n",
      "Iteration 2831, loss = 0.00235033\n",
      "Iteration 2832, loss = 0.00234829\n",
      "Iteration 2833, loss = 0.00234626\n",
      "Iteration 2834, loss = 0.00234422\n",
      "Iteration 2835, loss = 0.00234219\n",
      "Iteration 2836, loss = 0.00234016\n",
      "Iteration 2837, loss = 0.00233813\n",
      "Iteration 2838, loss = 0.00233610\n",
      "Iteration 2839, loss = 0.00233408\n",
      "Iteration 2840, loss = 0.00233206\n",
      "Iteration 2841, loss = 0.00233004\n",
      "Iteration 2842, loss = 0.00232802\n",
      "Iteration 2843, loss = 0.00232601\n",
      "Iteration 2844, loss = 0.00232400\n",
      "Iteration 2845, loss = 0.00232199\n",
      "Iteration 2846, loss = 0.00231998\n",
      "Iteration 2847, loss = 0.00231797\n",
      "Iteration 2848, loss = 0.00231597\n",
      "Iteration 2849, loss = 0.00231397\n",
      "Iteration 2850, loss = 0.00231197\n",
      "Iteration 2851, loss = 0.00230997\n",
      "Iteration 2852, loss = 0.00230798\n",
      "Iteration 2853, loss = 0.00230598\n",
      "Iteration 2854, loss = 0.00230399\n",
      "Iteration 2855, loss = 0.00230201\n",
      "Iteration 2856, loss = 0.00230002\n",
      "Iteration 2857, loss = 0.00229804\n",
      "Iteration 2858, loss = 0.00229606\n",
      "Iteration 2859, loss = 0.00229408\n",
      "Iteration 2860, loss = 0.00229210\n",
      "Iteration 2861, loss = 0.00229013\n",
      "Iteration 2862, loss = 0.00228815\n",
      "Iteration 2863, loss = 0.00228618\n",
      "Iteration 2864, loss = 0.00228421\n",
      "Iteration 2865, loss = 0.00228225\n",
      "Iteration 2866, loss = 0.00228029\n",
      "Iteration 2867, loss = 0.00227832\n",
      "Iteration 2868, loss = 0.00227636\n",
      "Iteration 2869, loss = 0.00227441\n",
      "Iteration 2870, loss = 0.00227245\n",
      "Iteration 2871, loss = 0.00227050\n",
      "Iteration 2872, loss = 0.00226855\n",
      "Iteration 2873, loss = 0.00226660\n",
      "Iteration 2874, loss = 0.00226465\n",
      "Iteration 2875, loss = 0.00226271\n",
      "Iteration 2876, loss = 0.00226077\n",
      "Iteration 2877, loss = 0.00225883\n",
      "Iteration 2878, loss = 0.00225689\n",
      "Iteration 2879, loss = 0.00225496\n",
      "Iteration 2880, loss = 0.00225302\n",
      "Iteration 2881, loss = 0.00225109\n",
      "Iteration 2882, loss = 0.00224916\n",
      "Iteration 2883, loss = 0.00224723\n",
      "Iteration 2884, loss = 0.00224531\n",
      "Iteration 2885, loss = 0.00224339\n",
      "Iteration 2886, loss = 0.00224147\n",
      "Iteration 2887, loss = 0.00223955\n",
      "Iteration 2888, loss = 0.00223763\n",
      "Iteration 2889, loss = 0.00223572\n",
      "Iteration 2890, loss = 0.00223381\n",
      "Iteration 2891, loss = 0.00223190\n",
      "Iteration 2892, loss = 0.00222999\n",
      "Iteration 2893, loss = 0.00222808\n",
      "Iteration 2894, loss = 0.00222618\n",
      "Iteration 2895, loss = 0.00222428\n",
      "Iteration 2896, loss = 0.00222238\n",
      "Iteration 2897, loss = 0.00222048\n",
      "Iteration 2898, loss = 0.00221859\n",
      "Iteration 2899, loss = 0.00221669\n",
      "Iteration 2900, loss = 0.00221480\n",
      "Iteration 2901, loss = 0.00221291\n",
      "Iteration 2902, loss = 0.00221102\n",
      "Iteration 2903, loss = 0.00220914\n",
      "Iteration 2904, loss = 0.00220726\n",
      "Iteration 2905, loss = 0.00220538\n",
      "Iteration 2906, loss = 0.00220350\n",
      "Iteration 2907, loss = 0.00220162\n",
      "Iteration 2908, loss = 0.00219975\n",
      "Iteration 2909, loss = 0.00219787\n",
      "Iteration 2910, loss = 0.00219600\n",
      "Iteration 2911, loss = 0.00219414\n",
      "Iteration 2912, loss = 0.00219227\n",
      "Iteration 2913, loss = 0.00219041\n",
      "Iteration 2914, loss = 0.00218854\n",
      "Iteration 2915, loss = 0.00218668\n",
      "Iteration 2916, loss = 0.00218483\n",
      "Iteration 2917, loss = 0.00218297\n",
      "Iteration 2918, loss = 0.00218111\n",
      "Iteration 2919, loss = 0.00217926\n",
      "Iteration 2920, loss = 0.00217741\n",
      "Iteration 2921, loss = 0.00217557\n",
      "Iteration 2922, loss = 0.00217372\n",
      "Iteration 2923, loss = 0.00217188\n",
      "Iteration 2924, loss = 0.00217003\n",
      "Iteration 2925, loss = 0.00216819\n",
      "Iteration 2926, loss = 0.00216636\n",
      "Iteration 2927, loss = 0.00216452\n",
      "Iteration 2928, loss = 0.00216269\n",
      "Iteration 2929, loss = 0.00216085\n",
      "Iteration 2930, loss = 0.00215902\n",
      "Iteration 2931, loss = 0.00215720\n",
      "Iteration 2932, loss = 0.00215537\n",
      "Iteration 2933, loss = 0.00215355\n",
      "Iteration 2934, loss = 0.00215173\n",
      "Iteration 2935, loss = 0.00214991\n",
      "Iteration 2936, loss = 0.00214809\n",
      "Iteration 2937, loss = 0.00214627\n",
      "Iteration 2938, loss = 0.00214446\n",
      "Iteration 2939, loss = 0.00214265\n",
      "Iteration 2940, loss = 0.00214084\n",
      "Iteration 2941, loss = 0.00213903\n",
      "Iteration 2942, loss = 0.00213722\n",
      "Iteration 2943, loss = 0.00213542\n",
      "Iteration 2944, loss = 0.00213362\n",
      "Iteration 2945, loss = 0.00213182\n",
      "Iteration 2946, loss = 0.00213002\n",
      "Iteration 2947, loss = 0.00212822\n",
      "Iteration 2948, loss = 0.00212643\n",
      "Iteration 2949, loss = 0.00212464\n",
      "Iteration 2950, loss = 0.00212285\n",
      "Iteration 2951, loss = 0.00212106\n",
      "Iteration 2952, loss = 0.00211927\n",
      "Iteration 2953, loss = 0.00211749\n",
      "Iteration 2954, loss = 0.00211570\n",
      "Iteration 2955, loss = 0.00211392\n",
      "Iteration 2956, loss = 0.00211214\n",
      "Iteration 2957, loss = 0.00211037\n",
      "Iteration 2958, loss = 0.00210859\n",
      "Iteration 2959, loss = 0.00210682\n",
      "Iteration 2960, loss = 0.00210505\n",
      "Iteration 2961, loss = 0.00210328\n",
      "Iteration 2962, loss = 0.00210151\n",
      "Iteration 2963, loss = 0.00209975\n",
      "Iteration 2964, loss = 0.00209798\n",
      "Iteration 2965, loss = 0.00209622\n",
      "Iteration 2966, loss = 0.00209446\n",
      "Iteration 2967, loss = 0.00209271\n",
      "Iteration 2968, loss = 0.00209095\n",
      "Iteration 2969, loss = 0.00208920\n",
      "Iteration 2970, loss = 0.00208744\n",
      "Iteration 2971, loss = 0.00208569\n",
      "Iteration 2972, loss = 0.00208395\n",
      "Iteration 2973, loss = 0.00208220\n",
      "Iteration 2974, loss = 0.00208046\n",
      "Iteration 2975, loss = 0.00207871\n",
      "Iteration 2976, loss = 0.00207697\n",
      "Iteration 2977, loss = 0.00207523\n",
      "Iteration 2978, loss = 0.00207350\n",
      "Iteration 2979, loss = 0.00207176\n",
      "Iteration 2980, loss = 0.00207003\n",
      "Iteration 2981, loss = 0.00206830\n",
      "Iteration 2982, loss = 0.00206657\n",
      "Iteration 2983, loss = 0.00206484\n",
      "Iteration 2984, loss = 0.00206312\n",
      "Iteration 2985, loss = 0.00206139\n",
      "Iteration 2986, loss = 0.00205967\n",
      "Iteration 2987, loss = 0.00205795\n",
      "Iteration 2988, loss = 0.00205623\n",
      "Iteration 2989, loss = 0.00205452\n",
      "Iteration 2990, loss = 0.00205280\n",
      "Iteration 2991, loss = 0.00205109\n",
      "Iteration 2992, loss = 0.00204938\n",
      "Iteration 2993, loss = 0.00204767\n",
      "Iteration 2994, loss = 0.00204596\n",
      "Iteration 2995, loss = 0.00204426\n",
      "Iteration 2996, loss = 0.00204255\n",
      "Iteration 2997, loss = 0.00204085\n",
      "Iteration 2998, loss = 0.00203915\n",
      "Iteration 2999, loss = 0.00203745\n",
      "Iteration 3000, loss = 0.00203576\n",
      "Iteration 3001, loss = 0.00203406\n",
      "Iteration 3002, loss = 0.00203237\n",
      "Iteration 3003, loss = 0.00203068\n",
      "Iteration 3004, loss = 0.00202899\n",
      "Iteration 3005, loss = 0.00202730\n",
      "Iteration 3006, loss = 0.00202562\n",
      "Iteration 3007, loss = 0.00202393\n",
      "Iteration 3008, loss = 0.00202225\n",
      "Iteration 3009, loss = 0.00202057\n",
      "Iteration 3010, loss = 0.00201890\n",
      "Iteration 3011, loss = 0.00201722\n",
      "Iteration 3012, loss = 0.00201554\n",
      "Iteration 3013, loss = 0.00201387\n",
      "Iteration 3014, loss = 0.00201220\n",
      "Iteration 3015, loss = 0.00201053\n",
      "Iteration 3016, loss = 0.00200886\n",
      "Iteration 3017, loss = 0.00200720\n",
      "Iteration 3018, loss = 0.00200554\n",
      "Iteration 3019, loss = 0.00200387\n",
      "Iteration 3020, loss = 0.00200221\n",
      "Iteration 3021, loss = 0.00200055\n",
      "Iteration 3022, loss = 0.00199890\n",
      "Iteration 3023, loss = 0.00199724\n",
      "Iteration 3024, loss = 0.00199559\n",
      "Iteration 3025, loss = 0.00199394\n",
      "Iteration 3026, loss = 0.00199229\n",
      "Iteration 3027, loss = 0.00199064\n",
      "Iteration 3028, loss = 0.00198900\n",
      "Iteration 3029, loss = 0.00198735\n",
      "Iteration 3030, loss = 0.00198571\n",
      "Iteration 3031, loss = 0.00198407\n",
      "Iteration 3032, loss = 0.00198243\n",
      "Iteration 3033, loss = 0.00198079\n",
      "Iteration 3034, loss = 0.00197916\n",
      "Iteration 3035, loss = 0.00197752\n",
      "Iteration 3036, loss = 0.00197589\n",
      "Iteration 3037, loss = 0.00197426\n",
      "Iteration 3038, loss = 0.00197263\n",
      "Iteration 3039, loss = 0.00197100\n",
      "Iteration 3040, loss = 0.00196938\n",
      "Iteration 3041, loss = 0.00196775\n",
      "Iteration 3042, loss = 0.00196613\n",
      "Iteration 3043, loss = 0.00196451\n",
      "Iteration 3044, loss = 0.00196289\n",
      "Iteration 3045, loss = 0.00196128\n",
      "Iteration 3046, loss = 0.00195966\n",
      "Iteration 3047, loss = 0.00195805\n",
      "Iteration 3048, loss = 0.00195644\n",
      "Iteration 3049, loss = 0.00195483\n",
      "Iteration 3050, loss = 0.00195322\n",
      "Iteration 3051, loss = 0.00195161\n",
      "Iteration 3052, loss = 0.00195001\n",
      "Iteration 3053, loss = 0.00194841\n",
      "Iteration 3054, loss = 0.00194681\n",
      "Iteration 3055, loss = 0.00194521\n",
      "Iteration 3056, loss = 0.00194361\n",
      "Iteration 3057, loss = 0.00194201\n",
      "Iteration 3058, loss = 0.00194042\n",
      "Iteration 3059, loss = 0.00193882\n",
      "Iteration 3060, loss = 0.00193723\n",
      "Iteration 3061, loss = 0.00193564\n",
      "Iteration 3062, loss = 0.00193406\n",
      "Iteration 3063, loss = 0.00193247\n",
      "Iteration 3064, loss = 0.00193089\n",
      "Iteration 3065, loss = 0.00192930\n",
      "Iteration 3066, loss = 0.00192772\n",
      "Iteration 3067, loss = 0.00192614\n",
      "Iteration 3068, loss = 0.00192456\n",
      "Iteration 3069, loss = 0.00192299\n",
      "Iteration 3070, loss = 0.00192141\n",
      "Iteration 3071, loss = 0.00191984\n",
      "Iteration 3072, loss = 0.00191827\n",
      "Iteration 3073, loss = 0.00191670\n",
      "Iteration 3074, loss = 0.00191513\n",
      "Iteration 3075, loss = 0.00191357\n",
      "Iteration 3076, loss = 0.00191200\n",
      "Iteration 3077, loss = 0.00191044\n",
      "Iteration 3078, loss = 0.00190888\n",
      "Iteration 3079, loss = 0.00190732\n",
      "Iteration 3080, loss = 0.00190576\n",
      "Iteration 3081, loss = 0.00190420\n",
      "Iteration 3082, loss = 0.00190265\n",
      "Iteration 3083, loss = 0.00190110\n",
      "Iteration 3084, loss = 0.00189955\n",
      "Iteration 3085, loss = 0.00189800\n",
      "Iteration 3086, loss = 0.00189645\n",
      "Iteration 3087, loss = 0.00189490\n",
      "Iteration 3088, loss = 0.00189336\n",
      "Iteration 3089, loss = 0.00189181\n",
      "Iteration 3090, loss = 0.00189027\n",
      "Iteration 3091, loss = 0.00188873\n",
      "Iteration 3092, loss = 0.00188719\n",
      "Iteration 3093, loss = 0.00188566\n",
      "Iteration 3094, loss = 0.00188412\n",
      "Iteration 3095, loss = 0.00188259\n",
      "Iteration 3096, loss = 0.00188106\n",
      "Iteration 3097, loss = 0.00187953\n",
      "Iteration 3098, loss = 0.00187800\n",
      "Iteration 3099, loss = 0.00187647\n",
      "Iteration 3100, loss = 0.00187494\n",
      "Iteration 3101, loss = 0.00187342\n",
      "Iteration 3102, loss = 0.00187190\n",
      "Iteration 3103, loss = 0.00187038\n",
      "Iteration 3104, loss = 0.00186886\n",
      "Iteration 3105, loss = 0.00186734\n",
      "Iteration 3106, loss = 0.00186582\n",
      "Iteration 3107, loss = 0.00186431\n",
      "Iteration 3108, loss = 0.00186280\n",
      "Iteration 3109, loss = 0.00186129\n",
      "Iteration 3110, loss = 0.00185978\n",
      "Iteration 3111, loss = 0.00185827\n",
      "Iteration 3112, loss = 0.00185676\n",
      "Iteration 3113, loss = 0.00185526\n",
      "Iteration 3114, loss = 0.00185375\n",
      "Iteration 3115, loss = 0.00185225\n",
      "Iteration 3116, loss = 0.00185075\n",
      "Iteration 3117, loss = 0.00184925\n",
      "Iteration 3118, loss = 0.00184776\n",
      "Iteration 3119, loss = 0.00184626\n",
      "Iteration 3120, loss = 0.00184477\n",
      "Iteration 3121, loss = 0.00184327\n",
      "Iteration 3122, loss = 0.00184178\n",
      "Iteration 3123, loss = 0.00184029\n",
      "Iteration 3124, loss = 0.00183880\n",
      "Iteration 3125, loss = 0.00183732\n",
      "Iteration 3126, loss = 0.00183583\n",
      "Iteration 3127, loss = 0.00183435\n",
      "Iteration 3128, loss = 0.00183287\n",
      "Iteration 3129, loss = 0.00183139\n",
      "Iteration 3130, loss = 0.00182991\n",
      "Iteration 3131, loss = 0.00182843\n",
      "Iteration 3132, loss = 0.00182696\n",
      "Iteration 3133, loss = 0.00182548\n",
      "Iteration 3134, loss = 0.00182401\n",
      "Iteration 3135, loss = 0.00182254\n",
      "Iteration 3136, loss = 0.00182107\n",
      "Iteration 3137, loss = 0.00181960\n",
      "Iteration 3138, loss = 0.00181814\n",
      "Iteration 3139, loss = 0.00181667\n",
      "Iteration 3140, loss = 0.00181521\n",
      "Iteration 3141, loss = 0.00181375\n",
      "Iteration 3142, loss = 0.00181229\n",
      "Iteration 3143, loss = 0.00181083\n",
      "Iteration 3144, loss = 0.00180937\n",
      "Iteration 3145, loss = 0.00180792\n",
      "Iteration 3146, loss = 0.00180646\n",
      "Iteration 3147, loss = 0.00180501\n",
      "Iteration 3148, loss = 0.00180356\n",
      "Iteration 3149, loss = 0.00180211\n",
      "Iteration 3150, loss = 0.00180066\n",
      "Iteration 3151, loss = 0.00179921\n",
      "Iteration 3152, loss = 0.00179777\n",
      "Iteration 3153, loss = 0.00179632\n",
      "Iteration 3154, loss = 0.00179488\n",
      "Iteration 3155, loss = 0.00179344\n",
      "Iteration 3156, loss = 0.00179200\n",
      "Iteration 3157, loss = 0.00179056\n",
      "Iteration 3158, loss = 0.00178913\n",
      "Iteration 3159, loss = 0.00178769\n",
      "Iteration 3160, loss = 0.00178626\n",
      "Iteration 3161, loss = 0.00178483\n",
      "Iteration 3162, loss = 0.00178340\n",
      "Iteration 3163, loss = 0.00178197\n",
      "Iteration 3164, loss = 0.00178054\n",
      "Iteration 3165, loss = 0.00177911\n",
      "Iteration 3166, loss = 0.00177769\n",
      "Iteration 3167, loss = 0.00177627\n",
      "Iteration 3168, loss = 0.00177484\n",
      "Iteration 3169, loss = 0.00177342\n",
      "Iteration 3170, loss = 0.00177201\n",
      "Iteration 3171, loss = 0.00177059\n",
      "Iteration 3172, loss = 0.00176917\n",
      "Iteration 3173, loss = 0.00176776\n",
      "Iteration 3174, loss = 0.00176635\n",
      "Iteration 3175, loss = 0.00176493\n",
      "Iteration 3176, loss = 0.00176352\n",
      "Iteration 3177, loss = 0.00176212\n",
      "Iteration 3178, loss = 0.00176071\n",
      "Iteration 3179, loss = 0.00175930\n",
      "Iteration 3180, loss = 0.00175790\n",
      "Iteration 3181, loss = 0.00175650\n",
      "Iteration 3182, loss = 0.00175510\n",
      "Iteration 3183, loss = 0.00175370\n",
      "Iteration 3184, loss = 0.00175230\n",
      "Iteration 3185, loss = 0.00175090\n",
      "Iteration 3186, loss = 0.00174950\n",
      "Iteration 3187, loss = 0.00174811\n",
      "Iteration 3188, loss = 0.00174672\n",
      "Iteration 3189, loss = 0.00174533\n",
      "Iteration 3190, loss = 0.00174394\n",
      "Iteration 3191, loss = 0.00174255\n",
      "Iteration 3192, loss = 0.00174116\n",
      "Iteration 3193, loss = 0.00173978\n",
      "Iteration 3194, loss = 0.00173839\n",
      "Iteration 3195, loss = 0.00173701\n",
      "Iteration 3196, loss = 0.00173563\n",
      "Iteration 3197, loss = 0.00173425\n",
      "Iteration 3198, loss = 0.00173287\n",
      "Iteration 3199, loss = 0.00173149\n",
      "Iteration 3200, loss = 0.00173012\n",
      "Iteration 3201, loss = 0.00172874\n",
      "Iteration 3202, loss = 0.00172737\n",
      "Iteration 3203, loss = 0.00172600\n",
      "Iteration 3204, loss = 0.00172463\n",
      "Iteration 3205, loss = 0.00172326\n",
      "Iteration 3206, loss = 0.00172189\n",
      "Iteration 3207, loss = 0.00172053\n",
      "Iteration 3208, loss = 0.00171916\n",
      "Iteration 3209, loss = 0.00171780\n",
      "Iteration 3210, loss = 0.00171644\n",
      "Iteration 3211, loss = 0.00171508\n",
      "Iteration 3212, loss = 0.00171372\n",
      "Iteration 3213, loss = 0.00171236\n",
      "Iteration 3214, loss = 0.00171100\n",
      "Iteration 3215, loss = 0.00170965\n",
      "Iteration 3216, loss = 0.00170829\n",
      "Iteration 3217, loss = 0.00170694\n",
      "Iteration 3218, loss = 0.00170559\n",
      "Iteration 3219, loss = 0.00170424\n",
      "Iteration 3220, loss = 0.00170289\n",
      "Iteration 3221, loss = 0.00170155\n",
      "Iteration 3222, loss = 0.00170020\n",
      "Iteration 3223, loss = 0.00169886\n",
      "Iteration 3224, loss = 0.00169752\n",
      "Iteration 3225, loss = 0.00169617\n",
      "Iteration 3226, loss = 0.00169484\n",
      "Iteration 3227, loss = 0.00169350\n",
      "Iteration 3228, loss = 0.00169216\n",
      "Iteration 3229, loss = 0.00169082\n",
      "Iteration 3230, loss = 0.00168949\n",
      "Iteration 3231, loss = 0.00168816\n",
      "Iteration 3232, loss = 0.00168683\n",
      "Iteration 3233, loss = 0.00168549\n",
      "Iteration 3234, loss = 0.00168417\n",
      "Iteration 3235, loss = 0.00168284\n",
      "Iteration 3236, loss = 0.00168151\n",
      "Iteration 3237, loss = 0.00168019\n",
      "Iteration 3238, loss = 0.00167886\n",
      "Iteration 3239, loss = 0.00167754\n",
      "Iteration 3240, loss = 0.00167622\n",
      "Iteration 3241, loss = 0.00167490\n",
      "Iteration 3242, loss = 0.00167358\n",
      "Iteration 3243, loss = 0.00167227\n",
      "Iteration 3244, loss = 0.00167095\n",
      "Iteration 3245, loss = 0.00166964\n",
      "Iteration 3246, loss = 0.00166832\n",
      "Iteration 3247, loss = 0.00166701\n",
      "Iteration 3248, loss = 0.00166570\n",
      "Iteration 3249, loss = 0.00166439\n",
      "Iteration 3250, loss = 0.00166308\n",
      "Iteration 3251, loss = 0.00166178\n",
      "Iteration 3252, loss = 0.00166047\n",
      "Iteration 3253, loss = 0.00165917\n",
      "Iteration 3254, loss = 0.00165787\n",
      "Iteration 3255, loss = 0.00165657\n",
      "Iteration 3256, loss = 0.00165527\n",
      "Iteration 3257, loss = 0.00165397\n",
      "Iteration 3258, loss = 0.00165267\n",
      "Iteration 3259, loss = 0.00165138\n",
      "Iteration 3260, loss = 0.00165008\n",
      "Iteration 3261, loss = 0.00164879\n",
      "Iteration 3262, loss = 0.00164750\n",
      "Iteration 3263, loss = 0.00164621\n",
      "Iteration 3264, loss = 0.00164492\n",
      "Iteration 3265, loss = 0.00164363\n",
      "Iteration 3266, loss = 0.00164234\n",
      "Iteration 3267, loss = 0.00164106\n",
      "Iteration 3268, loss = 0.00163977\n",
      "Iteration 3269, loss = 0.00163849\n",
      "Iteration 3270, loss = 0.00163721\n",
      "Iteration 3271, loss = 0.00163593\n",
      "Iteration 3272, loss = 0.00163465\n",
      "Iteration 3273, loss = 0.00163337\n",
      "Iteration 3274, loss = 0.00163209\n",
      "Iteration 3275, loss = 0.00163082\n",
      "Iteration 3276, loss = 0.00162955\n",
      "Iteration 3277, loss = 0.00162827\n",
      "Iteration 3278, loss = 0.00162700\n",
      "Iteration 3279, loss = 0.00162573\n",
      "Iteration 3280, loss = 0.00162446\n",
      "Iteration 3281, loss = 0.00162320\n",
      "Iteration 3282, loss = 0.00162193\n",
      "Iteration 3283, loss = 0.00162066\n",
      "Iteration 3284, loss = 0.00161940\n",
      "Iteration 3285, loss = 0.00161814\n",
      "Iteration 3286, loss = 0.00161688\n",
      "Iteration 3287, loss = 0.00161562\n",
      "Iteration 3288, loss = 0.00161436\n",
      "Iteration 3289, loss = 0.00161310\n",
      "Iteration 3290, loss = 0.00161185\n",
      "Iteration 3291, loss = 0.00161059\n",
      "Iteration 3292, loss = 0.00160934\n",
      "Iteration 3293, loss = 0.00160808\n",
      "Iteration 3294, loss = 0.00160683\n",
      "Iteration 3295, loss = 0.00160558\n",
      "Iteration 3296, loss = 0.00160433\n",
      "Iteration 3297, loss = 0.00160309\n",
      "Iteration 3298, loss = 0.00160184\n",
      "Iteration 3299, loss = 0.00160060\n",
      "Iteration 3300, loss = 0.00159935\n",
      "Iteration 3301, loss = 0.00159811\n",
      "Iteration 3302, loss = 0.00159687\n",
      "Iteration 3303, loss = 0.00159563\n",
      "Iteration 3304, loss = 0.00159439\n",
      "Iteration 3305, loss = 0.00159315\n",
      "Iteration 3306, loss = 0.00159192\n",
      "Iteration 3307, loss = 0.00159068\n",
      "Iteration 3308, loss = 0.00158945\n",
      "Iteration 3309, loss = 0.00158822\n",
      "Iteration 3310, loss = 0.00158698\n",
      "Iteration 3311, loss = 0.00158575\n",
      "Iteration 3312, loss = 0.00158453\n",
      "Iteration 3313, loss = 0.00158330\n",
      "Iteration 3314, loss = 0.00158207\n",
      "Iteration 3315, loss = 0.00158085\n",
      "Iteration 3316, loss = 0.00157962\n",
      "Iteration 3317, loss = 0.00157840\n",
      "Iteration 3318, loss = 0.00157718\n",
      "Iteration 3319, loss = 0.00157596\n",
      "Iteration 3320, loss = 0.00157474\n",
      "Iteration 3321, loss = 0.00157352\n",
      "Iteration 3322, loss = 0.00157230\n",
      "Iteration 3323, loss = 0.00157109\n",
      "Iteration 3324, loss = 0.00156988\n",
      "Iteration 3325, loss = 0.00156866\n",
      "Iteration 3326, loss = 0.00156745\n",
      "Iteration 3327, loss = 0.00156624\n",
      "Iteration 3328, loss = 0.00156503\n",
      "Iteration 3329, loss = 0.00156382\n",
      "Iteration 3330, loss = 0.00156261\n",
      "Iteration 3331, loss = 0.00156141\n",
      "Iteration 3332, loss = 0.00156020\n",
      "Iteration 3333, loss = 0.00155900\n",
      "Iteration 3334, loss = 0.00155780\n",
      "Iteration 3335, loss = 0.00155660\n",
      "Iteration 3336, loss = 0.00155540\n",
      "Iteration 3337, loss = 0.00155420\n",
      "Iteration 3338, loss = 0.00155300\n",
      "Iteration 3339, loss = 0.00155180\n",
      "Iteration 3340, loss = 0.00155061\n",
      "Iteration 3341, loss = 0.00154942\n",
      "Iteration 3342, loss = 0.00154822\n",
      "Iteration 3343, loss = 0.00154703\n",
      "Iteration 3344, loss = 0.00154584\n",
      "Iteration 3345, loss = 0.00154465\n",
      "Iteration 3346, loss = 0.00154346\n",
      "Iteration 3347, loss = 0.00154228\n",
      "Iteration 3348, loss = 0.00154109\n",
      "Iteration 3349, loss = 0.00153991\n",
      "Iteration 3350, loss = 0.00153872\n",
      "Iteration 3351, loss = 0.00153754\n",
      "Iteration 3352, loss = 0.00153636\n",
      "Iteration 3353, loss = 0.00153518\n",
      "Iteration 3354, loss = 0.00153400\n",
      "Iteration 3355, loss = 0.00153282\n",
      "Iteration 3356, loss = 0.00153165\n",
      "Iteration 3357, loss = 0.00153047\n",
      "Iteration 3358, loss = 0.00152930\n",
      "Iteration 3359, loss = 0.00152812\n",
      "Iteration 3360, loss = 0.00152695\n",
      "Iteration 3361, loss = 0.00152578\n",
      "Iteration 3362, loss = 0.00152461\n",
      "Iteration 3363, loss = 0.00152344\n",
      "Iteration 3364, loss = 0.00152228\n",
      "Iteration 3365, loss = 0.00152111\n",
      "Iteration 3366, loss = 0.00151995\n",
      "Iteration 3367, loss = 0.00151878\n",
      "Iteration 3368, loss = 0.00151762\n",
      "Iteration 3369, loss = 0.00151646\n",
      "Iteration 3370, loss = 0.00151530\n",
      "Iteration 3371, loss = 0.00151414\n",
      "Iteration 3372, loss = 0.00151298\n",
      "Iteration 3373, loss = 0.00151182\n",
      "Iteration 3374, loss = 0.00151067\n",
      "Iteration 3375, loss = 0.00150951\n",
      "Iteration 3376, loss = 0.00150836\n",
      "Iteration 3377, loss = 0.00150721\n",
      "Iteration 3378, loss = 0.00150606\n",
      "Iteration 3379, loss = 0.00150491\n",
      "Iteration 3380, loss = 0.00150376\n",
      "Iteration 3381, loss = 0.00150261\n",
      "Iteration 3382, loss = 0.00150146\n",
      "Iteration 3383, loss = 0.00150032\n",
      "Iteration 3384, loss = 0.00149917\n",
      "Iteration 3385, loss = 0.00149803\n",
      "Iteration 3386, loss = 0.00149689\n",
      "Iteration 3387, loss = 0.00149574\n",
      "Iteration 3388, loss = 0.00149460\n",
      "Iteration 3389, loss = 0.00149347\n",
      "Iteration 3390, loss = 0.00149233\n",
      "Iteration 3391, loss = 0.00149119\n",
      "Iteration 3392, loss = 0.00149006\n",
      "Iteration 3393, loss = 0.00148892\n",
      "Iteration 3394, loss = 0.00148779\n",
      "Iteration 3395, loss = 0.00148666\n",
      "Iteration 3396, loss = 0.00148552\n",
      "Iteration 3397, loss = 0.00148439\n",
      "Iteration 3398, loss = 0.00148327\n",
      "Iteration 3399, loss = 0.00148214\n",
      "Iteration 3400, loss = 0.00148101\n",
      "Iteration 3401, loss = 0.00147989\n",
      "Iteration 3402, loss = 0.00147876\n",
      "Iteration 3403, loss = 0.00147764\n",
      "Iteration 3404, loss = 0.00147651\n",
      "Iteration 3405, loss = 0.00147539\n",
      "Iteration 3406, loss = 0.00147427\n",
      "Iteration 3407, loss = 0.00147315\n",
      "Iteration 3408, loss = 0.00147204\n",
      "Iteration 3409, loss = 0.00147092\n",
      "Iteration 3410, loss = 0.00146980\n",
      "Iteration 3411, loss = 0.00146869\n",
      "Iteration 3412, loss = 0.00146757\n",
      "Iteration 3413, loss = 0.00146646\n",
      "Iteration 3414, loss = 0.00146535\n",
      "Iteration 3415, loss = 0.00146424\n",
      "Iteration 3416, loss = 0.00146313\n",
      "Iteration 3417, loss = 0.00146202\n",
      "Iteration 3418, loss = 0.00146092\n",
      "Iteration 3419, loss = 0.00145981\n",
      "Iteration 3420, loss = 0.00145870\n",
      "Iteration 3421, loss = 0.00145760\n",
      "Iteration 3422, loss = 0.00145650\n",
      "Iteration 3423, loss = 0.00145540\n",
      "Iteration 3424, loss = 0.00145429\n",
      "Iteration 3425, loss = 0.00145320\n",
      "Iteration 3426, loss = 0.00145210\n",
      "Iteration 3427, loss = 0.00145100\n",
      "Iteration 3428, loss = 0.00144990\n",
      "Iteration 3429, loss = 0.00144881\n",
      "Iteration 3430, loss = 0.00144771\n",
      "Iteration 3431, loss = 0.00144662\n",
      "Iteration 3432, loss = 0.00144553\n",
      "Iteration 3433, loss = 0.00144444\n",
      "Iteration 3434, loss = 0.00144335\n",
      "Iteration 3435, loss = 0.00144226\n",
      "Iteration 3436, loss = 0.00144117\n",
      "Iteration 3437, loss = 0.00144008\n",
      "Iteration 3438, loss = 0.00143900\n",
      "Iteration 3439, loss = 0.00143791\n",
      "Iteration 3440, loss = 0.00143683\n",
      "Iteration 3441, loss = 0.00143574\n",
      "Iteration 3442, loss = 0.00143466\n",
      "Iteration 3443, loss = 0.00143358\n",
      "Iteration 3444, loss = 0.00143250\n",
      "Iteration 3445, loss = 0.00143142\n",
      "Iteration 3446, loss = 0.00143035\n",
      "Iteration 3447, loss = 0.00142927\n",
      "Iteration 3448, loss = 0.00142819\n",
      "Iteration 3449, loss = 0.00142712\n",
      "Iteration 3450, loss = 0.00142605\n",
      "Iteration 3451, loss = 0.00142497\n",
      "Iteration 3452, loss = 0.00142390\n",
      "Iteration 3453, loss = 0.00142283\n",
      "Iteration 3454, loss = 0.00142176\n",
      "Iteration 3455, loss = 0.00142069\n",
      "Iteration 3456, loss = 0.00141963\n",
      "Iteration 3457, loss = 0.00141856\n",
      "Iteration 3458, loss = 0.00141750\n",
      "Iteration 3459, loss = 0.00141643\n",
      "Iteration 3460, loss = 0.00141537\n",
      "Iteration 3461, loss = 0.00141431\n",
      "Iteration 3462, loss = 0.00141325\n",
      "Iteration 3463, loss = 0.00141219\n",
      "Iteration 3464, loss = 0.00141113\n",
      "Iteration 3465, loss = 0.00141007\n",
      "Iteration 3466, loss = 0.00140901\n",
      "Iteration 3467, loss = 0.00140796\n",
      "Iteration 3468, loss = 0.00140690\n",
      "Iteration 3469, loss = 0.00140585\n",
      "Iteration 3470, loss = 0.00140479\n",
      "Iteration 3471, loss = 0.00140374\n",
      "Iteration 3472, loss = 0.00140269\n",
      "Iteration 3473, loss = 0.00140164\n",
      "Iteration 3474, loss = 0.00140059\n",
      "Iteration 3475, loss = 0.00139954\n",
      "Iteration 3476, loss = 0.00139850\n",
      "Iteration 3477, loss = 0.00139745\n",
      "Iteration 3478, loss = 0.00139641\n",
      "Iteration 3479, loss = 0.00139536\n",
      "Iteration 3480, loss = 0.00139432\n",
      "Iteration 3481, loss = 0.00139328\n",
      "Iteration 3482, loss = 0.00139224\n",
      "Iteration 3483, loss = 0.00139120\n",
      "Iteration 3484, loss = 0.00139016\n",
      "Iteration 3485, loss = 0.00138912\n",
      "Iteration 3486, loss = 0.00138808\n",
      "Iteration 3487, loss = 0.00138705\n",
      "Iteration 3488, loss = 0.00138601\n",
      "Iteration 3489, loss = 0.00138498\n",
      "Iteration 3490, loss = 0.00138395\n",
      "Iteration 3491, loss = 0.00138291\n",
      "Iteration 3492, loss = 0.00138188\n",
      "Iteration 3493, loss = 0.00138085\n",
      "Iteration 3494, loss = 0.00137982\n",
      "Iteration 3495, loss = 0.00137880\n",
      "Iteration 3496, loss = 0.00137777\n",
      "Iteration 3497, loss = 0.00137674\n",
      "Iteration 3498, loss = 0.00137572\n",
      "Iteration 3499, loss = 0.00137469\n",
      "Iteration 3500, loss = 0.00137367\n",
      "Iteration 3501, loss = 0.00137265\n",
      "Iteration 3502, loss = 0.00137163\n",
      "Iteration 3503, loss = 0.00137061\n",
      "Iteration 3504, loss = 0.00136959\n",
      "Iteration 3505, loss = 0.00136857\n",
      "Iteration 3506, loss = 0.00136755\n",
      "Iteration 3507, loss = 0.00136654\n",
      "Iteration 3508, loss = 0.00136552\n",
      "Iteration 3509, loss = 0.00136451\n",
      "Iteration 3510, loss = 0.00136349\n",
      "Iteration 3511, loss = 0.00136248\n",
      "Iteration 3512, loss = 0.00136147\n",
      "Iteration 3513, loss = 0.00136046\n",
      "Iteration 3514, loss = 0.00135945\n",
      "Iteration 3515, loss = 0.00135844\n",
      "Iteration 3516, loss = 0.00135743\n",
      "Iteration 3517, loss = 0.00135643\n",
      "Iteration 3518, loss = 0.00135542\n",
      "Iteration 3519, loss = 0.00135442\n",
      "Iteration 3520, loss = 0.00135341\n",
      "Iteration 3521, loss = 0.00135241\n",
      "Iteration 3522, loss = 0.00135141\n",
      "Iteration 3523, loss = 0.00135041\n",
      "Iteration 3524, loss = 0.00134941\n",
      "Iteration 3525, loss = 0.00134841\n",
      "Iteration 3526, loss = 0.00134741\n",
      "Iteration 3527, loss = 0.00134641\n",
      "Iteration 3528, loss = 0.00134542\n",
      "Iteration 3529, loss = 0.00134442\n",
      "Iteration 3530, loss = 0.00134343\n",
      "Iteration 3531, loss = 0.00134243\n",
      "Iteration 3532, loss = 0.00134144\n",
      "Iteration 3533, loss = 0.00134045\n",
      "Iteration 3534, loss = 0.00133946\n",
      "Training loss did not improve more than tol=0.000001 for 10 consecutive epochs. Stopping.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00        14\n",
      "           2       0.95      1.00      0.98        20\n",
      "           3       1.00      0.95      0.97        20\n",
      "\n",
      "    accuracy                           0.98        54\n",
      "   macro avg       0.98      0.98      0.98        54\n",
      "weighted avg       0.98      0.98      0.98        54\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "hidden_layers = (30,)\n",
    "activation = 'logistic'\n",
    "opts = dict(hidden_layer_sizes = hidden_layers,verbose = True,activation = activation,tol = 1e-6,max_iter = int(1e6))\n",
    "solver = 'adam'\n",
    "clf_MLP = MLPClassifier(solver = solver, **opts)\n",
    "clf_MLP.fit(X_train_,y_train)\n",
    "predictions = clf_MLP.predict(X_test_)\n",
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- hidden_layers = (30,) , activation = 'relu', solver = 'adam'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.61379971\n",
      "Iteration 2, loss = 1.58443495\n",
      "Iteration 3, loss = 1.55547224\n",
      "Iteration 4, loss = 1.52710410\n",
      "Iteration 5, loss = 1.49917418\n",
      "Iteration 6, loss = 1.47166037\n",
      "Iteration 7, loss = 1.44459367\n",
      "Iteration 8, loss = 1.41793431\n",
      "Iteration 9, loss = 1.39169964\n",
      "Iteration 10, loss = 1.36597658\n",
      "Iteration 11, loss = 1.34079763\n",
      "Iteration 12, loss = 1.31608679\n",
      "Iteration 13, loss = 1.29179000\n",
      "Iteration 14, loss = 1.26783615\n",
      "Iteration 15, loss = 1.24434067\n",
      "Iteration 16, loss = 1.22125097\n",
      "Iteration 17, loss = 1.19864769\n",
      "Iteration 18, loss = 1.17654330\n",
      "Iteration 19, loss = 1.15488640\n",
      "Iteration 20, loss = 1.13361936\n",
      "Iteration 21, loss = 1.11272568\n",
      "Iteration 22, loss = 1.09217938\n",
      "Iteration 23, loss = 1.07203572\n",
      "Iteration 24, loss = 1.05233570\n",
      "Iteration 25, loss = 1.03295441\n",
      "Iteration 26, loss = 1.01393830\n",
      "Iteration 27, loss = 0.99532049\n",
      "Iteration 28, loss = 0.97710473\n",
      "Iteration 29, loss = 0.95925667\n",
      "Iteration 30, loss = 0.94174553\n",
      "Iteration 31, loss = 0.92452875\n",
      "Iteration 32, loss = 0.90761929\n",
      "Iteration 33, loss = 0.89095241\n",
      "Iteration 34, loss = 0.87460796\n",
      "Iteration 35, loss = 0.85852248\n",
      "Iteration 36, loss = 0.84275015\n",
      "Iteration 37, loss = 0.82721936\n",
      "Iteration 38, loss = 0.81194318\n",
      "Iteration 39, loss = 0.79692034\n",
      "Iteration 40, loss = 0.78217840\n",
      "Iteration 41, loss = 0.76775351\n",
      "Iteration 42, loss = 0.75360353\n",
      "Iteration 43, loss = 0.73968895\n",
      "Iteration 44, loss = 0.72607841\n",
      "Iteration 45, loss = 0.71275179\n",
      "Iteration 46, loss = 0.69974144\n",
      "Iteration 47, loss = 0.68701328\n",
      "Iteration 48, loss = 0.67455964\n",
      "Iteration 49, loss = 0.66238210\n",
      "Iteration 50, loss = 0.65047695\n",
      "Iteration 51, loss = 0.63883968\n",
      "Iteration 52, loss = 0.62747881\n",
      "Iteration 53, loss = 0.61635755\n",
      "Iteration 54, loss = 0.60548070\n",
      "Iteration 55, loss = 0.59482947\n",
      "Iteration 56, loss = 0.58442126\n",
      "Iteration 57, loss = 0.57423117\n",
      "Iteration 58, loss = 0.56426220\n",
      "Iteration 59, loss = 0.55450471\n",
      "Iteration 60, loss = 0.54496174\n",
      "Iteration 61, loss = 0.53564656\n",
      "Iteration 62, loss = 0.52654559\n",
      "Iteration 63, loss = 0.51764889\n",
      "Iteration 64, loss = 0.50895505\n",
      "Iteration 65, loss = 0.50044387\n",
      "Iteration 66, loss = 0.49211383\n",
      "Iteration 67, loss = 0.48397991\n",
      "Iteration 68, loss = 0.47603966\n",
      "Iteration 69, loss = 0.46828886\n",
      "Iteration 70, loss = 0.46071914\n",
      "Iteration 71, loss = 0.45331763\n",
      "Iteration 72, loss = 0.44606480\n",
      "Iteration 73, loss = 0.43895517\n",
      "Iteration 74, loss = 0.43199981\n",
      "Iteration 75, loss = 0.42519552\n",
      "Iteration 76, loss = 0.41852124\n",
      "Iteration 77, loss = 0.41199036\n",
      "Iteration 78, loss = 0.40558583\n",
      "Iteration 79, loss = 0.39930866\n",
      "Iteration 80, loss = 0.39315547\n",
      "Iteration 81, loss = 0.38713471\n",
      "Iteration 82, loss = 0.38124266\n",
      "Iteration 83, loss = 0.37547486\n",
      "Iteration 84, loss = 0.36982049\n",
      "Iteration 85, loss = 0.36428781\n",
      "Iteration 86, loss = 0.35886877\n",
      "Iteration 87, loss = 0.35355725\n",
      "Iteration 88, loss = 0.34835394\n",
      "Iteration 89, loss = 0.34326213\n",
      "Iteration 90, loss = 0.33827706\n",
      "Iteration 91, loss = 0.33339334\n",
      "Iteration 92, loss = 0.32862127\n",
      "Iteration 93, loss = 0.32394724\n",
      "Iteration 94, loss = 0.31936298\n",
      "Iteration 95, loss = 0.31486186\n",
      "Iteration 96, loss = 0.31044891\n",
      "Iteration 97, loss = 0.30612270\n",
      "Iteration 98, loss = 0.30187369\n",
      "Iteration 99, loss = 0.29770901\n",
      "Iteration 100, loss = 0.29362710\n",
      "Iteration 101, loss = 0.28961961\n",
      "Iteration 102, loss = 0.28568978\n",
      "Iteration 103, loss = 0.28183081\n",
      "Iteration 104, loss = 0.27804445\n",
      "Iteration 105, loss = 0.27432934\n",
      "Iteration 106, loss = 0.27068454\n",
      "Iteration 107, loss = 0.26710640\n",
      "Iteration 108, loss = 0.26359645\n",
      "Iteration 109, loss = 0.26015403\n",
      "Iteration 110, loss = 0.25677300\n",
      "Iteration 111, loss = 0.25344544\n",
      "Iteration 112, loss = 0.25017727\n",
      "Iteration 113, loss = 0.24696482\n",
      "Iteration 114, loss = 0.24380066\n",
      "Iteration 115, loss = 0.24069262\n",
      "Iteration 116, loss = 0.23763739\n",
      "Iteration 117, loss = 0.23462932\n",
      "Iteration 118, loss = 0.23166508\n",
      "Iteration 119, loss = 0.22875238\n",
      "Iteration 120, loss = 0.22589041\n",
      "Iteration 121, loss = 0.22307541\n",
      "Iteration 122, loss = 0.22030315\n",
      "Iteration 123, loss = 0.21757542\n",
      "Iteration 124, loss = 0.21489525\n",
      "Iteration 125, loss = 0.21226577\n",
      "Iteration 126, loss = 0.20968627\n",
      "Iteration 127, loss = 0.20714946\n",
      "Iteration 128, loss = 0.20465420\n",
      "Iteration 129, loss = 0.20220129\n",
      "Iteration 130, loss = 0.19978888\n",
      "Iteration 131, loss = 0.19741531\n",
      "Iteration 132, loss = 0.19507919\n",
      "Iteration 133, loss = 0.19277933\n",
      "Iteration 134, loss = 0.19051667\n",
      "Iteration 135, loss = 0.18829083\n",
      "Iteration 136, loss = 0.18610039\n",
      "Iteration 137, loss = 0.18394642\n",
      "Iteration 138, loss = 0.18183038\n",
      "Iteration 139, loss = 0.17974921\n",
      "Iteration 140, loss = 0.17770184\n",
      "Iteration 141, loss = 0.17568750\n",
      "Iteration 142, loss = 0.17370665\n",
      "Iteration 143, loss = 0.17176059\n",
      "Iteration 144, loss = 0.16984712\n",
      "Iteration 145, loss = 0.16796520\n",
      "Iteration 146, loss = 0.16611317\n",
      "Iteration 147, loss = 0.16429218\n",
      "Iteration 148, loss = 0.16250036\n",
      "Iteration 149, loss = 0.16073699\n",
      "Iteration 150, loss = 0.15900029\n",
      "Iteration 151, loss = 0.15729136\n",
      "Iteration 152, loss = 0.15560958\n",
      "Iteration 153, loss = 0.15395378\n",
      "Iteration 154, loss = 0.15232351\n",
      "Iteration 155, loss = 0.15071876\n",
      "Iteration 156, loss = 0.14913851\n",
      "Iteration 157, loss = 0.14757923\n",
      "Iteration 158, loss = 0.14604320\n",
      "Iteration 159, loss = 0.14452977\n",
      "Iteration 160, loss = 0.14303879\n",
      "Iteration 161, loss = 0.14156887\n",
      "Iteration 162, loss = 0.14012175\n",
      "Iteration 163, loss = 0.13869949\n",
      "Iteration 164, loss = 0.13729685\n",
      "Iteration 165, loss = 0.13591544\n",
      "Iteration 166, loss = 0.13455410\n",
      "Iteration 167, loss = 0.13321109\n",
      "Iteration 168, loss = 0.13188656\n",
      "Iteration 169, loss = 0.13058107\n",
      "Iteration 170, loss = 0.12929391\n",
      "Iteration 171, loss = 0.12802482\n",
      "Iteration 172, loss = 0.12677350\n",
      "Iteration 173, loss = 0.12553967\n",
      "Iteration 174, loss = 0.12432339\n",
      "Iteration 175, loss = 0.12312511\n",
      "Iteration 176, loss = 0.12194365\n",
      "Iteration 177, loss = 0.12077850\n",
      "Iteration 178, loss = 0.11962940\n",
      "Iteration 179, loss = 0.11849631\n",
      "Iteration 180, loss = 0.11737890\n",
      "Iteration 181, loss = 0.11627671\n",
      "Iteration 182, loss = 0.11518949\n",
      "Iteration 183, loss = 0.11411167\n",
      "Iteration 184, loss = 0.11304510\n",
      "Iteration 185, loss = 0.11199134\n",
      "Iteration 186, loss = 0.11095128\n",
      "Iteration 187, loss = 0.10992534\n",
      "Iteration 188, loss = 0.10891174\n",
      "Iteration 189, loss = 0.10791118\n",
      "Iteration 190, loss = 0.10692357\n",
      "Iteration 191, loss = 0.10594817\n",
      "Iteration 192, loss = 0.10498474\n",
      "Iteration 193, loss = 0.10403369\n",
      "Iteration 194, loss = 0.10309417\n",
      "Iteration 195, loss = 0.10216678\n",
      "Iteration 196, loss = 0.10125074\n",
      "Iteration 197, loss = 0.10034658\n",
      "Iteration 198, loss = 0.09945370\n",
      "Iteration 199, loss = 0.09857239\n",
      "Iteration 200, loss = 0.09770280\n",
      "Iteration 201, loss = 0.09684437\n",
      "Iteration 202, loss = 0.09599713\n",
      "Iteration 203, loss = 0.09516025\n",
      "Iteration 204, loss = 0.09433376\n",
      "Iteration 205, loss = 0.09351789\n",
      "Iteration 206, loss = 0.09271497\n",
      "Iteration 207, loss = 0.09192212\n",
      "Iteration 208, loss = 0.09113925\n",
      "Iteration 209, loss = 0.09036615\n",
      "Iteration 210, loss = 0.08960281\n",
      "Iteration 211, loss = 0.08884880\n",
      "Iteration 212, loss = 0.08810337\n",
      "Iteration 213, loss = 0.08736643\n",
      "Iteration 214, loss = 0.08663829\n",
      "Iteration 215, loss = 0.08591897\n",
      "Iteration 216, loss = 0.08520861\n",
      "Iteration 217, loss = 0.08450623\n",
      "Iteration 218, loss = 0.08381221\n",
      "Iteration 219, loss = 0.08312674\n",
      "Iteration 220, loss = 0.08244961\n",
      "Iteration 221, loss = 0.08177925\n",
      "Iteration 222, loss = 0.08111526\n",
      "Iteration 223, loss = 0.08045856\n",
      "Iteration 224, loss = 0.07980873\n",
      "Iteration 225, loss = 0.07916605\n",
      "Iteration 226, loss = 0.07853055\n",
      "Iteration 227, loss = 0.07789807\n",
      "Iteration 228, loss = 0.07727198\n",
      "Iteration 229, loss = 0.07665196\n",
      "Iteration 230, loss = 0.07603859\n",
      "Iteration 231, loss = 0.07543164\n",
      "Iteration 232, loss = 0.07482948\n",
      "Iteration 233, loss = 0.07423299\n",
      "Iteration 234, loss = 0.07364199\n",
      "Iteration 235, loss = 0.07305650\n",
      "Iteration 236, loss = 0.07247707\n",
      "Iteration 237, loss = 0.07190404\n",
      "Iteration 238, loss = 0.07133697\n",
      "Iteration 239, loss = 0.07077580\n",
      "Iteration 240, loss = 0.07022004\n",
      "Iteration 241, loss = 0.06966989\n",
      "Iteration 242, loss = 0.06912566\n",
      "Iteration 243, loss = 0.06858724\n",
      "Iteration 244, loss = 0.06805465\n",
      "Iteration 245, loss = 0.06752779\n",
      "Iteration 246, loss = 0.06700672\n",
      "Iteration 247, loss = 0.06649145\n",
      "Iteration 248, loss = 0.06598174\n",
      "Iteration 249, loss = 0.06547755\n",
      "Iteration 250, loss = 0.06497885\n",
      "Iteration 251, loss = 0.06448553\n",
      "Iteration 252, loss = 0.06399755\n",
      "Iteration 253, loss = 0.06351485\n",
      "Iteration 254, loss = 0.06303736\n",
      "Iteration 255, loss = 0.06256510\n",
      "Iteration 256, loss = 0.06209795\n",
      "Iteration 257, loss = 0.06163563\n",
      "Iteration 258, loss = 0.06117826\n",
      "Iteration 259, loss = 0.06072576\n",
      "Iteration 260, loss = 0.06027812\n",
      "Iteration 261, loss = 0.05983522\n",
      "Iteration 262, loss = 0.05939692\n",
      "Iteration 263, loss = 0.05896284\n",
      "Iteration 264, loss = 0.05853327\n",
      "Iteration 265, loss = 0.05810821\n",
      "Iteration 266, loss = 0.05768764\n",
      "Iteration 267, loss = 0.05727149\n",
      "Iteration 268, loss = 0.05685987\n",
      "Iteration 269, loss = 0.05645255\n",
      "Iteration 270, loss = 0.05604934\n",
      "Iteration 271, loss = 0.05565022\n",
      "Iteration 272, loss = 0.05525528\n",
      "Iteration 273, loss = 0.05486437\n",
      "Iteration 274, loss = 0.05447744\n",
      "Iteration 275, loss = 0.05409448\n",
      "Iteration 276, loss = 0.05371552\n",
      "Iteration 277, loss = 0.05334046\n",
      "Iteration 278, loss = 0.05296922\n",
      "Iteration 279, loss = 0.05260176\n",
      "Iteration 280, loss = 0.05223804\n",
      "Iteration 281, loss = 0.05187800\n",
      "Iteration 282, loss = 0.05152166\n",
      "Iteration 283, loss = 0.05116895\n",
      "Iteration 284, loss = 0.05081979\n",
      "Iteration 285, loss = 0.05047412\n",
      "Iteration 286, loss = 0.05013189\n",
      "Iteration 287, loss = 0.04979306\n",
      "Iteration 288, loss = 0.04945767\n",
      "Iteration 289, loss = 0.04912555\n",
      "Iteration 290, loss = 0.04879638\n",
      "Iteration 291, loss = 0.04847031\n",
      "Iteration 292, loss = 0.04814731\n",
      "Iteration 293, loss = 0.04782731\n",
      "Iteration 294, loss = 0.04751031\n",
      "Iteration 295, loss = 0.04719626\n",
      "Iteration 296, loss = 0.04688543\n",
      "Iteration 297, loss = 0.04657755\n",
      "Iteration 298, loss = 0.04627257\n",
      "Iteration 299, loss = 0.04597047\n",
      "Iteration 300, loss = 0.04567120\n",
      "Iteration 301, loss = 0.04537468\n",
      "Iteration 302, loss = 0.04508089\n",
      "Iteration 303, loss = 0.04478984\n",
      "Iteration 304, loss = 0.04450158\n",
      "Iteration 305, loss = 0.04421579\n",
      "Iteration 306, loss = 0.04393240\n",
      "Iteration 307, loss = 0.04365165\n",
      "Iteration 308, loss = 0.04337348\n",
      "Iteration 309, loss = 0.04309784\n",
      "Iteration 310, loss = 0.04282472\n",
      "Iteration 311, loss = 0.04255409\n",
      "Iteration 312, loss = 0.04228592\n",
      "Iteration 313, loss = 0.04202019\n",
      "Iteration 314, loss = 0.04175691\n",
      "Iteration 315, loss = 0.04149602\n",
      "Iteration 316, loss = 0.04123749\n",
      "Iteration 317, loss = 0.04098129\n",
      "Iteration 318, loss = 0.04072741\n",
      "Iteration 319, loss = 0.04047662\n",
      "Iteration 320, loss = 0.04022842\n",
      "Iteration 321, loss = 0.03998212\n",
      "Iteration 322, loss = 0.03973772\n",
      "Iteration 323, loss = 0.03949575\n",
      "Iteration 324, loss = 0.03925612\n",
      "Iteration 325, loss = 0.03901858\n",
      "Iteration 326, loss = 0.03878311\n",
      "Iteration 327, loss = 0.03854970\n",
      "Iteration 328, loss = 0.03831999\n",
      "Iteration 329, loss = 0.03809386\n",
      "Iteration 330, loss = 0.03786928\n",
      "Iteration 331, loss = 0.03764615\n",
      "Iteration 332, loss = 0.03742467\n",
      "Iteration 333, loss = 0.03720485\n",
      "Iteration 334, loss = 0.03698671\n",
      "Iteration 335, loss = 0.03677022\n",
      "Iteration 336, loss = 0.03655543\n",
      "Iteration 337, loss = 0.03634232\n",
      "Iteration 338, loss = 0.03613134\n",
      "Iteration 339, loss = 0.03592185\n",
      "Iteration 340, loss = 0.03571389\n",
      "Iteration 341, loss = 0.03550780\n",
      "Iteration 342, loss = 0.03530366\n",
      "Iteration 343, loss = 0.03510263\n",
      "Iteration 344, loss = 0.03490352\n",
      "Iteration 345, loss = 0.03470583\n",
      "Iteration 346, loss = 0.03450954\n",
      "Iteration 347, loss = 0.03431472\n",
      "Iteration 348, loss = 0.03412136\n",
      "Iteration 349, loss = 0.03392935\n",
      "Iteration 350, loss = 0.03373841\n",
      "Iteration 351, loss = 0.03354938\n",
      "Iteration 352, loss = 0.03336273\n",
      "Iteration 353, loss = 0.03317807\n",
      "Iteration 354, loss = 0.03299458\n",
      "Iteration 355, loss = 0.03281279\n",
      "Iteration 356, loss = 0.03263245\n",
      "Iteration 357, loss = 0.03245343\n",
      "Iteration 358, loss = 0.03227589\n",
      "Iteration 359, loss = 0.03209977\n",
      "Iteration 360, loss = 0.03192498\n",
      "Iteration 361, loss = 0.03175178\n",
      "Iteration 362, loss = 0.03157993\n",
      "Iteration 363, loss = 0.03140949\n",
      "Iteration 364, loss = 0.03124124\n",
      "Iteration 365, loss = 0.03107387\n",
      "Iteration 366, loss = 0.03090738\n",
      "Iteration 367, loss = 0.03074256\n",
      "Iteration 368, loss = 0.03057927\n",
      "Iteration 369, loss = 0.03041717\n",
      "Iteration 370, loss = 0.03025627\n",
      "Iteration 371, loss = 0.03009659\n",
      "Iteration 372, loss = 0.02993813\n",
      "Iteration 373, loss = 0.02978104\n",
      "Iteration 374, loss = 0.02962499\n",
      "Iteration 375, loss = 0.02947051\n",
      "Iteration 376, loss = 0.02931721\n",
      "Iteration 377, loss = 0.02916476\n",
      "Iteration 378, loss = 0.02901322\n",
      "Iteration 379, loss = 0.02886343\n",
      "Iteration 380, loss = 0.02871459\n",
      "Iteration 381, loss = 0.02856685\n",
      "Iteration 382, loss = 0.02842041\n",
      "Iteration 383, loss = 0.02827534\n",
      "Iteration 384, loss = 0.02813173\n",
      "Iteration 385, loss = 0.02798884\n",
      "Iteration 386, loss = 0.02784674\n",
      "Iteration 387, loss = 0.02770610\n",
      "Iteration 388, loss = 0.02756660\n",
      "Iteration 389, loss = 0.02742804\n",
      "Iteration 390, loss = 0.02729041\n",
      "Iteration 391, loss = 0.02715372\n",
      "Iteration 392, loss = 0.02701796\n",
      "Iteration 393, loss = 0.02688326\n",
      "Iteration 394, loss = 0.02674945\n",
      "Iteration 395, loss = 0.02661676\n",
      "Iteration 396, loss = 0.02648531\n",
      "Iteration 397, loss = 0.02635452\n",
      "Iteration 398, loss = 0.02622490\n",
      "Iteration 399, loss = 0.02609631\n",
      "Iteration 400, loss = 0.02596860\n",
      "Iteration 401, loss = 0.02584173\n",
      "Iteration 402, loss = 0.02571599\n",
      "Iteration 403, loss = 0.02559105\n",
      "Iteration 404, loss = 0.02546694\n",
      "Iteration 405, loss = 0.02534388\n",
      "Iteration 406, loss = 0.02522166\n",
      "Iteration 407, loss = 0.02510001\n",
      "Iteration 408, loss = 0.02497952\n",
      "Iteration 409, loss = 0.02485991\n",
      "Iteration 410, loss = 0.02474107\n",
      "Iteration 411, loss = 0.02462303\n",
      "Iteration 412, loss = 0.02450586\n",
      "Iteration 413, loss = 0.02438945\n",
      "Iteration 414, loss = 0.02427381\n",
      "Iteration 415, loss = 0.02415893\n",
      "Iteration 416, loss = 0.02404501\n",
      "Iteration 417, loss = 0.02393183\n",
      "Iteration 418, loss = 0.02381940\n",
      "Iteration 419, loss = 0.02370777\n",
      "Iteration 420, loss = 0.02359696\n",
      "Iteration 421, loss = 0.02348702\n",
      "Iteration 422, loss = 0.02337764\n",
      "Iteration 423, loss = 0.02326907\n",
      "Iteration 424, loss = 0.02316142\n",
      "Iteration 425, loss = 0.02305443\n",
      "Iteration 426, loss = 0.02294812\n",
      "Iteration 427, loss = 0.02284250\n",
      "Iteration 428, loss = 0.02273756\n",
      "Iteration 429, loss = 0.02263368\n",
      "Iteration 430, loss = 0.02253033\n",
      "Iteration 431, loss = 0.02242734\n",
      "Iteration 432, loss = 0.02232535\n",
      "Iteration 433, loss = 0.02222418\n",
      "Iteration 434, loss = 0.02212358\n",
      "Iteration 435, loss = 0.02202360\n",
      "Iteration 436, loss = 0.02192429\n",
      "Iteration 437, loss = 0.02182560\n",
      "Iteration 438, loss = 0.02172756\n",
      "Iteration 439, loss = 0.02163014\n",
      "Iteration 440, loss = 0.02153332\n",
      "Iteration 441, loss = 0.02143742\n",
      "Iteration 442, loss = 0.02134203\n",
      "Iteration 443, loss = 0.02124706\n",
      "Iteration 444, loss = 0.02115278\n",
      "Iteration 445, loss = 0.02105920\n",
      "Iteration 446, loss = 0.02096613\n",
      "Iteration 447, loss = 0.02087366\n",
      "Iteration 448, loss = 0.02078175\n",
      "Iteration 449, loss = 0.02069034\n",
      "Iteration 450, loss = 0.02059951\n",
      "Iteration 451, loss = 0.02050927\n",
      "Iteration 452, loss = 0.02041996\n",
      "Iteration 453, loss = 0.02033094\n",
      "Iteration 454, loss = 0.02024231\n",
      "Iteration 455, loss = 0.02015436\n",
      "Iteration 456, loss = 0.02006705\n",
      "Iteration 457, loss = 0.01998030\n",
      "Iteration 458, loss = 0.01989403\n",
      "Iteration 459, loss = 0.01980829\n",
      "Iteration 460, loss = 0.01972305\n",
      "Iteration 461, loss = 0.01963838\n",
      "Iteration 462, loss = 0.01955421\n",
      "Iteration 463, loss = 0.01947058\n",
      "Iteration 464, loss = 0.01938772\n",
      "Iteration 465, loss = 0.01930519\n",
      "Iteration 466, loss = 0.01922301\n",
      "Iteration 467, loss = 0.01914148\n",
      "Iteration 468, loss = 0.01906048\n",
      "Iteration 469, loss = 0.01898000\n",
      "Iteration 470, loss = 0.01889999\n",
      "Iteration 471, loss = 0.01882041\n",
      "Iteration 472, loss = 0.01874130\n",
      "Iteration 473, loss = 0.01866262\n",
      "Iteration 474, loss = 0.01858438\n",
      "Iteration 475, loss = 0.01850663\n",
      "Iteration 476, loss = 0.01842946\n",
      "Iteration 477, loss = 0.01835272\n",
      "Iteration 478, loss = 0.01827620\n",
      "Iteration 479, loss = 0.01820043\n",
      "Iteration 480, loss = 0.01812509\n",
      "Iteration 481, loss = 0.01805021\n",
      "Iteration 482, loss = 0.01797574\n",
      "Iteration 483, loss = 0.01790167\n",
      "Iteration 484, loss = 0.01782802\n",
      "Iteration 485, loss = 0.01775481\n",
      "Iteration 486, loss = 0.01768200\n",
      "Iteration 487, loss = 0.01760958\n",
      "Iteration 488, loss = 0.01753780\n",
      "Iteration 489, loss = 0.01746639\n",
      "Iteration 490, loss = 0.01739521\n",
      "Iteration 491, loss = 0.01732463\n",
      "Iteration 492, loss = 0.01725451\n",
      "Iteration 493, loss = 0.01718479\n",
      "Iteration 494, loss = 0.01711545\n",
      "Iteration 495, loss = 0.01704646\n",
      "Iteration 496, loss = 0.01697792\n",
      "Iteration 497, loss = 0.01690983\n",
      "Iteration 498, loss = 0.01684207\n",
      "Iteration 499, loss = 0.01677475\n",
      "Iteration 500, loss = 0.01670794\n",
      "Iteration 501, loss = 0.01664150\n",
      "Iteration 502, loss = 0.01657520\n",
      "Iteration 503, loss = 0.01650943\n",
      "Iteration 504, loss = 0.01644403\n",
      "Iteration 505, loss = 0.01637895\n",
      "Iteration 506, loss = 0.01631449\n",
      "Iteration 507, loss = 0.01625048\n",
      "Iteration 508, loss = 0.01618692\n",
      "Iteration 509, loss = 0.01612364\n",
      "Iteration 510, loss = 0.01606070\n",
      "Iteration 511, loss = 0.01599803\n",
      "Iteration 512, loss = 0.01593567\n",
      "Iteration 513, loss = 0.01587363\n",
      "Iteration 514, loss = 0.01581191\n",
      "Iteration 515, loss = 0.01575062\n",
      "Iteration 516, loss = 0.01568955\n",
      "Iteration 517, loss = 0.01562891\n",
      "Iteration 518, loss = 0.01556858\n",
      "Iteration 519, loss = 0.01550853\n",
      "Iteration 520, loss = 0.01544887\n",
      "Iteration 521, loss = 0.01538967\n",
      "Iteration 522, loss = 0.01533077\n",
      "Iteration 523, loss = 0.01527215\n",
      "Iteration 524, loss = 0.01521398\n",
      "Iteration 525, loss = 0.01515601\n",
      "Iteration 526, loss = 0.01509827\n",
      "Iteration 527, loss = 0.01504094\n",
      "Iteration 528, loss = 0.01498392\n",
      "Iteration 529, loss = 0.01492732\n",
      "Iteration 530, loss = 0.01487102\n",
      "Iteration 531, loss = 0.01481501\n",
      "Iteration 532, loss = 0.01475934\n",
      "Iteration 533, loss = 0.01470397\n",
      "Iteration 534, loss = 0.01464885\n",
      "Iteration 535, loss = 0.01459403\n",
      "Iteration 536, loss = 0.01453954\n",
      "Iteration 537, loss = 0.01448534\n",
      "Iteration 538, loss = 0.01443139\n",
      "Iteration 539, loss = 0.01437772\n",
      "Iteration 540, loss = 0.01432445\n",
      "Iteration 541, loss = 0.01427151\n",
      "Iteration 542, loss = 0.01421870\n",
      "Iteration 543, loss = 0.01416620\n",
      "Iteration 544, loss = 0.01411399\n",
      "Iteration 545, loss = 0.01406208\n",
      "Iteration 546, loss = 0.01401050\n",
      "Iteration 547, loss = 0.01395916\n",
      "Iteration 548, loss = 0.01390808\n",
      "Iteration 549, loss = 0.01385727\n",
      "Iteration 550, loss = 0.01380670\n",
      "Iteration 551, loss = 0.01375641\n",
      "Iteration 552, loss = 0.01370632\n",
      "Iteration 553, loss = 0.01365641\n",
      "Iteration 554, loss = 0.01360679\n",
      "Iteration 555, loss = 0.01355727\n",
      "Iteration 556, loss = 0.01350804\n",
      "Iteration 557, loss = 0.01345912\n",
      "Iteration 558, loss = 0.01341042\n",
      "Iteration 559, loss = 0.01336202\n",
      "Iteration 560, loss = 0.01331374\n",
      "Iteration 561, loss = 0.01326582\n",
      "Iteration 562, loss = 0.01321810\n",
      "Iteration 563, loss = 0.01317064\n",
      "Iteration 564, loss = 0.01312342\n",
      "Iteration 565, loss = 0.01307656\n",
      "Iteration 566, loss = 0.01302987\n",
      "Iteration 567, loss = 0.01298338\n",
      "Iteration 568, loss = 0.01293709\n",
      "Iteration 569, loss = 0.01289109\n",
      "Iteration 570, loss = 0.01284530\n",
      "Iteration 571, loss = 0.01279987\n",
      "Iteration 572, loss = 0.01275464\n",
      "Iteration 573, loss = 0.01270966\n",
      "Iteration 574, loss = 0.01266490\n",
      "Iteration 575, loss = 0.01262033\n",
      "Iteration 576, loss = 0.01257605\n",
      "Iteration 577, loss = 0.01253196\n",
      "Iteration 578, loss = 0.01248806\n",
      "Iteration 579, loss = 0.01244438\n",
      "Iteration 580, loss = 0.01240093\n",
      "Iteration 581, loss = 0.01235773\n",
      "Iteration 582, loss = 0.01231475\n",
      "Iteration 583, loss = 0.01227197\n",
      "Iteration 584, loss = 0.01222945\n",
      "Iteration 585, loss = 0.01218708\n",
      "Iteration 586, loss = 0.01214491\n",
      "Iteration 587, loss = 0.01210295\n",
      "Iteration 588, loss = 0.01206125\n",
      "Iteration 589, loss = 0.01201982\n",
      "Iteration 590, loss = 0.01197843\n",
      "Iteration 591, loss = 0.01193733\n",
      "Iteration 592, loss = 0.01189645\n",
      "Iteration 593, loss = 0.01185575\n",
      "Iteration 594, loss = 0.01181525\n",
      "Iteration 595, loss = 0.01177493\n",
      "Iteration 596, loss = 0.01173479\n",
      "Iteration 597, loss = 0.01169487\n",
      "Iteration 598, loss = 0.01165514\n",
      "Iteration 599, loss = 0.01161561\n",
      "Iteration 600, loss = 0.01157638\n",
      "Iteration 601, loss = 0.01153724\n",
      "Iteration 602, loss = 0.01149822\n",
      "Iteration 603, loss = 0.01145949\n",
      "Iteration 604, loss = 0.01142094\n",
      "Iteration 605, loss = 0.01138259\n",
      "Iteration 606, loss = 0.01134441\n",
      "Iteration 607, loss = 0.01130638\n",
      "Iteration 608, loss = 0.01126854\n",
      "Iteration 609, loss = 0.01123091\n",
      "Iteration 610, loss = 0.01119358\n",
      "Iteration 611, loss = 0.01115628\n",
      "Iteration 612, loss = 0.01111909\n",
      "Iteration 613, loss = 0.01108222\n",
      "Iteration 614, loss = 0.01104551\n",
      "Iteration 615, loss = 0.01100896\n",
      "Iteration 616, loss = 0.01097254\n",
      "Iteration 617, loss = 0.01093633\n",
      "Iteration 618, loss = 0.01090029\n",
      "Iteration 619, loss = 0.01086441\n",
      "Iteration 620, loss = 0.01082874\n",
      "Iteration 621, loss = 0.01079320\n",
      "Iteration 622, loss = 0.01075785\n",
      "Iteration 623, loss = 0.01072268\n",
      "Iteration 624, loss = 0.01068769\n",
      "Iteration 625, loss = 0.01065286\n",
      "Iteration 626, loss = 0.01061817\n",
      "Iteration 627, loss = 0.01058363\n",
      "Iteration 628, loss = 0.01054925\n",
      "Iteration 629, loss = 0.01051508\n",
      "Iteration 630, loss = 0.01048103\n",
      "Iteration 631, loss = 0.01044714\n",
      "Iteration 632, loss = 0.01041350\n",
      "Iteration 633, loss = 0.01038004\n",
      "Iteration 634, loss = 0.01034672\n",
      "Iteration 635, loss = 0.01031358\n",
      "Iteration 636, loss = 0.01028052\n",
      "Iteration 637, loss = 0.01024790\n",
      "Iteration 638, loss = 0.01021572\n",
      "Iteration 639, loss = 0.01018364\n",
      "Iteration 640, loss = 0.01015167\n",
      "Iteration 641, loss = 0.01011988\n",
      "Iteration 642, loss = 0.01008823\n",
      "Iteration 643, loss = 0.01005676\n",
      "Iteration 644, loss = 0.01002536\n",
      "Iteration 645, loss = 0.00999406\n",
      "Iteration 646, loss = 0.00996296\n",
      "Iteration 647, loss = 0.00993208\n",
      "Iteration 648, loss = 0.00990117\n",
      "Iteration 649, loss = 0.00987045\n",
      "Iteration 650, loss = 0.00983988\n",
      "Iteration 651, loss = 0.00980946\n",
      "Iteration 652, loss = 0.00977918\n",
      "Iteration 653, loss = 0.00974900\n",
      "Iteration 654, loss = 0.00971894\n",
      "Iteration 655, loss = 0.00968904\n",
      "Iteration 656, loss = 0.00965928\n",
      "Iteration 657, loss = 0.00962964\n",
      "Iteration 658, loss = 0.00960011\n",
      "Iteration 659, loss = 0.00957079\n",
      "Iteration 660, loss = 0.00954153\n",
      "Iteration 661, loss = 0.00951242\n",
      "Iteration 662, loss = 0.00948347\n",
      "Iteration 663, loss = 0.00945463\n",
      "Iteration 664, loss = 0.00942590\n",
      "Iteration 665, loss = 0.00939732\n",
      "Iteration 666, loss = 0.00936884\n",
      "Iteration 667, loss = 0.00934048\n",
      "Iteration 668, loss = 0.00931222\n",
      "Iteration 669, loss = 0.00928423\n",
      "Iteration 670, loss = 0.00925623\n",
      "Iteration 671, loss = 0.00922831\n",
      "Iteration 672, loss = 0.00920060\n",
      "Iteration 673, loss = 0.00917297\n",
      "Iteration 674, loss = 0.00914547\n",
      "Iteration 675, loss = 0.00911811\n",
      "Iteration 676, loss = 0.00909085\n",
      "Iteration 677, loss = 0.00906368\n",
      "Iteration 678, loss = 0.00903668\n",
      "Iteration 679, loss = 0.00900976\n",
      "Iteration 680, loss = 0.00898302\n",
      "Iteration 681, loss = 0.00895642\n",
      "Iteration 682, loss = 0.00892990\n",
      "Iteration 683, loss = 0.00890347\n",
      "Iteration 684, loss = 0.00887711\n",
      "Iteration 685, loss = 0.00885085\n",
      "Iteration 686, loss = 0.00882477\n",
      "Iteration 687, loss = 0.00879881\n",
      "Iteration 688, loss = 0.00877295\n",
      "Iteration 689, loss = 0.00874720\n",
      "Iteration 690, loss = 0.00872154\n",
      "Iteration 691, loss = 0.00869600\n",
      "Iteration 692, loss = 0.00867054\n",
      "Iteration 693, loss = 0.00864523\n",
      "Iteration 694, loss = 0.00861998\n",
      "Iteration 695, loss = 0.00859486\n",
      "Iteration 696, loss = 0.00856982\n",
      "Iteration 697, loss = 0.00854492\n",
      "Iteration 698, loss = 0.00852012\n",
      "Iteration 699, loss = 0.00849541\n",
      "Iteration 700, loss = 0.00847078\n",
      "Iteration 701, loss = 0.00844628\n",
      "Iteration 702, loss = 0.00842184\n",
      "Iteration 703, loss = 0.00839751\n",
      "Iteration 704, loss = 0.00837331\n",
      "Iteration 705, loss = 0.00834912\n",
      "Iteration 706, loss = 0.00832509\n",
      "Iteration 707, loss = 0.00830113\n",
      "Iteration 708, loss = 0.00827728\n",
      "Iteration 709, loss = 0.00825354\n",
      "Iteration 710, loss = 0.00822985\n",
      "Iteration 711, loss = 0.00820630\n",
      "Iteration 712, loss = 0.00818283\n",
      "Iteration 713, loss = 0.00815948\n",
      "Iteration 714, loss = 0.00813619\n",
      "Iteration 715, loss = 0.00811309\n",
      "Iteration 716, loss = 0.00808999\n",
      "Iteration 717, loss = 0.00806703\n",
      "Iteration 718, loss = 0.00804420\n",
      "Iteration 719, loss = 0.00802143\n",
      "Iteration 720, loss = 0.00799872\n",
      "Iteration 721, loss = 0.00797612\n",
      "Iteration 722, loss = 0.00795360\n",
      "Iteration 723, loss = 0.00793118\n",
      "Iteration 724, loss = 0.00790885\n",
      "Iteration 725, loss = 0.00788661\n",
      "Iteration 726, loss = 0.00786450\n",
      "Iteration 727, loss = 0.00784247\n",
      "Iteration 728, loss = 0.00782053\n",
      "Iteration 729, loss = 0.00779869\n",
      "Iteration 730, loss = 0.00777694\n",
      "Iteration 731, loss = 0.00775529\n",
      "Iteration 732, loss = 0.00773368\n",
      "Iteration 733, loss = 0.00771214\n",
      "Iteration 734, loss = 0.00769073\n",
      "Iteration 735, loss = 0.00766938\n",
      "Iteration 736, loss = 0.00764811\n",
      "Iteration 737, loss = 0.00762697\n",
      "Iteration 738, loss = 0.00760586\n",
      "Iteration 739, loss = 0.00758487\n",
      "Iteration 740, loss = 0.00756399\n",
      "Iteration 741, loss = 0.00754315\n",
      "Iteration 742, loss = 0.00752238\n",
      "Iteration 743, loss = 0.00750169\n",
      "Iteration 744, loss = 0.00748112\n",
      "Iteration 745, loss = 0.00746063\n",
      "Iteration 746, loss = 0.00744020\n",
      "Iteration 747, loss = 0.00741982\n",
      "Iteration 748, loss = 0.00739951\n",
      "Iteration 749, loss = 0.00737938\n",
      "Iteration 750, loss = 0.00735927\n",
      "Iteration 751, loss = 0.00733919\n",
      "Iteration 752, loss = 0.00731924\n",
      "Iteration 753, loss = 0.00729940\n",
      "Iteration 754, loss = 0.00727962\n",
      "Iteration 755, loss = 0.00725990\n",
      "Iteration 756, loss = 0.00724023\n",
      "Iteration 757, loss = 0.00722066\n",
      "Iteration 758, loss = 0.00720119\n",
      "Iteration 759, loss = 0.00718177\n",
      "Iteration 760, loss = 0.00716243\n",
      "Iteration 761, loss = 0.00714319\n",
      "Iteration 762, loss = 0.00712401\n",
      "Iteration 763, loss = 0.00710492\n",
      "Iteration 764, loss = 0.00708589\n",
      "Iteration 765, loss = 0.00706693\n",
      "Iteration 766, loss = 0.00704804\n",
      "Iteration 767, loss = 0.00702922\n",
      "Iteration 768, loss = 0.00701051\n",
      "Iteration 769, loss = 0.00699184\n",
      "Iteration 770, loss = 0.00697321\n",
      "Iteration 771, loss = 0.00695469\n",
      "Iteration 772, loss = 0.00693628\n",
      "Iteration 773, loss = 0.00691793\n",
      "Iteration 774, loss = 0.00689961\n",
      "Iteration 775, loss = 0.00688137\n",
      "Iteration 776, loss = 0.00686321\n",
      "Iteration 777, loss = 0.00684512\n",
      "Iteration 778, loss = 0.00682708\n",
      "Iteration 779, loss = 0.00680911\n",
      "Iteration 780, loss = 0.00679124\n",
      "Iteration 781, loss = 0.00677342\n",
      "Iteration 782, loss = 0.00675569\n",
      "Iteration 783, loss = 0.00673801\n",
      "Iteration 784, loss = 0.00672042\n",
      "Iteration 785, loss = 0.00670285\n",
      "Iteration 786, loss = 0.00668537\n",
      "Iteration 787, loss = 0.00666801\n",
      "Iteration 788, loss = 0.00665065\n",
      "Iteration 789, loss = 0.00663337\n",
      "Iteration 790, loss = 0.00661615\n",
      "Iteration 791, loss = 0.00659904\n",
      "Iteration 792, loss = 0.00658196\n",
      "Iteration 793, loss = 0.00656490\n",
      "Iteration 794, loss = 0.00654795\n",
      "Iteration 795, loss = 0.00653108\n",
      "Iteration 796, loss = 0.00651427\n",
      "Iteration 797, loss = 0.00649752\n",
      "Iteration 798, loss = 0.00648079\n",
      "Iteration 799, loss = 0.00646414\n",
      "Iteration 800, loss = 0.00644760\n",
      "Iteration 801, loss = 0.00643105\n",
      "Iteration 802, loss = 0.00641462\n",
      "Iteration 803, loss = 0.00639824\n",
      "Iteration 804, loss = 0.00638192\n",
      "Iteration 805, loss = 0.00636565\n",
      "Iteration 806, loss = 0.00634944\n",
      "Iteration 807, loss = 0.00633327\n",
      "Iteration 808, loss = 0.00631720\n",
      "Iteration 809, loss = 0.00630121\n",
      "Iteration 810, loss = 0.00628529\n",
      "Iteration 811, loss = 0.00626941\n",
      "Iteration 812, loss = 0.00625361\n",
      "Iteration 813, loss = 0.00623785\n",
      "Iteration 814, loss = 0.00622220\n",
      "Iteration 815, loss = 0.00620658\n",
      "Iteration 816, loss = 0.00619100\n",
      "Iteration 817, loss = 0.00617550\n",
      "Iteration 818, loss = 0.00616005\n",
      "Iteration 819, loss = 0.00614468\n",
      "Iteration 820, loss = 0.00612933\n",
      "Iteration 821, loss = 0.00611405\n",
      "Iteration 822, loss = 0.00609882\n",
      "Iteration 823, loss = 0.00608367\n",
      "Iteration 824, loss = 0.00606855\n",
      "Iteration 825, loss = 0.00605346\n",
      "Iteration 826, loss = 0.00603853\n",
      "Iteration 827, loss = 0.00602358\n",
      "Iteration 828, loss = 0.00600862\n",
      "Iteration 829, loss = 0.00599380\n",
      "Iteration 830, loss = 0.00597904\n",
      "Iteration 831, loss = 0.00596431\n",
      "Iteration 832, loss = 0.00594962\n",
      "Iteration 833, loss = 0.00593498\n",
      "Iteration 834, loss = 0.00592040\n",
      "Iteration 835, loss = 0.00590586\n",
      "Iteration 836, loss = 0.00589137\n",
      "Iteration 837, loss = 0.00587693\n",
      "Iteration 838, loss = 0.00586258\n",
      "Iteration 839, loss = 0.00584825\n",
      "Iteration 840, loss = 0.00583398\n",
      "Iteration 841, loss = 0.00581977\n",
      "Iteration 842, loss = 0.00580560\n",
      "Iteration 843, loss = 0.00579147\n",
      "Iteration 844, loss = 0.00577741\n",
      "Iteration 845, loss = 0.00576339\n",
      "Iteration 846, loss = 0.00574941\n",
      "Iteration 847, loss = 0.00573547\n",
      "Iteration 848, loss = 0.00572159\n",
      "Iteration 849, loss = 0.00570776\n",
      "Iteration 850, loss = 0.00569397\n",
      "Iteration 851, loss = 0.00568027\n",
      "Iteration 852, loss = 0.00566658\n",
      "Iteration 853, loss = 0.00565293\n",
      "Iteration 854, loss = 0.00563935\n",
      "Iteration 855, loss = 0.00562581\n",
      "Iteration 856, loss = 0.00561231\n",
      "Iteration 857, loss = 0.00559886\n",
      "Iteration 858, loss = 0.00558548\n",
      "Iteration 859, loss = 0.00557213\n",
      "Iteration 860, loss = 0.00555880\n",
      "Iteration 861, loss = 0.00554551\n",
      "Iteration 862, loss = 0.00553230\n",
      "Iteration 863, loss = 0.00551915\n",
      "Iteration 864, loss = 0.00550601\n",
      "Iteration 865, loss = 0.00549294\n",
      "Iteration 866, loss = 0.00547990\n",
      "Iteration 867, loss = 0.00546691\n",
      "Iteration 868, loss = 0.00545398\n",
      "Iteration 869, loss = 0.00544111\n",
      "Iteration 870, loss = 0.00542825\n",
      "Iteration 871, loss = 0.00541540\n",
      "Iteration 872, loss = 0.00540262\n",
      "Iteration 873, loss = 0.00538989\n",
      "Iteration 874, loss = 0.00537719\n",
      "Iteration 875, loss = 0.00536454\n",
      "Iteration 876, loss = 0.00535196\n",
      "Iteration 877, loss = 0.00533940\n",
      "Iteration 878, loss = 0.00532686\n",
      "Iteration 879, loss = 0.00531440\n",
      "Iteration 880, loss = 0.00530198\n",
      "Iteration 881, loss = 0.00528959\n",
      "Iteration 882, loss = 0.00527723\n",
      "Iteration 883, loss = 0.00526491\n",
      "Iteration 884, loss = 0.00525265\n",
      "Iteration 885, loss = 0.00524044\n",
      "Iteration 886, loss = 0.00522823\n",
      "Iteration 887, loss = 0.00521608\n",
      "Iteration 888, loss = 0.00520398\n",
      "Iteration 889, loss = 0.00519196\n",
      "Iteration 890, loss = 0.00517992\n",
      "Iteration 891, loss = 0.00516792\n",
      "Iteration 892, loss = 0.00515598\n",
      "Iteration 893, loss = 0.00514408\n",
      "Iteration 894, loss = 0.00513222\n",
      "Iteration 895, loss = 0.00512040\n",
      "Iteration 896, loss = 0.00510860\n",
      "Iteration 897, loss = 0.00509686\n",
      "Iteration 898, loss = 0.00508516\n",
      "Iteration 899, loss = 0.00507349\n",
      "Iteration 900, loss = 0.00506184\n",
      "Iteration 901, loss = 0.00505027\n",
      "Iteration 902, loss = 0.00503870\n",
      "Iteration 903, loss = 0.00502719\n",
      "Iteration 904, loss = 0.00501573\n",
      "Iteration 905, loss = 0.00500430\n",
      "Iteration 906, loss = 0.00499290\n",
      "Iteration 907, loss = 0.00498153\n",
      "Iteration 908, loss = 0.00497021\n",
      "Iteration 909, loss = 0.00495892\n",
      "Iteration 910, loss = 0.00494767\n",
      "Iteration 911, loss = 0.00493644\n",
      "Iteration 912, loss = 0.00492525\n",
      "Iteration 913, loss = 0.00491410\n",
      "Iteration 914, loss = 0.00490299\n",
      "Iteration 915, loss = 0.00489192\n",
      "Iteration 916, loss = 0.00488089\n",
      "Iteration 917, loss = 0.00486989\n",
      "Iteration 918, loss = 0.00485894\n",
      "Iteration 919, loss = 0.00484801\n",
      "Iteration 920, loss = 0.00483713\n",
      "Iteration 921, loss = 0.00482627\n",
      "Iteration 922, loss = 0.00481544\n",
      "Iteration 923, loss = 0.00480466\n",
      "Iteration 924, loss = 0.00479390\n",
      "Iteration 925, loss = 0.00478317\n",
      "Iteration 926, loss = 0.00477248\n",
      "Iteration 927, loss = 0.00476187\n",
      "Iteration 928, loss = 0.00475126\n",
      "Iteration 929, loss = 0.00474066\n",
      "Iteration 930, loss = 0.00473011\n",
      "Iteration 931, loss = 0.00471961\n",
      "Iteration 932, loss = 0.00470914\n",
      "Iteration 933, loss = 0.00469871\n",
      "Iteration 934, loss = 0.00468829\n",
      "Iteration 935, loss = 0.00467791\n",
      "Iteration 936, loss = 0.00466755\n",
      "Iteration 937, loss = 0.00465723\n",
      "Iteration 938, loss = 0.00464695\n",
      "Iteration 939, loss = 0.00463670\n",
      "Iteration 940, loss = 0.00462652\n",
      "Iteration 941, loss = 0.00461634\n",
      "Iteration 942, loss = 0.00460617\n",
      "Iteration 943, loss = 0.00459606\n",
      "Iteration 944, loss = 0.00458597\n",
      "Iteration 945, loss = 0.00457593\n",
      "Iteration 946, loss = 0.00456592\n",
      "Iteration 947, loss = 0.00455591\n",
      "Iteration 948, loss = 0.00454596\n",
      "Iteration 949, loss = 0.00453603\n",
      "Iteration 950, loss = 0.00452613\n",
      "Iteration 951, loss = 0.00451625\n",
      "Iteration 952, loss = 0.00450642\n",
      "Iteration 953, loss = 0.00449660\n",
      "Iteration 954, loss = 0.00448685\n",
      "Iteration 955, loss = 0.00447709\n",
      "Iteration 956, loss = 0.00446739\n",
      "Iteration 957, loss = 0.00445772\n",
      "Iteration 958, loss = 0.00444810\n",
      "Iteration 959, loss = 0.00443853\n",
      "Iteration 960, loss = 0.00442898\n",
      "Iteration 961, loss = 0.00441945\n",
      "Iteration 962, loss = 0.00440995\n",
      "Iteration 963, loss = 0.00440049\n",
      "Iteration 964, loss = 0.00439105\n",
      "Iteration 965, loss = 0.00438167\n",
      "Iteration 966, loss = 0.00437231\n",
      "Iteration 967, loss = 0.00436297\n",
      "Iteration 968, loss = 0.00435364\n",
      "Iteration 969, loss = 0.00434437\n",
      "Iteration 970, loss = 0.00433513\n",
      "Iteration 971, loss = 0.00432590\n",
      "Iteration 972, loss = 0.00431671\n",
      "Iteration 973, loss = 0.00430755\n",
      "Iteration 974, loss = 0.00429840\n",
      "Iteration 975, loss = 0.00428929\n",
      "Iteration 976, loss = 0.00428021\n",
      "Iteration 977, loss = 0.00427114\n",
      "Iteration 978, loss = 0.00426210\n",
      "Iteration 979, loss = 0.00425313\n",
      "Iteration 980, loss = 0.00424415\n",
      "Iteration 981, loss = 0.00423517\n",
      "Iteration 982, loss = 0.00422627\n",
      "Iteration 983, loss = 0.00421740\n",
      "Iteration 984, loss = 0.00420856\n",
      "Iteration 985, loss = 0.00419976\n",
      "Iteration 986, loss = 0.00419097\n",
      "Iteration 987, loss = 0.00418220\n",
      "Iteration 988, loss = 0.00417346\n",
      "Iteration 989, loss = 0.00416476\n",
      "Iteration 990, loss = 0.00415607\n",
      "Iteration 991, loss = 0.00414743\n",
      "Iteration 992, loss = 0.00413877\n",
      "Iteration 993, loss = 0.00413015\n",
      "Iteration 994, loss = 0.00412158\n",
      "Iteration 995, loss = 0.00411303\n",
      "Iteration 996, loss = 0.00410448\n",
      "Iteration 997, loss = 0.00409598\n",
      "Iteration 998, loss = 0.00408749\n",
      "Iteration 999, loss = 0.00407901\n",
      "Iteration 1000, loss = 0.00407059\n",
      "Iteration 1001, loss = 0.00406218\n",
      "Iteration 1002, loss = 0.00405379\n",
      "Iteration 1003, loss = 0.00404541\n",
      "Iteration 1004, loss = 0.00403708\n",
      "Iteration 1005, loss = 0.00402876\n",
      "Iteration 1006, loss = 0.00402048\n",
      "Iteration 1007, loss = 0.00401223\n",
      "Iteration 1008, loss = 0.00400399\n",
      "Iteration 1009, loss = 0.00399577\n",
      "Iteration 1010, loss = 0.00398759\n",
      "Iteration 1011, loss = 0.00397944\n",
      "Iteration 1012, loss = 0.00397129\n",
      "Iteration 1013, loss = 0.00396317\n",
      "Iteration 1014, loss = 0.00395506\n",
      "Iteration 1015, loss = 0.00394698\n",
      "Iteration 1016, loss = 0.00393895\n",
      "Iteration 1017, loss = 0.00393094\n",
      "Iteration 1018, loss = 0.00392293\n",
      "Iteration 1019, loss = 0.00391496\n",
      "Iteration 1020, loss = 0.00390701\n",
      "Iteration 1021, loss = 0.00389910\n",
      "Iteration 1022, loss = 0.00389121\n",
      "Iteration 1023, loss = 0.00388333\n",
      "Iteration 1024, loss = 0.00387546\n",
      "Iteration 1025, loss = 0.00386761\n",
      "Iteration 1026, loss = 0.00385979\n",
      "Iteration 1027, loss = 0.00385201\n",
      "Iteration 1028, loss = 0.00384424\n",
      "Iteration 1029, loss = 0.00383648\n",
      "Iteration 1030, loss = 0.00382877\n",
      "Iteration 1031, loss = 0.00382106\n",
      "Iteration 1032, loss = 0.00381338\n",
      "Iteration 1033, loss = 0.00380574\n",
      "Iteration 1034, loss = 0.00379812\n",
      "Iteration 1035, loss = 0.00379050\n",
      "Iteration 1036, loss = 0.00378290\n",
      "Iteration 1037, loss = 0.00377534\n",
      "Iteration 1038, loss = 0.00376778\n",
      "Iteration 1039, loss = 0.00376024\n",
      "Iteration 1040, loss = 0.00375275\n",
      "Iteration 1041, loss = 0.00374526\n",
      "Iteration 1042, loss = 0.00373780\n",
      "Iteration 1043, loss = 0.00373038\n",
      "Iteration 1044, loss = 0.00372298\n",
      "Iteration 1045, loss = 0.00371560\n",
      "Iteration 1046, loss = 0.00370824\n",
      "Iteration 1047, loss = 0.00370092\n",
      "Iteration 1048, loss = 0.00369361\n",
      "Iteration 1049, loss = 0.00368633\n",
      "Iteration 1050, loss = 0.00367906\n",
      "Iteration 1051, loss = 0.00367179\n",
      "Iteration 1052, loss = 0.00366456\n",
      "Iteration 1053, loss = 0.00365735\n",
      "Iteration 1054, loss = 0.00365016\n",
      "Iteration 1055, loss = 0.00364298\n",
      "Iteration 1056, loss = 0.00363584\n",
      "Iteration 1057, loss = 0.00362870\n",
      "Iteration 1058, loss = 0.00362159\n",
      "Iteration 1059, loss = 0.00361451\n",
      "Iteration 1060, loss = 0.00360745\n",
      "Iteration 1061, loss = 0.00360041\n",
      "Iteration 1062, loss = 0.00359338\n",
      "Iteration 1063, loss = 0.00358636\n",
      "Iteration 1064, loss = 0.00357937\n",
      "Iteration 1065, loss = 0.00357241\n",
      "Iteration 1066, loss = 0.00356545\n",
      "Iteration 1067, loss = 0.00355851\n",
      "Iteration 1068, loss = 0.00355160\n",
      "Iteration 1069, loss = 0.00354470\n",
      "Iteration 1070, loss = 0.00353783\n",
      "Iteration 1071, loss = 0.00353098\n",
      "Iteration 1072, loss = 0.00352414\n",
      "Iteration 1073, loss = 0.00351733\n",
      "Iteration 1074, loss = 0.00351053\n",
      "Iteration 1075, loss = 0.00350375\n",
      "Iteration 1076, loss = 0.00349699\n",
      "Iteration 1077, loss = 0.00349026\n",
      "Iteration 1078, loss = 0.00348353\n",
      "Iteration 1079, loss = 0.00347681\n",
      "Iteration 1080, loss = 0.00347013\n",
      "Iteration 1081, loss = 0.00346346\n",
      "Iteration 1082, loss = 0.00345680\n",
      "Iteration 1083, loss = 0.00345016\n",
      "Iteration 1084, loss = 0.00344355\n",
      "Iteration 1085, loss = 0.00343697\n",
      "Iteration 1086, loss = 0.00343039\n",
      "Iteration 1087, loss = 0.00342383\n",
      "Iteration 1088, loss = 0.00341729\n",
      "Iteration 1089, loss = 0.00341076\n",
      "Iteration 1090, loss = 0.00340426\n",
      "Iteration 1091, loss = 0.00339778\n",
      "Iteration 1092, loss = 0.00339131\n",
      "Iteration 1093, loss = 0.00338483\n",
      "Iteration 1094, loss = 0.00337840\n",
      "Iteration 1095, loss = 0.00337198\n",
      "Iteration 1096, loss = 0.00336558\n",
      "Iteration 1097, loss = 0.00335921\n",
      "Iteration 1098, loss = 0.00335284\n",
      "Iteration 1099, loss = 0.00334649\n",
      "Iteration 1100, loss = 0.00334016\n",
      "Iteration 1101, loss = 0.00333385\n",
      "Iteration 1102, loss = 0.00332757\n",
      "Iteration 1103, loss = 0.00332128\n",
      "Iteration 1104, loss = 0.00331501\n",
      "Iteration 1105, loss = 0.00330876\n",
      "Iteration 1106, loss = 0.00330253\n",
      "Iteration 1107, loss = 0.00329633\n",
      "Iteration 1108, loss = 0.00329013\n",
      "Iteration 1109, loss = 0.00328395\n",
      "Iteration 1110, loss = 0.00327778\n",
      "Iteration 1111, loss = 0.00327164\n",
      "Iteration 1112, loss = 0.00326551\n",
      "Iteration 1113, loss = 0.00325940\n",
      "Iteration 1114, loss = 0.00325330\n",
      "Iteration 1115, loss = 0.00324723\n",
      "Iteration 1116, loss = 0.00324116\n",
      "Iteration 1117, loss = 0.00323512\n",
      "Iteration 1118, loss = 0.00322909\n",
      "Iteration 1119, loss = 0.00322306\n",
      "Iteration 1120, loss = 0.00321707\n",
      "Iteration 1121, loss = 0.00321108\n",
      "Iteration 1122, loss = 0.00320511\n",
      "Iteration 1123, loss = 0.00319917\n",
      "Iteration 1124, loss = 0.00319322\n",
      "Iteration 1125, loss = 0.00318730\n",
      "Iteration 1126, loss = 0.00318140\n",
      "Iteration 1127, loss = 0.00317551\n",
      "Iteration 1128, loss = 0.00316964\n",
      "Iteration 1129, loss = 0.00316378\n",
      "Iteration 1130, loss = 0.00315794\n",
      "Iteration 1131, loss = 0.00315210\n",
      "Iteration 1132, loss = 0.00314629\n",
      "Iteration 1133, loss = 0.00314049\n",
      "Iteration 1134, loss = 0.00313471\n",
      "Iteration 1135, loss = 0.00312895\n",
      "Iteration 1136, loss = 0.00312319\n",
      "Iteration 1137, loss = 0.00311746\n",
      "Iteration 1138, loss = 0.00311174\n",
      "Iteration 1139, loss = 0.00310604\n",
      "Iteration 1140, loss = 0.00310034\n",
      "Iteration 1141, loss = 0.00309466\n",
      "Iteration 1142, loss = 0.00308900\n",
      "Iteration 1143, loss = 0.00308335\n",
      "Iteration 1144, loss = 0.00307771\n",
      "Iteration 1145, loss = 0.00307210\n",
      "Iteration 1146, loss = 0.00306650\n",
      "Iteration 1147, loss = 0.00306090\n",
      "Iteration 1148, loss = 0.00305534\n",
      "Iteration 1149, loss = 0.00304978\n",
      "Iteration 1150, loss = 0.00304423\n",
      "Iteration 1151, loss = 0.00303870\n",
      "Iteration 1152, loss = 0.00303318\n",
      "Iteration 1153, loss = 0.00302768\n",
      "Iteration 1154, loss = 0.00302219\n",
      "Iteration 1155, loss = 0.00301672\n",
      "Iteration 1156, loss = 0.00301125\n",
      "Iteration 1157, loss = 0.00300582\n",
      "Iteration 1158, loss = 0.00300038\n",
      "Iteration 1159, loss = 0.00299497\n",
      "Iteration 1160, loss = 0.00298955\n",
      "Iteration 1161, loss = 0.00298416\n",
      "Iteration 1162, loss = 0.00297879\n",
      "Iteration 1163, loss = 0.00297343\n",
      "Iteration 1164, loss = 0.00296808\n",
      "Iteration 1165, loss = 0.00296274\n",
      "Iteration 1166, loss = 0.00295742\n",
      "Iteration 1167, loss = 0.00295213\n",
      "Iteration 1168, loss = 0.00294685\n",
      "Iteration 1169, loss = 0.00294158\n",
      "Iteration 1170, loss = 0.00293632\n",
      "Iteration 1171, loss = 0.00293108\n",
      "Iteration 1172, loss = 0.00292585\n",
      "Iteration 1173, loss = 0.00292063\n",
      "Iteration 1174, loss = 0.00291543\n",
      "Iteration 1175, loss = 0.00291024\n",
      "Iteration 1176, loss = 0.00290506\n",
      "Iteration 1177, loss = 0.00289989\n",
      "Iteration 1178, loss = 0.00289475\n",
      "Iteration 1179, loss = 0.00288961\n",
      "Iteration 1180, loss = 0.00288448\n",
      "Iteration 1181, loss = 0.00287937\n",
      "Iteration 1182, loss = 0.00287427\n",
      "Iteration 1183, loss = 0.00286919\n",
      "Iteration 1184, loss = 0.00286411\n",
      "Iteration 1185, loss = 0.00285904\n",
      "Iteration 1186, loss = 0.00285398\n",
      "Iteration 1187, loss = 0.00284895\n",
      "Iteration 1188, loss = 0.00284393\n",
      "Iteration 1189, loss = 0.00283892\n",
      "Iteration 1190, loss = 0.00283392\n",
      "Iteration 1191, loss = 0.00282893\n",
      "Iteration 1192, loss = 0.00282397\n",
      "Iteration 1193, loss = 0.00281899\n",
      "Iteration 1194, loss = 0.00281405\n",
      "Iteration 1195, loss = 0.00280912\n",
      "Iteration 1196, loss = 0.00280419\n",
      "Iteration 1197, loss = 0.00279929\n",
      "Iteration 1198, loss = 0.00279438\n",
      "Iteration 1199, loss = 0.00278951\n",
      "Iteration 1200, loss = 0.00278462\n",
      "Iteration 1201, loss = 0.00277976\n",
      "Iteration 1202, loss = 0.00277491\n",
      "Iteration 1203, loss = 0.00277008\n",
      "Iteration 1204, loss = 0.00276525\n",
      "Iteration 1205, loss = 0.00276044\n",
      "Iteration 1206, loss = 0.00275564\n",
      "Iteration 1207, loss = 0.00275084\n",
      "Iteration 1208, loss = 0.00274607\n",
      "Iteration 1209, loss = 0.00274130\n",
      "Iteration 1210, loss = 0.00273655\n",
      "Iteration 1211, loss = 0.00273181\n",
      "Iteration 1212, loss = 0.00272708\n",
      "Iteration 1213, loss = 0.00272236\n",
      "Iteration 1214, loss = 0.00271765\n",
      "Iteration 1215, loss = 0.00271296\n",
      "Iteration 1216, loss = 0.00270827\n",
      "Iteration 1217, loss = 0.00270360\n",
      "Iteration 1218, loss = 0.00269893\n",
      "Iteration 1219, loss = 0.00269428\n",
      "Iteration 1220, loss = 0.00268964\n",
      "Iteration 1221, loss = 0.00268502\n",
      "Iteration 1222, loss = 0.00268040\n",
      "Iteration 1223, loss = 0.00267580\n",
      "Iteration 1224, loss = 0.00267121\n",
      "Iteration 1225, loss = 0.00266663\n",
      "Iteration 1226, loss = 0.00266206\n",
      "Iteration 1227, loss = 0.00265750\n",
      "Iteration 1228, loss = 0.00265295\n",
      "Iteration 1229, loss = 0.00264842\n",
      "Iteration 1230, loss = 0.00264390\n",
      "Iteration 1231, loss = 0.00263939\n",
      "Iteration 1232, loss = 0.00263489\n",
      "Iteration 1233, loss = 0.00263040\n",
      "Iteration 1234, loss = 0.00262593\n",
      "Iteration 1235, loss = 0.00262147\n",
      "Iteration 1236, loss = 0.00261702\n",
      "Iteration 1237, loss = 0.00261257\n",
      "Iteration 1238, loss = 0.00260814\n",
      "Iteration 1239, loss = 0.00260372\n",
      "Iteration 1240, loss = 0.00259931\n",
      "Iteration 1241, loss = 0.00259491\n",
      "Iteration 1242, loss = 0.00259053\n",
      "Iteration 1243, loss = 0.00258614\n",
      "Iteration 1244, loss = 0.00258177\n",
      "Iteration 1245, loss = 0.00257742\n",
      "Iteration 1246, loss = 0.00257307\n",
      "Iteration 1247, loss = 0.00256873\n",
      "Iteration 1248, loss = 0.00256441\n",
      "Iteration 1249, loss = 0.00256011\n",
      "Iteration 1250, loss = 0.00255580\n",
      "Iteration 1251, loss = 0.00255150\n",
      "Iteration 1252, loss = 0.00254721\n",
      "Iteration 1253, loss = 0.00254294\n",
      "Iteration 1254, loss = 0.00253869\n",
      "Iteration 1255, loss = 0.00253443\n",
      "Iteration 1256, loss = 0.00253019\n",
      "Iteration 1257, loss = 0.00252595\n",
      "Iteration 1258, loss = 0.00252174\n",
      "Iteration 1259, loss = 0.00251751\n",
      "Iteration 1260, loss = 0.00251331\n",
      "Iteration 1261, loss = 0.00250912\n",
      "Iteration 1262, loss = 0.00250494\n",
      "Iteration 1263, loss = 0.00250077\n",
      "Iteration 1264, loss = 0.00249660\n",
      "Iteration 1265, loss = 0.00249245\n",
      "Iteration 1266, loss = 0.00248831\n",
      "Iteration 1267, loss = 0.00248418\n",
      "Iteration 1268, loss = 0.00248006\n",
      "Iteration 1269, loss = 0.00247595\n",
      "Iteration 1270, loss = 0.00247184\n",
      "Iteration 1271, loss = 0.00246774\n",
      "Iteration 1272, loss = 0.00246366\n",
      "Iteration 1273, loss = 0.00245959\n",
      "Iteration 1274, loss = 0.00245552\n",
      "Iteration 1275, loss = 0.00245146\n",
      "Iteration 1276, loss = 0.00244742\n",
      "Iteration 1277, loss = 0.00244338\n",
      "Iteration 1278, loss = 0.00243935\n",
      "Iteration 1279, loss = 0.00243534\n",
      "Iteration 1280, loss = 0.00243134\n",
      "Iteration 1281, loss = 0.00242733\n",
      "Iteration 1282, loss = 0.00242335\n",
      "Iteration 1283, loss = 0.00241937\n",
      "Iteration 1284, loss = 0.00241539\n",
      "Iteration 1285, loss = 0.00241144\n",
      "Iteration 1286, loss = 0.00240748\n",
      "Iteration 1287, loss = 0.00240354\n",
      "Iteration 1288, loss = 0.00239961\n",
      "Iteration 1289, loss = 0.00239568\n",
      "Iteration 1290, loss = 0.00239177\n",
      "Iteration 1291, loss = 0.00238786\n",
      "Iteration 1292, loss = 0.00238397\n",
      "Iteration 1293, loss = 0.00238008\n",
      "Iteration 1294, loss = 0.00237620\n",
      "Iteration 1295, loss = 0.00237234\n",
      "Iteration 1296, loss = 0.00236848\n",
      "Iteration 1297, loss = 0.00236463\n",
      "Iteration 1298, loss = 0.00236079\n",
      "Iteration 1299, loss = 0.00235696\n",
      "Iteration 1300, loss = 0.00235313\n",
      "Iteration 1301, loss = 0.00234932\n",
      "Iteration 1302, loss = 0.00234551\n",
      "Iteration 1303, loss = 0.00234173\n",
      "Iteration 1304, loss = 0.00233793\n",
      "Iteration 1305, loss = 0.00233416\n",
      "Iteration 1306, loss = 0.00233038\n",
      "Iteration 1307, loss = 0.00232662\n",
      "Iteration 1308, loss = 0.00232287\n",
      "Iteration 1309, loss = 0.00231913\n",
      "Iteration 1310, loss = 0.00231539\n",
      "Iteration 1311, loss = 0.00231167\n",
      "Iteration 1312, loss = 0.00230795\n",
      "Iteration 1313, loss = 0.00230424\n",
      "Iteration 1314, loss = 0.00230054\n",
      "Iteration 1315, loss = 0.00229685\n",
      "Iteration 1316, loss = 0.00229316\n",
      "Iteration 1317, loss = 0.00228949\n",
      "Iteration 1318, loss = 0.00228582\n",
      "Iteration 1319, loss = 0.00228216\n",
      "Iteration 1320, loss = 0.00227851\n",
      "Iteration 1321, loss = 0.00227487\n",
      "Iteration 1322, loss = 0.00227123\n",
      "Iteration 1323, loss = 0.00226760\n",
      "Iteration 1324, loss = 0.00226399\n",
      "Iteration 1325, loss = 0.00226038\n",
      "Iteration 1326, loss = 0.00225678\n",
      "Iteration 1327, loss = 0.00225318\n",
      "Iteration 1328, loss = 0.00224959\n",
      "Iteration 1329, loss = 0.00224602\n",
      "Iteration 1330, loss = 0.00224245\n",
      "Iteration 1331, loss = 0.00223888\n",
      "Iteration 1332, loss = 0.00223533\n",
      "Iteration 1333, loss = 0.00223177\n",
      "Iteration 1334, loss = 0.00222825\n",
      "Iteration 1335, loss = 0.00222471\n",
      "Iteration 1336, loss = 0.00222120\n",
      "Iteration 1337, loss = 0.00221768\n",
      "Iteration 1338, loss = 0.00221418\n",
      "Iteration 1339, loss = 0.00221068\n",
      "Iteration 1340, loss = 0.00220718\n",
      "Iteration 1341, loss = 0.00220371\n",
      "Iteration 1342, loss = 0.00220023\n",
      "Iteration 1343, loss = 0.00219676\n",
      "Iteration 1344, loss = 0.00219330\n",
      "Iteration 1345, loss = 0.00218984\n",
      "Iteration 1346, loss = 0.00218640\n",
      "Iteration 1347, loss = 0.00218296\n",
      "Iteration 1348, loss = 0.00217953\n",
      "Iteration 1349, loss = 0.00217611\n",
      "Iteration 1350, loss = 0.00217269\n",
      "Iteration 1351, loss = 0.00216929\n",
      "Iteration 1352, loss = 0.00216589\n",
      "Iteration 1353, loss = 0.00216250\n",
      "Iteration 1354, loss = 0.00215911\n",
      "Iteration 1355, loss = 0.00215574\n",
      "Iteration 1356, loss = 0.00215237\n",
      "Iteration 1357, loss = 0.00214901\n",
      "Iteration 1358, loss = 0.00214566\n",
      "Iteration 1359, loss = 0.00214231\n",
      "Iteration 1360, loss = 0.00213897\n",
      "Iteration 1361, loss = 0.00213564\n",
      "Iteration 1362, loss = 0.00213231\n",
      "Iteration 1363, loss = 0.00212899\n",
      "Iteration 1364, loss = 0.00212568\n",
      "Iteration 1365, loss = 0.00212238\n",
      "Iteration 1366, loss = 0.00211909\n",
      "Iteration 1367, loss = 0.00211580\n",
      "Iteration 1368, loss = 0.00211252\n",
      "Iteration 1369, loss = 0.00210924\n",
      "Iteration 1370, loss = 0.00210597\n",
      "Iteration 1371, loss = 0.00210272\n",
      "Iteration 1372, loss = 0.00209946\n",
      "Iteration 1373, loss = 0.00209622\n",
      "Iteration 1374, loss = 0.00209299\n",
      "Iteration 1375, loss = 0.00208976\n",
      "Iteration 1376, loss = 0.00208653\n",
      "Iteration 1377, loss = 0.00208331\n",
      "Iteration 1378, loss = 0.00208011\n",
      "Iteration 1379, loss = 0.00207691\n",
      "Iteration 1380, loss = 0.00207371\n",
      "Iteration 1381, loss = 0.00207053\n",
      "Iteration 1382, loss = 0.00206735\n",
      "Iteration 1383, loss = 0.00206417\n",
      "Iteration 1384, loss = 0.00206100\n",
      "Iteration 1385, loss = 0.00205784\n",
      "Iteration 1386, loss = 0.00205469\n",
      "Iteration 1387, loss = 0.00205154\n",
      "Iteration 1388, loss = 0.00204841\n",
      "Iteration 1389, loss = 0.00204528\n",
      "Iteration 1390, loss = 0.00204215\n",
      "Iteration 1391, loss = 0.00203903\n",
      "Iteration 1392, loss = 0.00203591\n",
      "Iteration 1393, loss = 0.00203281\n",
      "Iteration 1394, loss = 0.00202971\n",
      "Iteration 1395, loss = 0.00202662\n",
      "Iteration 1396, loss = 0.00202353\n",
      "Iteration 1397, loss = 0.00202045\n",
      "Iteration 1398, loss = 0.00201739\n",
      "Iteration 1399, loss = 0.00201432\n",
      "Iteration 1400, loss = 0.00201126\n",
      "Iteration 1401, loss = 0.00200821\n",
      "Iteration 1402, loss = 0.00200517\n",
      "Iteration 1403, loss = 0.00200212\n",
      "Iteration 1404, loss = 0.00199909\n",
      "Iteration 1405, loss = 0.00199606\n",
      "Iteration 1406, loss = 0.00199304\n",
      "Iteration 1407, loss = 0.00199003\n",
      "Iteration 1408, loss = 0.00198703\n",
      "Iteration 1409, loss = 0.00198402\n",
      "Iteration 1410, loss = 0.00198103\n",
      "Iteration 1411, loss = 0.00197804\n",
      "Iteration 1412, loss = 0.00197506\n",
      "Iteration 1413, loss = 0.00197209\n",
      "Iteration 1414, loss = 0.00196912\n",
      "Iteration 1415, loss = 0.00196616\n",
      "Iteration 1416, loss = 0.00196320\n",
      "Iteration 1417, loss = 0.00196025\n",
      "Iteration 1418, loss = 0.00195732\n",
      "Iteration 1419, loss = 0.00195437\n",
      "Iteration 1420, loss = 0.00195144\n",
      "Iteration 1421, loss = 0.00194852\n",
      "Iteration 1422, loss = 0.00194560\n",
      "Iteration 1423, loss = 0.00194269\n",
      "Iteration 1424, loss = 0.00193978\n",
      "Iteration 1425, loss = 0.00193688\n",
      "Iteration 1426, loss = 0.00193399\n",
      "Iteration 1427, loss = 0.00193110\n",
      "Iteration 1428, loss = 0.00192822\n",
      "Iteration 1429, loss = 0.00192534\n",
      "Iteration 1430, loss = 0.00192247\n",
      "Iteration 1431, loss = 0.00191961\n",
      "Iteration 1432, loss = 0.00191675\n",
      "Iteration 1433, loss = 0.00191389\n",
      "Iteration 1434, loss = 0.00191104\n",
      "Iteration 1435, loss = 0.00190822\n",
      "Iteration 1436, loss = 0.00190538\n",
      "Iteration 1437, loss = 0.00190254\n",
      "Iteration 1438, loss = 0.00189973\n",
      "Iteration 1439, loss = 0.00189691\n",
      "Iteration 1440, loss = 0.00189410\n",
      "Iteration 1441, loss = 0.00189130\n",
      "Iteration 1442, loss = 0.00188850\n",
      "Iteration 1443, loss = 0.00188571\n",
      "Iteration 1444, loss = 0.00188292\n",
      "Iteration 1445, loss = 0.00188014\n",
      "Iteration 1446, loss = 0.00187736\n",
      "Iteration 1447, loss = 0.00187460\n",
      "Iteration 1448, loss = 0.00187184\n",
      "Iteration 1449, loss = 0.00186909\n",
      "Iteration 1450, loss = 0.00186634\n",
      "Iteration 1451, loss = 0.00186360\n",
      "Iteration 1452, loss = 0.00186086\n",
      "Iteration 1453, loss = 0.00185813\n",
      "Iteration 1454, loss = 0.00185541\n",
      "Iteration 1455, loss = 0.00185269\n",
      "Iteration 1456, loss = 0.00184998\n",
      "Iteration 1457, loss = 0.00184727\n",
      "Iteration 1458, loss = 0.00184457\n",
      "Iteration 1459, loss = 0.00184187\n",
      "Iteration 1460, loss = 0.00183918\n",
      "Iteration 1461, loss = 0.00183649\n",
      "Iteration 1462, loss = 0.00183381\n",
      "Iteration 1463, loss = 0.00183114\n",
      "Iteration 1464, loss = 0.00182847\n",
      "Iteration 1465, loss = 0.00182580\n",
      "Iteration 1466, loss = 0.00182315\n",
      "Iteration 1467, loss = 0.00182049\n",
      "Iteration 1468, loss = 0.00181784\n",
      "Iteration 1469, loss = 0.00181520\n",
      "Iteration 1470, loss = 0.00181257\n",
      "Iteration 1471, loss = 0.00180994\n",
      "Iteration 1472, loss = 0.00180732\n",
      "Iteration 1473, loss = 0.00180469\n",
      "Iteration 1474, loss = 0.00180208\n",
      "Iteration 1475, loss = 0.00179947\n",
      "Iteration 1476, loss = 0.00179686\n",
      "Iteration 1477, loss = 0.00179426\n",
      "Iteration 1478, loss = 0.00179167\n",
      "Iteration 1479, loss = 0.00178908\n",
      "Iteration 1480, loss = 0.00178650\n",
      "Iteration 1481, loss = 0.00178392\n",
      "Iteration 1482, loss = 0.00178135\n",
      "Iteration 1483, loss = 0.00177879\n",
      "Iteration 1484, loss = 0.00177622\n",
      "Iteration 1485, loss = 0.00177367\n",
      "Iteration 1486, loss = 0.00177111\n",
      "Iteration 1487, loss = 0.00176857\n",
      "Iteration 1488, loss = 0.00176603\n",
      "Iteration 1489, loss = 0.00176349\n",
      "Iteration 1490, loss = 0.00176096\n",
      "Iteration 1491, loss = 0.00175843\n",
      "Iteration 1492, loss = 0.00175591\n",
      "Iteration 1493, loss = 0.00175339\n",
      "Iteration 1494, loss = 0.00175088\n",
      "Iteration 1495, loss = 0.00174837\n",
      "Iteration 1496, loss = 0.00174587\n",
      "Iteration 1497, loss = 0.00174338\n",
      "Iteration 1498, loss = 0.00174089\n",
      "Iteration 1499, loss = 0.00173840\n",
      "Iteration 1500, loss = 0.00173592\n",
      "Iteration 1501, loss = 0.00173344\n",
      "Iteration 1502, loss = 0.00173097\n",
      "Iteration 1503, loss = 0.00172851\n",
      "Iteration 1504, loss = 0.00172605\n",
      "Iteration 1505, loss = 0.00172359\n",
      "Iteration 1506, loss = 0.00172114\n",
      "Iteration 1507, loss = 0.00171870\n",
      "Iteration 1508, loss = 0.00171625\n",
      "Iteration 1509, loss = 0.00171382\n",
      "Iteration 1510, loss = 0.00171138\n",
      "Iteration 1511, loss = 0.00170896\n",
      "Iteration 1512, loss = 0.00170654\n",
      "Iteration 1513, loss = 0.00170413\n",
      "Iteration 1514, loss = 0.00170171\n",
      "Iteration 1515, loss = 0.00169930\n",
      "Iteration 1516, loss = 0.00169690\n",
      "Iteration 1517, loss = 0.00169450\n",
      "Iteration 1518, loss = 0.00169210\n",
      "Iteration 1519, loss = 0.00168972\n",
      "Iteration 1520, loss = 0.00168733\n",
      "Iteration 1521, loss = 0.00168496\n",
      "Iteration 1522, loss = 0.00168258\n",
      "Iteration 1523, loss = 0.00168021\n",
      "Iteration 1524, loss = 0.00167784\n",
      "Iteration 1525, loss = 0.00167548\n",
      "Iteration 1526, loss = 0.00167312\n",
      "Iteration 1527, loss = 0.00167077\n",
      "Iteration 1528, loss = 0.00166842\n",
      "Iteration 1529, loss = 0.00166609\n",
      "Iteration 1530, loss = 0.00166375\n",
      "Iteration 1531, loss = 0.00166141\n",
      "Iteration 1532, loss = 0.00165909\n",
      "Iteration 1533, loss = 0.00165676\n",
      "Iteration 1534, loss = 0.00165444\n",
      "Iteration 1535, loss = 0.00165213\n",
      "Iteration 1536, loss = 0.00164981\n",
      "Iteration 1537, loss = 0.00164751\n",
      "Iteration 1538, loss = 0.00164521\n",
      "Iteration 1539, loss = 0.00164291\n",
      "Iteration 1540, loss = 0.00164062\n",
      "Iteration 1541, loss = 0.00163836\n",
      "Iteration 1542, loss = 0.00163609\n",
      "Iteration 1543, loss = 0.00163383\n",
      "Iteration 1544, loss = 0.00163157\n",
      "Iteration 1545, loss = 0.00162931\n",
      "Iteration 1546, loss = 0.00162705\n",
      "Iteration 1547, loss = 0.00162480\n",
      "Iteration 1548, loss = 0.00162255\n",
      "Iteration 1549, loss = 0.00162030\n",
      "Iteration 1550, loss = 0.00161805\n",
      "Iteration 1551, loss = 0.00161581\n",
      "Iteration 1552, loss = 0.00161359\n",
      "Iteration 1553, loss = 0.00161137\n",
      "Iteration 1554, loss = 0.00160914\n",
      "Iteration 1555, loss = 0.00160693\n",
      "Iteration 1556, loss = 0.00160472\n",
      "Iteration 1557, loss = 0.00160251\n",
      "Iteration 1558, loss = 0.00160031\n",
      "Iteration 1559, loss = 0.00159810\n",
      "Iteration 1560, loss = 0.00159591\n",
      "Iteration 1561, loss = 0.00159372\n",
      "Iteration 1562, loss = 0.00159153\n",
      "Iteration 1563, loss = 0.00158935\n",
      "Iteration 1564, loss = 0.00158717\n",
      "Iteration 1565, loss = 0.00158500\n",
      "Iteration 1566, loss = 0.00158283\n",
      "Iteration 1567, loss = 0.00158066\n",
      "Iteration 1568, loss = 0.00157850\n",
      "Iteration 1569, loss = 0.00157635\n",
      "Iteration 1570, loss = 0.00157420\n",
      "Iteration 1571, loss = 0.00157204\n",
      "Iteration 1572, loss = 0.00156990\n",
      "Iteration 1573, loss = 0.00156776\n",
      "Iteration 1574, loss = 0.00156562\n",
      "Iteration 1575, loss = 0.00156349\n",
      "Iteration 1576, loss = 0.00156136\n",
      "Iteration 1577, loss = 0.00155923\n",
      "Iteration 1578, loss = 0.00155711\n",
      "Iteration 1579, loss = 0.00155501\n",
      "Iteration 1580, loss = 0.00155289\n",
      "Iteration 1581, loss = 0.00155078\n",
      "Iteration 1582, loss = 0.00154868\n",
      "Iteration 1583, loss = 0.00154659\n",
      "Iteration 1584, loss = 0.00154449\n",
      "Iteration 1585, loss = 0.00154240\n",
      "Iteration 1586, loss = 0.00154032\n",
      "Iteration 1587, loss = 0.00153824\n",
      "Iteration 1588, loss = 0.00153616\n",
      "Iteration 1589, loss = 0.00153408\n",
      "Iteration 1590, loss = 0.00153201\n",
      "Iteration 1591, loss = 0.00152995\n",
      "Iteration 1592, loss = 0.00152789\n",
      "Iteration 1593, loss = 0.00152583\n",
      "Iteration 1594, loss = 0.00152378\n",
      "Iteration 1595, loss = 0.00152173\n",
      "Iteration 1596, loss = 0.00151968\n",
      "Iteration 1597, loss = 0.00151763\n",
      "Iteration 1598, loss = 0.00151559\n",
      "Iteration 1599, loss = 0.00151356\n",
      "Iteration 1600, loss = 0.00151153\n",
      "Iteration 1601, loss = 0.00150950\n",
      "Iteration 1602, loss = 0.00150747\n",
      "Iteration 1603, loss = 0.00150545\n",
      "Iteration 1604, loss = 0.00150343\n",
      "Iteration 1605, loss = 0.00150142\n",
      "Iteration 1606, loss = 0.00149941\n",
      "Iteration 1607, loss = 0.00149740\n",
      "Iteration 1608, loss = 0.00149539\n",
      "Iteration 1609, loss = 0.00149339\n",
      "Iteration 1610, loss = 0.00149140\n",
      "Iteration 1611, loss = 0.00148941\n",
      "Iteration 1612, loss = 0.00148742\n",
      "Iteration 1613, loss = 0.00148543\n",
      "Iteration 1614, loss = 0.00148345\n",
      "Iteration 1615, loss = 0.00148148\n",
      "Iteration 1616, loss = 0.00147951\n",
      "Iteration 1617, loss = 0.00147754\n",
      "Iteration 1618, loss = 0.00147557\n",
      "Iteration 1619, loss = 0.00147361\n",
      "Iteration 1620, loss = 0.00147165\n",
      "Iteration 1621, loss = 0.00146970\n",
      "Iteration 1622, loss = 0.00146775\n",
      "Iteration 1623, loss = 0.00146580\n",
      "Iteration 1624, loss = 0.00146386\n",
      "Iteration 1625, loss = 0.00146192\n",
      "Iteration 1626, loss = 0.00145998\n",
      "Iteration 1627, loss = 0.00145805\n",
      "Iteration 1628, loss = 0.00145612\n",
      "Iteration 1629, loss = 0.00145419\n",
      "Iteration 1630, loss = 0.00145227\n",
      "Iteration 1631, loss = 0.00145036\n",
      "Iteration 1632, loss = 0.00144844\n",
      "Iteration 1633, loss = 0.00144653\n",
      "Iteration 1634, loss = 0.00144462\n",
      "Iteration 1635, loss = 0.00144272\n",
      "Iteration 1636, loss = 0.00144082\n",
      "Iteration 1637, loss = 0.00143892\n",
      "Iteration 1638, loss = 0.00143703\n",
      "Iteration 1639, loss = 0.00143514\n",
      "Iteration 1640, loss = 0.00143325\n",
      "Iteration 1641, loss = 0.00143137\n",
      "Iteration 1642, loss = 0.00142949\n",
      "Iteration 1643, loss = 0.00142761\n",
      "Iteration 1644, loss = 0.00142574\n",
      "Iteration 1645, loss = 0.00142387\n",
      "Iteration 1646, loss = 0.00142200\n",
      "Iteration 1647, loss = 0.00142014\n",
      "Iteration 1648, loss = 0.00141828\n",
      "Iteration 1649, loss = 0.00141642\n",
      "Iteration 1650, loss = 0.00141457\n",
      "Iteration 1651, loss = 0.00141272\n",
      "Iteration 1652, loss = 0.00141088\n",
      "Iteration 1653, loss = 0.00140903\n",
      "Iteration 1654, loss = 0.00140719\n",
      "Iteration 1655, loss = 0.00140536\n",
      "Iteration 1656, loss = 0.00140352\n",
      "Iteration 1657, loss = 0.00140169\n",
      "Iteration 1658, loss = 0.00139986\n",
      "Iteration 1659, loss = 0.00139804\n",
      "Iteration 1660, loss = 0.00139622\n",
      "Iteration 1661, loss = 0.00139440\n",
      "Iteration 1662, loss = 0.00139259\n",
      "Iteration 1663, loss = 0.00139078\n",
      "Iteration 1664, loss = 0.00138897\n",
      "Iteration 1665, loss = 0.00138717\n",
      "Iteration 1666, loss = 0.00138537\n",
      "Iteration 1667, loss = 0.00138357\n",
      "Iteration 1668, loss = 0.00138178\n",
      "Iteration 1669, loss = 0.00137999\n",
      "Iteration 1670, loss = 0.00137820\n",
      "Iteration 1671, loss = 0.00137642\n",
      "Iteration 1672, loss = 0.00137464\n",
      "Iteration 1673, loss = 0.00137286\n",
      "Iteration 1674, loss = 0.00137109\n",
      "Iteration 1675, loss = 0.00136931\n",
      "Iteration 1676, loss = 0.00136754\n",
      "Iteration 1677, loss = 0.00136578\n",
      "Iteration 1678, loss = 0.00136402\n",
      "Iteration 1679, loss = 0.00136226\n",
      "Iteration 1680, loss = 0.00136051\n",
      "Iteration 1681, loss = 0.00135875\n",
      "Iteration 1682, loss = 0.00135700\n",
      "Iteration 1683, loss = 0.00135526\n",
      "Iteration 1684, loss = 0.00135352\n",
      "Iteration 1685, loss = 0.00135178\n",
      "Iteration 1686, loss = 0.00135004\n",
      "Iteration 1687, loss = 0.00134831\n",
      "Iteration 1688, loss = 0.00134658\n",
      "Iteration 1689, loss = 0.00134485\n",
      "Iteration 1690, loss = 0.00134313\n",
      "Iteration 1691, loss = 0.00134141\n",
      "Iteration 1692, loss = 0.00133969\n",
      "Iteration 1693, loss = 0.00133797\n",
      "Iteration 1694, loss = 0.00133626\n",
      "Iteration 1695, loss = 0.00133456\n",
      "Iteration 1696, loss = 0.00133285\n",
      "Iteration 1697, loss = 0.00133114\n",
      "Iteration 1698, loss = 0.00132945\n",
      "Iteration 1699, loss = 0.00132776\n",
      "Iteration 1700, loss = 0.00132606\n",
      "Iteration 1701, loss = 0.00132438\n",
      "Iteration 1702, loss = 0.00132269\n",
      "Iteration 1703, loss = 0.00132099\n",
      "Iteration 1704, loss = 0.00131932\n",
      "Iteration 1705, loss = 0.00131764\n",
      "Iteration 1706, loss = 0.00131596\n",
      "Iteration 1707, loss = 0.00131428\n",
      "Iteration 1708, loss = 0.00131261\n",
      "Iteration 1709, loss = 0.00131094\n",
      "Iteration 1710, loss = 0.00130928\n",
      "Iteration 1711, loss = 0.00130762\n",
      "Iteration 1712, loss = 0.00130596\n",
      "Iteration 1713, loss = 0.00130430\n",
      "Iteration 1714, loss = 0.00130265\n",
      "Iteration 1715, loss = 0.00130100\n",
      "Iteration 1716, loss = 0.00129935\n",
      "Iteration 1717, loss = 0.00129770\n",
      "Iteration 1718, loss = 0.00129606\n",
      "Iteration 1719, loss = 0.00129442\n",
      "Iteration 1720, loss = 0.00129278\n",
      "Iteration 1721, loss = 0.00129115\n",
      "Iteration 1722, loss = 0.00128952\n",
      "Iteration 1723, loss = 0.00128789\n",
      "Iteration 1724, loss = 0.00128627\n",
      "Iteration 1725, loss = 0.00128465\n",
      "Iteration 1726, loss = 0.00128303\n",
      "Iteration 1727, loss = 0.00128142\n",
      "Iteration 1728, loss = 0.00127980\n",
      "Iteration 1729, loss = 0.00127819\n",
      "Iteration 1730, loss = 0.00127658\n",
      "Iteration 1731, loss = 0.00127498\n",
      "Iteration 1732, loss = 0.00127337\n",
      "Iteration 1733, loss = 0.00127177\n",
      "Iteration 1734, loss = 0.00127017\n",
      "Iteration 1735, loss = 0.00126859\n",
      "Iteration 1736, loss = 0.00126699\n",
      "Iteration 1737, loss = 0.00126540\n",
      "Iteration 1738, loss = 0.00126382\n",
      "Iteration 1739, loss = 0.00126224\n",
      "Iteration 1740, loss = 0.00126066\n",
      "Iteration 1741, loss = 0.00125908\n",
      "Iteration 1742, loss = 0.00125750\n",
      "Iteration 1743, loss = 0.00125593\n",
      "Iteration 1744, loss = 0.00125437\n",
      "Iteration 1745, loss = 0.00125280\n",
      "Iteration 1746, loss = 0.00125123\n",
      "Iteration 1747, loss = 0.00124967\n",
      "Iteration 1748, loss = 0.00124811\n",
      "Iteration 1749, loss = 0.00124656\n",
      "Iteration 1750, loss = 0.00124501\n",
      "Iteration 1751, loss = 0.00124346\n",
      "Iteration 1752, loss = 0.00124191\n",
      "Iteration 1753, loss = 0.00124037\n",
      "Iteration 1754, loss = 0.00123882\n",
      "Iteration 1755, loss = 0.00123728\n",
      "Iteration 1756, loss = 0.00123575\n",
      "Iteration 1757, loss = 0.00123422\n",
      "Iteration 1758, loss = 0.00123269\n",
      "Iteration 1759, loss = 0.00123115\n",
      "Iteration 1760, loss = 0.00122963\n",
      "Iteration 1761, loss = 0.00122811\n",
      "Iteration 1762, loss = 0.00122659\n",
      "Iteration 1763, loss = 0.00122507\n",
      "Iteration 1764, loss = 0.00122355\n",
      "Iteration 1765, loss = 0.00122204\n",
      "Iteration 1766, loss = 0.00122053\n",
      "Iteration 1767, loss = 0.00121902\n",
      "Iteration 1768, loss = 0.00121752\n",
      "Iteration 1769, loss = 0.00121602\n",
      "Iteration 1770, loss = 0.00121452\n",
      "Iteration 1771, loss = 0.00121302\n",
      "Iteration 1772, loss = 0.00121153\n",
      "Iteration 1773, loss = 0.00121004\n",
      "Iteration 1774, loss = 0.00120855\n",
      "Iteration 1775, loss = 0.00120706\n",
      "Iteration 1776, loss = 0.00120557\n",
      "Iteration 1777, loss = 0.00120409\n",
      "Iteration 1778, loss = 0.00120261\n",
      "Iteration 1779, loss = 0.00120114\n",
      "Iteration 1780, loss = 0.00119967\n",
      "Iteration 1781, loss = 0.00119819\n",
      "Iteration 1782, loss = 0.00119672\n",
      "Iteration 1783, loss = 0.00119525\n",
      "Iteration 1784, loss = 0.00119379\n",
      "Iteration 1785, loss = 0.00119233\n",
      "Iteration 1786, loss = 0.00119087\n",
      "Iteration 1787, loss = 0.00118941\n",
      "Iteration 1788, loss = 0.00118796\n",
      "Iteration 1789, loss = 0.00118650\n",
      "Iteration 1790, loss = 0.00118506\n",
      "Iteration 1791, loss = 0.00118361\n",
      "Iteration 1792, loss = 0.00118216\n",
      "Iteration 1793, loss = 0.00118072\n",
      "Iteration 1794, loss = 0.00117928\n",
      "Iteration 1795, loss = 0.00117784\n",
      "Iteration 1796, loss = 0.00117641\n",
      "Iteration 1797, loss = 0.00117498\n",
      "Iteration 1798, loss = 0.00117355\n",
      "Iteration 1799, loss = 0.00117212\n",
      "Iteration 1800, loss = 0.00117070\n",
      "Iteration 1801, loss = 0.00116927\n",
      "Iteration 1802, loss = 0.00116785\n",
      "Iteration 1803, loss = 0.00116644\n",
      "Iteration 1804, loss = 0.00116502\n",
      "Iteration 1805, loss = 0.00116361\n",
      "Iteration 1806, loss = 0.00116220\n",
      "Iteration 1807, loss = 0.00116079\n",
      "Iteration 1808, loss = 0.00115938\n",
      "Iteration 1809, loss = 0.00115798\n",
      "Iteration 1810, loss = 0.00115658\n",
      "Iteration 1811, loss = 0.00115518\n",
      "Iteration 1812, loss = 0.00115378\n",
      "Iteration 1813, loss = 0.00115239\n",
      "Iteration 1814, loss = 0.00115100\n",
      "Iteration 1815, loss = 0.00114961\n",
      "Iteration 1816, loss = 0.00114822\n",
      "Iteration 1817, loss = 0.00114684\n",
      "Iteration 1818, loss = 0.00114545\n",
      "Iteration 1819, loss = 0.00114407\n",
      "Iteration 1820, loss = 0.00114270\n",
      "Iteration 1821, loss = 0.00114132\n",
      "Iteration 1822, loss = 0.00113995\n",
      "Iteration 1823, loss = 0.00113858\n",
      "Iteration 1824, loss = 0.00113721\n",
      "Iteration 1825, loss = 0.00113584\n",
      "Iteration 1826, loss = 0.00113448\n",
      "Iteration 1827, loss = 0.00113312\n",
      "Iteration 1828, loss = 0.00113176\n",
      "Iteration 1829, loss = 0.00113040\n",
      "Iteration 1830, loss = 0.00112904\n",
      "Iteration 1831, loss = 0.00112769\n",
      "Iteration 1832, loss = 0.00112634\n",
      "Iteration 1833, loss = 0.00112499\n",
      "Iteration 1834, loss = 0.00112365\n",
      "Iteration 1835, loss = 0.00112230\n",
      "Iteration 1836, loss = 0.00112096\n",
      "Iteration 1837, loss = 0.00111962\n",
      "Iteration 1838, loss = 0.00111829\n",
      "Iteration 1839, loss = 0.00111695\n",
      "Iteration 1840, loss = 0.00111562\n",
      "Iteration 1841, loss = 0.00111429\n",
      "Iteration 1842, loss = 0.00111296\n",
      "Iteration 1843, loss = 0.00111163\n",
      "Iteration 1844, loss = 0.00111031\n",
      "Iteration 1845, loss = 0.00110899\n",
      "Iteration 1846, loss = 0.00110767\n",
      "Iteration 1847, loss = 0.00110635\n",
      "Iteration 1848, loss = 0.00110504\n",
      "Iteration 1849, loss = 0.00110373\n",
      "Iteration 1850, loss = 0.00110242\n",
      "Iteration 1851, loss = 0.00110111\n",
      "Iteration 1852, loss = 0.00109980\n",
      "Iteration 1853, loss = 0.00109850\n",
      "Iteration 1854, loss = 0.00109719\n",
      "Iteration 1855, loss = 0.00109589\n",
      "Iteration 1856, loss = 0.00109459\n",
      "Iteration 1857, loss = 0.00109330\n",
      "Iteration 1858, loss = 0.00109201\n",
      "Iteration 1859, loss = 0.00109072\n",
      "Iteration 1860, loss = 0.00108943\n",
      "Iteration 1861, loss = 0.00108814\n",
      "Iteration 1862, loss = 0.00108686\n",
      "Iteration 1863, loss = 0.00108557\n",
      "Iteration 1864, loss = 0.00108429\n",
      "Iteration 1865, loss = 0.00108301\n",
      "Iteration 1866, loss = 0.00108174\n",
      "Iteration 1867, loss = 0.00108046\n",
      "Iteration 1868, loss = 0.00107919\n",
      "Iteration 1869, loss = 0.00107792\n",
      "Iteration 1870, loss = 0.00107665\n",
      "Iteration 1871, loss = 0.00107538\n",
      "Iteration 1872, loss = 0.00107412\n",
      "Iteration 1873, loss = 0.00107286\n",
      "Iteration 1874, loss = 0.00107160\n",
      "Iteration 1875, loss = 0.00107034\n",
      "Iteration 1876, loss = 0.00106909\n",
      "Iteration 1877, loss = 0.00106783\n",
      "Iteration 1878, loss = 0.00106658\n",
      "Iteration 1879, loss = 0.00106533\n",
      "Iteration 1880, loss = 0.00106408\n",
      "Iteration 1881, loss = 0.00106284\n",
      "Iteration 1882, loss = 0.00106159\n",
      "Iteration 1883, loss = 0.00106035\n",
      "Iteration 1884, loss = 0.00105911\n",
      "Iteration 1885, loss = 0.00105787\n",
      "Iteration 1886, loss = 0.00105664\n",
      "Iteration 1887, loss = 0.00105541\n",
      "Iteration 1888, loss = 0.00105417\n",
      "Iteration 1889, loss = 0.00105295\n",
      "Iteration 1890, loss = 0.00105172\n",
      "Iteration 1891, loss = 0.00105049\n",
      "Iteration 1892, loss = 0.00104927\n",
      "Iteration 1893, loss = 0.00104805\n",
      "Iteration 1894, loss = 0.00104683\n",
      "Iteration 1895, loss = 0.00104561\n",
      "Iteration 1896, loss = 0.00104439\n",
      "Iteration 1897, loss = 0.00104318\n",
      "Iteration 1898, loss = 0.00104197\n",
      "Iteration 1899, loss = 0.00104076\n",
      "Iteration 1900, loss = 0.00103955\n",
      "Iteration 1901, loss = 0.00103835\n",
      "Iteration 1902, loss = 0.00103714\n",
      "Iteration 1903, loss = 0.00103594\n",
      "Iteration 1904, loss = 0.00103474\n",
      "Iteration 1905, loss = 0.00103355\n",
      "Iteration 1906, loss = 0.00103235\n",
      "Iteration 1907, loss = 0.00103116\n",
      "Iteration 1908, loss = 0.00102997\n",
      "Iteration 1909, loss = 0.00102877\n",
      "Iteration 1910, loss = 0.00102759\n",
      "Iteration 1911, loss = 0.00102640\n",
      "Iteration 1912, loss = 0.00102522\n",
      "Iteration 1913, loss = 0.00102404\n",
      "Iteration 1914, loss = 0.00102286\n",
      "Iteration 1915, loss = 0.00102168\n",
      "Iteration 1916, loss = 0.00102051\n",
      "Iteration 1917, loss = 0.00101933\n",
      "Iteration 1918, loss = 0.00101816\n",
      "Iteration 1919, loss = 0.00101699\n",
      "Iteration 1920, loss = 0.00101583\n",
      "Iteration 1921, loss = 0.00101466\n",
      "Iteration 1922, loss = 0.00101349\n",
      "Iteration 1923, loss = 0.00101233\n",
      "Iteration 1924, loss = 0.00101117\n",
      "Iteration 1925, loss = 0.00101001\n",
      "Iteration 1926, loss = 0.00100885\n",
      "Iteration 1927, loss = 0.00100770\n",
      "Iteration 1928, loss = 0.00100655\n",
      "Iteration 1929, loss = 0.00100540\n",
      "Iteration 1930, loss = 0.00100425\n",
      "Iteration 1931, loss = 0.00100310\n",
      "Iteration 1932, loss = 0.00100196\n",
      "Iteration 1933, loss = 0.00100081\n",
      "Iteration 1934, loss = 0.00099967\n",
      "Iteration 1935, loss = 0.00099853\n",
      "Iteration 1936, loss = 0.00099739\n",
      "Iteration 1937, loss = 0.00099626\n",
      "Iteration 1938, loss = 0.00099512\n",
      "Iteration 1939, loss = 0.00099399\n",
      "Iteration 1940, loss = 0.00099287\n",
      "Iteration 1941, loss = 0.00099173\n",
      "Iteration 1942, loss = 0.00099061\n",
      "Iteration 1943, loss = 0.00098948\n",
      "Iteration 1944, loss = 0.00098836\n",
      "Iteration 1945, loss = 0.00098724\n",
      "Iteration 1946, loss = 0.00098612\n",
      "Iteration 1947, loss = 0.00098500\n",
      "Iteration 1948, loss = 0.00098389\n",
      "Iteration 1949, loss = 0.00098277\n",
      "Iteration 1950, loss = 0.00098166\n",
      "Iteration 1951, loss = 0.00098055\n",
      "Iteration 1952, loss = 0.00097944\n",
      "Iteration 1953, loss = 0.00097833\n",
      "Iteration 1954, loss = 0.00097723\n",
      "Iteration 1955, loss = 0.00097612\n",
      "Iteration 1956, loss = 0.00097502\n",
      "Iteration 1957, loss = 0.00097392\n",
      "Iteration 1958, loss = 0.00097282\n",
      "Iteration 1959, loss = 0.00097173\n",
      "Iteration 1960, loss = 0.00097063\n",
      "Iteration 1961, loss = 0.00096954\n",
      "Iteration 1962, loss = 0.00096845\n",
      "Iteration 1963, loss = 0.00096736\n",
      "Iteration 1964, loss = 0.00096627\n",
      "Iteration 1965, loss = 0.00096519\n",
      "Iteration 1966, loss = 0.00096410\n",
      "Iteration 1967, loss = 0.00096302\n",
      "Iteration 1968, loss = 0.00096194\n",
      "Iteration 1969, loss = 0.00096086\n",
      "Iteration 1970, loss = 0.00095978\n",
      "Iteration 1971, loss = 0.00095870\n",
      "Iteration 1972, loss = 0.00095763\n",
      "Iteration 1973, loss = 0.00095656\n",
      "Iteration 1974, loss = 0.00095548\n",
      "Iteration 1975, loss = 0.00095441\n",
      "Iteration 1976, loss = 0.00095335\n",
      "Iteration 1977, loss = 0.00095228\n",
      "Iteration 1978, loss = 0.00095122\n",
      "Iteration 1979, loss = 0.00095016\n",
      "Iteration 1980, loss = 0.00094910\n",
      "Iteration 1981, loss = 0.00094804\n",
      "Iteration 1982, loss = 0.00094698\n",
      "Iteration 1983, loss = 0.00094592\n",
      "Iteration 1984, loss = 0.00094487\n",
      "Iteration 1985, loss = 0.00094381\n",
      "Iteration 1986, loss = 0.00094276\n",
      "Iteration 1987, loss = 0.00094171\n",
      "Iteration 1988, loss = 0.00094066\n",
      "Iteration 1989, loss = 0.00093961\n",
      "Iteration 1990, loss = 0.00093857\n",
      "Iteration 1991, loss = 0.00093753\n",
      "Iteration 1992, loss = 0.00093649\n",
      "Iteration 1993, loss = 0.00093544\n",
      "Iteration 1994, loss = 0.00093440\n",
      "Iteration 1995, loss = 0.00093337\n",
      "Iteration 1996, loss = 0.00093233\n",
      "Iteration 1997, loss = 0.00093130\n",
      "Iteration 1998, loss = 0.00093026\n",
      "Iteration 1999, loss = 0.00092924\n",
      "Iteration 2000, loss = 0.00092821\n",
      "Iteration 2001, loss = 0.00092718\n",
      "Iteration 2002, loss = 0.00092615\n",
      "Iteration 2003, loss = 0.00092513\n",
      "Iteration 2004, loss = 0.00092411\n",
      "Iteration 2005, loss = 0.00092309\n",
      "Iteration 2006, loss = 0.00092207\n",
      "Iteration 2007, loss = 0.00092105\n",
      "Iteration 2008, loss = 0.00092003\n",
      "Iteration 2009, loss = 0.00091902\n",
      "Iteration 2010, loss = 0.00091800\n",
      "Iteration 2011, loss = 0.00091699\n",
      "Iteration 2012, loss = 0.00091598\n",
      "Iteration 2013, loss = 0.00091497\n",
      "Iteration 2014, loss = 0.00091397\n",
      "Iteration 2015, loss = 0.00091296\n",
      "Iteration 2016, loss = 0.00091195\n",
      "Iteration 2017, loss = 0.00091095\n",
      "Iteration 2018, loss = 0.00090995\n",
      "Iteration 2019, loss = 0.00090895\n",
      "Iteration 2020, loss = 0.00090795\n",
      "Iteration 2021, loss = 0.00090696\n",
      "Iteration 2022, loss = 0.00090596\n",
      "Iteration 2023, loss = 0.00090496\n",
      "Iteration 2024, loss = 0.00090397\n",
      "Iteration 2025, loss = 0.00090298\n",
      "Iteration 2026, loss = 0.00090200\n",
      "Iteration 2027, loss = 0.00090101\n",
      "Iteration 2028, loss = 0.00090002\n",
      "Iteration 2029, loss = 0.00089903\n",
      "Iteration 2030, loss = 0.00089805\n",
      "Training loss did not improve more than tol=0.000001 for 10 consecutive epochs. Stopping.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.93      1.00      0.97        14\n",
      "           2       0.95      0.95      0.95        20\n",
      "           3       1.00      0.95      0.97        20\n",
      "\n",
      "    accuracy                           0.96        54\n",
      "   macro avg       0.96      0.97      0.96        54\n",
      "weighted avg       0.96      0.96      0.96        54\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "hidden_layers = (30,)\n",
    "activation = 'relu'\n",
    "opts = dict(hidden_layer_sizes = hidden_layers,verbose = True,activation = activation,tol = 1e-6,max_iter = int(1e6))\n",
    "solver = 'adam'\n",
    "clf_MLP = MLPClassifier(solver = solver, **opts)\n",
    "clf_MLP.fit(X_train_,y_train)\n",
    "predictions = clf_MLP.predict(X_test_)\n",
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- hidden_layers = (512,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.09985299\n",
      "Iteration 2, loss = 1.04282847\n",
      "Iteration 3, loss = 1.02569089\n",
      "Iteration 4, loss = 1.01622552\n",
      "Iteration 5, loss = 0.99830429\n",
      "Iteration 6, loss = 0.97295542\n",
      "Iteration 7, loss = 0.94632683\n",
      "Iteration 8, loss = 0.92303893\n",
      "Iteration 9, loss = 0.90448078\n",
      "Iteration 10, loss = 0.88871232\n",
      "Iteration 11, loss = 0.87226694\n",
      "Iteration 12, loss = 0.85313248\n",
      "Iteration 13, loss = 0.83193468\n",
      "Iteration 14, loss = 0.81064671\n",
      "Iteration 15, loss = 0.79082802\n",
      "Iteration 16, loss = 0.77278823\n",
      "Iteration 17, loss = 0.75578367\n",
      "Iteration 18, loss = 0.73874194\n",
      "Iteration 19, loss = 0.72095376\n",
      "Iteration 20, loss = 0.70238167\n",
      "Iteration 21, loss = 0.68352619\n",
      "Iteration 22, loss = 0.66503578\n",
      "Iteration 23, loss = 0.64732052\n",
      "Iteration 24, loss = 0.63036529\n",
      "Iteration 25, loss = 0.61381617\n",
      "Iteration 26, loss = 0.59725970\n",
      "Iteration 27, loss = 0.58049385\n",
      "Iteration 28, loss = 0.56361418\n",
      "Iteration 29, loss = 0.54689800\n",
      "Iteration 30, loss = 0.53060865\n",
      "Iteration 31, loss = 0.51485860\n",
      "Iteration 32, loss = 0.49959326\n",
      "Iteration 33, loss = 0.48467092\n",
      "Iteration 34, loss = 0.46996910\n",
      "Iteration 35, loss = 0.45545329\n",
      "Iteration 36, loss = 0.44118143\n",
      "Iteration 37, loss = 0.42725811\n",
      "Iteration 38, loss = 0.41377459\n",
      "Iteration 39, loss = 0.40076855\n",
      "Iteration 40, loss = 0.38821984\n",
      "Iteration 41, loss = 0.37607603\n",
      "Iteration 42, loss = 0.36428779\n",
      "Iteration 43, loss = 0.35283331\n",
      "Iteration 44, loss = 0.34172207\n",
      "Iteration 45, loss = 0.33098150\n",
      "Iteration 46, loss = 0.32063781\n",
      "Iteration 47, loss = 0.31070197\n",
      "Iteration 48, loss = 0.30116598\n",
      "Iteration 49, loss = 0.29200836\n",
      "Iteration 50, loss = 0.28320375\n",
      "Iteration 51, loss = 0.27473121\n",
      "Iteration 52, loss = 0.26657810\n",
      "Iteration 53, loss = 0.25873902\n",
      "Iteration 54, loss = 0.25121174\n",
      "Iteration 55, loss = 0.24399274\n",
      "Iteration 56, loss = 0.23707433\n",
      "Iteration 57, loss = 0.23044412\n",
      "Iteration 58, loss = 0.22408658\n",
      "Iteration 59, loss = 0.21798537\n",
      "Iteration 60, loss = 0.21212556\n",
      "Iteration 61, loss = 0.20649478\n",
      "Iteration 62, loss = 0.20108326\n",
      "Iteration 63, loss = 0.19588307\n",
      "Iteration 64, loss = 0.19088700\n",
      "Iteration 65, loss = 0.18608758\n",
      "Iteration 66, loss = 0.18147667\n",
      "Iteration 67, loss = 0.17704540\n",
      "Iteration 68, loss = 0.17278458\n",
      "Iteration 69, loss = 0.16868520\n",
      "Iteration 70, loss = 0.16473884\n",
      "Iteration 71, loss = 0.16093795\n",
      "Iteration 72, loss = 0.15727586\n",
      "Iteration 73, loss = 0.15374656\n",
      "Iteration 74, loss = 0.15034453\n",
      "Iteration 75, loss = 0.14706449\n",
      "Iteration 76, loss = 0.14390123\n",
      "Iteration 77, loss = 0.14084957\n",
      "Iteration 78, loss = 0.13790439\n",
      "Iteration 79, loss = 0.13506074\n",
      "Iteration 80, loss = 0.13231389\n",
      "Iteration 81, loss = 0.12965942\n",
      "Iteration 82, loss = 0.12709328\n",
      "Iteration 83, loss = 0.12461171\n",
      "Iteration 84, loss = 0.12221124\n",
      "Iteration 85, loss = 0.11988860\n",
      "Iteration 86, loss = 0.11764071\n",
      "Iteration 87, loss = 0.11546455\n",
      "Iteration 88, loss = 0.11335721\n",
      "Iteration 89, loss = 0.11131588\n",
      "Iteration 90, loss = 0.10933782\n",
      "Iteration 91, loss = 0.10742042\n",
      "Iteration 92, loss = 0.10556116\n",
      "Iteration 93, loss = 0.10375768\n",
      "Iteration 94, loss = 0.10200774\n",
      "Iteration 95, loss = 0.10030924\n",
      "Iteration 96, loss = 0.09866016\n",
      "Iteration 97, loss = 0.09705858\n",
      "Iteration 98, loss = 0.09550269\n",
      "Iteration 99, loss = 0.09399071\n",
      "Iteration 100, loss = 0.09252096\n",
      "Iteration 101, loss = 0.09109181\n",
      "Iteration 102, loss = 0.08970168\n",
      "Iteration 103, loss = 0.08834908\n",
      "Iteration 104, loss = 0.08703258\n",
      "Iteration 105, loss = 0.08575080\n",
      "Iteration 106, loss = 0.08450244\n",
      "Iteration 107, loss = 0.08328627\n",
      "Iteration 108, loss = 0.08210110\n",
      "Iteration 109, loss = 0.08094581\n",
      "Iteration 110, loss = 0.07981933\n",
      "Iteration 111, loss = 0.07872061\n",
      "Iteration 112, loss = 0.07764869\n",
      "Iteration 113, loss = 0.07660260\n",
      "Iteration 114, loss = 0.07558144\n",
      "Iteration 115, loss = 0.07458435\n",
      "Iteration 116, loss = 0.07361048\n",
      "Iteration 117, loss = 0.07265906\n",
      "Iteration 118, loss = 0.07172931\n",
      "Iteration 119, loss = 0.07082051\n",
      "Iteration 120, loss = 0.06993198\n",
      "Iteration 121, loss = 0.06906304\n",
      "Iteration 122, loss = 0.06821306\n",
      "Iteration 123, loss = 0.06738143\n",
      "Iteration 124, loss = 0.06656758\n",
      "Iteration 125, loss = 0.06577094\n",
      "Iteration 126, loss = 0.06499097\n",
      "Iteration 127, loss = 0.06422717\n",
      "Iteration 128, loss = 0.06347904\n",
      "Iteration 129, loss = 0.06274611\n",
      "Iteration 130, loss = 0.06202791\n",
      "Iteration 131, loss = 0.06132402\n",
      "Iteration 132, loss = 0.06063400\n",
      "Iteration 133, loss = 0.05995746\n",
      "Iteration 134, loss = 0.05929402\n",
      "Iteration 135, loss = 0.05864328\n",
      "Iteration 136, loss = 0.05800490\n",
      "Iteration 137, loss = 0.05737854\n",
      "Iteration 138, loss = 0.05676384\n",
      "Iteration 139, loss = 0.05616050\n",
      "Iteration 140, loss = 0.05556820\n",
      "Iteration 141, loss = 0.05498664\n",
      "Iteration 142, loss = 0.05441554\n",
      "Iteration 143, loss = 0.05385462\n",
      "Iteration 144, loss = 0.05330359\n",
      "Iteration 145, loss = 0.05276222\n",
      "Iteration 146, loss = 0.05223024\n",
      "Iteration 147, loss = 0.05170741\n",
      "Iteration 148, loss = 0.05119350\n",
      "Iteration 149, loss = 0.05068828\n",
      "Iteration 150, loss = 0.05019153\n",
      "Iteration 151, loss = 0.04970304\n",
      "Iteration 152, loss = 0.04922260\n",
      "Iteration 153, loss = 0.04875003\n",
      "Iteration 154, loss = 0.04828512\n",
      "Iteration 155, loss = 0.04782768\n",
      "Iteration 156, loss = 0.04737755\n",
      "Iteration 157, loss = 0.04693454\n",
      "Iteration 158, loss = 0.04649849\n",
      "Iteration 159, loss = 0.04606923\n",
      "Iteration 160, loss = 0.04564661\n",
      "Iteration 161, loss = 0.04523046\n",
      "Iteration 162, loss = 0.04482065\n",
      "Iteration 163, loss = 0.04441703\n",
      "Iteration 164, loss = 0.04401946\n",
      "Iteration 165, loss = 0.04362780\n",
      "Iteration 166, loss = 0.04324192\n",
      "Iteration 167, loss = 0.04286169\n",
      "Iteration 168, loss = 0.04248699\n",
      "Iteration 169, loss = 0.04211771\n",
      "Iteration 170, loss = 0.04175372\n",
      "Iteration 171, loss = 0.04139490\n",
      "Iteration 172, loss = 0.04104116\n",
      "Iteration 173, loss = 0.04069238\n",
      "Iteration 174, loss = 0.04034846\n",
      "Iteration 175, loss = 0.04000930\n",
      "Iteration 176, loss = 0.03967480\n",
      "Iteration 177, loss = 0.03934486\n",
      "Iteration 178, loss = 0.03901940\n",
      "Iteration 179, loss = 0.03869832\n",
      "Iteration 180, loss = 0.03838153\n",
      "Iteration 181, loss = 0.03806896\n",
      "Iteration 182, loss = 0.03776051\n",
      "Iteration 183, loss = 0.03745610\n",
      "Iteration 184, loss = 0.03715567\n",
      "Iteration 185, loss = 0.03685912\n",
      "Iteration 186, loss = 0.03656639\n",
      "Iteration 187, loss = 0.03627741\n",
      "Iteration 188, loss = 0.03599210\n",
      "Iteration 189, loss = 0.03571040\n",
      "Iteration 190, loss = 0.03543223\n",
      "Iteration 191, loss = 0.03515754\n",
      "Iteration 192, loss = 0.03488625\n",
      "Iteration 193, loss = 0.03461831\n",
      "Iteration 194, loss = 0.03435366\n",
      "Iteration 195, loss = 0.03409224\n",
      "Iteration 196, loss = 0.03383399\n",
      "Iteration 197, loss = 0.03357885\n",
      "Iteration 198, loss = 0.03332677\n",
      "Iteration 199, loss = 0.03307769\n",
      "Iteration 200, loss = 0.03283157\n",
      "Iteration 201, loss = 0.03258836\n",
      "Iteration 202, loss = 0.03234799\n",
      "Iteration 203, loss = 0.03211043\n",
      "Iteration 204, loss = 0.03187563\n",
      "Iteration 205, loss = 0.03164354\n",
      "Iteration 206, loss = 0.03141411\n",
      "Iteration 207, loss = 0.03118731\n",
      "Iteration 208, loss = 0.03096308\n",
      "Iteration 209, loss = 0.03074140\n",
      "Iteration 210, loss = 0.03052220\n",
      "Iteration 211, loss = 0.03030546\n",
      "Iteration 212, loss = 0.03009114\n",
      "Iteration 213, loss = 0.02987919\n",
      "Iteration 214, loss = 0.02966958\n",
      "Iteration 215, loss = 0.02946227\n",
      "Iteration 216, loss = 0.02925723\n",
      "Iteration 217, loss = 0.02905442\n",
      "Iteration 218, loss = 0.02885381\n",
      "Iteration 219, loss = 0.02865535\n",
      "Iteration 220, loss = 0.02845903\n",
      "Iteration 221, loss = 0.02826480\n",
      "Iteration 222, loss = 0.02807264\n",
      "Iteration 223, loss = 0.02788251\n",
      "Iteration 224, loss = 0.02769439\n",
      "Iteration 225, loss = 0.02750824\n",
      "Iteration 226, loss = 0.02732403\n",
      "Iteration 227, loss = 0.02714174\n",
      "Iteration 228, loss = 0.02696133\n",
      "Iteration 229, loss = 0.02678279\n",
      "Iteration 230, loss = 0.02660607\n",
      "Iteration 231, loss = 0.02643117\n",
      "Iteration 232, loss = 0.02625804\n",
      "Iteration 233, loss = 0.02608667\n",
      "Iteration 234, loss = 0.02591703\n",
      "Iteration 235, loss = 0.02574909\n",
      "Iteration 236, loss = 0.02558283\n",
      "Iteration 237, loss = 0.02541824\n",
      "Iteration 238, loss = 0.02525528\n",
      "Iteration 239, loss = 0.02509393\n",
      "Iteration 240, loss = 0.02493416\n",
      "Iteration 241, loss = 0.02477597\n",
      "Iteration 242, loss = 0.02461933\n",
      "Iteration 243, loss = 0.02446420\n",
      "Iteration 244, loss = 0.02431059\n",
      "Iteration 245, loss = 0.02415846\n",
      "Iteration 246, loss = 0.02400779\n",
      "Iteration 247, loss = 0.02385857\n",
      "Iteration 248, loss = 0.02371077\n",
      "Iteration 249, loss = 0.02356438\n",
      "Iteration 250, loss = 0.02341938\n",
      "Iteration 251, loss = 0.02327574\n",
      "Iteration 252, loss = 0.02313346\n",
      "Iteration 253, loss = 0.02299251\n",
      "Iteration 254, loss = 0.02285288\n",
      "Iteration 255, loss = 0.02271456\n",
      "Iteration 256, loss = 0.02257751\n",
      "Iteration 257, loss = 0.02244173\n",
      "Iteration 258, loss = 0.02230720\n",
      "Iteration 259, loss = 0.02217390\n",
      "Iteration 260, loss = 0.02204183\n",
      "Iteration 261, loss = 0.02191096\n",
      "Iteration 262, loss = 0.02178127\n",
      "Iteration 263, loss = 0.02165276\n",
      "Iteration 264, loss = 0.02152541\n",
      "Iteration 265, loss = 0.02139920\n",
      "Iteration 266, loss = 0.02127413\n",
      "Iteration 267, loss = 0.02115017\n",
      "Iteration 268, loss = 0.02102731\n",
      "Iteration 269, loss = 0.02090554\n",
      "Iteration 270, loss = 0.02078485\n",
      "Iteration 271, loss = 0.02066522\n",
      "Iteration 272, loss = 0.02054664\n",
      "Iteration 273, loss = 0.02042910\n",
      "Iteration 274, loss = 0.02031258\n",
      "Iteration 275, loss = 0.02019708\n",
      "Iteration 276, loss = 0.02008258\n",
      "Iteration 277, loss = 0.01996906\n",
      "Iteration 278, loss = 0.01985652\n",
      "Iteration 279, loss = 0.01974495\n",
      "Iteration 280, loss = 0.01963433\n",
      "Iteration 281, loss = 0.01952465\n",
      "Iteration 282, loss = 0.01941591\n",
      "Iteration 283, loss = 0.01930809\n",
      "Iteration 284, loss = 0.01920117\n",
      "Iteration 285, loss = 0.01909516\n",
      "Iteration 286, loss = 0.01899004\n",
      "Iteration 287, loss = 0.01888579\n",
      "Iteration 288, loss = 0.01878241\n",
      "Iteration 289, loss = 0.01867990\n",
      "Iteration 290, loss = 0.01857823\n",
      "Iteration 291, loss = 0.01847740\n",
      "Iteration 292, loss = 0.01837741\n",
      "Iteration 293, loss = 0.01827823\n",
      "Iteration 294, loss = 0.01817986\n",
      "Iteration 295, loss = 0.01808230\n",
      "Iteration 296, loss = 0.01798553\n",
      "Iteration 297, loss = 0.01788955\n",
      "Iteration 298, loss = 0.01779434\n",
      "Iteration 299, loss = 0.01769990\n",
      "Iteration 300, loss = 0.01760622\n",
      "Iteration 301, loss = 0.01751329\n",
      "Iteration 302, loss = 0.01742110\n",
      "Iteration 303, loss = 0.01732965\n",
      "Iteration 304, loss = 0.01723892\n",
      "Iteration 305, loss = 0.01714891\n",
      "Iteration 306, loss = 0.01705961\n",
      "Iteration 307, loss = 0.01697102\n",
      "Iteration 308, loss = 0.01688311\n",
      "Iteration 309, loss = 0.01679590\n",
      "Iteration 310, loss = 0.01670937\n",
      "Iteration 311, loss = 0.01662351\n",
      "Iteration 312, loss = 0.01653832\n",
      "Iteration 313, loss = 0.01645379\n",
      "Iteration 314, loss = 0.01636991\n",
      "Iteration 315, loss = 0.01628667\n",
      "Iteration 316, loss = 0.01620408\n",
      "Iteration 317, loss = 0.01612211\n",
      "Iteration 318, loss = 0.01604077\n",
      "Iteration 319, loss = 0.01596005\n",
      "Iteration 320, loss = 0.01587994\n",
      "Iteration 321, loss = 0.01580044\n",
      "Iteration 322, loss = 0.01572154\n",
      "Iteration 323, loss = 0.01564323\n",
      "Iteration 324, loss = 0.01556551\n",
      "Iteration 325, loss = 0.01548837\n",
      "Iteration 326, loss = 0.01541180\n",
      "Iteration 327, loss = 0.01533580\n",
      "Iteration 328, loss = 0.01526037\n",
      "Iteration 329, loss = 0.01518550\n",
      "Iteration 330, loss = 0.01511118\n",
      "Iteration 331, loss = 0.01503741\n",
      "Iteration 332, loss = 0.01496417\n",
      "Iteration 333, loss = 0.01489148\n",
      "Iteration 334, loss = 0.01481931\n",
      "Iteration 335, loss = 0.01474767\n",
      "Iteration 336, loss = 0.01467655\n",
      "Iteration 337, loss = 0.01460594\n",
      "Iteration 338, loss = 0.01453585\n",
      "Iteration 339, loss = 0.01446626\n",
      "Iteration 340, loss = 0.01439717\n",
      "Iteration 341, loss = 0.01432857\n",
      "Iteration 342, loss = 0.01426047\n",
      "Iteration 343, loss = 0.01419285\n",
      "Iteration 344, loss = 0.01412571\n",
      "Iteration 345, loss = 0.01405904\n",
      "Iteration 346, loss = 0.01399285\n",
      "Iteration 347, loss = 0.01392713\n",
      "Iteration 348, loss = 0.01386187\n",
      "Iteration 349, loss = 0.01379706\n",
      "Iteration 350, loss = 0.01373271\n",
      "Iteration 351, loss = 0.01366881\n",
      "Iteration 352, loss = 0.01360535\n",
      "Iteration 353, loss = 0.01354234\n",
      "Iteration 354, loss = 0.01347976\n",
      "Iteration 355, loss = 0.01341761\n",
      "Iteration 356, loss = 0.01335589\n",
      "Iteration 357, loss = 0.01329460\n",
      "Iteration 358, loss = 0.01323373\n",
      "Iteration 359, loss = 0.01317327\n",
      "Iteration 360, loss = 0.01311323\n",
      "Iteration 361, loss = 0.01305359\n",
      "Iteration 362, loss = 0.01299436\n",
      "Iteration 363, loss = 0.01293553\n",
      "Iteration 364, loss = 0.01287710\n",
      "Iteration 365, loss = 0.01281906\n",
      "Iteration 366, loss = 0.01276141\n",
      "Iteration 367, loss = 0.01270415\n",
      "Iteration 368, loss = 0.01264727\n",
      "Iteration 369, loss = 0.01259078\n",
      "Iteration 370, loss = 0.01253466\n",
      "Iteration 371, loss = 0.01247891\n",
      "Iteration 372, loss = 0.01242353\n",
      "Iteration 373, loss = 0.01236851\n",
      "Iteration 374, loss = 0.01231386\n",
      "Iteration 375, loss = 0.01225957\n",
      "Iteration 376, loss = 0.01220564\n",
      "Iteration 377, loss = 0.01215206\n",
      "Iteration 378, loss = 0.01209883\n",
      "Iteration 379, loss = 0.01204594\n",
      "Iteration 380, loss = 0.01199340\n",
      "Iteration 381, loss = 0.01194120\n",
      "Iteration 382, loss = 0.01188934\n",
      "Iteration 383, loss = 0.01183781\n",
      "Iteration 384, loss = 0.01178662\n",
      "Iteration 385, loss = 0.01173575\n",
      "Iteration 386, loss = 0.01168521\n",
      "Iteration 387, loss = 0.01163500\n",
      "Iteration 388, loss = 0.01158510\n",
      "Iteration 389, loss = 0.01153552\n",
      "Iteration 390, loss = 0.01148626\n",
      "Iteration 391, loss = 0.01143731\n",
      "Iteration 392, loss = 0.01138866\n",
      "Iteration 393, loss = 0.01134033\n",
      "Iteration 394, loss = 0.01129230\n",
      "Iteration 395, loss = 0.01124457\n",
      "Iteration 396, loss = 0.01119714\n",
      "Iteration 397, loss = 0.01115001\n",
      "Iteration 398, loss = 0.01110317\n",
      "Iteration 399, loss = 0.01105662\n",
      "Iteration 400, loss = 0.01101036\n",
      "Iteration 401, loss = 0.01096439\n",
      "Iteration 402, loss = 0.01091870\n",
      "Iteration 403, loss = 0.01087330\n",
      "Iteration 404, loss = 0.01082817\n",
      "Iteration 405, loss = 0.01078332\n",
      "Iteration 406, loss = 0.01073875\n",
      "Iteration 407, loss = 0.01069445\n",
      "Iteration 408, loss = 0.01065042\n",
      "Iteration 409, loss = 0.01060666\n",
      "Iteration 410, loss = 0.01056316\n",
      "Iteration 411, loss = 0.01051993\n",
      "Iteration 412, loss = 0.01047695\n",
      "Iteration 413, loss = 0.01043424\n",
      "Iteration 414, loss = 0.01039179\n",
      "Iteration 415, loss = 0.01034959\n",
      "Iteration 416, loss = 0.01030764\n",
      "Iteration 417, loss = 0.01026594\n",
      "Iteration 418, loss = 0.01022450\n",
      "Iteration 419, loss = 0.01018330\n",
      "Iteration 420, loss = 0.01014234\n",
      "Iteration 421, loss = 0.01010163\n",
      "Iteration 422, loss = 0.01006116\n",
      "Iteration 423, loss = 0.01002093\n",
      "Iteration 424, loss = 0.00998094\n",
      "Iteration 425, loss = 0.00994118\n",
      "Iteration 426, loss = 0.00990165\n",
      "Iteration 427, loss = 0.00986236\n",
      "Iteration 428, loss = 0.00982330\n",
      "Iteration 429, loss = 0.00978446\n",
      "Iteration 430, loss = 0.00974585\n",
      "Iteration 431, loss = 0.00970747\n",
      "Iteration 432, loss = 0.00966930\n",
      "Iteration 433, loss = 0.00963136\n",
      "Iteration 434, loss = 0.00959364\n",
      "Iteration 435, loss = 0.00955614\n",
      "Iteration 436, loss = 0.00951885\n",
      "Iteration 437, loss = 0.00948177\n",
      "Iteration 438, loss = 0.00944491\n",
      "Iteration 439, loss = 0.00940826\n",
      "Iteration 440, loss = 0.00937182\n",
      "Iteration 441, loss = 0.00933558\n",
      "Iteration 442, loss = 0.00929955\n",
      "Iteration 443, loss = 0.00926373\n",
      "Iteration 444, loss = 0.00922811\n",
      "Iteration 445, loss = 0.00919269\n",
      "Iteration 446, loss = 0.00915747\n",
      "Iteration 447, loss = 0.00912245\n",
      "Iteration 448, loss = 0.00908762\n",
      "Iteration 449, loss = 0.00905299\n",
      "Iteration 450, loss = 0.00901855\n",
      "Iteration 451, loss = 0.00898431\n",
      "Iteration 452, loss = 0.00895026\n",
      "Iteration 453, loss = 0.00891639\n",
      "Iteration 454, loss = 0.00888272\n",
      "Iteration 455, loss = 0.00884923\n",
      "Iteration 456, loss = 0.00881593\n",
      "Iteration 457, loss = 0.00878281\n",
      "Iteration 458, loss = 0.00874987\n",
      "Iteration 459, loss = 0.00871711\n",
      "Iteration 460, loss = 0.00868454\n",
      "Iteration 461, loss = 0.00865214\n",
      "Iteration 462, loss = 0.00861992\n",
      "Iteration 463, loss = 0.00858788\n",
      "Iteration 464, loss = 0.00855601\n",
      "Iteration 465, loss = 0.00852431\n",
      "Iteration 466, loss = 0.00849278\n",
      "Iteration 467, loss = 0.00846143\n",
      "Iteration 468, loss = 0.00843025\n",
      "Iteration 469, loss = 0.00839923\n",
      "Iteration 470, loss = 0.00836838\n",
      "Iteration 471, loss = 0.00833770\n",
      "Iteration 472, loss = 0.00830718\n",
      "Iteration 473, loss = 0.00827682\n",
      "Iteration 474, loss = 0.00824663\n",
      "Iteration 475, loss = 0.00821660\n",
      "Iteration 476, loss = 0.00818673\n",
      "Iteration 477, loss = 0.00815702\n",
      "Iteration 478, loss = 0.00812746\n",
      "Iteration 479, loss = 0.00809806\n",
      "Iteration 480, loss = 0.00806882\n",
      "Iteration 481, loss = 0.00803973\n",
      "Iteration 482, loss = 0.00801080\n",
      "Iteration 483, loss = 0.00798202\n",
      "Iteration 484, loss = 0.00795339\n",
      "Iteration 485, loss = 0.00792490\n",
      "Iteration 486, loss = 0.00789657\n",
      "Iteration 487, loss = 0.00786839\n",
      "Iteration 488, loss = 0.00784035\n",
      "Iteration 489, loss = 0.00781246\n",
      "Iteration 490, loss = 0.00778471\n",
      "Iteration 491, loss = 0.00775711\n",
      "Iteration 492, loss = 0.00772965\n",
      "Iteration 493, loss = 0.00770233\n",
      "Iteration 494, loss = 0.00767516\n",
      "Iteration 495, loss = 0.00764812\n",
      "Iteration 496, loss = 0.00762122\n",
      "Iteration 497, loss = 0.00759446\n",
      "Iteration 498, loss = 0.00756784\n",
      "Iteration 499, loss = 0.00754135\n",
      "Iteration 500, loss = 0.00751500\n",
      "Iteration 501, loss = 0.00748879\n",
      "Iteration 502, loss = 0.00746270\n",
      "Iteration 503, loss = 0.00743675\n",
      "Iteration 504, loss = 0.00741093\n",
      "Iteration 505, loss = 0.00738524\n",
      "Iteration 506, loss = 0.00735969\n",
      "Iteration 507, loss = 0.00733426\n",
      "Iteration 508, loss = 0.00730895\n",
      "Iteration 509, loss = 0.00728378\n",
      "Iteration 510, loss = 0.00725873\n",
      "Iteration 511, loss = 0.00723381\n",
      "Iteration 512, loss = 0.00720901\n",
      "Iteration 513, loss = 0.00718434\n",
      "Iteration 514, loss = 0.00715979\n",
      "Iteration 515, loss = 0.00713536\n",
      "Iteration 516, loss = 0.00711105\n",
      "Iteration 517, loss = 0.00708686\n",
      "Iteration 518, loss = 0.00706280\n",
      "Iteration 519, loss = 0.00703885\n",
      "Iteration 520, loss = 0.00701502\n",
      "Iteration 521, loss = 0.00699131\n",
      "Iteration 522, loss = 0.00696771\n",
      "Iteration 523, loss = 0.00694423\n",
      "Iteration 524, loss = 0.00692087\n",
      "Iteration 525, loss = 0.00689762\n",
      "Iteration 526, loss = 0.00687448\n",
      "Iteration 527, loss = 0.00685146\n",
      "Iteration 528, loss = 0.00682855\n",
      "Iteration 529, loss = 0.00680575\n",
      "Iteration 530, loss = 0.00678306\n",
      "Iteration 531, loss = 0.00676048\n",
      "Iteration 532, loss = 0.00673801\n",
      "Iteration 533, loss = 0.00671565\n",
      "Iteration 534, loss = 0.00669340\n",
      "Iteration 535, loss = 0.00667125\n",
      "Iteration 536, loss = 0.00664921\n",
      "Iteration 537, loss = 0.00662728\n",
      "Iteration 538, loss = 0.00660545\n",
      "Iteration 539, loss = 0.00658373\n",
      "Iteration 540, loss = 0.00656211\n",
      "Iteration 541, loss = 0.00654059\n",
      "Iteration 542, loss = 0.00651918\n",
      "Iteration 543, loss = 0.00649787\n",
      "Iteration 544, loss = 0.00647666\n",
      "Iteration 545, loss = 0.00645555\n",
      "Iteration 546, loss = 0.00643454\n",
      "Iteration 547, loss = 0.00641363\n",
      "Iteration 548, loss = 0.00639281\n",
      "Iteration 549, loss = 0.00637210\n",
      "Iteration 550, loss = 0.00635148\n",
      "Iteration 551, loss = 0.00633096\n",
      "Iteration 552, loss = 0.00631054\n",
      "Iteration 553, loss = 0.00629021\n",
      "Iteration 554, loss = 0.00626998\n",
      "Iteration 555, loss = 0.00624984\n",
      "Iteration 556, loss = 0.00622980\n",
      "Iteration 557, loss = 0.00620985\n",
      "Iteration 558, loss = 0.00618999\n",
      "Iteration 559, loss = 0.00617022\n",
      "Iteration 560, loss = 0.00615055\n",
      "Iteration 561, loss = 0.00613097\n",
      "Iteration 562, loss = 0.00611147\n",
      "Iteration 563, loss = 0.00609207\n",
      "Iteration 564, loss = 0.00607276\n",
      "Iteration 565, loss = 0.00605353\n",
      "Iteration 566, loss = 0.00603440\n",
      "Iteration 567, loss = 0.00601535\n",
      "Iteration 568, loss = 0.00599639\n",
      "Iteration 569, loss = 0.00597751\n",
      "Iteration 570, loss = 0.00595873\n",
      "Iteration 571, loss = 0.00594002\n",
      "Iteration 572, loss = 0.00592141\n",
      "Iteration 573, loss = 0.00590287\n",
      "Iteration 574, loss = 0.00588443\n",
      "Iteration 575, loss = 0.00586606\n",
      "Iteration 576, loss = 0.00584778\n",
      "Iteration 577, loss = 0.00582958\n",
      "Iteration 578, loss = 0.00581146\n",
      "Iteration 579, loss = 0.00579343\n",
      "Iteration 580, loss = 0.00577548\n",
      "Iteration 581, loss = 0.00575760\n",
      "Iteration 582, loss = 0.00573981\n",
      "Iteration 583, loss = 0.00572210\n",
      "Iteration 584, loss = 0.00570446\n",
      "Iteration 585, loss = 0.00568691\n",
      "Iteration 586, loss = 0.00566943\n",
      "Iteration 587, loss = 0.00565203\n",
      "Iteration 588, loss = 0.00563471\n",
      "Iteration 589, loss = 0.00561746\n",
      "Iteration 590, loss = 0.00560030\n",
      "Iteration 591, loss = 0.00558320\n",
      "Iteration 592, loss = 0.00556619\n",
      "Iteration 593, loss = 0.00554925\n",
      "Iteration 594, loss = 0.00553238\n",
      "Iteration 595, loss = 0.00551559\n",
      "Iteration 596, loss = 0.00549887\n",
      "Iteration 597, loss = 0.00548222\n",
      "Iteration 598, loss = 0.00546565\n",
      "Iteration 599, loss = 0.00544915\n",
      "Iteration 600, loss = 0.00543272\n",
      "Iteration 601, loss = 0.00541637\n",
      "Iteration 602, loss = 0.00540008\n",
      "Iteration 603, loss = 0.00538387\n",
      "Iteration 604, loss = 0.00536773\n",
      "Iteration 605, loss = 0.00535166\n",
      "Iteration 606, loss = 0.00533565\n",
      "Iteration 607, loss = 0.00531972\n",
      "Iteration 608, loss = 0.00530386\n",
      "Iteration 609, loss = 0.00528806\n",
      "Iteration 610, loss = 0.00527233\n",
      "Iteration 611, loss = 0.00525667\n",
      "Iteration 612, loss = 0.00524108\n",
      "Iteration 613, loss = 0.00522555\n",
      "Iteration 614, loss = 0.00521009\n",
      "Iteration 615, loss = 0.00519470\n",
      "Iteration 616, loss = 0.00517937\n",
      "Iteration 617, loss = 0.00516411\n",
      "Iteration 618, loss = 0.00514892\n",
      "Iteration 619, loss = 0.00513378\n",
      "Iteration 620, loss = 0.00511872\n",
      "Iteration 621, loss = 0.00510371\n",
      "Iteration 622, loss = 0.00508877\n",
      "Iteration 623, loss = 0.00507390\n",
      "Iteration 624, loss = 0.00505909\n",
      "Iteration 625, loss = 0.00504434\n",
      "Iteration 626, loss = 0.00502965\n",
      "Iteration 627, loss = 0.00501502\n",
      "Iteration 628, loss = 0.00500046\n",
      "Iteration 629, loss = 0.00498595\n",
      "Iteration 630, loss = 0.00497151\n",
      "Iteration 631, loss = 0.00495713\n",
      "Iteration 632, loss = 0.00494281\n",
      "Iteration 633, loss = 0.00492854\n",
      "Iteration 634, loss = 0.00491434\n",
      "Iteration 635, loss = 0.00490020\n",
      "Iteration 636, loss = 0.00488611\n",
      "Iteration 637, loss = 0.00487209\n",
      "Iteration 638, loss = 0.00485812\n",
      "Iteration 639, loss = 0.00484421\n",
      "Iteration 640, loss = 0.00483036\n",
      "Iteration 641, loss = 0.00481657\n",
      "Iteration 642, loss = 0.00480283\n",
      "Iteration 643, loss = 0.00478915\n",
      "Iteration 644, loss = 0.00477552\n",
      "Iteration 645, loss = 0.00476196\n",
      "Iteration 646, loss = 0.00474844\n",
      "Iteration 647, loss = 0.00473499\n",
      "Iteration 648, loss = 0.00472159\n",
      "Iteration 649, loss = 0.00470824\n",
      "Iteration 650, loss = 0.00469495\n",
      "Iteration 651, loss = 0.00468171\n",
      "Iteration 652, loss = 0.00466853\n",
      "Iteration 653, loss = 0.00465540\n",
      "Iteration 654, loss = 0.00464232\n",
      "Iteration 655, loss = 0.00462930\n",
      "Iteration 656, loss = 0.00461633\n",
      "Iteration 657, loss = 0.00460341\n",
      "Iteration 658, loss = 0.00459054\n",
      "Iteration 659, loss = 0.00457773\n",
      "Iteration 660, loss = 0.00456497\n",
      "Iteration 661, loss = 0.00455226\n",
      "Iteration 662, loss = 0.00453960\n",
      "Iteration 663, loss = 0.00452699\n",
      "Iteration 664, loss = 0.00451443\n",
      "Iteration 665, loss = 0.00450193\n",
      "Iteration 666, loss = 0.00448947\n",
      "Iteration 667, loss = 0.00447706\n",
      "Iteration 668, loss = 0.00446471\n",
      "Iteration 669, loss = 0.00445240\n",
      "Iteration 670, loss = 0.00444014\n",
      "Iteration 671, loss = 0.00442793\n",
      "Iteration 672, loss = 0.00441577\n",
      "Iteration 673, loss = 0.00440366\n",
      "Iteration 674, loss = 0.00439159\n",
      "Iteration 675, loss = 0.00437957\n",
      "Iteration 676, loss = 0.00436761\n",
      "Iteration 677, loss = 0.00435568\n",
      "Iteration 678, loss = 0.00434381\n",
      "Iteration 679, loss = 0.00433198\n",
      "Iteration 680, loss = 0.00432020\n",
      "Iteration 681, loss = 0.00430846\n",
      "Iteration 682, loss = 0.00429677\n",
      "Iteration 683, loss = 0.00428513\n",
      "Iteration 684, loss = 0.00427353\n",
      "Iteration 685, loss = 0.00426198\n",
      "Iteration 686, loss = 0.00425048\n",
      "Iteration 687, loss = 0.00423901\n",
      "Iteration 688, loss = 0.00422760\n",
      "Iteration 689, loss = 0.00421622\n",
      "Iteration 690, loss = 0.00420490\n",
      "Iteration 691, loss = 0.00419361\n",
      "Iteration 692, loss = 0.00418237\n",
      "Iteration 693, loss = 0.00417117\n",
      "Iteration 694, loss = 0.00416002\n",
      "Iteration 695, loss = 0.00414891\n",
      "Iteration 696, loss = 0.00413784\n",
      "Iteration 697, loss = 0.00412682\n",
      "Iteration 698, loss = 0.00411584\n",
      "Iteration 699, loss = 0.00410490\n",
      "Iteration 700, loss = 0.00409400\n",
      "Iteration 701, loss = 0.00408314\n",
      "Iteration 702, loss = 0.00407233\n",
      "Iteration 703, loss = 0.00406156\n",
      "Iteration 704, loss = 0.00405083\n",
      "Iteration 705, loss = 0.00404014\n",
      "Iteration 706, loss = 0.00402949\n",
      "Iteration 707, loss = 0.00401888\n",
      "Iteration 708, loss = 0.00400831\n",
      "Iteration 709, loss = 0.00399778\n",
      "Iteration 710, loss = 0.00398729\n",
      "Iteration 711, loss = 0.00397685\n",
      "Iteration 712, loss = 0.00396644\n",
      "Iteration 713, loss = 0.00395607\n",
      "Iteration 714, loss = 0.00394574\n",
      "Iteration 715, loss = 0.00393545\n",
      "Iteration 716, loss = 0.00392520\n",
      "Iteration 717, loss = 0.00391498\n",
      "Iteration 718, loss = 0.00390481\n",
      "Iteration 719, loss = 0.00389467\n",
      "Iteration 720, loss = 0.00388457\n",
      "Iteration 721, loss = 0.00387451\n",
      "Iteration 722, loss = 0.00386449\n",
      "Iteration 723, loss = 0.00385450\n",
      "Iteration 724, loss = 0.00384456\n",
      "Iteration 725, loss = 0.00383465\n",
      "Iteration 726, loss = 0.00382477\n",
      "Iteration 727, loss = 0.00381493\n",
      "Iteration 728, loss = 0.00380513\n",
      "Iteration 729, loss = 0.00379537\n",
      "Iteration 730, loss = 0.00378564\n",
      "Iteration 731, loss = 0.00377595\n",
      "Iteration 732, loss = 0.00376630\n",
      "Iteration 733, loss = 0.00375668\n",
      "Iteration 734, loss = 0.00374709\n",
      "Iteration 735, loss = 0.00373754\n",
      "Iteration 736, loss = 0.00372803\n",
      "Iteration 737, loss = 0.00371855\n",
      "Iteration 738, loss = 0.00370911\n",
      "Iteration 739, loss = 0.00369970\n",
      "Iteration 740, loss = 0.00369033\n",
      "Iteration 741, loss = 0.00368099\n",
      "Iteration 742, loss = 0.00367168\n",
      "Iteration 743, loss = 0.00366241\n",
      "Iteration 744, loss = 0.00365317\n",
      "Iteration 745, loss = 0.00364397\n",
      "Iteration 746, loss = 0.00363480\n",
      "Iteration 747, loss = 0.00362566\n",
      "Iteration 748, loss = 0.00361656\n",
      "Iteration 749, loss = 0.00360749\n",
      "Iteration 750, loss = 0.00359845\n",
      "Iteration 751, loss = 0.00358945\n",
      "Iteration 752, loss = 0.00358047\n",
      "Iteration 753, loss = 0.00357153\n",
      "Iteration 754, loss = 0.00356263\n",
      "Iteration 755, loss = 0.00355375\n",
      "Iteration 756, loss = 0.00354491\n",
      "Iteration 757, loss = 0.00353610\n",
      "Iteration 758, loss = 0.00352732\n",
      "Iteration 759, loss = 0.00351857\n",
      "Iteration 760, loss = 0.00350986\n",
      "Iteration 761, loss = 0.00350117\n",
      "Iteration 762, loss = 0.00349252\n",
      "Iteration 763, loss = 0.00348389\n",
      "Iteration 764, loss = 0.00347530\n",
      "Iteration 765, loss = 0.00346674\n",
      "Iteration 766, loss = 0.00345821\n",
      "Iteration 767, loss = 0.00344971\n",
      "Iteration 768, loss = 0.00344124\n",
      "Iteration 769, loss = 0.00343280\n",
      "Iteration 770, loss = 0.00342439\n",
      "Iteration 771, loss = 0.00341601\n",
      "Iteration 772, loss = 0.00340766\n",
      "Iteration 773, loss = 0.00339934\n",
      "Iteration 774, loss = 0.00339105\n",
      "Iteration 775, loss = 0.00338279\n",
      "Iteration 776, loss = 0.00337456\n",
      "Iteration 777, loss = 0.00336635\n",
      "Iteration 778, loss = 0.00335818\n",
      "Iteration 779, loss = 0.00335003\n",
      "Iteration 780, loss = 0.00334191\n",
      "Iteration 781, loss = 0.00333383\n",
      "Iteration 782, loss = 0.00332577\n",
      "Iteration 783, loss = 0.00331773\n",
      "Iteration 784, loss = 0.00330973\n",
      "Iteration 785, loss = 0.00330175\n",
      "Iteration 786, loss = 0.00329381\n",
      "Iteration 787, loss = 0.00328589\n",
      "Iteration 788, loss = 0.00327799\n",
      "Iteration 789, loss = 0.00327013\n",
      "Iteration 790, loss = 0.00326229\n",
      "Iteration 791, loss = 0.00325448\n",
      "Iteration 792, loss = 0.00324670\n",
      "Iteration 793, loss = 0.00323894\n",
      "Iteration 794, loss = 0.00323121\n",
      "Iteration 795, loss = 0.00322351\n",
      "Iteration 796, loss = 0.00321583\n",
      "Iteration 797, loss = 0.00320818\n",
      "Iteration 798, loss = 0.00320056\n",
      "Iteration 799, loss = 0.00319296\n",
      "Iteration 800, loss = 0.00318539\n",
      "Iteration 801, loss = 0.00317785\n",
      "Iteration 802, loss = 0.00317033\n",
      "Iteration 803, loss = 0.00316284\n",
      "Iteration 804, loss = 0.00315537\n",
      "Iteration 805, loss = 0.00314793\n",
      "Iteration 806, loss = 0.00314051\n",
      "Iteration 807, loss = 0.00313312\n",
      "Iteration 808, loss = 0.00312576\n",
      "Iteration 809, loss = 0.00311842\n",
      "Iteration 810, loss = 0.00311110\n",
      "Iteration 811, loss = 0.00310381\n",
      "Iteration 812, loss = 0.00309655\n",
      "Iteration 813, loss = 0.00308930\n",
      "Iteration 814, loss = 0.00308209\n",
      "Iteration 815, loss = 0.00307490\n",
      "Iteration 816, loss = 0.00306773\n",
      "Iteration 817, loss = 0.00306059\n",
      "Iteration 818, loss = 0.00305347\n",
      "Iteration 819, loss = 0.00304637\n",
      "Iteration 820, loss = 0.00303930\n",
      "Iteration 821, loss = 0.00303225\n",
      "Iteration 822, loss = 0.00302523\n",
      "Iteration 823, loss = 0.00301823\n",
      "Iteration 824, loss = 0.00301125\n",
      "Iteration 825, loss = 0.00300430\n",
      "Iteration 826, loss = 0.00299737\n",
      "Iteration 827, loss = 0.00299047\n",
      "Iteration 828, loss = 0.00298358\n",
      "Iteration 829, loss = 0.00297672\n",
      "Iteration 830, loss = 0.00296989\n",
      "Iteration 831, loss = 0.00296307\n",
      "Iteration 832, loss = 0.00295628\n",
      "Iteration 833, loss = 0.00294951\n",
      "Iteration 834, loss = 0.00294276\n",
      "Iteration 835, loss = 0.00293604\n",
      "Iteration 836, loss = 0.00292934\n",
      "Iteration 837, loss = 0.00292266\n",
      "Iteration 838, loss = 0.00291600\n",
      "Iteration 839, loss = 0.00290937\n",
      "Iteration 840, loss = 0.00290275\n",
      "Iteration 841, loss = 0.00289616\n",
      "Iteration 842, loss = 0.00288959\n",
      "Iteration 843, loss = 0.00288304\n",
      "Iteration 844, loss = 0.00287652\n",
      "Iteration 845, loss = 0.00287001\n",
      "Iteration 846, loss = 0.00286353\n",
      "Iteration 847, loss = 0.00285706\n",
      "Iteration 848, loss = 0.00285062\n",
      "Iteration 849, loss = 0.00284420\n",
      "Iteration 850, loss = 0.00283780\n",
      "Iteration 851, loss = 0.00283143\n",
      "Iteration 852, loss = 0.00282507\n",
      "Iteration 853, loss = 0.00281873\n",
      "Iteration 854, loss = 0.00281242\n",
      "Iteration 855, loss = 0.00280612\n",
      "Iteration 856, loss = 0.00279985\n",
      "Iteration 857, loss = 0.00279359\n",
      "Iteration 858, loss = 0.00278736\n",
      "Iteration 859, loss = 0.00278114\n",
      "Iteration 860, loss = 0.00277495\n",
      "Iteration 861, loss = 0.00276878\n",
      "Iteration 862, loss = 0.00276262\n",
      "Iteration 863, loss = 0.00275649\n",
      "Iteration 864, loss = 0.00275038\n",
      "Iteration 865, loss = 0.00274428\n",
      "Iteration 866, loss = 0.00273821\n",
      "Iteration 867, loss = 0.00273215\n",
      "Iteration 868, loss = 0.00272612\n",
      "Iteration 869, loss = 0.00272010\n",
      "Iteration 870, loss = 0.00271411\n",
      "Iteration 871, loss = 0.00270813\n",
      "Iteration 872, loss = 0.00270217\n",
      "Iteration 873, loss = 0.00269623\n",
      "Iteration 874, loss = 0.00269031\n",
      "Iteration 875, loss = 0.00268441\n",
      "Iteration 876, loss = 0.00267853\n",
      "Iteration 877, loss = 0.00267266\n",
      "Iteration 878, loss = 0.00266682\n",
      "Iteration 879, loss = 0.00266099\n",
      "Iteration 880, loss = 0.00265518\n",
      "Iteration 881, loss = 0.00264939\n",
      "Iteration 882, loss = 0.00264362\n",
      "Iteration 883, loss = 0.00263787\n",
      "Iteration 884, loss = 0.00263214\n",
      "Iteration 885, loss = 0.00262642\n",
      "Iteration 886, loss = 0.00262072\n",
      "Iteration 887, loss = 0.00261504\n",
      "Iteration 888, loss = 0.00260938\n",
      "Iteration 889, loss = 0.00260374\n",
      "Iteration 890, loss = 0.00259811\n",
      "Iteration 891, loss = 0.00259250\n",
      "Iteration 892, loss = 0.00258691\n",
      "Iteration 893, loss = 0.00258134\n",
      "Iteration 894, loss = 0.00257578\n",
      "Iteration 895, loss = 0.00257024\n",
      "Iteration 896, loss = 0.00256472\n",
      "Iteration 897, loss = 0.00255922\n",
      "Iteration 898, loss = 0.00255373\n",
      "Iteration 899, loss = 0.00254826\n",
      "Iteration 900, loss = 0.00254281\n",
      "Iteration 901, loss = 0.00253737\n",
      "Iteration 902, loss = 0.00253195\n",
      "Iteration 903, loss = 0.00252655\n",
      "Iteration 904, loss = 0.00252117\n",
      "Iteration 905, loss = 0.00251580\n",
      "Iteration 906, loss = 0.00251045\n",
      "Iteration 907, loss = 0.00250511\n",
      "Iteration 908, loss = 0.00249979\n",
      "Iteration 909, loss = 0.00249449\n",
      "Iteration 910, loss = 0.00248921\n",
      "Iteration 911, loss = 0.00248394\n",
      "Iteration 912, loss = 0.00247868\n",
      "Iteration 913, loss = 0.00247345\n",
      "Iteration 914, loss = 0.00246823\n",
      "Iteration 915, loss = 0.00246302\n",
      "Iteration 916, loss = 0.00245783\n",
      "Iteration 917, loss = 0.00245266\n",
      "Iteration 918, loss = 0.00244751\n",
      "Iteration 919, loss = 0.00244237\n",
      "Iteration 920, loss = 0.00243724\n",
      "Iteration 921, loss = 0.00243213\n",
      "Iteration 922, loss = 0.00242704\n",
      "Iteration 923, loss = 0.00242196\n",
      "Iteration 924, loss = 0.00241690\n",
      "Iteration 925, loss = 0.00241185\n",
      "Iteration 926, loss = 0.00240682\n",
      "Iteration 927, loss = 0.00240180\n",
      "Iteration 928, loss = 0.00239680\n",
      "Iteration 929, loss = 0.00239181\n",
      "Iteration 930, loss = 0.00238684\n",
      "Iteration 931, loss = 0.00238189\n",
      "Iteration 932, loss = 0.00237695\n",
      "Iteration 933, loss = 0.00237202\n",
      "Iteration 934, loss = 0.00236711\n",
      "Iteration 935, loss = 0.00236221\n",
      "Iteration 936, loss = 0.00235733\n",
      "Iteration 937, loss = 0.00235247\n",
      "Iteration 938, loss = 0.00234762\n",
      "Iteration 939, loss = 0.00234278\n",
      "Iteration 940, loss = 0.00233796\n",
      "Iteration 941, loss = 0.00233315\n",
      "Iteration 942, loss = 0.00232835\n",
      "Iteration 943, loss = 0.00232358\n",
      "Iteration 944, loss = 0.00231881\n",
      "Iteration 945, loss = 0.00231406\n",
      "Iteration 946, loss = 0.00230932\n",
      "Iteration 947, loss = 0.00230460\n",
      "Iteration 948, loss = 0.00229989\n",
      "Iteration 949, loss = 0.00229520\n",
      "Iteration 950, loss = 0.00229052\n",
      "Iteration 951, loss = 0.00228585\n",
      "Iteration 952, loss = 0.00228120\n",
      "Iteration 953, loss = 0.00227656\n",
      "Iteration 954, loss = 0.00227194\n",
      "Iteration 955, loss = 0.00226733\n",
      "Iteration 956, loss = 0.00226273\n",
      "Iteration 957, loss = 0.00225815\n",
      "Iteration 958, loss = 0.00225358\n",
      "Iteration 959, loss = 0.00224902\n",
      "Iteration 960, loss = 0.00224448\n",
      "Iteration 961, loss = 0.00223995\n",
      "Iteration 962, loss = 0.00223543\n",
      "Iteration 963, loss = 0.00223093\n",
      "Iteration 964, loss = 0.00222644\n",
      "Iteration 965, loss = 0.00222197\n",
      "Iteration 966, loss = 0.00221750\n",
      "Iteration 967, loss = 0.00221305\n",
      "Iteration 968, loss = 0.00220862\n",
      "Iteration 969, loss = 0.00220419\n",
      "Iteration 970, loss = 0.00219978\n",
      "Iteration 971, loss = 0.00219538\n",
      "Iteration 972, loss = 0.00219100\n",
      "Iteration 973, loss = 0.00218663\n",
      "Iteration 974, loss = 0.00218227\n",
      "Iteration 975, loss = 0.00217792\n",
      "Iteration 976, loss = 0.00217359\n",
      "Iteration 977, loss = 0.00216926\n",
      "Iteration 978, loss = 0.00216496\n",
      "Iteration 979, loss = 0.00216066\n",
      "Iteration 980, loss = 0.00215638\n",
      "Iteration 981, loss = 0.00215210\n",
      "Iteration 982, loss = 0.00214785\n",
      "Iteration 983, loss = 0.00214360\n",
      "Iteration 984, loss = 0.00213937\n",
      "Iteration 985, loss = 0.00213514\n",
      "Iteration 986, loss = 0.00213093\n",
      "Iteration 987, loss = 0.00212674\n",
      "Iteration 988, loss = 0.00212255\n",
      "Iteration 989, loss = 0.00211838\n",
      "Iteration 990, loss = 0.00211422\n",
      "Iteration 991, loss = 0.00211007\n",
      "Iteration 992, loss = 0.00210593\n",
      "Iteration 993, loss = 0.00210180\n",
      "Iteration 994, loss = 0.00209769\n",
      "Iteration 995, loss = 0.00209359\n",
      "Iteration 996, loss = 0.00208950\n",
      "Iteration 997, loss = 0.00208542\n",
      "Iteration 998, loss = 0.00208135\n",
      "Iteration 999, loss = 0.00207730\n",
      "Iteration 1000, loss = 0.00207325\n",
      "Iteration 1001, loss = 0.00206922\n",
      "Iteration 1002, loss = 0.00206520\n",
      "Iteration 1003, loss = 0.00206119\n",
      "Iteration 1004, loss = 0.00205719\n",
      "Iteration 1005, loss = 0.00205321\n",
      "Iteration 1006, loss = 0.00204923\n",
      "Iteration 1007, loss = 0.00204527\n",
      "Iteration 1008, loss = 0.00204132\n",
      "Iteration 1009, loss = 0.00203738\n",
      "Iteration 1010, loss = 0.00203345\n",
      "Iteration 1011, loss = 0.00202953\n",
      "Iteration 1012, loss = 0.00202562\n",
      "Iteration 1013, loss = 0.00202172\n",
      "Iteration 1014, loss = 0.00201784\n",
      "Iteration 1015, loss = 0.00201396\n",
      "Iteration 1016, loss = 0.00201010\n",
      "Iteration 1017, loss = 0.00200625\n",
      "Iteration 1018, loss = 0.00200240\n",
      "Iteration 1019, loss = 0.00199857\n",
      "Iteration 1020, loss = 0.00199475\n",
      "Iteration 1021, loss = 0.00199094\n",
      "Iteration 1022, loss = 0.00198714\n",
      "Iteration 1023, loss = 0.00198336\n",
      "Iteration 1024, loss = 0.00197958\n",
      "Iteration 1025, loss = 0.00197581\n",
      "Iteration 1026, loss = 0.00197205\n",
      "Iteration 1027, loss = 0.00196831\n",
      "Iteration 1028, loss = 0.00196457\n",
      "Iteration 1029, loss = 0.00196085\n",
      "Iteration 1030, loss = 0.00195713\n",
      "Iteration 1031, loss = 0.00195343\n",
      "Iteration 1032, loss = 0.00194973\n",
      "Iteration 1033, loss = 0.00194605\n",
      "Iteration 1034, loss = 0.00194238\n",
      "Iteration 1035, loss = 0.00193871\n",
      "Iteration 1036, loss = 0.00193506\n",
      "Iteration 1037, loss = 0.00193142\n",
      "Iteration 1038, loss = 0.00192778\n",
      "Iteration 1039, loss = 0.00192416\n",
      "Iteration 1040, loss = 0.00192055\n",
      "Iteration 1041, loss = 0.00191694\n",
      "Iteration 1042, loss = 0.00191335\n",
      "Iteration 1043, loss = 0.00190977\n",
      "Iteration 1044, loss = 0.00190619\n",
      "Iteration 1045, loss = 0.00190263\n",
      "Iteration 1046, loss = 0.00189908\n",
      "Iteration 1047, loss = 0.00189553\n",
      "Iteration 1048, loss = 0.00189200\n",
      "Iteration 1049, loss = 0.00188847\n",
      "Iteration 1050, loss = 0.00188496\n",
      "Iteration 1051, loss = 0.00188145\n",
      "Iteration 1052, loss = 0.00187796\n",
      "Iteration 1053, loss = 0.00187447\n",
      "Iteration 1054, loss = 0.00187100\n",
      "Iteration 1055, loss = 0.00186753\n",
      "Iteration 1056, loss = 0.00186407\n",
      "Iteration 1057, loss = 0.00186062\n",
      "Iteration 1058, loss = 0.00185718\n",
      "Iteration 1059, loss = 0.00185375\n",
      "Iteration 1060, loss = 0.00185033\n",
      "Iteration 1061, loss = 0.00184692\n",
      "Iteration 1062, loss = 0.00184352\n",
      "Iteration 1063, loss = 0.00184013\n",
      "Iteration 1064, loss = 0.00183675\n",
      "Iteration 1065, loss = 0.00183337\n",
      "Iteration 1066, loss = 0.00183001\n",
      "Iteration 1067, loss = 0.00182665\n",
      "Iteration 1068, loss = 0.00182331\n",
      "Iteration 1069, loss = 0.00181997\n",
      "Iteration 1070, loss = 0.00181664\n",
      "Iteration 1071, loss = 0.00181332\n",
      "Iteration 1072, loss = 0.00181001\n",
      "Iteration 1073, loss = 0.00180671\n",
      "Iteration 1074, loss = 0.00180342\n",
      "Iteration 1075, loss = 0.00180013\n",
      "Iteration 1076, loss = 0.00179686\n",
      "Iteration 1077, loss = 0.00179359\n",
      "Iteration 1078, loss = 0.00179033\n",
      "Iteration 1079, loss = 0.00178708\n",
      "Iteration 1080, loss = 0.00178384\n",
      "Iteration 1081, loss = 0.00178061\n",
      "Iteration 1082, loss = 0.00177739\n",
      "Iteration 1083, loss = 0.00177418\n",
      "Iteration 1084, loss = 0.00177097\n",
      "Iteration 1085, loss = 0.00176777\n",
      "Iteration 1086, loss = 0.00176458\n",
      "Iteration 1087, loss = 0.00176140\n",
      "Iteration 1088, loss = 0.00175823\n",
      "Iteration 1089, loss = 0.00175507\n",
      "Iteration 1090, loss = 0.00175192\n",
      "Iteration 1091, loss = 0.00174877\n",
      "Iteration 1092, loss = 0.00174563\n",
      "Iteration 1093, loss = 0.00174250\n",
      "Iteration 1094, loss = 0.00173938\n",
      "Iteration 1095, loss = 0.00173627\n",
      "Iteration 1096, loss = 0.00173316\n",
      "Iteration 1097, loss = 0.00173006\n",
      "Iteration 1098, loss = 0.00172698\n",
      "Iteration 1099, loss = 0.00172390\n",
      "Iteration 1100, loss = 0.00172082\n",
      "Iteration 1101, loss = 0.00171776\n",
      "Iteration 1102, loss = 0.00171470\n",
      "Iteration 1103, loss = 0.00171166\n",
      "Iteration 1104, loss = 0.00170862\n",
      "Iteration 1105, loss = 0.00170558\n",
      "Iteration 1106, loss = 0.00170256\n",
      "Iteration 1107, loss = 0.00169954\n",
      "Iteration 1108, loss = 0.00169653\n",
      "Iteration 1109, loss = 0.00169353\n",
      "Iteration 1110, loss = 0.00169054\n",
      "Iteration 1111, loss = 0.00168756\n",
      "Iteration 1112, loss = 0.00168458\n",
      "Iteration 1113, loss = 0.00168161\n",
      "Iteration 1114, loss = 0.00167865\n",
      "Iteration 1115, loss = 0.00167570\n",
      "Iteration 1116, loss = 0.00167275\n",
      "Iteration 1117, loss = 0.00166981\n",
      "Iteration 1118, loss = 0.00166688\n",
      "Iteration 1119, loss = 0.00166396\n",
      "Iteration 1120, loss = 0.00166104\n",
      "Iteration 1121, loss = 0.00165813\n",
      "Iteration 1122, loss = 0.00165523\n",
      "Iteration 1123, loss = 0.00165234\n",
      "Iteration 1124, loss = 0.00164945\n",
      "Iteration 1125, loss = 0.00164658\n",
      "Iteration 1126, loss = 0.00164371\n",
      "Iteration 1127, loss = 0.00164084\n",
      "Iteration 1128, loss = 0.00163799\n",
      "Iteration 1129, loss = 0.00163514\n",
      "Iteration 1130, loss = 0.00163230\n",
      "Iteration 1131, loss = 0.00162946\n",
      "Iteration 1132, loss = 0.00162664\n",
      "Iteration 1133, loss = 0.00162382\n",
      "Iteration 1134, loss = 0.00162101\n",
      "Iteration 1135, loss = 0.00161820\n",
      "Iteration 1136, loss = 0.00161540\n",
      "Iteration 1137, loss = 0.00161261\n",
      "Iteration 1138, loss = 0.00160983\n",
      "Iteration 1139, loss = 0.00160706\n",
      "Iteration 1140, loss = 0.00160429\n",
      "Iteration 1141, loss = 0.00160152\n",
      "Iteration 1142, loss = 0.00159877\n",
      "Iteration 1143, loss = 0.00159602\n",
      "Iteration 1144, loss = 0.00159328\n",
      "Iteration 1145, loss = 0.00159055\n",
      "Iteration 1146, loss = 0.00158782\n",
      "Iteration 1147, loss = 0.00158510\n",
      "Iteration 1148, loss = 0.00158239\n",
      "Iteration 1149, loss = 0.00157968\n",
      "Iteration 1150, loss = 0.00157698\n",
      "Iteration 1151, loss = 0.00157429\n",
      "Iteration 1152, loss = 0.00157160\n",
      "Iteration 1153, loss = 0.00156893\n",
      "Iteration 1154, loss = 0.00156625\n",
      "Iteration 1155, loss = 0.00156359\n",
      "Iteration 1156, loss = 0.00156093\n",
      "Iteration 1157, loss = 0.00155828\n",
      "Iteration 1158, loss = 0.00155563\n",
      "Iteration 1159, loss = 0.00155299\n",
      "Iteration 1160, loss = 0.00155036\n",
      "Iteration 1161, loss = 0.00154774\n",
      "Iteration 1162, loss = 0.00154512\n",
      "Iteration 1163, loss = 0.00154251\n",
      "Iteration 1164, loss = 0.00153990\n",
      "Iteration 1165, loss = 0.00153730\n",
      "Iteration 1166, loss = 0.00153471\n",
      "Iteration 1167, loss = 0.00153212\n",
      "Iteration 1168, loss = 0.00152955\n",
      "Iteration 1169, loss = 0.00152697\n",
      "Iteration 1170, loss = 0.00152441\n",
      "Iteration 1171, loss = 0.00152185\n",
      "Iteration 1172, loss = 0.00151929\n",
      "Iteration 1173, loss = 0.00151675\n",
      "Iteration 1174, loss = 0.00151420\n",
      "Iteration 1175, loss = 0.00151167\n",
      "Iteration 1176, loss = 0.00150914\n",
      "Iteration 1177, loss = 0.00150662\n",
      "Iteration 1178, loss = 0.00150410\n",
      "Iteration 1179, loss = 0.00150159\n",
      "Iteration 1180, loss = 0.00149909\n",
      "Iteration 1181, loss = 0.00149659\n",
      "Iteration 1182, loss = 0.00149410\n",
      "Iteration 1183, loss = 0.00149162\n",
      "Iteration 1184, loss = 0.00148914\n",
      "Iteration 1185, loss = 0.00148667\n",
      "Iteration 1186, loss = 0.00148420\n",
      "Iteration 1187, loss = 0.00148174\n",
      "Iteration 1188, loss = 0.00147929\n",
      "Iteration 1189, loss = 0.00147684\n",
      "Iteration 1190, loss = 0.00147440\n",
      "Iteration 1191, loss = 0.00147196\n",
      "Iteration 1192, loss = 0.00146953\n",
      "Iteration 1193, loss = 0.00146711\n",
      "Iteration 1194, loss = 0.00146469\n",
      "Iteration 1195, loss = 0.00146228\n",
      "Iteration 1196, loss = 0.00145987\n",
      "Iteration 1197, loss = 0.00145747\n",
      "Iteration 1198, loss = 0.00145508\n",
      "Iteration 1199, loss = 0.00145269\n",
      "Iteration 1200, loss = 0.00145030\n",
      "Iteration 1201, loss = 0.00144793\n",
      "Iteration 1202, loss = 0.00144556\n",
      "Iteration 1203, loss = 0.00144319\n",
      "Iteration 1204, loss = 0.00144083\n",
      "Iteration 1205, loss = 0.00143848\n",
      "Iteration 1206, loss = 0.00143613\n",
      "Iteration 1207, loss = 0.00143379\n",
      "Iteration 1208, loss = 0.00143145\n",
      "Iteration 1209, loss = 0.00142912\n",
      "Iteration 1210, loss = 0.00142679\n",
      "Iteration 1211, loss = 0.00142447\n",
      "Iteration 1212, loss = 0.00142216\n",
      "Iteration 1213, loss = 0.00141985\n",
      "Iteration 1214, loss = 0.00141755\n",
      "Iteration 1215, loss = 0.00141525\n",
      "Iteration 1216, loss = 0.00141296\n",
      "Iteration 1217, loss = 0.00141067\n",
      "Iteration 1218, loss = 0.00140839\n",
      "Iteration 1219, loss = 0.00140612\n",
      "Iteration 1220, loss = 0.00140385\n",
      "Iteration 1221, loss = 0.00140159\n",
      "Iteration 1222, loss = 0.00139933\n",
      "Iteration 1223, loss = 0.00139707\n",
      "Iteration 1224, loss = 0.00139483\n",
      "Iteration 1225, loss = 0.00139258\n",
      "Iteration 1226, loss = 0.00139035\n",
      "Iteration 1227, loss = 0.00138811\n",
      "Iteration 1228, loss = 0.00138589\n",
      "Iteration 1229, loss = 0.00138367\n",
      "Iteration 1230, loss = 0.00138145\n",
      "Iteration 1231, loss = 0.00137924\n",
      "Iteration 1232, loss = 0.00137704\n",
      "Iteration 1233, loss = 0.00137484\n",
      "Iteration 1234, loss = 0.00137264\n",
      "Iteration 1235, loss = 0.00137045\n",
      "Iteration 1236, loss = 0.00136827\n",
      "Iteration 1237, loss = 0.00136609\n",
      "Iteration 1238, loss = 0.00136392\n",
      "Iteration 1239, loss = 0.00136175\n",
      "Iteration 1240, loss = 0.00135958\n",
      "Iteration 1241, loss = 0.00135743\n",
      "Iteration 1242, loss = 0.00135527\n",
      "Iteration 1243, loss = 0.00135313\n",
      "Iteration 1244, loss = 0.00135098\n",
      "Iteration 1245, loss = 0.00134884\n",
      "Iteration 1246, loss = 0.00134671\n",
      "Iteration 1247, loss = 0.00134458\n",
      "Iteration 1248, loss = 0.00134246\n",
      "Iteration 1249, loss = 0.00134034\n",
      "Iteration 1250, loss = 0.00133823\n",
      "Iteration 1251, loss = 0.00133612\n",
      "Iteration 1252, loss = 0.00133402\n",
      "Iteration 1253, loss = 0.00133192\n",
      "Iteration 1254, loss = 0.00132983\n",
      "Iteration 1255, loss = 0.00132774\n",
      "Iteration 1256, loss = 0.00132566\n",
      "Iteration 1257, loss = 0.00132358\n",
      "Iteration 1258, loss = 0.00132151\n",
      "Iteration 1259, loss = 0.00131944\n",
      "Iteration 1260, loss = 0.00131738\n",
      "Iteration 1261, loss = 0.00131532\n",
      "Iteration 1262, loss = 0.00131327\n",
      "Iteration 1263, loss = 0.00131122\n",
      "Iteration 1264, loss = 0.00130917\n",
      "Iteration 1265, loss = 0.00130713\n",
      "Iteration 1266, loss = 0.00130510\n",
      "Iteration 1267, loss = 0.00130307\n",
      "Iteration 1268, loss = 0.00130104\n",
      "Iteration 1269, loss = 0.00129902\n",
      "Iteration 1270, loss = 0.00129701\n",
      "Iteration 1271, loss = 0.00129500\n",
      "Iteration 1272, loss = 0.00129299\n",
      "Iteration 1273, loss = 0.00129099\n",
      "Iteration 1274, loss = 0.00128899\n",
      "Iteration 1275, loss = 0.00128700\n",
      "Iteration 1276, loss = 0.00128501\n",
      "Iteration 1277, loss = 0.00128303\n",
      "Iteration 1278, loss = 0.00128105\n",
      "Iteration 1279, loss = 0.00127908\n",
      "Iteration 1280, loss = 0.00127711\n",
      "Iteration 1281, loss = 0.00127514\n",
      "Iteration 1282, loss = 0.00127318\n",
      "Iteration 1283, loss = 0.00127123\n",
      "Iteration 1284, loss = 0.00126928\n",
      "Iteration 1285, loss = 0.00126733\n",
      "Iteration 1286, loss = 0.00126539\n",
      "Iteration 1287, loss = 0.00126345\n",
      "Iteration 1288, loss = 0.00126152\n",
      "Iteration 1289, loss = 0.00125959\n",
      "Iteration 1290, loss = 0.00125766\n",
      "Iteration 1291, loss = 0.00125574\n",
      "Iteration 1292, loss = 0.00125383\n",
      "Iteration 1293, loss = 0.00125192\n",
      "Iteration 1294, loss = 0.00125001\n",
      "Iteration 1295, loss = 0.00124811\n",
      "Iteration 1296, loss = 0.00124621\n",
      "Iteration 1297, loss = 0.00124432\n",
      "Iteration 1298, loss = 0.00124243\n",
      "Iteration 1299, loss = 0.00124054\n",
      "Iteration 1300, loss = 0.00123866\n",
      "Iteration 1301, loss = 0.00123678\n",
      "Iteration 1302, loss = 0.00123491\n",
      "Iteration 1303, loss = 0.00123304\n",
      "Iteration 1304, loss = 0.00123118\n",
      "Iteration 1305, loss = 0.00122932\n",
      "Iteration 1306, loss = 0.00122747\n",
      "Iteration 1307, loss = 0.00122561\n",
      "Iteration 1308, loss = 0.00122377\n",
      "Iteration 1309, loss = 0.00122193\n",
      "Iteration 1310, loss = 0.00122009\n",
      "Iteration 1311, loss = 0.00121825\n",
      "Iteration 1312, loss = 0.00121642\n",
      "Iteration 1313, loss = 0.00121460\n",
      "Iteration 1314, loss = 0.00121278\n",
      "Iteration 1315, loss = 0.00121096\n",
      "Iteration 1316, loss = 0.00120914\n",
      "Iteration 1317, loss = 0.00120733\n",
      "Iteration 1318, loss = 0.00120553\n",
      "Iteration 1319, loss = 0.00120373\n",
      "Iteration 1320, loss = 0.00120193\n",
      "Iteration 1321, loss = 0.00120014\n",
      "Iteration 1322, loss = 0.00119835\n",
      "Iteration 1323, loss = 0.00119656\n",
      "Iteration 1324, loss = 0.00119478\n",
      "Iteration 1325, loss = 0.00119301\n",
      "Iteration 1326, loss = 0.00119123\n",
      "Iteration 1327, loss = 0.00118946\n",
      "Iteration 1328, loss = 0.00118770\n",
      "Iteration 1329, loss = 0.00118594\n",
      "Iteration 1330, loss = 0.00118418\n",
      "Iteration 1331, loss = 0.00118243\n",
      "Iteration 1332, loss = 0.00118068\n",
      "Iteration 1333, loss = 0.00117893\n",
      "Iteration 1334, loss = 0.00117719\n",
      "Iteration 1335, loss = 0.00117545\n",
      "Iteration 1336, loss = 0.00117372\n",
      "Iteration 1337, loss = 0.00117199\n",
      "Iteration 1338, loss = 0.00117026\n",
      "Iteration 1339, loss = 0.00116854\n",
      "Iteration 1340, loss = 0.00116682\n",
      "Iteration 1341, loss = 0.00116511\n",
      "Iteration 1342, loss = 0.00116340\n",
      "Iteration 1343, loss = 0.00116169\n",
      "Iteration 1344, loss = 0.00115999\n",
      "Iteration 1345, loss = 0.00115829\n",
      "Iteration 1346, loss = 0.00115659\n",
      "Iteration 1347, loss = 0.00115490\n",
      "Iteration 1348, loss = 0.00115321\n",
      "Iteration 1349, loss = 0.00115153\n",
      "Iteration 1350, loss = 0.00114985\n",
      "Iteration 1351, loss = 0.00114817\n",
      "Iteration 1352, loss = 0.00114650\n",
      "Iteration 1353, loss = 0.00114483\n",
      "Iteration 1354, loss = 0.00114316\n",
      "Iteration 1355, loss = 0.00114150\n",
      "Iteration 1356, loss = 0.00113984\n",
      "Iteration 1357, loss = 0.00113819\n",
      "Iteration 1358, loss = 0.00113653\n",
      "Iteration 1359, loss = 0.00113489\n",
      "Iteration 1360, loss = 0.00113324\n",
      "Iteration 1361, loss = 0.00113160\n",
      "Iteration 1362, loss = 0.00112997\n",
      "Iteration 1363, loss = 0.00112833\n",
      "Iteration 1364, loss = 0.00112670\n",
      "Iteration 1365, loss = 0.00112508\n",
      "Iteration 1366, loss = 0.00112345\n",
      "Iteration 1367, loss = 0.00112184\n",
      "Iteration 1368, loss = 0.00112022\n",
      "Iteration 1369, loss = 0.00111861\n",
      "Iteration 1370, loss = 0.00111700\n",
      "Iteration 1371, loss = 0.00111539\n",
      "Iteration 1372, loss = 0.00111379\n",
      "Iteration 1373, loss = 0.00111220\n",
      "Iteration 1374, loss = 0.00111060\n",
      "Iteration 1375, loss = 0.00110901\n",
      "Iteration 1376, loss = 0.00110742\n",
      "Iteration 1377, loss = 0.00110584\n",
      "Iteration 1378, loss = 0.00110426\n",
      "Iteration 1379, loss = 0.00110268\n",
      "Iteration 1380, loss = 0.00110111\n",
      "Iteration 1381, loss = 0.00109954\n",
      "Iteration 1382, loss = 0.00109797\n",
      "Iteration 1383, loss = 0.00109641\n",
      "Iteration 1384, loss = 0.00109485\n",
      "Iteration 1385, loss = 0.00109329\n",
      "Iteration 1386, loss = 0.00109174\n",
      "Iteration 1387, loss = 0.00109019\n",
      "Iteration 1388, loss = 0.00108864\n",
      "Iteration 1389, loss = 0.00108710\n",
      "Iteration 1390, loss = 0.00108556\n",
      "Iteration 1391, loss = 0.00108402\n",
      "Iteration 1392, loss = 0.00108249\n",
      "Iteration 1393, loss = 0.00108096\n",
      "Iteration 1394, loss = 0.00107943\n",
      "Iteration 1395, loss = 0.00107791\n",
      "Iteration 1396, loss = 0.00107639\n",
      "Iteration 1397, loss = 0.00107487\n",
      "Iteration 1398, loss = 0.00107336\n",
      "Iteration 1399, loss = 0.00107185\n",
      "Iteration 1400, loss = 0.00107034\n",
      "Iteration 1401, loss = 0.00106883\n",
      "Iteration 1402, loss = 0.00106733\n",
      "Iteration 1403, loss = 0.00106584\n",
      "Iteration 1404, loss = 0.00106434\n",
      "Iteration 1405, loss = 0.00106285\n",
      "Iteration 1406, loss = 0.00106136\n",
      "Iteration 1407, loss = 0.00105988\n",
      "Iteration 1408, loss = 0.00105840\n",
      "Iteration 1409, loss = 0.00105692\n",
      "Iteration 1410, loss = 0.00105544\n",
      "Iteration 1411, loss = 0.00105397\n",
      "Iteration 1412, loss = 0.00105250\n",
      "Iteration 1413, loss = 0.00105103\n",
      "Iteration 1414, loss = 0.00104957\n",
      "Iteration 1415, loss = 0.00104811\n",
      "Iteration 1416, loss = 0.00104666\n",
      "Iteration 1417, loss = 0.00104520\n",
      "Iteration 1418, loss = 0.00104375\n",
      "Iteration 1419, loss = 0.00104230\n",
      "Iteration 1420, loss = 0.00104086\n",
      "Iteration 1421, loss = 0.00103942\n",
      "Iteration 1422, loss = 0.00103798\n",
      "Iteration 1423, loss = 0.00103654\n",
      "Iteration 1424, loss = 0.00103511\n",
      "Iteration 1425, loss = 0.00103368\n",
      "Iteration 1426, loss = 0.00103226\n",
      "Iteration 1427, loss = 0.00103083\n",
      "Iteration 1428, loss = 0.00102941\n",
      "Iteration 1429, loss = 0.00102800\n",
      "Iteration 1430, loss = 0.00102658\n",
      "Iteration 1431, loss = 0.00102517\n",
      "Iteration 1432, loss = 0.00102376\n",
      "Iteration 1433, loss = 0.00102236\n",
      "Iteration 1434, loss = 0.00102096\n",
      "Iteration 1435, loss = 0.00101956\n",
      "Iteration 1436, loss = 0.00101816\n",
      "Iteration 1437, loss = 0.00101677\n",
      "Iteration 1438, loss = 0.00101538\n",
      "Iteration 1439, loss = 0.00101399\n",
      "Iteration 1440, loss = 0.00101260\n",
      "Iteration 1441, loss = 0.00101122\n",
      "Iteration 1442, loss = 0.00100984\n",
      "Iteration 1443, loss = 0.00100847\n",
      "Iteration 1444, loss = 0.00100709\n",
      "Iteration 1445, loss = 0.00100572\n",
      "Iteration 1446, loss = 0.00100436\n",
      "Iteration 1447, loss = 0.00100299\n",
      "Iteration 1448, loss = 0.00100163\n",
      "Iteration 1449, loss = 0.00100027\n",
      "Iteration 1450, loss = 0.00099891\n",
      "Iteration 1451, loss = 0.00099756\n",
      "Iteration 1452, loss = 0.00099621\n",
      "Iteration 1453, loss = 0.00099486\n",
      "Iteration 1454, loss = 0.00099352\n",
      "Iteration 1455, loss = 0.00099217\n",
      "Iteration 1456, loss = 0.00099084\n",
      "Iteration 1457, loss = 0.00098950\n",
      "Iteration 1458, loss = 0.00098816\n",
      "Iteration 1459, loss = 0.00098683\n",
      "Iteration 1460, loss = 0.00098551\n",
      "Iteration 1461, loss = 0.00098418\n",
      "Iteration 1462, loss = 0.00098286\n",
      "Iteration 1463, loss = 0.00098154\n",
      "Iteration 1464, loss = 0.00098022\n",
      "Iteration 1465, loss = 0.00097890\n",
      "Iteration 1466, loss = 0.00097759\n",
      "Iteration 1467, loss = 0.00097628\n",
      "Iteration 1468, loss = 0.00097498\n",
      "Iteration 1469, loss = 0.00097367\n",
      "Iteration 1470, loss = 0.00097237\n",
      "Iteration 1471, loss = 0.00097107\n",
      "Iteration 1472, loss = 0.00096978\n",
      "Iteration 1473, loss = 0.00096848\n",
      "Iteration 1474, loss = 0.00096719\n",
      "Iteration 1475, loss = 0.00096591\n",
      "Iteration 1476, loss = 0.00096462\n",
      "Iteration 1477, loss = 0.00096334\n",
      "Iteration 1478, loss = 0.00096206\n",
      "Iteration 1479, loss = 0.00096078\n",
      "Iteration 1480, loss = 0.00095950\n",
      "Iteration 1481, loss = 0.00095823\n",
      "Iteration 1482, loss = 0.00095696\n",
      "Iteration 1483, loss = 0.00095570\n",
      "Iteration 1484, loss = 0.00095443\n",
      "Iteration 1485, loss = 0.00095317\n",
      "Iteration 1486, loss = 0.00095191\n",
      "Iteration 1487, loss = 0.00095065\n",
      "Iteration 1488, loss = 0.00094940\n",
      "Iteration 1489, loss = 0.00094815\n",
      "Iteration 1490, loss = 0.00094690\n",
      "Iteration 1491, loss = 0.00094565\n",
      "Iteration 1492, loss = 0.00094441\n",
      "Iteration 1493, loss = 0.00094317\n",
      "Iteration 1494, loss = 0.00094193\n",
      "Iteration 1495, loss = 0.00094069\n",
      "Iteration 1496, loss = 0.00093946\n",
      "Iteration 1497, loss = 0.00093822\n",
      "Iteration 1498, loss = 0.00093700\n",
      "Iteration 1499, loss = 0.00093577\n",
      "Iteration 1500, loss = 0.00093455\n",
      "Iteration 1501, loss = 0.00093332\n",
      "Iteration 1502, loss = 0.00093210\n",
      "Iteration 1503, loss = 0.00093089\n",
      "Iteration 1504, loss = 0.00092967\n",
      "Iteration 1505, loss = 0.00092846\n",
      "Iteration 1506, loss = 0.00092725\n",
      "Iteration 1507, loss = 0.00092605\n",
      "Iteration 1508, loss = 0.00092484\n",
      "Iteration 1509, loss = 0.00092364\n",
      "Iteration 1510, loss = 0.00092244\n",
      "Iteration 1511, loss = 0.00092124\n",
      "Iteration 1512, loss = 0.00092005\n",
      "Iteration 1513, loss = 0.00091885\n",
      "Iteration 1514, loss = 0.00091766\n",
      "Iteration 1515, loss = 0.00091648\n",
      "Iteration 1516, loss = 0.00091529\n",
      "Iteration 1517, loss = 0.00091411\n",
      "Iteration 1518, loss = 0.00091293\n",
      "Iteration 1519, loss = 0.00091175\n",
      "Iteration 1520, loss = 0.00091057\n",
      "Iteration 1521, loss = 0.00090940\n",
      "Iteration 1522, loss = 0.00090823\n",
      "Iteration 1523, loss = 0.00090706\n",
      "Iteration 1524, loss = 0.00090589\n",
      "Iteration 1525, loss = 0.00090473\n",
      "Iteration 1526, loss = 0.00090357\n",
      "Iteration 1527, loss = 0.00090241\n",
      "Iteration 1528, loss = 0.00090125\n",
      "Iteration 1529, loss = 0.00090010\n",
      "Iteration 1530, loss = 0.00089894\n",
      "Iteration 1531, loss = 0.00089779\n",
      "Iteration 1532, loss = 0.00089664\n",
      "Iteration 1533, loss = 0.00089550\n",
      "Iteration 1534, loss = 0.00089435\n",
      "Iteration 1535, loss = 0.00089321\n",
      "Iteration 1536, loss = 0.00089207\n",
      "Iteration 1537, loss = 0.00089094\n",
      "Iteration 1538, loss = 0.00088980\n",
      "Iteration 1539, loss = 0.00088867\n",
      "Iteration 1540, loss = 0.00088754\n",
      "Iteration 1541, loss = 0.00088641\n",
      "Iteration 1542, loss = 0.00088529\n",
      "Iteration 1543, loss = 0.00088416\n",
      "Iteration 1544, loss = 0.00088304\n",
      "Iteration 1545, loss = 0.00088192\n",
      "Iteration 1546, loss = 0.00088081\n",
      "Iteration 1547, loss = 0.00087969\n",
      "Iteration 1548, loss = 0.00087858\n",
      "Iteration 1549, loss = 0.00087747\n",
      "Iteration 1550, loss = 0.00087636\n",
      "Iteration 1551, loss = 0.00087526\n",
      "Iteration 1552, loss = 0.00087415\n",
      "Iteration 1553, loss = 0.00087305\n",
      "Iteration 1554, loss = 0.00087195\n",
      "Iteration 1555, loss = 0.00087085\n",
      "Iteration 1556, loss = 0.00086976\n",
      "Iteration 1557, loss = 0.00086866\n",
      "Iteration 1558, loss = 0.00086757\n",
      "Iteration 1559, loss = 0.00086649\n",
      "Iteration 1560, loss = 0.00086540\n",
      "Iteration 1561, loss = 0.00086431\n",
      "Iteration 1562, loss = 0.00086323\n",
      "Iteration 1563, loss = 0.00086215\n",
      "Iteration 1564, loss = 0.00086107\n",
      "Iteration 1565, loss = 0.00086000\n",
      "Iteration 1566, loss = 0.00085892\n",
      "Iteration 1567, loss = 0.00085785\n",
      "Iteration 1568, loss = 0.00085678\n",
      "Iteration 1569, loss = 0.00085571\n",
      "Iteration 1570, loss = 0.00085465\n",
      "Iteration 1571, loss = 0.00085358\n",
      "Iteration 1572, loss = 0.00085252\n",
      "Iteration 1573, loss = 0.00085146\n",
      "Iteration 1574, loss = 0.00085041\n",
      "Iteration 1575, loss = 0.00084935\n",
      "Iteration 1576, loss = 0.00084830\n",
      "Iteration 1577, loss = 0.00084725\n",
      "Iteration 1578, loss = 0.00084620\n",
      "Iteration 1579, loss = 0.00084515\n",
      "Iteration 1580, loss = 0.00084410\n",
      "Iteration 1581, loss = 0.00084306\n",
      "Iteration 1582, loss = 0.00084202\n",
      "Iteration 1583, loss = 0.00084098\n",
      "Iteration 1584, loss = 0.00083994\n",
      "Iteration 1585, loss = 0.00083891\n",
      "Iteration 1586, loss = 0.00083788\n",
      "Iteration 1587, loss = 0.00083684\n",
      "Iteration 1588, loss = 0.00083582\n",
      "Iteration 1589, loss = 0.00083479\n",
      "Iteration 1590, loss = 0.00083376\n",
      "Iteration 1591, loss = 0.00083274\n",
      "Iteration 1592, loss = 0.00083172\n",
      "Iteration 1593, loss = 0.00083070\n",
      "Iteration 1594, loss = 0.00082968\n",
      "Iteration 1595, loss = 0.00082867\n",
      "Iteration 1596, loss = 0.00082765\n",
      "Iteration 1597, loss = 0.00082664\n",
      "Iteration 1598, loss = 0.00082563\n",
      "Iteration 1599, loss = 0.00082462\n",
      "Iteration 1600, loss = 0.00082362\n",
      "Iteration 1601, loss = 0.00082261\n",
      "Iteration 1602, loss = 0.00082161\n",
      "Iteration 1603, loss = 0.00082061\n",
      "Iteration 1604, loss = 0.00081961\n",
      "Iteration 1605, loss = 0.00081862\n",
      "Iteration 1606, loss = 0.00081762\n",
      "Iteration 1607, loss = 0.00081663\n",
      "Iteration 1608, loss = 0.00081564\n",
      "Iteration 1609, loss = 0.00081465\n",
      "Iteration 1610, loss = 0.00081366\n",
      "Iteration 1611, loss = 0.00081268\n",
      "Iteration 1612, loss = 0.00081170\n",
      "Iteration 1613, loss = 0.00081071\n",
      "Iteration 1614, loss = 0.00080973\n",
      "Training loss did not improve more than tol=0.000001 for 10 consecutive epochs. Stopping.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00        14\n",
      "           2       0.91      1.00      0.95        20\n",
      "           3       1.00      0.90      0.95        20\n",
      "\n",
      "    accuracy                           0.96        54\n",
      "   macro avg       0.97      0.97      0.97        54\n",
      "weighted avg       0.97      0.96      0.96        54\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "hidden_layers = (512,)\n",
    "activation = 'logistic'\n",
    "opts = dict(hidden_layer_sizes = hidden_layers,verbose = True,activation = activation,tol = 1e-6,max_iter = int(1e6))\n",
    "solver = 'adam'\n",
    "clf_MLP = MLPClassifier(solver = solver, **opts)\n",
    "clf_MLP.fit(X_train_,y_train)\n",
    "predictions = clf_MLP.predict(X_test_)\n",
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "訓練資料的classification report "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00        45\n",
      "           2       1.00      1.00      1.00        51\n",
      "           3       1.00      1.00      1.00        28\n",
      "\n",
      "    accuracy                           1.00       124\n",
      "   macro avg       1.00      1.00      1.00       124\n",
      "weighted avg       1.00      1.00      1.00       124\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred = clf_MLP.predict(X_train_)\n",
    "print(classification_report(y_train,pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- hidden_layers = (60, 60, 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.08756968\n",
      "Iteration 2, loss = 1.07820776\n",
      "Iteration 3, loss = 1.07212742\n",
      "Iteration 4, loss = 1.06915770\n",
      "Iteration 5, loss = 1.06869760\n",
      "Iteration 6, loss = 1.06964583\n",
      "Iteration 7, loss = 1.07073085\n",
      "Iteration 8, loss = 1.07109747\n",
      "Iteration 9, loss = 1.07053781\n",
      "Iteration 10, loss = 1.06928073\n",
      "Iteration 11, loss = 1.06770834\n",
      "Iteration 12, loss = 1.06617760\n",
      "Iteration 13, loss = 1.06493379\n",
      "Iteration 14, loss = 1.06407729\n",
      "Iteration 15, loss = 1.06356735\n",
      "Iteration 16, loss = 1.06325890\n",
      "Iteration 17, loss = 1.06296406\n",
      "Iteration 18, loss = 1.06251783\n",
      "Iteration 19, loss = 1.06182368\n",
      "Iteration 20, loss = 1.06086634\n",
      "Iteration 21, loss = 1.05969593\n",
      "Iteration 22, loss = 1.05839679\n",
      "Iteration 23, loss = 1.05705406\n",
      "Iteration 24, loss = 1.05572679\n",
      "Iteration 25, loss = 1.05443241\n",
      "Iteration 26, loss = 1.05314502\n",
      "Iteration 27, loss = 1.05180665\n",
      "Iteration 28, loss = 1.05034732\n",
      "Iteration 29, loss = 1.04870619\n",
      "Iteration 30, loss = 1.04684629\n",
      "Iteration 31, loss = 1.04475816\n",
      "Iteration 32, loss = 1.04245325\n",
      "Iteration 33, loss = 1.03995128\n",
      "Iteration 34, loss = 1.03726694\n",
      "Iteration 35, loss = 1.03440042\n",
      "Iteration 36, loss = 1.03133418\n",
      "Iteration 37, loss = 1.02803574\n",
      "Iteration 38, loss = 1.02446476\n",
      "Iteration 39, loss = 1.02058105\n",
      "Iteration 40, loss = 1.01635082\n",
      "Iteration 41, loss = 1.01174913\n",
      "Iteration 42, loss = 1.00675874\n",
      "Iteration 43, loss = 1.00136635\n",
      "Iteration 44, loss = 0.99555833\n",
      "Iteration 45, loss = 0.98931752\n",
      "Iteration 46, loss = 0.98262202\n",
      "Iteration 47, loss = 0.97544629\n",
      "Iteration 48, loss = 0.96776370\n",
      "Iteration 49, loss = 0.95954954\n",
      "Iteration 50, loss = 0.95078359\n",
      "Iteration 51, loss = 0.94145162\n",
      "Iteration 52, loss = 0.93154577\n",
      "Iteration 53, loss = 0.92106415\n",
      "Iteration 54, loss = 0.91001022\n",
      "Iteration 55, loss = 0.89839237\n",
      "Iteration 56, loss = 0.88622401\n",
      "Iteration 57, loss = 0.87352404\n",
      "Iteration 58, loss = 0.86031770\n",
      "Iteration 59, loss = 0.84663727\n",
      "Iteration 60, loss = 0.83252269\n",
      "Iteration 61, loss = 0.81802159\n",
      "Iteration 62, loss = 0.80318898\n",
      "Iteration 63, loss = 0.78808641\n",
      "Iteration 64, loss = 0.77278084\n",
      "Iteration 65, loss = 0.75734330\n",
      "Iteration 66, loss = 0.74184724\n",
      "Iteration 67, loss = 0.72636695\n",
      "Iteration 68, loss = 0.71097583\n",
      "Iteration 69, loss = 0.69574469\n",
      "Iteration 70, loss = 0.68074010\n",
      "Iteration 71, loss = 0.66602290\n",
      "Iteration 72, loss = 0.65164697\n",
      "Iteration 73, loss = 0.63765822\n",
      "Iteration 74, loss = 0.62409400\n",
      "Iteration 75, loss = 0.61098283\n",
      "Iteration 76, loss = 0.59834449\n",
      "Iteration 77, loss = 0.58619041\n",
      "Iteration 78, loss = 0.57452421\n",
      "Iteration 79, loss = 0.56334244\n",
      "Iteration 80, loss = 0.55263548\n",
      "Iteration 81, loss = 0.54238840\n",
      "Iteration 82, loss = 0.53258190\n",
      "Iteration 83, loss = 0.52319323\n",
      "Iteration 84, loss = 0.51419706\n",
      "Iteration 85, loss = 0.50556634\n",
      "Iteration 86, loss = 0.49727300\n",
      "Iteration 87, loss = 0.48928868\n",
      "Iteration 88, loss = 0.48158524\n",
      "Iteration 89, loss = 0.47413534\n",
      "Iteration 90, loss = 0.46691274\n",
      "Iteration 91, loss = 0.45989266\n",
      "Iteration 92, loss = 0.45305200\n",
      "Iteration 93, loss = 0.44636939\n",
      "Iteration 94, loss = 0.43982531\n",
      "Iteration 95, loss = 0.43340206\n",
      "Iteration 96, loss = 0.42708371\n",
      "Iteration 97, loss = 0.42085602\n",
      "Iteration 98, loss = 0.41470638\n",
      "Iteration 99, loss = 0.40862369\n",
      "Iteration 100, loss = 0.40259829\n",
      "Iteration 101, loss = 0.39662182\n",
      "Iteration 102, loss = 0.39068713\n",
      "Iteration 103, loss = 0.38478817\n",
      "Iteration 104, loss = 0.37891989\n",
      "Iteration 105, loss = 0.37307812\n",
      "Iteration 106, loss = 0.36725946\n",
      "Iteration 107, loss = 0.36146126\n",
      "Iteration 108, loss = 0.35568147\n",
      "Iteration 109, loss = 0.34991866\n",
      "Iteration 110, loss = 0.34417190\n",
      "Iteration 111, loss = 0.33844074\n",
      "Iteration 112, loss = 0.33272521\n",
      "Iteration 113, loss = 0.32702569\n",
      "Iteration 114, loss = 0.32134295\n",
      "Iteration 115, loss = 0.31567805\n",
      "Iteration 116, loss = 0.31003235\n",
      "Iteration 117, loss = 0.30440744\n",
      "Iteration 118, loss = 0.29880514\n",
      "Iteration 119, loss = 0.29322746\n",
      "Iteration 120, loss = 0.28767658\n",
      "Iteration 121, loss = 0.28215483\n",
      "Iteration 122, loss = 0.27666465\n",
      "Iteration 123, loss = 0.27120858\n",
      "Iteration 124, loss = 0.26578924\n",
      "Iteration 125, loss = 0.26040928\n",
      "Iteration 126, loss = 0.25507141\n",
      "Iteration 127, loss = 0.24977832\n",
      "Iteration 128, loss = 0.24453270\n",
      "Iteration 129, loss = 0.23933720\n",
      "Iteration 130, loss = 0.23419445\n",
      "Iteration 131, loss = 0.22910702\n",
      "Iteration 132, loss = 0.22407739\n",
      "Iteration 133, loss = 0.21910799\n",
      "Iteration 134, loss = 0.21420112\n",
      "Iteration 135, loss = 0.20935900\n",
      "Iteration 136, loss = 0.20458375\n",
      "Iteration 137, loss = 0.19987736\n",
      "Iteration 138, loss = 0.19524172\n",
      "Iteration 139, loss = 0.19067857\n",
      "Iteration 140, loss = 0.18618956\n",
      "Iteration 141, loss = 0.18177621\n",
      "Iteration 142, loss = 0.17743989\n",
      "Iteration 143, loss = 0.17318188\n",
      "Iteration 144, loss = 0.16900329\n",
      "Iteration 145, loss = 0.16490514\n",
      "Iteration 146, loss = 0.16088828\n",
      "Iteration 147, loss = 0.15695346\n",
      "Iteration 148, loss = 0.15310126\n",
      "Iteration 149, loss = 0.14933215\n",
      "Iteration 150, loss = 0.14564645\n",
      "Iteration 151, loss = 0.14204435\n",
      "Iteration 152, loss = 0.13852588\n",
      "Iteration 153, loss = 0.13509095\n",
      "Iteration 154, loss = 0.13173932\n",
      "Iteration 155, loss = 0.12847061\n",
      "Iteration 156, loss = 0.12528432\n",
      "Iteration 157, loss = 0.12217981\n",
      "Iteration 158, loss = 0.11915632\n",
      "Iteration 159, loss = 0.11621295\n",
      "Iteration 160, loss = 0.11334873\n",
      "Iteration 161, loss = 0.11056255\n",
      "Iteration 162, loss = 0.10785322\n",
      "Iteration 163, loss = 0.10521946\n",
      "Iteration 164, loss = 0.10265991\n",
      "Iteration 165, loss = 0.10017316\n",
      "Iteration 166, loss = 0.09775773\n",
      "Iteration 167, loss = 0.09541209\n",
      "Iteration 168, loss = 0.09313467\n",
      "Iteration 169, loss = 0.09092388\n",
      "Iteration 170, loss = 0.08877811\n",
      "Iteration 171, loss = 0.08669572\n",
      "Iteration 172, loss = 0.08467507\n",
      "Iteration 173, loss = 0.08271453\n",
      "Iteration 174, loss = 0.08081245\n",
      "Iteration 175, loss = 0.07896721\n",
      "Iteration 176, loss = 0.07717718\n",
      "Iteration 177, loss = 0.07544078\n",
      "Iteration 178, loss = 0.07375641\n",
      "Iteration 179, loss = 0.07212254\n",
      "Iteration 180, loss = 0.07053761\n",
      "Iteration 181, loss = 0.06900012\n",
      "Iteration 182, loss = 0.06750861\n",
      "Iteration 183, loss = 0.06606162\n",
      "Iteration 184, loss = 0.06465774\n",
      "Iteration 185, loss = 0.06329557\n",
      "Iteration 186, loss = 0.06197378\n",
      "Iteration 187, loss = 0.06069104\n",
      "Iteration 188, loss = 0.05944606\n",
      "Iteration 189, loss = 0.05823761\n",
      "Iteration 190, loss = 0.05706445\n",
      "Iteration 191, loss = 0.05592541\n",
      "Iteration 192, loss = 0.05481933\n",
      "Iteration 193, loss = 0.05374511\n",
      "Iteration 194, loss = 0.05270165\n",
      "Iteration 195, loss = 0.05168792\n",
      "Iteration 196, loss = 0.05070289\n",
      "Iteration 197, loss = 0.04974559\n",
      "Iteration 198, loss = 0.04881505\n",
      "Iteration 199, loss = 0.04791037\n",
      "Iteration 200, loss = 0.04703066\n",
      "Iteration 201, loss = 0.04617504\n",
      "Iteration 202, loss = 0.04534271\n",
      "Iteration 203, loss = 0.04453285\n",
      "Iteration 204, loss = 0.04374471\n",
      "Iteration 205, loss = 0.04297752\n",
      "Iteration 206, loss = 0.04223059\n",
      "Iteration 207, loss = 0.04150321\n",
      "Iteration 208, loss = 0.04079473\n",
      "Iteration 209, loss = 0.04010450\n",
      "Iteration 210, loss = 0.03943191\n",
      "Iteration 211, loss = 0.03877637\n",
      "Iteration 212, loss = 0.03813732\n",
      "Iteration 213, loss = 0.03751419\n",
      "Iteration 214, loss = 0.03690647\n",
      "Iteration 215, loss = 0.03631365\n",
      "Iteration 216, loss = 0.03573524\n",
      "Iteration 217, loss = 0.03517078\n",
      "Iteration 218, loss = 0.03461981\n",
      "Iteration 219, loss = 0.03408191\n",
      "Iteration 220, loss = 0.03355666\n",
      "Iteration 221, loss = 0.03304366\n",
      "Iteration 222, loss = 0.03254253\n",
      "Iteration 223, loss = 0.03205289\n",
      "Iteration 224, loss = 0.03157440\n",
      "Iteration 225, loss = 0.03110672\n",
      "Iteration 226, loss = 0.03064952\n",
      "Iteration 227, loss = 0.03020247\n",
      "Iteration 228, loss = 0.02976529\n",
      "Iteration 229, loss = 0.02933767\n",
      "Iteration 230, loss = 0.02891934\n",
      "Iteration 231, loss = 0.02851003\n",
      "Iteration 232, loss = 0.02810948\n",
      "Iteration 233, loss = 0.02771743\n",
      "Iteration 234, loss = 0.02733365\n",
      "Iteration 235, loss = 0.02695790\n",
      "Iteration 236, loss = 0.02658997\n",
      "Iteration 237, loss = 0.02622962\n",
      "Iteration 238, loss = 0.02587667\n",
      "Iteration 239, loss = 0.02553089\n",
      "Iteration 240, loss = 0.02519211\n",
      "Iteration 241, loss = 0.02486013\n",
      "Iteration 242, loss = 0.02453478\n",
      "Iteration 243, loss = 0.02421587\n",
      "Iteration 244, loss = 0.02390325\n",
      "Iteration 245, loss = 0.02359674\n",
      "Iteration 246, loss = 0.02329619\n",
      "Iteration 247, loss = 0.02300144\n",
      "Iteration 248, loss = 0.02271236\n",
      "Iteration 249, loss = 0.02242879\n",
      "Iteration 250, loss = 0.02215060\n",
      "Iteration 251, loss = 0.02187765\n",
      "Iteration 252, loss = 0.02160982\n",
      "Iteration 253, loss = 0.02134698\n",
      "Iteration 254, loss = 0.02108900\n",
      "Iteration 255, loss = 0.02083578\n",
      "Iteration 256, loss = 0.02058719\n",
      "Iteration 257, loss = 0.02034313\n",
      "Iteration 258, loss = 0.02010348\n",
      "Iteration 259, loss = 0.01986815\n",
      "Iteration 260, loss = 0.01963702\n",
      "Iteration 261, loss = 0.01941002\n",
      "Iteration 262, loss = 0.01918703\n",
      "Iteration 263, loss = 0.01896796\n",
      "Iteration 264, loss = 0.01875273\n",
      "Iteration 265, loss = 0.01854125\n",
      "Iteration 266, loss = 0.01833344\n",
      "Iteration 267, loss = 0.01812920\n",
      "Iteration 268, loss = 0.01792846\n",
      "Iteration 269, loss = 0.01773115\n",
      "Iteration 270, loss = 0.01753718\n",
      "Iteration 271, loss = 0.01734648\n",
      "Iteration 272, loss = 0.01715899\n",
      "Iteration 273, loss = 0.01697462\n",
      "Iteration 274, loss = 0.01679332\n",
      "Iteration 275, loss = 0.01661502\n",
      "Iteration 276, loss = 0.01643964\n",
      "Iteration 277, loss = 0.01626714\n",
      "Iteration 278, loss = 0.01609744\n",
      "Iteration 279, loss = 0.01593049\n",
      "Iteration 280, loss = 0.01576623\n",
      "Iteration 281, loss = 0.01560461\n",
      "Iteration 282, loss = 0.01544556\n",
      "Iteration 283, loss = 0.01528904\n",
      "Iteration 284, loss = 0.01513499\n",
      "Iteration 285, loss = 0.01498337\n",
      "Iteration 286, loss = 0.01483411\n",
      "Iteration 287, loss = 0.01468718\n",
      "Iteration 288, loss = 0.01454253\n",
      "Iteration 289, loss = 0.01440010\n",
      "Iteration 290, loss = 0.01425986\n",
      "Iteration 291, loss = 0.01412177\n",
      "Iteration 292, loss = 0.01398577\n",
      "Iteration 293, loss = 0.01385182\n",
      "Iteration 294, loss = 0.01371990\n",
      "Iteration 295, loss = 0.01358995\n",
      "Iteration 296, loss = 0.01346193\n",
      "Iteration 297, loss = 0.01333582\n",
      "Iteration 298, loss = 0.01321157\n",
      "Iteration 299, loss = 0.01308914\n",
      "Iteration 300, loss = 0.01296850\n",
      "Iteration 301, loss = 0.01284962\n",
      "Iteration 302, loss = 0.01273247\n",
      "Iteration 303, loss = 0.01261700\n",
      "Iteration 304, loss = 0.01250319\n",
      "Iteration 305, loss = 0.01239101\n",
      "Iteration 306, loss = 0.01228042\n",
      "Iteration 307, loss = 0.01217139\n",
      "Iteration 308, loss = 0.01206391\n",
      "Iteration 309, loss = 0.01195793\n",
      "Iteration 310, loss = 0.01185343\n",
      "Iteration 311, loss = 0.01175038\n",
      "Iteration 312, loss = 0.01164876\n",
      "Iteration 313, loss = 0.01154853\n",
      "Iteration 314, loss = 0.01144968\n",
      "Iteration 315, loss = 0.01135218\n",
      "Iteration 316, loss = 0.01125601\n",
      "Iteration 317, loss = 0.01116113\n",
      "Iteration 318, loss = 0.01106753\n",
      "Iteration 319, loss = 0.01097518\n",
      "Iteration 320, loss = 0.01088407\n",
      "Iteration 321, loss = 0.01079416\n",
      "Iteration 322, loss = 0.01070545\n",
      "Iteration 323, loss = 0.01061790\n",
      "Iteration 324, loss = 0.01053149\n",
      "Iteration 325, loss = 0.01044622\n",
      "Iteration 326, loss = 0.01036205\n",
      "Iteration 327, loss = 0.01027897\n",
      "Iteration 328, loss = 0.01019696\n",
      "Iteration 329, loss = 0.01011599\n",
      "Iteration 330, loss = 0.01003606\n",
      "Iteration 331, loss = 0.00995715\n",
      "Iteration 332, loss = 0.00987923\n",
      "Iteration 333, loss = 0.00980230\n",
      "Iteration 334, loss = 0.00972632\n",
      "Iteration 335, loss = 0.00965130\n",
      "Iteration 336, loss = 0.00957721\n",
      "Iteration 337, loss = 0.00950403\n",
      "Iteration 338, loss = 0.00943175\n",
      "Iteration 339, loss = 0.00936036\n",
      "Iteration 340, loss = 0.00928985\n",
      "Iteration 341, loss = 0.00922019\n",
      "Iteration 342, loss = 0.00915137\n",
      "Iteration 343, loss = 0.00908338\n",
      "Iteration 344, loss = 0.00901621\n",
      "Iteration 345, loss = 0.00894984\n",
      "Iteration 346, loss = 0.00888426\n",
      "Iteration 347, loss = 0.00881945\n",
      "Iteration 348, loss = 0.00875542\n",
      "Iteration 349, loss = 0.00869213\n",
      "Iteration 350, loss = 0.00862958\n",
      "Iteration 351, loss = 0.00856777\n",
      "Iteration 352, loss = 0.00850666\n",
      "Iteration 353, loss = 0.00844627\n",
      "Iteration 354, loss = 0.00838657\n",
      "Iteration 355, loss = 0.00832755\n",
      "Iteration 356, loss = 0.00826921\n",
      "Iteration 357, loss = 0.00821153\n",
      "Iteration 358, loss = 0.00815450\n",
      "Iteration 359, loss = 0.00809812\n",
      "Iteration 360, loss = 0.00804237\n",
      "Iteration 361, loss = 0.00798724\n",
      "Iteration 362, loss = 0.00793272\n",
      "Iteration 363, loss = 0.00787881\n",
      "Iteration 364, loss = 0.00782550\n",
      "Iteration 365, loss = 0.00777277\n",
      "Iteration 366, loss = 0.00772062\n",
      "Iteration 367, loss = 0.00766903\n",
      "Iteration 368, loss = 0.00761801\n",
      "Iteration 369, loss = 0.00756754\n",
      "Iteration 370, loss = 0.00751762\n",
      "Iteration 371, loss = 0.00746823\n",
      "Iteration 372, loss = 0.00741936\n",
      "Iteration 373, loss = 0.00737102\n",
      "Iteration 374, loss = 0.00732319\n",
      "Iteration 375, loss = 0.00727587\n",
      "Iteration 376, loss = 0.00722904\n",
      "Iteration 377, loss = 0.00718271\n",
      "Iteration 378, loss = 0.00713686\n",
      "Iteration 379, loss = 0.00709149\n",
      "Iteration 380, loss = 0.00704658\n",
      "Iteration 381, loss = 0.00700214\n",
      "Iteration 382, loss = 0.00695816\n",
      "Iteration 383, loss = 0.00691463\n",
      "Iteration 384, loss = 0.00687154\n",
      "Iteration 385, loss = 0.00682889\n",
      "Iteration 386, loss = 0.00678667\n",
      "Iteration 387, loss = 0.00674488\n",
      "Iteration 388, loss = 0.00670351\n",
      "Iteration 389, loss = 0.00666255\n",
      "Iteration 390, loss = 0.00662201\n",
      "Iteration 391, loss = 0.00658186\n",
      "Iteration 392, loss = 0.00654211\n",
      "Iteration 393, loss = 0.00650276\n",
      "Iteration 394, loss = 0.00646379\n",
      "Iteration 395, loss = 0.00642520\n",
      "Iteration 396, loss = 0.00638699\n",
      "Iteration 397, loss = 0.00634915\n",
      "Iteration 398, loss = 0.00631168\n",
      "Iteration 399, loss = 0.00627457\n",
      "Iteration 400, loss = 0.00623781\n",
      "Iteration 401, loss = 0.00620141\n",
      "Iteration 402, loss = 0.00616536\n",
      "Iteration 403, loss = 0.00612964\n",
      "Iteration 404, loss = 0.00609427\n",
      "Iteration 405, loss = 0.00605923\n",
      "Iteration 406, loss = 0.00602452\n",
      "Iteration 407, loss = 0.00599014\n",
      "Iteration 408, loss = 0.00595608\n",
      "Iteration 409, loss = 0.00592233\n",
      "Iteration 410, loss = 0.00588890\n",
      "Iteration 411, loss = 0.00585577\n",
      "Iteration 412, loss = 0.00582296\n",
      "Iteration 413, loss = 0.00579044\n",
      "Iteration 414, loss = 0.00575822\n",
      "Iteration 415, loss = 0.00572629\n",
      "Iteration 416, loss = 0.00569466\n",
      "Iteration 417, loss = 0.00566331\n",
      "Iteration 418, loss = 0.00563224\n",
      "Iteration 419, loss = 0.00560146\n",
      "Iteration 420, loss = 0.00557094\n",
      "Iteration 421, loss = 0.00554071\n",
      "Iteration 422, loss = 0.00551074\n",
      "Iteration 423, loss = 0.00548103\n",
      "Iteration 424, loss = 0.00545159\n",
      "Iteration 425, loss = 0.00542241\n",
      "Iteration 426, loss = 0.00539348\n",
      "Iteration 427, loss = 0.00536481\n",
      "Iteration 428, loss = 0.00533639\n",
      "Iteration 429, loss = 0.00530822\n",
      "Iteration 430, loss = 0.00528028\n",
      "Iteration 431, loss = 0.00525260\n",
      "Iteration 432, loss = 0.00522514\n",
      "Iteration 433, loss = 0.00519793\n",
      "Iteration 434, loss = 0.00517095\n",
      "Iteration 435, loss = 0.00514419\n",
      "Iteration 436, loss = 0.00511767\n",
      "Iteration 437, loss = 0.00509137\n",
      "Iteration 438, loss = 0.00506529\n",
      "Iteration 439, loss = 0.00503943\n",
      "Iteration 440, loss = 0.00501379\n",
      "Iteration 441, loss = 0.00498836\n",
      "Iteration 442, loss = 0.00496314\n",
      "Iteration 443, loss = 0.00493814\n",
      "Iteration 444, loss = 0.00491334\n",
      "Iteration 445, loss = 0.00488874\n",
      "Iteration 446, loss = 0.00486435\n",
      "Iteration 447, loss = 0.00484016\n",
      "Iteration 448, loss = 0.00481616\n",
      "Iteration 449, loss = 0.00479236\n",
      "Iteration 450, loss = 0.00476876\n",
      "Iteration 451, loss = 0.00474534\n",
      "Iteration 452, loss = 0.00472212\n",
      "Iteration 453, loss = 0.00469908\n",
      "Iteration 454, loss = 0.00467623\n",
      "Iteration 455, loss = 0.00465356\n",
      "Iteration 456, loss = 0.00463107\n",
      "Iteration 457, loss = 0.00460876\n",
      "Iteration 458, loss = 0.00458663\n",
      "Iteration 459, loss = 0.00456467\n",
      "Iteration 460, loss = 0.00454288\n",
      "Iteration 461, loss = 0.00452127\n",
      "Iteration 462, loss = 0.00449982\n",
      "Iteration 463, loss = 0.00447855\n",
      "Iteration 464, loss = 0.00445743\n",
      "Iteration 465, loss = 0.00443649\n",
      "Iteration 466, loss = 0.00441570\n",
      "Iteration 467, loss = 0.00439508\n",
      "Iteration 468, loss = 0.00437461\n",
      "Iteration 469, loss = 0.00435430\n",
      "Iteration 470, loss = 0.00433415\n",
      "Iteration 471, loss = 0.00431415\n",
      "Iteration 472, loss = 0.00429430\n",
      "Iteration 473, loss = 0.00427460\n",
      "Iteration 474, loss = 0.00425506\n",
      "Iteration 475, loss = 0.00423566\n",
      "Iteration 476, loss = 0.00421640\n",
      "Iteration 477, loss = 0.00419729\n",
      "Iteration 478, loss = 0.00417832\n",
      "Iteration 479, loss = 0.00415950\n",
      "Iteration 480, loss = 0.00414081\n",
      "Iteration 481, loss = 0.00412226\n",
      "Iteration 482, loss = 0.00410385\n",
      "Iteration 483, loss = 0.00408558\n",
      "Iteration 484, loss = 0.00406744\n",
      "Iteration 485, loss = 0.00404943\n",
      "Iteration 486, loss = 0.00403156\n",
      "Iteration 487, loss = 0.00401381\n",
      "Iteration 488, loss = 0.00399620\n",
      "Iteration 489, loss = 0.00397871\n",
      "Iteration 490, loss = 0.00396135\n",
      "Iteration 491, loss = 0.00394411\n",
      "Iteration 492, loss = 0.00392700\n",
      "Iteration 493, loss = 0.00391001\n",
      "Iteration 494, loss = 0.00389314\n",
      "Iteration 495, loss = 0.00387639\n",
      "Iteration 496, loss = 0.00385976\n",
      "Iteration 497, loss = 0.00384325\n",
      "Iteration 498, loss = 0.00382686\n",
      "Iteration 499, loss = 0.00381058\n",
      "Iteration 500, loss = 0.00379441\n",
      "Iteration 501, loss = 0.00377836\n",
      "Iteration 502, loss = 0.00376243\n",
      "Iteration 503, loss = 0.00374660\n",
      "Iteration 504, loss = 0.00373088\n",
      "Iteration 505, loss = 0.00371528\n",
      "Iteration 506, loss = 0.00369978\n",
      "Iteration 507, loss = 0.00368438\n",
      "Iteration 508, loss = 0.00366910\n",
      "Iteration 509, loss = 0.00365392\n",
      "Iteration 510, loss = 0.00363884\n",
      "Iteration 511, loss = 0.00362387\n",
      "Iteration 512, loss = 0.00360900\n",
      "Iteration 513, loss = 0.00359422\n",
      "Iteration 514, loss = 0.00357955\n",
      "Iteration 515, loss = 0.00356498\n",
      "Iteration 516, loss = 0.00355051\n",
      "Iteration 517, loss = 0.00353614\n",
      "Iteration 518, loss = 0.00352186\n",
      "Iteration 519, loss = 0.00350767\n",
      "Iteration 520, loss = 0.00349359\n",
      "Iteration 521, loss = 0.00347959\n",
      "Iteration 522, loss = 0.00346569\n",
      "Iteration 523, loss = 0.00345188\n",
      "Iteration 524, loss = 0.00343816\n",
      "Iteration 525, loss = 0.00342454\n",
      "Iteration 526, loss = 0.00341100\n",
      "Iteration 527, loss = 0.00339755\n",
      "Iteration 528, loss = 0.00338419\n",
      "Iteration 529, loss = 0.00337092\n",
      "Iteration 530, loss = 0.00335773\n",
      "Iteration 531, loss = 0.00334463\n",
      "Iteration 532, loss = 0.00333161\n",
      "Iteration 533, loss = 0.00331868\n",
      "Iteration 534, loss = 0.00330583\n",
      "Iteration 535, loss = 0.00329307\n",
      "Iteration 536, loss = 0.00328039\n",
      "Iteration 537, loss = 0.00326778\n",
      "Iteration 538, loss = 0.00325526\n",
      "Iteration 539, loss = 0.00324282\n",
      "Iteration 540, loss = 0.00323046\n",
      "Iteration 541, loss = 0.00321817\n",
      "Iteration 542, loss = 0.00320597\n",
      "Iteration 543, loss = 0.00319384\n",
      "Iteration 544, loss = 0.00318179\n",
      "Iteration 545, loss = 0.00316981\n",
      "Iteration 546, loss = 0.00315791\n",
      "Iteration 547, loss = 0.00314608\n",
      "Iteration 548, loss = 0.00313433\n",
      "Iteration 549, loss = 0.00312265\n",
      "Iteration 550, loss = 0.00311104\n",
      "Iteration 551, loss = 0.00309950\n",
      "Iteration 552, loss = 0.00308804\n",
      "Iteration 553, loss = 0.00307664\n",
      "Iteration 554, loss = 0.00306532\n",
      "Iteration 555, loss = 0.00305407\n",
      "Iteration 556, loss = 0.00304288\n",
      "Iteration 557, loss = 0.00303177\n",
      "Iteration 558, loss = 0.00302072\n",
      "Iteration 559, loss = 0.00300974\n",
      "Iteration 560, loss = 0.00299882\n",
      "Iteration 561, loss = 0.00298797\n",
      "Iteration 562, loss = 0.00297719\n",
      "Iteration 563, loss = 0.00296647\n",
      "Iteration 564, loss = 0.00295582\n",
      "Iteration 565, loss = 0.00294523\n",
      "Iteration 566, loss = 0.00293470\n",
      "Iteration 567, loss = 0.00292424\n",
      "Iteration 568, loss = 0.00291384\n",
      "Iteration 569, loss = 0.00290350\n",
      "Iteration 570, loss = 0.00289322\n",
      "Iteration 571, loss = 0.00288301\n",
      "Iteration 572, loss = 0.00287285\n",
      "Iteration 573, loss = 0.00286275\n",
      "Iteration 574, loss = 0.00285272\n",
      "Iteration 575, loss = 0.00284274\n",
      "Iteration 576, loss = 0.00283282\n",
      "Iteration 577, loss = 0.00282296\n",
      "Iteration 578, loss = 0.00281316\n",
      "Iteration 579, loss = 0.00280341\n",
      "Iteration 580, loss = 0.00279372\n",
      "Iteration 581, loss = 0.00278409\n",
      "Iteration 582, loss = 0.00277451\n",
      "Iteration 583, loss = 0.00276498\n",
      "Iteration 584, loss = 0.00275552\n",
      "Iteration 585, loss = 0.00274610\n",
      "Iteration 586, loss = 0.00273674\n",
      "Iteration 587, loss = 0.00272744\n",
      "Iteration 588, loss = 0.00271818\n",
      "Iteration 589, loss = 0.00270898\n",
      "Iteration 590, loss = 0.00269984\n",
      "Iteration 591, loss = 0.00269074\n",
      "Iteration 592, loss = 0.00268170\n",
      "Iteration 593, loss = 0.00267270\n",
      "Iteration 594, loss = 0.00266376\n",
      "Iteration 595, loss = 0.00265487\n",
      "Iteration 596, loss = 0.00264603\n",
      "Iteration 597, loss = 0.00263723\n",
      "Iteration 598, loss = 0.00262849\n",
      "Iteration 599, loss = 0.00261980\n",
      "Iteration 600, loss = 0.00261115\n",
      "Iteration 601, loss = 0.00260255\n",
      "Iteration 602, loss = 0.00259400\n",
      "Iteration 603, loss = 0.00258550\n",
      "Iteration 604, loss = 0.00257704\n",
      "Iteration 605, loss = 0.00256863\n",
      "Iteration 606, loss = 0.00256027\n",
      "Iteration 607, loss = 0.00255195\n",
      "Iteration 608, loss = 0.00254368\n",
      "Iteration 609, loss = 0.00253545\n",
      "Iteration 610, loss = 0.00252727\n",
      "Iteration 611, loss = 0.00251913\n",
      "Iteration 612, loss = 0.00251104\n",
      "Iteration 613, loss = 0.00250299\n",
      "Iteration 614, loss = 0.00249499\n",
      "Iteration 615, loss = 0.00248702\n",
      "Iteration 616, loss = 0.00247910\n",
      "Iteration 617, loss = 0.00247123\n",
      "Iteration 618, loss = 0.00246339\n",
      "Iteration 619, loss = 0.00245560\n",
      "Iteration 620, loss = 0.00244785\n",
      "Iteration 621, loss = 0.00244014\n",
      "Iteration 622, loss = 0.00243247\n",
      "Iteration 623, loss = 0.00242484\n",
      "Iteration 624, loss = 0.00241726\n",
      "Iteration 625, loss = 0.00240971\n",
      "Iteration 626, loss = 0.00240220\n",
      "Iteration 627, loss = 0.00239474\n",
      "Iteration 628, loss = 0.00238731\n",
      "Iteration 629, loss = 0.00237992\n",
      "Iteration 630, loss = 0.00237257\n",
      "Iteration 631, loss = 0.00236526\n",
      "Iteration 632, loss = 0.00235798\n",
      "Iteration 633, loss = 0.00235075\n",
      "Iteration 634, loss = 0.00234355\n",
      "Iteration 635, loss = 0.00233639\n",
      "Iteration 636, loss = 0.00232927\n",
      "Iteration 637, loss = 0.00232218\n",
      "Iteration 638, loss = 0.00231513\n",
      "Iteration 639, loss = 0.00230811\n",
      "Iteration 640, loss = 0.00230114\n",
      "Iteration 641, loss = 0.00229420\n",
      "Iteration 642, loss = 0.00228729\n",
      "Iteration 643, loss = 0.00228042\n",
      "Iteration 644, loss = 0.00227358\n",
      "Iteration 645, loss = 0.00226678\n",
      "Iteration 646, loss = 0.00226001\n",
      "Iteration 647, loss = 0.00225328\n",
      "Iteration 648, loss = 0.00224658\n",
      "Iteration 649, loss = 0.00223992\n",
      "Iteration 650, loss = 0.00223329\n",
      "Iteration 651, loss = 0.00222669\n",
      "Iteration 652, loss = 0.00222013\n",
      "Iteration 653, loss = 0.00221360\n",
      "Iteration 654, loss = 0.00220710\n",
      "Iteration 655, loss = 0.00220063\n",
      "Iteration 656, loss = 0.00219420\n",
      "Iteration 657, loss = 0.00218780\n",
      "Iteration 658, loss = 0.00218143\n",
      "Iteration 659, loss = 0.00217509\n",
      "Iteration 660, loss = 0.00216878\n",
      "Iteration 661, loss = 0.00216251\n",
      "Iteration 662, loss = 0.00215627\n",
      "Iteration 663, loss = 0.00215005\n",
      "Iteration 664, loss = 0.00214387\n",
      "Iteration 665, loss = 0.00213772\n",
      "Iteration 666, loss = 0.00213159\n",
      "Iteration 667, loss = 0.00212550\n",
      "Iteration 668, loss = 0.00211944\n",
      "Iteration 669, loss = 0.00211340\n",
      "Iteration 670, loss = 0.00210740\n",
      "Iteration 671, loss = 0.00210143\n",
      "Iteration 672, loss = 0.00209548\n",
      "Iteration 673, loss = 0.00208956\n",
      "Iteration 674, loss = 0.00208367\n",
      "Iteration 675, loss = 0.00207781\n",
      "Iteration 676, loss = 0.00207198\n",
      "Iteration 677, loss = 0.00206618\n",
      "Iteration 678, loss = 0.00206040\n",
      "Iteration 679, loss = 0.00205465\n",
      "Iteration 680, loss = 0.00204893\n",
      "Iteration 681, loss = 0.00204324\n",
      "Iteration 682, loss = 0.00203757\n",
      "Iteration 683, loss = 0.00203193\n",
      "Iteration 684, loss = 0.00202632\n",
      "Iteration 685, loss = 0.00202074\n",
      "Iteration 686, loss = 0.00201518\n",
      "Iteration 687, loss = 0.00200964\n",
      "Iteration 688, loss = 0.00200414\n",
      "Iteration 689, loss = 0.00199865\n",
      "Iteration 690, loss = 0.00199320\n",
      "Iteration 691, loss = 0.00198777\n",
      "Iteration 692, loss = 0.00198236\n",
      "Iteration 693, loss = 0.00197698\n",
      "Iteration 694, loss = 0.00197163\n",
      "Iteration 695, loss = 0.00196630\n",
      "Iteration 696, loss = 0.00196100\n",
      "Iteration 697, loss = 0.00195571\n",
      "Iteration 698, loss = 0.00195046\n",
      "Iteration 699, loss = 0.00194523\n",
      "Iteration 700, loss = 0.00194002\n",
      "Iteration 701, loss = 0.00193484\n",
      "Iteration 702, loss = 0.00192968\n",
      "Iteration 703, loss = 0.00192454\n",
      "Iteration 704, loss = 0.00191943\n",
      "Iteration 705, loss = 0.00191434\n",
      "Iteration 706, loss = 0.00190928\n",
      "Iteration 707, loss = 0.00190423\n",
      "Iteration 708, loss = 0.00189921\n",
      "Iteration 709, loss = 0.00189422\n",
      "Iteration 710, loss = 0.00188924\n",
      "Iteration 711, loss = 0.00188429\n",
      "Iteration 712, loss = 0.00187936\n",
      "Iteration 713, loss = 0.00187446\n",
      "Iteration 714, loss = 0.00186957\n",
      "Iteration 715, loss = 0.00186471\n",
      "Iteration 716, loss = 0.00185987\n",
      "Iteration 717, loss = 0.00185505\n",
      "Iteration 718, loss = 0.00185025\n",
      "Iteration 719, loss = 0.00184548\n",
      "Iteration 720, loss = 0.00184072\n",
      "Iteration 721, loss = 0.00183599\n",
      "Iteration 722, loss = 0.00183127\n",
      "Iteration 723, loss = 0.00182658\n",
      "Iteration 724, loss = 0.00182191\n",
      "Iteration 725, loss = 0.00181726\n",
      "Iteration 726, loss = 0.00181263\n",
      "Iteration 727, loss = 0.00180802\n",
      "Iteration 728, loss = 0.00180343\n",
      "Iteration 729, loss = 0.00179887\n",
      "Iteration 730, loss = 0.00179432\n",
      "Iteration 731, loss = 0.00178979\n",
      "Iteration 732, loss = 0.00178528\n",
      "Iteration 733, loss = 0.00178079\n",
      "Iteration 734, loss = 0.00177632\n",
      "Iteration 735, loss = 0.00177187\n",
      "Iteration 736, loss = 0.00176744\n",
      "Iteration 737, loss = 0.00176303\n",
      "Iteration 738, loss = 0.00175864\n",
      "Iteration 739, loss = 0.00175426\n",
      "Iteration 740, loss = 0.00174991\n",
      "Iteration 741, loss = 0.00174557\n",
      "Iteration 742, loss = 0.00174125\n",
      "Iteration 743, loss = 0.00173696\n",
      "Iteration 744, loss = 0.00173268\n",
      "Iteration 745, loss = 0.00172841\n",
      "Iteration 746, loss = 0.00172417\n",
      "Iteration 747, loss = 0.00171994\n",
      "Iteration 748, loss = 0.00171574\n",
      "Iteration 749, loss = 0.00171155\n",
      "Iteration 750, loss = 0.00170738\n",
      "Iteration 751, loss = 0.00170322\n",
      "Iteration 752, loss = 0.00169909\n",
      "Iteration 753, loss = 0.00169497\n",
      "Iteration 754, loss = 0.00169086\n",
      "Iteration 755, loss = 0.00168678\n",
      "Iteration 756, loss = 0.00168271\n",
      "Iteration 757, loss = 0.00167866\n",
      "Iteration 758, loss = 0.00167463\n",
      "Iteration 759, loss = 0.00167061\n",
      "Iteration 760, loss = 0.00166662\n",
      "Iteration 761, loss = 0.00166263\n",
      "Iteration 762, loss = 0.00165867\n",
      "Iteration 763, loss = 0.00165472\n",
      "Iteration 764, loss = 0.00165078\n",
      "Iteration 765, loss = 0.00164687\n",
      "Iteration 766, loss = 0.00164297\n",
      "Iteration 767, loss = 0.00163908\n",
      "Iteration 768, loss = 0.00163522\n",
      "Iteration 769, loss = 0.00163136\n",
      "Iteration 770, loss = 0.00162753\n",
      "Iteration 771, loss = 0.00162371\n",
      "Iteration 772, loss = 0.00161990\n",
      "Iteration 773, loss = 0.00161611\n",
      "Iteration 774, loss = 0.00161234\n",
      "Iteration 775, loss = 0.00160858\n",
      "Iteration 776, loss = 0.00160484\n",
      "Iteration 777, loss = 0.00160111\n",
      "Iteration 778, loss = 0.00159740\n",
      "Iteration 779, loss = 0.00159370\n",
      "Iteration 780, loss = 0.00159002\n",
      "Iteration 781, loss = 0.00158635\n",
      "Iteration 782, loss = 0.00158270\n",
      "Iteration 783, loss = 0.00157906\n",
      "Iteration 784, loss = 0.00157544\n",
      "Iteration 785, loss = 0.00157183\n",
      "Iteration 786, loss = 0.00156823\n",
      "Iteration 787, loss = 0.00156465\n",
      "Iteration 788, loss = 0.00156109\n",
      "Iteration 789, loss = 0.00155754\n",
      "Iteration 790, loss = 0.00155400\n",
      "Iteration 791, loss = 0.00155048\n",
      "Iteration 792, loss = 0.00154697\n",
      "Iteration 793, loss = 0.00154347\n",
      "Iteration 794, loss = 0.00153999\n",
      "Iteration 795, loss = 0.00153652\n",
      "Iteration 796, loss = 0.00153307\n",
      "Iteration 797, loss = 0.00152963\n",
      "Iteration 798, loss = 0.00152621\n",
      "Iteration 799, loss = 0.00152279\n",
      "Iteration 800, loss = 0.00151939\n",
      "Iteration 801, loss = 0.00151601\n",
      "Iteration 802, loss = 0.00151264\n",
      "Iteration 803, loss = 0.00150928\n",
      "Iteration 804, loss = 0.00150593\n",
      "Iteration 805, loss = 0.00150260\n",
      "Iteration 806, loss = 0.00149928\n",
      "Iteration 807, loss = 0.00149597\n",
      "Iteration 808, loss = 0.00149268\n",
      "Iteration 809, loss = 0.00148939\n",
      "Iteration 810, loss = 0.00148613\n",
      "Iteration 811, loss = 0.00148287\n",
      "Iteration 812, loss = 0.00147963\n",
      "Iteration 813, loss = 0.00147640\n",
      "Iteration 814, loss = 0.00147318\n",
      "Iteration 815, loss = 0.00146997\n",
      "Iteration 816, loss = 0.00146678\n",
      "Iteration 817, loss = 0.00146360\n",
      "Iteration 818, loss = 0.00146043\n",
      "Iteration 819, loss = 0.00145727\n",
      "Iteration 820, loss = 0.00145412\n",
      "Iteration 821, loss = 0.00145099\n",
      "Iteration 822, loss = 0.00144787\n",
      "Iteration 823, loss = 0.00144476\n",
      "Iteration 824, loss = 0.00144166\n",
      "Iteration 825, loss = 0.00143858\n",
      "Iteration 826, loss = 0.00143550\n",
      "Iteration 827, loss = 0.00143244\n",
      "Iteration 828, loss = 0.00142939\n",
      "Iteration 829, loss = 0.00142635\n",
      "Iteration 830, loss = 0.00142333\n",
      "Iteration 831, loss = 0.00142031\n",
      "Iteration 832, loss = 0.00141730\n",
      "Iteration 833, loss = 0.00141431\n",
      "Iteration 834, loss = 0.00141133\n",
      "Iteration 835, loss = 0.00140836\n",
      "Iteration 836, loss = 0.00140540\n",
      "Iteration 837, loss = 0.00140245\n",
      "Iteration 838, loss = 0.00139951\n",
      "Iteration 839, loss = 0.00139658\n",
      "Iteration 840, loss = 0.00139367\n",
      "Iteration 841, loss = 0.00139076\n",
      "Iteration 842, loss = 0.00138787\n",
      "Iteration 843, loss = 0.00138498\n",
      "Iteration 844, loss = 0.00138211\n",
      "Iteration 845, loss = 0.00137925\n",
      "Iteration 846, loss = 0.00137640\n",
      "Iteration 847, loss = 0.00137355\n",
      "Iteration 848, loss = 0.00137072\n",
      "Iteration 849, loss = 0.00136790\n",
      "Iteration 850, loss = 0.00136509\n",
      "Iteration 851, loss = 0.00136229\n",
      "Iteration 852, loss = 0.00135950\n",
      "Iteration 853, loss = 0.00135672\n",
      "Iteration 854, loss = 0.00135395\n",
      "Iteration 855, loss = 0.00135119\n",
      "Iteration 856, loss = 0.00134845\n",
      "Iteration 857, loss = 0.00134571\n",
      "Iteration 858, loss = 0.00134298\n",
      "Iteration 859, loss = 0.00134026\n",
      "Iteration 860, loss = 0.00133755\n",
      "Iteration 861, loss = 0.00133485\n",
      "Iteration 862, loss = 0.00133216\n",
      "Iteration 863, loss = 0.00132948\n",
      "Iteration 864, loss = 0.00132681\n",
      "Iteration 865, loss = 0.00132415\n",
      "Iteration 866, loss = 0.00132149\n",
      "Iteration 867, loss = 0.00131885\n",
      "Iteration 868, loss = 0.00131622\n",
      "Iteration 869, loss = 0.00131360\n",
      "Iteration 870, loss = 0.00131098\n",
      "Iteration 871, loss = 0.00130838\n",
      "Iteration 872, loss = 0.00130578\n",
      "Iteration 873, loss = 0.00130320\n",
      "Iteration 874, loss = 0.00130062\n",
      "Iteration 875, loss = 0.00129805\n",
      "Iteration 876, loss = 0.00129549\n",
      "Iteration 877, loss = 0.00129294\n",
      "Iteration 878, loss = 0.00129040\n",
      "Iteration 879, loss = 0.00128787\n",
      "Iteration 880, loss = 0.00128535\n",
      "Iteration 881, loss = 0.00128284\n",
      "Iteration 882, loss = 0.00128033\n",
      "Iteration 883, loss = 0.00127784\n",
      "Iteration 884, loss = 0.00127535\n",
      "Iteration 885, loss = 0.00127287\n",
      "Iteration 886, loss = 0.00127040\n",
      "Iteration 887, loss = 0.00126794\n",
      "Iteration 888, loss = 0.00126549\n",
      "Iteration 889, loss = 0.00126304\n",
      "Iteration 890, loss = 0.00126061\n",
      "Iteration 891, loss = 0.00125818\n",
      "Iteration 892, loss = 0.00125576\n",
      "Iteration 893, loss = 0.00125335\n",
      "Iteration 894, loss = 0.00125095\n",
      "Iteration 895, loss = 0.00124855\n",
      "Iteration 896, loss = 0.00124617\n",
      "Iteration 897, loss = 0.00124379\n",
      "Iteration 898, loss = 0.00124142\n",
      "Iteration 899, loss = 0.00123906\n",
      "Iteration 900, loss = 0.00123671\n",
      "Iteration 901, loss = 0.00123436\n",
      "Iteration 902, loss = 0.00123203\n",
      "Iteration 903, loss = 0.00122970\n",
      "Iteration 904, loss = 0.00122738\n",
      "Iteration 905, loss = 0.00122506\n",
      "Iteration 906, loss = 0.00122276\n",
      "Iteration 907, loss = 0.00122046\n",
      "Iteration 908, loss = 0.00121817\n",
      "Iteration 909, loss = 0.00121589\n",
      "Iteration 910, loss = 0.00121362\n",
      "Iteration 911, loss = 0.00121135\n",
      "Iteration 912, loss = 0.00120909\n",
      "Iteration 913, loss = 0.00120684\n",
      "Iteration 914, loss = 0.00120460\n",
      "Iteration 915, loss = 0.00120236\n",
      "Iteration 916, loss = 0.00120013\n",
      "Iteration 917, loss = 0.00119791\n",
      "Iteration 918, loss = 0.00119570\n",
      "Iteration 919, loss = 0.00119350\n",
      "Iteration 920, loss = 0.00119130\n",
      "Iteration 921, loss = 0.00118911\n",
      "Iteration 922, loss = 0.00118692\n",
      "Iteration 923, loss = 0.00118475\n",
      "Iteration 924, loss = 0.00118258\n",
      "Iteration 925, loss = 0.00118041\n",
      "Iteration 926, loss = 0.00117826\n",
      "Iteration 927, loss = 0.00117611\n",
      "Iteration 928, loss = 0.00117397\n",
      "Iteration 929, loss = 0.00117184\n",
      "Iteration 930, loss = 0.00116971\n",
      "Iteration 931, loss = 0.00116759\n",
      "Iteration 932, loss = 0.00116548\n",
      "Iteration 933, loss = 0.00116337\n",
      "Iteration 934, loss = 0.00116128\n",
      "Iteration 935, loss = 0.00115918\n",
      "Iteration 936, loss = 0.00115710\n",
      "Iteration 937, loss = 0.00115502\n",
      "Iteration 938, loss = 0.00115295\n",
      "Iteration 939, loss = 0.00115089\n",
      "Iteration 940, loss = 0.00114883\n",
      "Iteration 941, loss = 0.00114678\n",
      "Iteration 942, loss = 0.00114473\n",
      "Iteration 943, loss = 0.00114270\n",
      "Iteration 944, loss = 0.00114067\n",
      "Iteration 945, loss = 0.00113864\n",
      "Iteration 946, loss = 0.00113662\n",
      "Iteration 947, loss = 0.00113461\n",
      "Iteration 948, loss = 0.00113261\n",
      "Iteration 949, loss = 0.00113061\n",
      "Iteration 950, loss = 0.00112862\n",
      "Iteration 951, loss = 0.00112663\n",
      "Iteration 952, loss = 0.00112465\n",
      "Iteration 953, loss = 0.00112268\n",
      "Iteration 954, loss = 0.00112072\n",
      "Iteration 955, loss = 0.00111876\n",
      "Iteration 956, loss = 0.00111680\n",
      "Iteration 957, loss = 0.00111485\n",
      "Iteration 958, loss = 0.00111291\n",
      "Iteration 959, loss = 0.00111098\n",
      "Iteration 960, loss = 0.00110905\n",
      "Iteration 961, loss = 0.00110713\n",
      "Iteration 962, loss = 0.00110521\n",
      "Iteration 963, loss = 0.00110330\n",
      "Iteration 964, loss = 0.00110140\n",
      "Iteration 965, loss = 0.00109950\n",
      "Iteration 966, loss = 0.00109761\n",
      "Iteration 967, loss = 0.00109572\n",
      "Iteration 968, loss = 0.00109384\n",
      "Iteration 969, loss = 0.00109196\n",
      "Iteration 970, loss = 0.00109010\n",
      "Iteration 971, loss = 0.00108823\n",
      "Iteration 972, loss = 0.00108638\n",
      "Iteration 973, loss = 0.00108453\n",
      "Iteration 974, loss = 0.00108268\n",
      "Iteration 975, loss = 0.00108084\n",
      "Iteration 976, loss = 0.00107901\n",
      "Iteration 977, loss = 0.00107718\n",
      "Iteration 978, loss = 0.00107536\n",
      "Iteration 979, loss = 0.00107354\n",
      "Iteration 980, loss = 0.00107173\n",
      "Iteration 981, loss = 0.00106993\n",
      "Iteration 982, loss = 0.00106813\n",
      "Iteration 983, loss = 0.00106633\n",
      "Iteration 984, loss = 0.00106454\n",
      "Iteration 985, loss = 0.00106276\n",
      "Iteration 986, loss = 0.00106098\n",
      "Iteration 987, loss = 0.00105921\n",
      "Iteration 988, loss = 0.00105745\n",
      "Iteration 989, loss = 0.00105568\n",
      "Iteration 990, loss = 0.00105393\n",
      "Iteration 991, loss = 0.00105218\n",
      "Iteration 992, loss = 0.00105043\n",
      "Iteration 993, loss = 0.00104870\n",
      "Iteration 994, loss = 0.00104696\n",
      "Iteration 995, loss = 0.00104523\n",
      "Iteration 996, loss = 0.00104351\n",
      "Iteration 997, loss = 0.00104179\n",
      "Iteration 998, loss = 0.00104008\n",
      "Iteration 999, loss = 0.00103837\n",
      "Iteration 1000, loss = 0.00103667\n",
      "Iteration 1001, loss = 0.00103497\n",
      "Iteration 1002, loss = 0.00103328\n",
      "Iteration 1003, loss = 0.00103159\n",
      "Iteration 1004, loss = 0.00102991\n",
      "Iteration 1005, loss = 0.00102823\n",
      "Iteration 1006, loss = 0.00102656\n",
      "Iteration 1007, loss = 0.00102490\n",
      "Iteration 1008, loss = 0.00102323\n",
      "Iteration 1009, loss = 0.00102158\n",
      "Iteration 1010, loss = 0.00101993\n",
      "Iteration 1011, loss = 0.00101828\n",
      "Iteration 1012, loss = 0.00101664\n",
      "Iteration 1013, loss = 0.00101500\n",
      "Iteration 1014, loss = 0.00101337\n",
      "Iteration 1015, loss = 0.00101174\n",
      "Iteration 1016, loss = 0.00101012\n",
      "Iteration 1017, loss = 0.00100850\n",
      "Iteration 1018, loss = 0.00100689\n",
      "Iteration 1019, loss = 0.00100528\n",
      "Iteration 1020, loss = 0.00100368\n",
      "Iteration 1021, loss = 0.00100208\n",
      "Iteration 1022, loss = 0.00100049\n",
      "Iteration 1023, loss = 0.00099890\n",
      "Iteration 1024, loss = 0.00099732\n",
      "Iteration 1025, loss = 0.00099574\n",
      "Iteration 1026, loss = 0.00099416\n",
      "Iteration 1027, loss = 0.00099259\n",
      "Iteration 1028, loss = 0.00099103\n",
      "Iteration 1029, loss = 0.00098947\n",
      "Iteration 1030, loss = 0.00098791\n",
      "Iteration 1031, loss = 0.00098636\n",
      "Iteration 1032, loss = 0.00098481\n",
      "Iteration 1033, loss = 0.00098327\n",
      "Iteration 1034, loss = 0.00098173\n",
      "Iteration 1035, loss = 0.00098020\n",
      "Iteration 1036, loss = 0.00097867\n",
      "Iteration 1037, loss = 0.00097715\n",
      "Iteration 1038, loss = 0.00097563\n",
      "Iteration 1039, loss = 0.00097411\n",
      "Iteration 1040, loss = 0.00097260\n",
      "Iteration 1041, loss = 0.00097109\n",
      "Iteration 1042, loss = 0.00096959\n",
      "Iteration 1043, loss = 0.00096809\n",
      "Iteration 1044, loss = 0.00096660\n",
      "Iteration 1045, loss = 0.00096511\n",
      "Iteration 1046, loss = 0.00096363\n",
      "Iteration 1047, loss = 0.00096215\n",
      "Iteration 1048, loss = 0.00096067\n",
      "Iteration 1049, loss = 0.00095920\n",
      "Iteration 1050, loss = 0.00095773\n",
      "Iteration 1051, loss = 0.00095627\n",
      "Iteration 1052, loss = 0.00095481\n",
      "Iteration 1053, loss = 0.00095335\n",
      "Iteration 1054, loss = 0.00095190\n",
      "Iteration 1055, loss = 0.00095045\n",
      "Iteration 1056, loss = 0.00094901\n",
      "Iteration 1057, loss = 0.00094757\n",
      "Iteration 1058, loss = 0.00094614\n",
      "Iteration 1059, loss = 0.00094471\n",
      "Iteration 1060, loss = 0.00094328\n",
      "Iteration 1061, loss = 0.00094186\n",
      "Iteration 1062, loss = 0.00094044\n",
      "Iteration 1063, loss = 0.00093903\n",
      "Iteration 1064, loss = 0.00093762\n",
      "Iteration 1065, loss = 0.00093621\n",
      "Iteration 1066, loss = 0.00093481\n",
      "Iteration 1067, loss = 0.00093341\n",
      "Iteration 1068, loss = 0.00093202\n",
      "Iteration 1069, loss = 0.00093063\n",
      "Iteration 1070, loss = 0.00092924\n",
      "Iteration 1071, loss = 0.00092786\n",
      "Iteration 1072, loss = 0.00092648\n",
      "Iteration 1073, loss = 0.00092511\n",
      "Iteration 1074, loss = 0.00092374\n",
      "Iteration 1075, loss = 0.00092237\n",
      "Iteration 1076, loss = 0.00092101\n",
      "Iteration 1077, loss = 0.00091965\n",
      "Iteration 1078, loss = 0.00091829\n",
      "Iteration 1079, loss = 0.00091694\n",
      "Iteration 1080, loss = 0.00091559\n",
      "Iteration 1081, loss = 0.00091425\n",
      "Iteration 1082, loss = 0.00091291\n",
      "Iteration 1083, loss = 0.00091157\n",
      "Iteration 1084, loss = 0.00091024\n",
      "Iteration 1085, loss = 0.00090891\n",
      "Iteration 1086, loss = 0.00090759\n",
      "Iteration 1087, loss = 0.00090626\n",
      "Iteration 1088, loss = 0.00090495\n",
      "Iteration 1089, loss = 0.00090363\n",
      "Iteration 1090, loss = 0.00090232\n",
      "Iteration 1091, loss = 0.00090101\n",
      "Iteration 1092, loss = 0.00089971\n",
      "Iteration 1093, loss = 0.00089841\n",
      "Iteration 1094, loss = 0.00089711\n",
      "Iteration 1095, loss = 0.00089582\n",
      "Iteration 1096, loss = 0.00089453\n",
      "Iteration 1097, loss = 0.00089325\n",
      "Iteration 1098, loss = 0.00089196\n",
      "Iteration 1099, loss = 0.00089069\n",
      "Iteration 1100, loss = 0.00088941\n",
      "Iteration 1101, loss = 0.00088814\n",
      "Iteration 1102, loss = 0.00088687\n",
      "Iteration 1103, loss = 0.00088561\n",
      "Iteration 1104, loss = 0.00088435\n",
      "Iteration 1105, loss = 0.00088309\n",
      "Iteration 1106, loss = 0.00088183\n",
      "Iteration 1107, loss = 0.00088058\n",
      "Iteration 1108, loss = 0.00087934\n",
      "Iteration 1109, loss = 0.00087809\n",
      "Iteration 1110, loss = 0.00087685\n",
      "Iteration 1111, loss = 0.00087561\n",
      "Iteration 1112, loss = 0.00087438\n",
      "Iteration 1113, loss = 0.00087315\n",
      "Iteration 1114, loss = 0.00087192\n",
      "Iteration 1115, loss = 0.00087070\n",
      "Iteration 1116, loss = 0.00086948\n",
      "Iteration 1117, loss = 0.00086826\n",
      "Iteration 1118, loss = 0.00086704\n",
      "Iteration 1119, loss = 0.00086583\n",
      "Iteration 1120, loss = 0.00086463\n",
      "Iteration 1121, loss = 0.00086342\n",
      "Iteration 1122, loss = 0.00086222\n",
      "Iteration 1123, loss = 0.00086102\n",
      "Iteration 1124, loss = 0.00085983\n",
      "Iteration 1125, loss = 0.00085864\n",
      "Iteration 1126, loss = 0.00085745\n",
      "Iteration 1127, loss = 0.00085626\n",
      "Iteration 1128, loss = 0.00085508\n",
      "Iteration 1129, loss = 0.00085390\n",
      "Iteration 1130, loss = 0.00085273\n",
      "Iteration 1131, loss = 0.00085155\n",
      "Iteration 1132, loss = 0.00085038\n",
      "Iteration 1133, loss = 0.00084922\n",
      "Iteration 1134, loss = 0.00084805\n",
      "Iteration 1135, loss = 0.00084689\n",
      "Iteration 1136, loss = 0.00084574\n",
      "Iteration 1137, loss = 0.00084458\n",
      "Iteration 1138, loss = 0.00084343\n",
      "Iteration 1139, loss = 0.00084228\n",
      "Iteration 1140, loss = 0.00084114\n",
      "Iteration 1141, loss = 0.00084000\n",
      "Iteration 1142, loss = 0.00083886\n",
      "Iteration 1143, loss = 0.00083772\n",
      "Iteration 1144, loss = 0.00083659\n",
      "Iteration 1145, loss = 0.00083546\n",
      "Iteration 1146, loss = 0.00083433\n",
      "Iteration 1147, loss = 0.00083321\n",
      "Iteration 1148, loss = 0.00083209\n",
      "Iteration 1149, loss = 0.00083097\n",
      "Iteration 1150, loss = 0.00082985\n",
      "Iteration 1151, loss = 0.00082874\n",
      "Iteration 1152, loss = 0.00082763\n",
      "Iteration 1153, loss = 0.00082652\n",
      "Iteration 1154, loss = 0.00082542\n",
      "Iteration 1155, loss = 0.00082432\n",
      "Iteration 1156, loss = 0.00082322\n",
      "Iteration 1157, loss = 0.00082213\n",
      "Iteration 1158, loss = 0.00082103\n",
      "Iteration 1159, loss = 0.00081994\n",
      "Iteration 1160, loss = 0.00081886\n",
      "Iteration 1161, loss = 0.00081777\n",
      "Iteration 1162, loss = 0.00081669\n",
      "Iteration 1163, loss = 0.00081561\n",
      "Iteration 1164, loss = 0.00081454\n",
      "Iteration 1165, loss = 0.00081347\n",
      "Iteration 1166, loss = 0.00081240\n",
      "Iteration 1167, loss = 0.00081133\n",
      "Iteration 1168, loss = 0.00081026\n",
      "Iteration 1169, loss = 0.00080920\n",
      "Iteration 1170, loss = 0.00080814\n",
      "Iteration 1171, loss = 0.00080709\n",
      "Iteration 1172, loss = 0.00080603\n",
      "Iteration 1173, loss = 0.00080498\n",
      "Iteration 1174, loss = 0.00080393\n",
      "Iteration 1175, loss = 0.00080289\n",
      "Iteration 1176, loss = 0.00080185\n",
      "Iteration 1177, loss = 0.00080081\n",
      "Iteration 1178, loss = 0.00079977\n",
      "Iteration 1179, loss = 0.00079873\n",
      "Iteration 1180, loss = 0.00079770\n",
      "Iteration 1181, loss = 0.00079667\n",
      "Iteration 1182, loss = 0.00079564\n",
      "Iteration 1183, loss = 0.00079462\n",
      "Iteration 1184, loss = 0.00079360\n",
      "Iteration 1185, loss = 0.00079258\n",
      "Iteration 1186, loss = 0.00079156\n",
      "Iteration 1187, loss = 0.00079055\n",
      "Iteration 1188, loss = 0.00078954\n",
      "Iteration 1189, loss = 0.00078853\n",
      "Iteration 1190, loss = 0.00078752\n",
      "Iteration 1191, loss = 0.00078652\n",
      "Iteration 1192, loss = 0.00078551\n",
      "Iteration 1193, loss = 0.00078452\n",
      "Iteration 1194, loss = 0.00078352\n",
      "Iteration 1195, loss = 0.00078253\n",
      "Iteration 1196, loss = 0.00078153\n",
      "Iteration 1197, loss = 0.00078054\n",
      "Iteration 1198, loss = 0.00077956\n",
      "Iteration 1199, loss = 0.00077857\n",
      "Iteration 1200, loss = 0.00077759\n",
      "Iteration 1201, loss = 0.00077661\n",
      "Iteration 1202, loss = 0.00077564\n",
      "Iteration 1203, loss = 0.00077466\n",
      "Training loss did not improve more than tol=0.000001 for 10 consecutive epochs. Stopping.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00        14\n",
      "           2       0.95      1.00      0.98        20\n",
      "           3       1.00      0.95      0.97        20\n",
      "\n",
      "    accuracy                           0.98        54\n",
      "   macro avg       0.98      0.98      0.98        54\n",
      "weighted avg       0.98      0.98      0.98        54\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "hidden_layers = (60,60,60)\n",
    "activation = 'logistic'\n",
    "opts = dict(hidden_layer_sizes = hidden_layers,verbose = True,activation = activation,tol = 1e-6,max_iter = int(1e6))\n",
    "solver = 'adam'\n",
    "clf_MLP = MLPClassifier(solver = solver, **opts)\n",
    "clf_MLP.fit(X_train_,y_train)\n",
    "predictions = clf_MLP.predict(X_test_)\n",
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- hidden_layers = (512, 512, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.22132922\n",
      "Iteration 2, loss = 1.09902323\n",
      "Iteration 3, loss = 1.20627408\n",
      "Iteration 4, loss = 1.14754799\n",
      "Iteration 5, loss = 1.09991528\n",
      "Iteration 6, loss = 1.08204642\n",
      "Iteration 7, loss = 1.08576131\n",
      "Iteration 8, loss = 1.10617276\n",
      "Iteration 9, loss = 1.11062644\n",
      "Iteration 10, loss = 1.09218677\n",
      "Iteration 11, loss = 1.07107543\n",
      "Iteration 12, loss = 1.06267834\n",
      "Iteration 13, loss = 1.06733741\n",
      "Iteration 14, loss = 1.07620278\n",
      "Iteration 15, loss = 1.08054425\n",
      "Iteration 16, loss = 1.07745992\n",
      "Iteration 17, loss = 1.06942146\n",
      "Iteration 18, loss = 1.06078627\n",
      "Iteration 19, loss = 1.05494924\n",
      "Iteration 20, loss = 1.05299842\n",
      "Iteration 21, loss = 1.05361053\n",
      "Iteration 22, loss = 1.05411373\n",
      "Iteration 23, loss = 1.05224739\n",
      "Iteration 24, loss = 1.04736614\n",
      "Iteration 25, loss = 1.04029506\n",
      "Iteration 26, loss = 1.03240271\n",
      "Iteration 27, loss = 1.02482320\n",
      "Iteration 28, loss = 1.01794448\n",
      "Iteration 29, loss = 1.01096275\n",
      "Iteration 30, loss = 1.00202908\n",
      "Iteration 31, loss = 0.98948517\n",
      "Iteration 32, loss = 0.97312155\n",
      "Iteration 33, loss = 0.95390558\n",
      "Iteration 34, loss = 0.93263063\n",
      "Iteration 35, loss = 0.90928862\n",
      "Iteration 36, loss = 0.88340975\n",
      "Iteration 37, loss = 0.85424758\n",
      "Iteration 38, loss = 0.82109854\n",
      "Iteration 39, loss = 0.78444433\n",
      "Iteration 40, loss = 0.74622314\n",
      "Iteration 41, loss = 0.70816309\n",
      "Iteration 42, loss = 0.67081184\n",
      "Iteration 43, loss = 0.63449567\n",
      "Iteration 44, loss = 0.59984130\n",
      "Iteration 45, loss = 0.56777554\n",
      "Iteration 46, loss = 0.53938218\n",
      "Iteration 47, loss = 0.51468811\n",
      "Iteration 48, loss = 0.49223872\n",
      "Iteration 49, loss = 0.47079670\n",
      "Iteration 50, loss = 0.45055567\n",
      "Iteration 51, loss = 0.43203833\n",
      "Iteration 52, loss = 0.41467353\n",
      "Iteration 53, loss = 0.39736505\n",
      "Iteration 54, loss = 0.37993230\n",
      "Iteration 55, loss = 0.36297179\n",
      "Iteration 56, loss = 0.34642983\n",
      "Iteration 57, loss = 0.32950040\n",
      "Iteration 58, loss = 0.31212251\n",
      "Iteration 59, loss = 0.29499780\n",
      "Iteration 60, loss = 0.27809512\n",
      "Iteration 61, loss = 0.26087727\n",
      "Iteration 62, loss = 0.24362889\n",
      "Iteration 63, loss = 0.22693260\n",
      "Iteration 64, loss = 0.21062815\n",
      "Iteration 65, loss = 0.19455416\n",
      "Iteration 66, loss = 0.17916836\n",
      "Iteration 67, loss = 0.16470772\n",
      "Iteration 68, loss = 0.15094455\n",
      "Iteration 69, loss = 0.13793931\n",
      "Iteration 70, loss = 0.12596080\n",
      "Iteration 71, loss = 0.11492491\n",
      "Iteration 72, loss = 0.10467279\n",
      "Iteration 73, loss = 0.09531381\n",
      "Iteration 74, loss = 0.08689719\n",
      "Iteration 75, loss = 0.07925602\n",
      "Iteration 76, loss = 0.07230812\n",
      "Iteration 77, loss = 0.06609674\n",
      "Iteration 78, loss = 0.06057247\n",
      "Iteration 79, loss = 0.05561205\n",
      "Iteration 80, loss = 0.05116075\n",
      "Iteration 81, loss = 0.04720640\n",
      "Iteration 82, loss = 0.04369485\n",
      "Iteration 83, loss = 0.04055080\n",
      "Iteration 84, loss = 0.03773012\n",
      "Iteration 85, loss = 0.03520953\n",
      "Iteration 86, loss = 0.03295534\n",
      "Iteration 87, loss = 0.03092648\n",
      "Iteration 88, loss = 0.02909268\n",
      "Iteration 89, loss = 0.02743475\n",
      "Iteration 90, loss = 0.02593307\n",
      "Iteration 91, loss = 0.02456496\n",
      "Iteration 92, loss = 0.02331163\n",
      "Iteration 93, loss = 0.02216147\n",
      "Iteration 94, loss = 0.02110555\n",
      "Iteration 95, loss = 0.02013308\n",
      "Iteration 96, loss = 0.01923245\n",
      "Iteration 97, loss = 0.01839499\n",
      "Iteration 98, loss = 0.01761583\n",
      "Iteration 99, loss = 0.01689127\n",
      "Iteration 100, loss = 0.01621630\n",
      "Iteration 101, loss = 0.01558496\n",
      "Iteration 102, loss = 0.01499232\n",
      "Iteration 103, loss = 0.01443550\n",
      "Iteration 104, loss = 0.01391281\n",
      "Iteration 105, loss = 0.01342224\n",
      "Iteration 106, loss = 0.01296105\n",
      "Iteration 107, loss = 0.01252640\n",
      "Iteration 108, loss = 0.01211607\n",
      "Iteration 109, loss = 0.01172861\n",
      "Iteration 110, loss = 0.01136282\n",
      "Iteration 111, loss = 0.01101736\n",
      "Iteration 112, loss = 0.01069070\n",
      "Iteration 113, loss = 0.01038138\n",
      "Iteration 114, loss = 0.01008820\n",
      "Iteration 115, loss = 0.00981013\n",
      "Iteration 116, loss = 0.00954625\n",
      "Iteration 117, loss = 0.00929561\n",
      "Iteration 118, loss = 0.00905728\n",
      "Iteration 119, loss = 0.00883045\n",
      "Iteration 120, loss = 0.00861440\n",
      "Iteration 121, loss = 0.00840846\n",
      "Iteration 122, loss = 0.00821200\n",
      "Iteration 123, loss = 0.00802436\n",
      "Iteration 124, loss = 0.00784494\n",
      "Iteration 125, loss = 0.00767321\n",
      "Iteration 126, loss = 0.00750871\n",
      "Iteration 127, loss = 0.00735105\n",
      "Iteration 128, loss = 0.00719982\n",
      "Iteration 129, loss = 0.00705463\n",
      "Iteration 130, loss = 0.00691509\n",
      "Iteration 131, loss = 0.00678084\n",
      "Iteration 132, loss = 0.00665158\n",
      "Iteration 133, loss = 0.00652703\n",
      "Iteration 134, loss = 0.00640696\n",
      "Iteration 135, loss = 0.00629113\n",
      "Iteration 136, loss = 0.00617930\n",
      "Iteration 137, loss = 0.00607126\n",
      "Iteration 138, loss = 0.00596678\n",
      "Iteration 139, loss = 0.00586570\n",
      "Iteration 140, loss = 0.00576784\n",
      "Iteration 141, loss = 0.00567305\n",
      "Iteration 142, loss = 0.00558118\n",
      "Iteration 143, loss = 0.00549209\n",
      "Iteration 144, loss = 0.00540565\n",
      "Iteration 145, loss = 0.00532174\n",
      "Iteration 146, loss = 0.00524024\n",
      "Iteration 147, loss = 0.00516105\n",
      "Iteration 148, loss = 0.00508407\n",
      "Iteration 149, loss = 0.00500920\n",
      "Iteration 150, loss = 0.00493636\n",
      "Iteration 151, loss = 0.00486545\n",
      "Iteration 152, loss = 0.00479641\n",
      "Iteration 153, loss = 0.00472915\n",
      "Iteration 154, loss = 0.00466361\n",
      "Iteration 155, loss = 0.00459973\n",
      "Iteration 156, loss = 0.00453744\n",
      "Iteration 157, loss = 0.00447668\n",
      "Iteration 158, loss = 0.00441740\n",
      "Iteration 159, loss = 0.00435953\n",
      "Iteration 160, loss = 0.00430304\n",
      "Iteration 161, loss = 0.00424787\n",
      "Iteration 162, loss = 0.00419399\n",
      "Iteration 163, loss = 0.00414133\n",
      "Iteration 164, loss = 0.00408987\n",
      "Iteration 165, loss = 0.00403957\n",
      "Iteration 166, loss = 0.00399038\n",
      "Iteration 167, loss = 0.00394227\n",
      "Iteration 168, loss = 0.00389521\n",
      "Iteration 169, loss = 0.00384916\n",
      "Iteration 170, loss = 0.00380410\n",
      "Iteration 171, loss = 0.00375998\n",
      "Iteration 172, loss = 0.00371679\n",
      "Iteration 173, loss = 0.00367450\n",
      "Iteration 174, loss = 0.00363308\n",
      "Iteration 175, loss = 0.00359250\n",
      "Iteration 176, loss = 0.00355274\n",
      "Iteration 177, loss = 0.00351378\n",
      "Iteration 178, loss = 0.00347558\n",
      "Iteration 179, loss = 0.00343815\n",
      "Iteration 180, loss = 0.00340144\n",
      "Iteration 181, loss = 0.00336544\n",
      "Iteration 182, loss = 0.00333014\n",
      "Iteration 183, loss = 0.00329551\n",
      "Iteration 184, loss = 0.00326153\n",
      "Iteration 185, loss = 0.00322819\n",
      "Iteration 186, loss = 0.00319547\n",
      "Iteration 187, loss = 0.00316336\n",
      "Iteration 188, loss = 0.00313183\n",
      "Iteration 189, loss = 0.00310088\n",
      "Iteration 190, loss = 0.00307049\n",
      "Iteration 191, loss = 0.00304065\n",
      "Iteration 192, loss = 0.00301133\n",
      "Iteration 193, loss = 0.00298254\n",
      "Iteration 194, loss = 0.00295425\n",
      "Iteration 195, loss = 0.00292645\n",
      "Iteration 196, loss = 0.00289914\n",
      "Iteration 197, loss = 0.00287229\n",
      "Iteration 198, loss = 0.00284591\n",
      "Iteration 199, loss = 0.00281997\n",
      "Iteration 200, loss = 0.00279447\n",
      "Iteration 201, loss = 0.00276940\n",
      "Iteration 202, loss = 0.00274474\n",
      "Iteration 203, loss = 0.00272050\n",
      "Iteration 204, loss = 0.00269665\n",
      "Iteration 205, loss = 0.00267319\n",
      "Iteration 206, loss = 0.00265011\n",
      "Iteration 207, loss = 0.00262740\n",
      "Iteration 208, loss = 0.00260506\n",
      "Iteration 209, loss = 0.00258307\n",
      "Iteration 210, loss = 0.00256143\n",
      "Iteration 211, loss = 0.00254014\n",
      "Iteration 212, loss = 0.00251917\n",
      "Iteration 213, loss = 0.00249853\n",
      "Iteration 214, loss = 0.00247821\n",
      "Iteration 215, loss = 0.00245821\n",
      "Iteration 216, loss = 0.00243850\n",
      "Iteration 217, loss = 0.00241910\n",
      "Iteration 218, loss = 0.00239999\n",
      "Iteration 219, loss = 0.00238117\n",
      "Iteration 220, loss = 0.00236263\n",
      "Iteration 221, loss = 0.00234436\n",
      "Iteration 222, loss = 0.00232637\n",
      "Iteration 223, loss = 0.00230863\n",
      "Iteration 224, loss = 0.00229116\n",
      "Iteration 225, loss = 0.00227394\n",
      "Iteration 226, loss = 0.00225697\n",
      "Iteration 227, loss = 0.00224024\n",
      "Iteration 228, loss = 0.00222376\n",
      "Iteration 229, loss = 0.00220750\n",
      "Iteration 230, loss = 0.00219148\n",
      "Iteration 231, loss = 0.00217568\n",
      "Iteration 232, loss = 0.00216010\n",
      "Iteration 233, loss = 0.00214474\n",
      "Iteration 234, loss = 0.00212960\n",
      "Iteration 235, loss = 0.00211466\n",
      "Iteration 236, loss = 0.00209992\n",
      "Iteration 237, loss = 0.00208539\n",
      "Iteration 238, loss = 0.00207105\n",
      "Iteration 239, loss = 0.00205691\n",
      "Iteration 240, loss = 0.00204296\n",
      "Iteration 241, loss = 0.00202919\n",
      "Iteration 242, loss = 0.00201560\n",
      "Iteration 243, loss = 0.00200220\n",
      "Iteration 244, loss = 0.00198897\n",
      "Iteration 245, loss = 0.00197592\n",
      "Iteration 246, loss = 0.00196303\n",
      "Iteration 247, loss = 0.00195032\n",
      "Iteration 248, loss = 0.00193776\n",
      "Iteration 249, loss = 0.00192537\n",
      "Iteration 250, loss = 0.00191314\n",
      "Iteration 251, loss = 0.00190106\n",
      "Iteration 252, loss = 0.00188914\n",
      "Iteration 253, loss = 0.00187736\n",
      "Iteration 254, loss = 0.00186574\n",
      "Iteration 255, loss = 0.00185426\n",
      "Iteration 256, loss = 0.00184292\n",
      "Iteration 257, loss = 0.00183172\n",
      "Iteration 258, loss = 0.00182066\n",
      "Iteration 259, loss = 0.00180974\n",
      "Iteration 260, loss = 0.00179894\n",
      "Iteration 261, loss = 0.00178828\n",
      "Iteration 262, loss = 0.00177775\n",
      "Iteration 263, loss = 0.00176735\n",
      "Iteration 264, loss = 0.00175707\n",
      "Iteration 265, loss = 0.00174691\n",
      "Iteration 266, loss = 0.00173688\n",
      "Iteration 267, loss = 0.00172696\n",
      "Iteration 268, loss = 0.00171716\n",
      "Iteration 269, loss = 0.00170748\n",
      "Iteration 270, loss = 0.00169791\n",
      "Iteration 271, loss = 0.00168845\n",
      "Iteration 272, loss = 0.00167909\n",
      "Iteration 273, loss = 0.00166985\n",
      "Iteration 274, loss = 0.00166071\n",
      "Iteration 275, loss = 0.00165168\n",
      "Iteration 276, loss = 0.00164275\n",
      "Iteration 277, loss = 0.00163392\n",
      "Iteration 278, loss = 0.00162520\n",
      "Iteration 279, loss = 0.00161657\n",
      "Iteration 280, loss = 0.00160803\n",
      "Iteration 281, loss = 0.00159959\n",
      "Iteration 282, loss = 0.00159125\n",
      "Iteration 283, loss = 0.00158300\n",
      "Iteration 284, loss = 0.00157484\n",
      "Iteration 285, loss = 0.00156676\n",
      "Iteration 286, loss = 0.00155878\n",
      "Iteration 287, loss = 0.00155088\n",
      "Iteration 288, loss = 0.00154307\n",
      "Iteration 289, loss = 0.00153535\n",
      "Iteration 290, loss = 0.00152770\n",
      "Iteration 291, loss = 0.00152014\n",
      "Iteration 292, loss = 0.00151266\n",
      "Iteration 293, loss = 0.00150526\n",
      "Iteration 294, loss = 0.00149794\n",
      "Iteration 295, loss = 0.00149069\n",
      "Iteration 296, loss = 0.00148352\n",
      "Iteration 297, loss = 0.00147643\n",
      "Iteration 298, loss = 0.00146941\n",
      "Iteration 299, loss = 0.00146246\n",
      "Iteration 300, loss = 0.00145559\n",
      "Iteration 301, loss = 0.00144878\n",
      "Iteration 302, loss = 0.00144205\n",
      "Iteration 303, loss = 0.00143538\n",
      "Iteration 304, loss = 0.00142879\n",
      "Iteration 305, loss = 0.00142226\n",
      "Iteration 306, loss = 0.00141579\n",
      "Iteration 307, loss = 0.00140939\n",
      "Iteration 308, loss = 0.00140306\n",
      "Iteration 309, loss = 0.00139679\n",
      "Iteration 310, loss = 0.00139058\n",
      "Iteration 311, loss = 0.00138443\n",
      "Iteration 312, loss = 0.00137835\n",
      "Iteration 313, loss = 0.00137232\n",
      "Iteration 314, loss = 0.00136635\n",
      "Iteration 315, loss = 0.00136045\n",
      "Iteration 316, loss = 0.00135459\n",
      "Iteration 317, loss = 0.00134880\n",
      "Iteration 318, loss = 0.00134306\n",
      "Iteration 319, loss = 0.00133738\n",
      "Iteration 320, loss = 0.00133175\n",
      "Iteration 321, loss = 0.00132618\n",
      "Iteration 322, loss = 0.00132066\n",
      "Iteration 323, loss = 0.00131519\n",
      "Iteration 324, loss = 0.00130978\n",
      "Iteration 325, loss = 0.00130441\n",
      "Iteration 326, loss = 0.00129910\n",
      "Iteration 327, loss = 0.00129384\n",
      "Iteration 328, loss = 0.00128862\n",
      "Iteration 329, loss = 0.00128346\n",
      "Iteration 330, loss = 0.00127834\n",
      "Iteration 331, loss = 0.00127327\n",
      "Iteration 332, loss = 0.00126825\n",
      "Iteration 333, loss = 0.00126327\n",
      "Iteration 334, loss = 0.00125834\n",
      "Iteration 335, loss = 0.00125345\n",
      "Iteration 336, loss = 0.00124861\n",
      "Iteration 337, loss = 0.00124381\n",
      "Iteration 338, loss = 0.00123906\n",
      "Iteration 339, loss = 0.00123435\n",
      "Iteration 340, loss = 0.00122968\n",
      "Iteration 341, loss = 0.00122505\n",
      "Iteration 342, loss = 0.00122047\n",
      "Iteration 343, loss = 0.00121592\n",
      "Iteration 344, loss = 0.00121142\n",
      "Iteration 345, loss = 0.00120695\n",
      "Iteration 346, loss = 0.00120253\n",
      "Iteration 347, loss = 0.00119814\n",
      "Iteration 348, loss = 0.00119379\n",
      "Iteration 349, loss = 0.00118948\n",
      "Iteration 350, loss = 0.00118521\n",
      "Iteration 351, loss = 0.00118098\n",
      "Iteration 352, loss = 0.00117678\n",
      "Iteration 353, loss = 0.00117262\n",
      "Iteration 354, loss = 0.00116849\n",
      "Iteration 355, loss = 0.00116440\n",
      "Iteration 356, loss = 0.00116034\n",
      "Iteration 357, loss = 0.00115632\n",
      "Iteration 358, loss = 0.00115234\n",
      "Iteration 359, loss = 0.00114838\n",
      "Iteration 360, loss = 0.00114446\n",
      "Iteration 361, loss = 0.00114058\n",
      "Iteration 362, loss = 0.00113672\n",
      "Iteration 363, loss = 0.00113290\n",
      "Iteration 364, loss = 0.00112911\n",
      "Iteration 365, loss = 0.00112535\n",
      "Iteration 366, loss = 0.00112163\n",
      "Iteration 367, loss = 0.00111793\n",
      "Iteration 368, loss = 0.00111426\n",
      "Iteration 369, loss = 0.00111063\n",
      "Iteration 370, loss = 0.00110702\n",
      "Iteration 371, loss = 0.00110345\n",
      "Iteration 372, loss = 0.00109990\n",
      "Iteration 373, loss = 0.00109638\n",
      "Iteration 374, loss = 0.00109289\n",
      "Iteration 375, loss = 0.00108943\n",
      "Iteration 376, loss = 0.00108600\n",
      "Iteration 377, loss = 0.00108259\n",
      "Iteration 378, loss = 0.00107921\n",
      "Iteration 379, loss = 0.00107586\n",
      "Iteration 380, loss = 0.00107253\n",
      "Iteration 381, loss = 0.00106924\n",
      "Iteration 382, loss = 0.00106596\n",
      "Iteration 383, loss = 0.00106272\n",
      "Iteration 384, loss = 0.00105950\n",
      "Iteration 385, loss = 0.00105630\n",
      "Iteration 386, loss = 0.00105313\n",
      "Iteration 387, loss = 0.00104998\n",
      "Iteration 388, loss = 0.00104686\n",
      "Iteration 389, loss = 0.00104376\n",
      "Iteration 390, loss = 0.00104069\n",
      "Iteration 391, loss = 0.00103764\n",
      "Iteration 392, loss = 0.00103462\n",
      "Iteration 393, loss = 0.00103161\n",
      "Iteration 394, loss = 0.00102863\n",
      "Iteration 395, loss = 0.00102568\n",
      "Iteration 396, loss = 0.00102274\n",
      "Iteration 397, loss = 0.00101983\n",
      "Iteration 398, loss = 0.00101694\n",
      "Iteration 399, loss = 0.00101407\n",
      "Iteration 400, loss = 0.00101122\n",
      "Iteration 401, loss = 0.00100840\n",
      "Iteration 402, loss = 0.00100559\n",
      "Iteration 403, loss = 0.00100281\n",
      "Iteration 404, loss = 0.00100005\n",
      "Iteration 405, loss = 0.00099731\n",
      "Iteration 406, loss = 0.00099459\n",
      "Iteration 407, loss = 0.00099188\n",
      "Iteration 408, loss = 0.00098920\n",
      "Iteration 409, loss = 0.00098654\n",
      "Iteration 410, loss = 0.00098390\n",
      "Iteration 411, loss = 0.00098128\n",
      "Iteration 412, loss = 0.00097867\n",
      "Iteration 413, loss = 0.00097609\n",
      "Iteration 414, loss = 0.00097352\n",
      "Iteration 415, loss = 0.00097097\n",
      "Iteration 416, loss = 0.00096845\n",
      "Iteration 417, loss = 0.00096593\n",
      "Iteration 418, loss = 0.00096344\n",
      "Iteration 419, loss = 0.00096097\n",
      "Iteration 420, loss = 0.00095851\n",
      "Iteration 421, loss = 0.00095607\n",
      "Iteration 422, loss = 0.00095365\n",
      "Iteration 423, loss = 0.00095124\n",
      "Iteration 424, loss = 0.00094885\n",
      "Iteration 425, loss = 0.00094648\n",
      "Iteration 426, loss = 0.00094413\n",
      "Iteration 427, loss = 0.00094179\n",
      "Iteration 428, loss = 0.00093947\n",
      "Iteration 429, loss = 0.00093716\n",
      "Iteration 430, loss = 0.00093487\n",
      "Iteration 431, loss = 0.00093260\n",
      "Iteration 432, loss = 0.00093034\n",
      "Iteration 433, loss = 0.00092810\n",
      "Iteration 434, loss = 0.00092587\n",
      "Iteration 435, loss = 0.00092366\n",
      "Iteration 436, loss = 0.00092147\n",
      "Iteration 437, loss = 0.00091929\n",
      "Iteration 438, loss = 0.00091712\n",
      "Iteration 439, loss = 0.00091497\n",
      "Iteration 440, loss = 0.00091283\n",
      "Iteration 441, loss = 0.00091071\n",
      "Iteration 442, loss = 0.00090860\n",
      "Iteration 443, loss = 0.00090651\n",
      "Iteration 444, loss = 0.00090443\n",
      "Iteration 445, loss = 0.00090236\n",
      "Iteration 446, loss = 0.00090031\n",
      "Iteration 447, loss = 0.00089827\n",
      "Iteration 448, loss = 0.00089625\n",
      "Iteration 449, loss = 0.00089424\n",
      "Iteration 450, loss = 0.00089224\n",
      "Iteration 451, loss = 0.00089026\n",
      "Iteration 452, loss = 0.00088829\n",
      "Iteration 453, loss = 0.00088633\n",
      "Iteration 454, loss = 0.00088438\n",
      "Iteration 455, loss = 0.00088245\n",
      "Iteration 456, loss = 0.00088053\n",
      "Iteration 457, loss = 0.00087862\n",
      "Iteration 458, loss = 0.00087673\n",
      "Iteration 459, loss = 0.00087484\n",
      "Iteration 460, loss = 0.00087297\n",
      "Iteration 461, loss = 0.00087112\n",
      "Iteration 462, loss = 0.00086927\n",
      "Iteration 463, loss = 0.00086744\n",
      "Iteration 464, loss = 0.00086561\n",
      "Iteration 465, loss = 0.00086380\n",
      "Iteration 466, loss = 0.00086200\n",
      "Iteration 467, loss = 0.00086021\n",
      "Iteration 468, loss = 0.00085844\n",
      "Iteration 469, loss = 0.00085667\n",
      "Iteration 470, loss = 0.00085492\n",
      "Iteration 471, loss = 0.00085318\n",
      "Iteration 472, loss = 0.00085144\n",
      "Iteration 473, loss = 0.00084972\n",
      "Iteration 474, loss = 0.00084801\n",
      "Iteration 475, loss = 0.00084631\n",
      "Iteration 476, loss = 0.00084462\n",
      "Iteration 477, loss = 0.00084295\n",
      "Iteration 478, loss = 0.00084128\n",
      "Iteration 479, loss = 0.00083962\n",
      "Iteration 480, loss = 0.00083797\n",
      "Iteration 481, loss = 0.00083634\n",
      "Iteration 482, loss = 0.00083471\n",
      "Iteration 483, loss = 0.00083309\n",
      "Iteration 484, loss = 0.00083149\n",
      "Iteration 485, loss = 0.00082989\n",
      "Iteration 486, loss = 0.00082830\n",
      "Iteration 487, loss = 0.00082672\n",
      "Iteration 488, loss = 0.00082516\n",
      "Iteration 489, loss = 0.00082360\n",
      "Iteration 490, loss = 0.00082205\n",
      "Iteration 491, loss = 0.00082051\n",
      "Iteration 492, loss = 0.00081898\n",
      "Iteration 493, loss = 0.00081746\n",
      "Iteration 494, loss = 0.00081594\n",
      "Iteration 495, loss = 0.00081444\n",
      "Iteration 496, loss = 0.00081295\n",
      "Iteration 497, loss = 0.00081146\n",
      "Iteration 498, loss = 0.00080999\n",
      "Iteration 499, loss = 0.00080852\n",
      "Iteration 500, loss = 0.00080706\n",
      "Iteration 501, loss = 0.00080561\n",
      "Iteration 502, loss = 0.00080417\n",
      "Iteration 503, loss = 0.00080273\n",
      "Iteration 504, loss = 0.00080131\n",
      "Iteration 505, loss = 0.00079989\n",
      "Iteration 506, loss = 0.00079849\n",
      "Iteration 507, loss = 0.00079709\n",
      "Iteration 508, loss = 0.00079569\n",
      "Iteration 509, loss = 0.00079431\n",
      "Iteration 510, loss = 0.00079294\n",
      "Iteration 511, loss = 0.00079157\n",
      "Iteration 512, loss = 0.00079021\n",
      "Iteration 513, loss = 0.00078886\n",
      "Iteration 514, loss = 0.00078751\n",
      "Iteration 515, loss = 0.00078618\n",
      "Iteration 516, loss = 0.00078485\n",
      "Iteration 517, loss = 0.00078353\n",
      "Iteration 518, loss = 0.00078221\n",
      "Iteration 519, loss = 0.00078091\n",
      "Iteration 520, loss = 0.00077961\n",
      "Iteration 521, loss = 0.00077832\n",
      "Iteration 522, loss = 0.00077703\n",
      "Iteration 523, loss = 0.00077576\n",
      "Iteration 524, loss = 0.00077449\n",
      "Iteration 525, loss = 0.00077323\n",
      "Iteration 526, loss = 0.00077197\n",
      "Iteration 527, loss = 0.00077072\n",
      "Iteration 528, loss = 0.00076948\n",
      "Iteration 529, loss = 0.00076825\n",
      "Iteration 530, loss = 0.00076702\n",
      "Iteration 531, loss = 0.00076580\n",
      "Iteration 532, loss = 0.00076459\n",
      "Iteration 533, loss = 0.00076338\n",
      "Iteration 534, loss = 0.00076218\n",
      "Iteration 535, loss = 0.00076099\n",
      "Iteration 536, loss = 0.00075980\n",
      "Iteration 537, loss = 0.00075862\n",
      "Iteration 538, loss = 0.00075744\n",
      "Iteration 539, loss = 0.00075628\n",
      "Iteration 540, loss = 0.00075512\n",
      "Iteration 541, loss = 0.00075396\n",
      "Iteration 542, loss = 0.00075281\n",
      "Iteration 543, loss = 0.00075167\n",
      "Iteration 544, loss = 0.00075053\n",
      "Iteration 545, loss = 0.00074940\n",
      "Iteration 546, loss = 0.00074828\n",
      "Iteration 547, loss = 0.00074716\n",
      "Iteration 548, loss = 0.00074605\n",
      "Iteration 549, loss = 0.00074494\n",
      "Iteration 550, loss = 0.00074385\n",
      "Iteration 551, loss = 0.00074275\n",
      "Iteration 552, loss = 0.00074166\n",
      "Iteration 553, loss = 0.00074058\n",
      "Iteration 554, loss = 0.00073950\n",
      "Iteration 555, loss = 0.00073843\n",
      "Iteration 556, loss = 0.00073737\n",
      "Iteration 557, loss = 0.00073631\n",
      "Iteration 558, loss = 0.00073525\n",
      "Iteration 559, loss = 0.00073421\n",
      "Iteration 560, loss = 0.00073316\n",
      "Iteration 561, loss = 0.00073213\n",
      "Iteration 562, loss = 0.00073109\n",
      "Iteration 563, loss = 0.00073007\n",
      "Iteration 564, loss = 0.00072904\n",
      "Iteration 565, loss = 0.00072803\n",
      "Iteration 566, loss = 0.00072702\n",
      "Iteration 567, loss = 0.00072601\n",
      "Iteration 568, loss = 0.00072501\n",
      "Iteration 569, loss = 0.00072402\n",
      "Iteration 570, loss = 0.00072303\n",
      "Iteration 571, loss = 0.00072204\n",
      "Iteration 572, loss = 0.00072106\n",
      "Iteration 573, loss = 0.00072009\n",
      "Iteration 574, loss = 0.00071912\n",
      "Iteration 575, loss = 0.00071815\n",
      "Iteration 576, loss = 0.00071719\n",
      "Iteration 577, loss = 0.00071624\n",
      "Iteration 578, loss = 0.00071529\n",
      "Iteration 579, loss = 0.00071434\n",
      "Training loss did not improve more than tol=0.000001 for 10 consecutive epochs. Stopping.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00        14\n",
      "           2       0.91      1.00      0.95        20\n",
      "           3       1.00      0.90      0.95        20\n",
      "\n",
      "    accuracy                           0.96        54\n",
      "   macro avg       0.97      0.97      0.97        54\n",
      "weighted avg       0.97      0.96      0.96        54\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "hidden_layers = (512,512,512)\n",
    "activation = 'logistic'\n",
    "opts = dict(hidden_layer_sizes = hidden_layers,verbose = True,activation = activation,tol = 1e-6,max_iter = int(1e6))\n",
    "solver = 'adam'\n",
    "clf_MLP = MLPClassifier(solver = solver, **opts)\n",
    "clf_MLP.fit(X_train_,y_train)\n",
    "predictions = clf_MLP.predict(X_test_)\n",
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "訓練資料的classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00        45\n",
      "           2       1.00      1.00      1.00        51\n",
      "           3       1.00      1.00      1.00        28\n",
      "\n",
      "    accuracy                           1.00       124\n",
      "   macro avg       1.00      1.00      1.00       124\n",
      "weighted avg       1.00      1.00      1.00       124\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred = clf_MLP.predict(X_train_)\n",
    "print(classification_report(y_train,pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 結論:\n",
    "### 1. hidden layers = (30,)，選擇不同的activation 跟 solver ，，因為資料量很小，效果並沒有差太多，準確率皆在90%以上\n",
    "### 2. 當hidden layers = (512,) 跟 hidden layers = (60,60,60) 或 hidden layers = (512,512,512)，差別並沒有太大，只用一層512個神經元就可以達到跟三層60個神經元或是三層512個神經元的效果，所以下面採hidden layers = (512,)\n",
    "### 3. 由2.可以知道，低維度的資料如果擴展到過多的神經元，其實資料無法提供足夠多的細節，也因此造成了許多無用、重複的數據。有時候不是模型大、模型深就是會比較好，要根據不同的狀況做適當的設計才是最好的選擇"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA主成分分析(成分比例採0.4)+神經網路NN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- hidden_layers = (30,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.98988869\n",
      "Iteration 2, loss = 0.98629936\n",
      "Iteration 3, loss = 0.98271912\n",
      "Iteration 4, loss = 0.97914649\n",
      "Iteration 5, loss = 0.97557874\n",
      "Iteration 6, loss = 0.97201442\n",
      "Iteration 7, loss = 0.96845310\n",
      "Iteration 8, loss = 0.96489461\n",
      "Iteration 9, loss = 0.96133883\n",
      "Iteration 10, loss = 0.95778564\n",
      "Iteration 11, loss = 0.95423495\n",
      "Iteration 12, loss = 0.95068665\n",
      "Iteration 13, loss = 0.94714067\n",
      "Iteration 14, loss = 0.94359693\n",
      "Iteration 15, loss = 0.94005534\n",
      "Iteration 16, loss = 0.93651584\n",
      "Iteration 17, loss = 0.93297834\n",
      "Iteration 18, loss = 0.92944275\n",
      "Iteration 19, loss = 0.92590898\n",
      "Iteration 20, loss = 0.92237694\n",
      "Iteration 21, loss = 0.91884651\n",
      "Iteration 22, loss = 0.91531758\n",
      "Iteration 23, loss = 0.91179003\n",
      "Iteration 24, loss = 0.90826373\n",
      "Iteration 25, loss = 0.90473854\n",
      "Iteration 26, loss = 0.90121434\n",
      "Iteration 27, loss = 0.89769099\n",
      "Iteration 28, loss = 0.89416835\n",
      "Iteration 29, loss = 0.89064630\n",
      "Iteration 30, loss = 0.88712472\n",
      "Iteration 31, loss = 0.88360346\n",
      "Iteration 32, loss = 0.88008242\n",
      "Iteration 33, loss = 0.87656149\n",
      "Iteration 34, loss = 0.87304055\n",
      "Iteration 35, loss = 0.86951951\n",
      "Iteration 36, loss = 0.86599828\n",
      "Iteration 37, loss = 0.86247677\n",
      "Iteration 38, loss = 0.85895490\n",
      "Iteration 39, loss = 0.85543261\n",
      "Iteration 40, loss = 0.85190985\n",
      "Iteration 41, loss = 0.84838655\n",
      "Iteration 42, loss = 0.84486269\n",
      "Iteration 43, loss = 0.84133822\n",
      "Iteration 44, loss = 0.83781312\n",
      "Iteration 45, loss = 0.83428739\n",
      "Iteration 46, loss = 0.83076099\n",
      "Iteration 47, loss = 0.82723395\n",
      "Iteration 48, loss = 0.82370626\n",
      "Iteration 49, loss = 0.82017794\n",
      "Iteration 50, loss = 0.81664902\n",
      "Iteration 51, loss = 0.81311951\n",
      "Iteration 52, loss = 0.80958947\n",
      "Iteration 53, loss = 0.80605893\n",
      "Iteration 54, loss = 0.80252795\n",
      "Iteration 55, loss = 0.79899658\n",
      "Iteration 56, loss = 0.79546488\n",
      "Iteration 57, loss = 0.79193293\n",
      "Iteration 58, loss = 0.78840081\n",
      "Iteration 59, loss = 0.78486859\n",
      "Iteration 60, loss = 0.78133637\n",
      "Iteration 61, loss = 0.77780423\n",
      "Iteration 62, loss = 0.77427228\n",
      "Iteration 63, loss = 0.77074062\n",
      "Iteration 64, loss = 0.76720936\n",
      "Iteration 65, loss = 0.76367862\n",
      "Iteration 66, loss = 0.76014851\n",
      "Iteration 67, loss = 0.75661917\n",
      "Iteration 68, loss = 0.75309071\n",
      "Iteration 69, loss = 0.74956328\n",
      "Iteration 70, loss = 0.74603702\n",
      "Iteration 71, loss = 0.74251206\n",
      "Iteration 72, loss = 0.73898857\n",
      "Iteration 73, loss = 0.73546668\n",
      "Iteration 74, loss = 0.73194657\n",
      "Iteration 75, loss = 0.72842838\n",
      "Iteration 76, loss = 0.72491229\n",
      "Iteration 77, loss = 0.72139847\n",
      "Iteration 78, loss = 0.71788708\n",
      "Iteration 79, loss = 0.71437831\n",
      "Iteration 80, loss = 0.71087233\n",
      "Iteration 81, loss = 0.70736933\n",
      "Iteration 82, loss = 0.70386949\n",
      "Iteration 83, loss = 0.70037300\n",
      "Iteration 84, loss = 0.69688006\n",
      "Iteration 85, loss = 0.69339085\n",
      "Iteration 86, loss = 0.68990558\n",
      "Iteration 87, loss = 0.68642445\n",
      "Iteration 88, loss = 0.68294765\n",
      "Iteration 89, loss = 0.67947539\n",
      "Iteration 90, loss = 0.67600787\n",
      "Iteration 91, loss = 0.67254531\n",
      "Iteration 92, loss = 0.66908790\n",
      "Iteration 93, loss = 0.66563586\n",
      "Iteration 94, loss = 0.66218940\n",
      "Iteration 95, loss = 0.65874873\n",
      "Iteration 96, loss = 0.65531406\n",
      "Iteration 97, loss = 0.65188561\n",
      "Iteration 98, loss = 0.64846359\n",
      "Iteration 99, loss = 0.64504822\n",
      "Iteration 100, loss = 0.64163970\n",
      "Iteration 101, loss = 0.63823825\n",
      "Iteration 102, loss = 0.63484409\n",
      "Iteration 103, loss = 0.63145744\n",
      "Iteration 104, loss = 0.62807850\n",
      "Iteration 105, loss = 0.62470749\n",
      "Iteration 106, loss = 0.62134462\n",
      "Iteration 107, loss = 0.61799011\n",
      "Iteration 108, loss = 0.61464416\n",
      "Iteration 109, loss = 0.61130699\n",
      "Iteration 110, loss = 0.60797881\n",
      "Iteration 111, loss = 0.60465982\n",
      "Iteration 112, loss = 0.60135023\n",
      "Iteration 113, loss = 0.59805025\n",
      "Iteration 114, loss = 0.59476008\n",
      "Iteration 115, loss = 0.59147992\n",
      "Iteration 116, loss = 0.58820998\n",
      "Iteration 117, loss = 0.58495044\n",
      "Iteration 118, loss = 0.58170151\n",
      "Iteration 119, loss = 0.57846338\n",
      "Iteration 120, loss = 0.57523624\n",
      "Iteration 121, loss = 0.57202028\n",
      "Iteration 122, loss = 0.56881569\n",
      "Iteration 123, loss = 0.56562264\n",
      "Iteration 124, loss = 0.56244133\n",
      "Iteration 125, loss = 0.55927193\n",
      "Iteration 126, loss = 0.55611462\n",
      "Iteration 127, loss = 0.55296956\n",
      "Iteration 128, loss = 0.54983693\n",
      "Iteration 129, loss = 0.54671690\n",
      "Iteration 130, loss = 0.54360962\n",
      "Iteration 131, loss = 0.54051526\n",
      "Iteration 132, loss = 0.53743397\n",
      "Iteration 133, loss = 0.53436590\n",
      "Iteration 134, loss = 0.53131121\n",
      "Iteration 135, loss = 0.52827005\n",
      "Iteration 136, loss = 0.52524254\n",
      "Iteration 137, loss = 0.52222883\n",
      "Iteration 138, loss = 0.51922906\n",
      "Iteration 139, loss = 0.51624336\n",
      "Iteration 140, loss = 0.51327185\n",
      "Iteration 141, loss = 0.51031465\n",
      "Iteration 142, loss = 0.50737190\n",
      "Iteration 143, loss = 0.50444369\n",
      "Iteration 144, loss = 0.50153015\n",
      "Iteration 145, loss = 0.49863137\n",
      "Iteration 146, loss = 0.49574747\n",
      "Iteration 147, loss = 0.49287855\n",
      "Iteration 148, loss = 0.49002469\n",
      "Iteration 149, loss = 0.48718600\n",
      "Iteration 150, loss = 0.48436255\n",
      "Iteration 151, loss = 0.48155443\n",
      "Iteration 152, loss = 0.47876172\n",
      "Iteration 153, loss = 0.47598450\n",
      "Iteration 154, loss = 0.47322284\n",
      "Iteration 155, loss = 0.47047680\n",
      "Iteration 156, loss = 0.46774645\n",
      "Iteration 157, loss = 0.46503185\n",
      "Iteration 158, loss = 0.46233305\n",
      "Iteration 159, loss = 0.45965011\n",
      "Iteration 160, loss = 0.45698307\n",
      "Iteration 161, loss = 0.45433197\n",
      "Iteration 162, loss = 0.45169687\n",
      "Iteration 163, loss = 0.44907779\n",
      "Iteration 164, loss = 0.44647476\n",
      "Iteration 165, loss = 0.44388782\n",
      "Iteration 166, loss = 0.44131699\n",
      "Iteration 167, loss = 0.43876229\n",
      "Iteration 168, loss = 0.43622374\n",
      "Iteration 169, loss = 0.43370136\n",
      "Iteration 170, loss = 0.43119516\n",
      "Iteration 171, loss = 0.42870513\n",
      "Iteration 172, loss = 0.42623130\n",
      "Iteration 173, loss = 0.42377366\n",
      "Iteration 174, loss = 0.42133220\n",
      "Iteration 175, loss = 0.41890693\n",
      "Iteration 176, loss = 0.41649783\n",
      "Iteration 177, loss = 0.41410489\n",
      "Iteration 178, loss = 0.41172811\n",
      "Iteration 179, loss = 0.40936745\n",
      "Iteration 180, loss = 0.40702290\n",
      "Iteration 181, loss = 0.40469445\n",
      "Iteration 182, loss = 0.40238205\n",
      "Iteration 183, loss = 0.40008569\n",
      "Iteration 184, loss = 0.39780533\n",
      "Iteration 185, loss = 0.39554093\n",
      "Iteration 186, loss = 0.39329248\n",
      "Iteration 187, loss = 0.39105991\n",
      "Iteration 188, loss = 0.38884320\n",
      "Iteration 189, loss = 0.38664230\n",
      "Iteration 190, loss = 0.38445716\n",
      "Iteration 191, loss = 0.38228774\n",
      "Iteration 192, loss = 0.38013398\n",
      "Iteration 193, loss = 0.37799585\n",
      "Iteration 194, loss = 0.37587327\n",
      "Iteration 195, loss = 0.37376620\n",
      "Iteration 196, loss = 0.37167457\n",
      "Iteration 197, loss = 0.36959834\n",
      "Iteration 198, loss = 0.36753743\n",
      "Iteration 199, loss = 0.36549178\n",
      "Iteration 200, loss = 0.36346134\n",
      "Iteration 201, loss = 0.36144603\n",
      "Iteration 202, loss = 0.35944578\n",
      "Iteration 203, loss = 0.35746054\n",
      "Iteration 204, loss = 0.35549021\n",
      "Iteration 205, loss = 0.35353475\n",
      "Iteration 206, loss = 0.35159406\n",
      "Iteration 207, loss = 0.34966808\n",
      "Iteration 208, loss = 0.34775673\n",
      "Iteration 209, loss = 0.34585994\n",
      "Iteration 210, loss = 0.34397762\n",
      "Iteration 211, loss = 0.34210970\n",
      "Iteration 212, loss = 0.34025610\n",
      "Iteration 213, loss = 0.33841674\n",
      "Iteration 214, loss = 0.33659153\n",
      "Iteration 215, loss = 0.33478039\n",
      "Iteration 216, loss = 0.33298324\n",
      "Iteration 217, loss = 0.33120000\n",
      "Iteration 218, loss = 0.32943057\n",
      "Iteration 219, loss = 0.32767488\n",
      "Iteration 220, loss = 0.32593284\n",
      "Iteration 221, loss = 0.32420436\n",
      "Iteration 222, loss = 0.32248935\n",
      "Iteration 223, loss = 0.32078773\n",
      "Iteration 224, loss = 0.31909940\n",
      "Iteration 225, loss = 0.31742429\n",
      "Iteration 226, loss = 0.31576229\n",
      "Iteration 227, loss = 0.31411332\n",
      "Iteration 228, loss = 0.31247729\n",
      "Iteration 229, loss = 0.31085411\n",
      "Iteration 230, loss = 0.30924369\n",
      "Iteration 231, loss = 0.30764594\n",
      "Iteration 232, loss = 0.30606077\n",
      "Iteration 233, loss = 0.30448808\n",
      "Iteration 234, loss = 0.30292778\n",
      "Iteration 235, loss = 0.30137979\n",
      "Iteration 236, loss = 0.29984401\n",
      "Iteration 237, loss = 0.29832036\n",
      "Iteration 238, loss = 0.29680873\n",
      "Iteration 239, loss = 0.29530904\n",
      "Iteration 240, loss = 0.29382120\n",
      "Iteration 241, loss = 0.29234511\n",
      "Iteration 242, loss = 0.29088068\n",
      "Iteration 243, loss = 0.28942783\n",
      "Iteration 244, loss = 0.28798646\n",
      "Iteration 245, loss = 0.28655648\n",
      "Iteration 246, loss = 0.28513780\n",
      "Iteration 247, loss = 0.28373033\n",
      "Iteration 248, loss = 0.28233398\n",
      "Iteration 249, loss = 0.28094866\n",
      "Iteration 250, loss = 0.27957427\n",
      "Iteration 251, loss = 0.27821074\n",
      "Iteration 252, loss = 0.27685797\n",
      "Iteration 253, loss = 0.27551586\n",
      "Iteration 254, loss = 0.27418434\n",
      "Iteration 255, loss = 0.27286332\n",
      "Iteration 256, loss = 0.27155270\n",
      "Iteration 257, loss = 0.27025240\n",
      "Iteration 258, loss = 0.26896233\n",
      "Iteration 259, loss = 0.26768241\n",
      "Iteration 260, loss = 0.26641255\n",
      "Iteration 261, loss = 0.26515265\n",
      "Iteration 262, loss = 0.26390265\n",
      "Iteration 263, loss = 0.26266244\n",
      "Iteration 264, loss = 0.26143196\n",
      "Iteration 265, loss = 0.26021110\n",
      "Iteration 266, loss = 0.25899980\n",
      "Iteration 267, loss = 0.25779796\n",
      "Iteration 268, loss = 0.25660550\n",
      "Iteration 269, loss = 0.25542234\n",
      "Iteration 270, loss = 0.25424840\n",
      "Iteration 271, loss = 0.25308360\n",
      "Iteration 272, loss = 0.25192786\n",
      "Iteration 273, loss = 0.25078109\n",
      "Iteration 274, loss = 0.24964322\n",
      "Iteration 275, loss = 0.24851416\n",
      "Iteration 276, loss = 0.24739385\n",
      "Iteration 277, loss = 0.24628219\n",
      "Iteration 278, loss = 0.24517912\n",
      "Iteration 279, loss = 0.24408455\n",
      "Iteration 280, loss = 0.24299842\n",
      "Iteration 281, loss = 0.24192063\n",
      "Iteration 282, loss = 0.24085113\n",
      "Iteration 283, loss = 0.23978983\n",
      "Iteration 284, loss = 0.23873666\n",
      "Iteration 285, loss = 0.23769154\n",
      "Iteration 286, loss = 0.23665441\n",
      "Iteration 287, loss = 0.23562518\n",
      "Iteration 288, loss = 0.23460380\n",
      "Iteration 289, loss = 0.23359018\n",
      "Iteration 290, loss = 0.23258426\n",
      "Iteration 291, loss = 0.23158596\n",
      "Iteration 292, loss = 0.23059523\n",
      "Iteration 293, loss = 0.22961198\n",
      "Iteration 294, loss = 0.22863614\n",
      "Iteration 295, loss = 0.22766766\n",
      "Iteration 296, loss = 0.22670647\n",
      "Iteration 297, loss = 0.22575249\n",
      "Iteration 298, loss = 0.22480566\n",
      "Iteration 299, loss = 0.22386592\n",
      "Iteration 300, loss = 0.22293320\n",
      "Iteration 301, loss = 0.22200744\n",
      "Iteration 302, loss = 0.22108857\n",
      "Iteration 303, loss = 0.22017652\n",
      "Iteration 304, loss = 0.21927125\n",
      "Iteration 305, loss = 0.21837268\n",
      "Iteration 306, loss = 0.21748076\n",
      "Iteration 307, loss = 0.21659541\n",
      "Iteration 308, loss = 0.21571659\n",
      "Iteration 309, loss = 0.21484424\n",
      "Iteration 310, loss = 0.21397828\n",
      "Iteration 311, loss = 0.21311867\n",
      "Iteration 312, loss = 0.21226535\n",
      "Iteration 313, loss = 0.21141825\n",
      "Iteration 314, loss = 0.21057733\n",
      "Iteration 315, loss = 0.20974252\n",
      "Iteration 316, loss = 0.20891378\n",
      "Iteration 317, loss = 0.20809103\n",
      "Iteration 318, loss = 0.20727424\n",
      "Iteration 319, loss = 0.20646334\n",
      "Iteration 320, loss = 0.20565828\n",
      "Iteration 321, loss = 0.20485901\n",
      "Iteration 322, loss = 0.20406547\n",
      "Iteration 323, loss = 0.20327762\n",
      "Iteration 324, loss = 0.20249540\n",
      "Iteration 325, loss = 0.20171876\n",
      "Iteration 326, loss = 0.20094764\n",
      "Iteration 327, loss = 0.20018201\n",
      "Iteration 328, loss = 0.19942181\n",
      "Iteration 329, loss = 0.19866698\n",
      "Iteration 330, loss = 0.19791749\n",
      "Iteration 331, loss = 0.19717328\n",
      "Iteration 332, loss = 0.19643430\n",
      "Iteration 333, loss = 0.19570051\n",
      "Iteration 334, loss = 0.19497186\n",
      "Iteration 335, loss = 0.19424831\n",
      "Iteration 336, loss = 0.19352981\n",
      "Iteration 337, loss = 0.19281631\n",
      "Iteration 338, loss = 0.19210777\n",
      "Iteration 339, loss = 0.19140414\n",
      "Iteration 340, loss = 0.19070538\n",
      "Iteration 341, loss = 0.19001145\n",
      "Iteration 342, loss = 0.18932230\n",
      "Iteration 343, loss = 0.18863789\n",
      "Iteration 344, loss = 0.18795818\n",
      "Iteration 345, loss = 0.18728312\n",
      "Iteration 346, loss = 0.18661268\n",
      "Iteration 347, loss = 0.18594681\n",
      "Iteration 348, loss = 0.18528548\n",
      "Iteration 349, loss = 0.18462864\n",
      "Iteration 350, loss = 0.18397625\n",
      "Iteration 351, loss = 0.18332827\n",
      "Iteration 352, loss = 0.18268467\n",
      "Iteration 353, loss = 0.18204541\n",
      "Iteration 354, loss = 0.18141044\n",
      "Iteration 355, loss = 0.18077973\n",
      "Iteration 356, loss = 0.18015325\n",
      "Iteration 357, loss = 0.17953095\n",
      "Iteration 358, loss = 0.17891280\n",
      "Iteration 359, loss = 0.17829876\n",
      "Iteration 360, loss = 0.17768879\n",
      "Iteration 361, loss = 0.17708287\n",
      "Iteration 362, loss = 0.17648096\n",
      "Iteration 363, loss = 0.17588301\n",
      "Iteration 364, loss = 0.17528900\n",
      "Iteration 365, loss = 0.17469889\n",
      "Iteration 366, loss = 0.17411265\n",
      "Iteration 367, loss = 0.17353025\n",
      "Iteration 368, loss = 0.17295165\n",
      "Iteration 369, loss = 0.17237681\n",
      "Iteration 370, loss = 0.17180572\n",
      "Iteration 371, loss = 0.17123832\n",
      "Iteration 372, loss = 0.17067460\n",
      "Iteration 373, loss = 0.17011452\n",
      "Iteration 374, loss = 0.16955806\n",
      "Iteration 375, loss = 0.16900517\n",
      "Iteration 376, loss = 0.16845582\n",
      "Iteration 377, loss = 0.16791000\n",
      "Iteration 378, loss = 0.16736767\n",
      "Iteration 379, loss = 0.16682879\n",
      "Iteration 380, loss = 0.16629334\n",
      "Iteration 381, loss = 0.16576130\n",
      "Iteration 382, loss = 0.16523262\n",
      "Iteration 383, loss = 0.16470729\n",
      "Iteration 384, loss = 0.16418528\n",
      "Iteration 385, loss = 0.16366655\n",
      "Iteration 386, loss = 0.16315108\n",
      "Iteration 387, loss = 0.16263885\n",
      "Iteration 388, loss = 0.16212982\n",
      "Iteration 389, loss = 0.16162396\n",
      "Iteration 390, loss = 0.16112126\n",
      "Iteration 391, loss = 0.16062169\n",
      "Iteration 392, loss = 0.16012522\n",
      "Iteration 393, loss = 0.15963182\n",
      "Iteration 394, loss = 0.15914147\n",
      "Iteration 395, loss = 0.15865414\n",
      "Iteration 396, loss = 0.15816981\n",
      "Iteration 397, loss = 0.15768846\n",
      "Iteration 398, loss = 0.15721006\n",
      "Iteration 399, loss = 0.15673458\n",
      "Iteration 400, loss = 0.15626200\n",
      "Iteration 401, loss = 0.15579230\n",
      "Iteration 402, loss = 0.15532545\n",
      "Iteration 403, loss = 0.15486144\n",
      "Iteration 404, loss = 0.15440023\n",
      "Iteration 405, loss = 0.15394181\n",
      "Iteration 406, loss = 0.15348615\n",
      "Iteration 407, loss = 0.15303323\n",
      "Iteration 408, loss = 0.15258303\n",
      "Iteration 409, loss = 0.15213553\n",
      "Iteration 410, loss = 0.15169070\n",
      "Iteration 411, loss = 0.15124852\n",
      "Iteration 412, loss = 0.15080898\n",
      "Iteration 413, loss = 0.15037204\n",
      "Iteration 414, loss = 0.14993770\n",
      "Iteration 415, loss = 0.14950592\n",
      "Iteration 416, loss = 0.14907670\n",
      "Iteration 417, loss = 0.14865000\n",
      "Iteration 418, loss = 0.14822581\n",
      "Iteration 419, loss = 0.14780411\n",
      "Iteration 420, loss = 0.14738488\n",
      "Iteration 421, loss = 0.14696810\n",
      "Iteration 422, loss = 0.14655374\n",
      "Iteration 423, loss = 0.14614180\n",
      "Iteration 424, loss = 0.14573225\n",
      "Iteration 425, loss = 0.14532508\n",
      "Iteration 426, loss = 0.14492026\n",
      "Iteration 427, loss = 0.14451777\n",
      "Iteration 428, loss = 0.14411760\n",
      "Iteration 429, loss = 0.14371974\n",
      "Iteration 430, loss = 0.14332416\n",
      "Iteration 431, loss = 0.14293084\n",
      "Iteration 432, loss = 0.14253977\n",
      "Iteration 433, loss = 0.14215092\n",
      "Iteration 434, loss = 0.14176429\n",
      "Iteration 435, loss = 0.14137986\n",
      "Iteration 436, loss = 0.14099761\n",
      "Iteration 437, loss = 0.14061752\n",
      "Iteration 438, loss = 0.14023957\n",
      "Iteration 439, loss = 0.13986375\n",
      "Iteration 440, loss = 0.13949005\n",
      "Iteration 441, loss = 0.13911844\n",
      "Iteration 442, loss = 0.13874892\n",
      "Iteration 443, loss = 0.13838145\n",
      "Iteration 444, loss = 0.13801604\n",
      "Iteration 445, loss = 0.13765267\n",
      "Iteration 446, loss = 0.13729131\n",
      "Iteration 447, loss = 0.13693196\n",
      "Iteration 448, loss = 0.13657459\n",
      "Iteration 449, loss = 0.13621920\n",
      "Iteration 450, loss = 0.13586577\n",
      "Iteration 451, loss = 0.13551428\n",
      "Iteration 452, loss = 0.13516472\n",
      "Iteration 453, loss = 0.13481708\n",
      "Iteration 454, loss = 0.13447134\n",
      "Iteration 455, loss = 0.13412749\n",
      "Iteration 456, loss = 0.13378551\n",
      "Iteration 457, loss = 0.13344539\n",
      "Iteration 458, loss = 0.13310711\n",
      "Iteration 459, loss = 0.13277067\n",
      "Iteration 460, loss = 0.13243605\n",
      "Iteration 461, loss = 0.13210323\n",
      "Iteration 462, loss = 0.13177220\n",
      "Iteration 463, loss = 0.13144296\n",
      "Iteration 464, loss = 0.13111548\n",
      "Iteration 465, loss = 0.13078975\n",
      "Iteration 466, loss = 0.13046577\n",
      "Iteration 467, loss = 0.13014351\n",
      "Iteration 468, loss = 0.12982297\n",
      "Iteration 469, loss = 0.12950413\n",
      "Iteration 470, loss = 0.12918698\n",
      "Iteration 471, loss = 0.12887151\n",
      "Iteration 472, loss = 0.12855771\n",
      "Iteration 473, loss = 0.12824557\n",
      "Iteration 474, loss = 0.12793506\n",
      "Iteration 475, loss = 0.12762619\n",
      "Iteration 476, loss = 0.12731894\n",
      "Iteration 477, loss = 0.12701329\n",
      "Iteration 478, loss = 0.12670924\n",
      "Iteration 479, loss = 0.12640678\n",
      "Iteration 480, loss = 0.12610589\n",
      "Iteration 481, loss = 0.12580657\n",
      "Iteration 482, loss = 0.12550880\n",
      "Iteration 483, loss = 0.12521256\n",
      "Iteration 484, loss = 0.12491786\n",
      "Iteration 485, loss = 0.12462468\n",
      "Iteration 486, loss = 0.12433301\n",
      "Iteration 487, loss = 0.12404283\n",
      "Iteration 488, loss = 0.12375415\n",
      "Iteration 489, loss = 0.12346694\n",
      "Iteration 490, loss = 0.12318120\n",
      "Iteration 491, loss = 0.12289692\n",
      "Iteration 492, loss = 0.12261408\n",
      "Iteration 493, loss = 0.12233268\n",
      "Iteration 494, loss = 0.12205271\n",
      "Iteration 495, loss = 0.12177416\n",
      "Iteration 496, loss = 0.12149701\n",
      "Iteration 497, loss = 0.12122127\n",
      "Iteration 498, loss = 0.12094691\n",
      "Iteration 499, loss = 0.12067393\n",
      "Iteration 500, loss = 0.12040232\n",
      "Iteration 501, loss = 0.12013207\n",
      "Iteration 502, loss = 0.11986317\n",
      "Iteration 503, loss = 0.11959561\n",
      "Iteration 504, loss = 0.11932939\n",
      "Iteration 505, loss = 0.11906449\n",
      "Iteration 506, loss = 0.11880090\n",
      "Iteration 507, loss = 0.11853863\n",
      "Iteration 508, loss = 0.11827765\n",
      "Iteration 509, loss = 0.11801795\n",
      "Iteration 510, loss = 0.11775954\n",
      "Iteration 511, loss = 0.11750240\n",
      "Iteration 512, loss = 0.11724652\n",
      "Iteration 513, loss = 0.11699190\n",
      "Iteration 514, loss = 0.11673853\n",
      "Iteration 515, loss = 0.11648639\n",
      "Iteration 516, loss = 0.11623548\n",
      "Iteration 517, loss = 0.11598579\n",
      "Iteration 518, loss = 0.11573732\n",
      "Iteration 519, loss = 0.11549005\n",
      "Iteration 520, loss = 0.11524398\n",
      "Iteration 521, loss = 0.11499911\n",
      "Iteration 522, loss = 0.11475541\n",
      "Iteration 523, loss = 0.11451289\n",
      "Iteration 524, loss = 0.11427153\n",
      "Iteration 525, loss = 0.11403133\n",
      "Iteration 526, loss = 0.11379229\n",
      "Iteration 527, loss = 0.11355439\n",
      "Iteration 528, loss = 0.11331762\n",
      "Iteration 529, loss = 0.11308199\n",
      "Iteration 530, loss = 0.11284748\n",
      "Iteration 531, loss = 0.11261408\n",
      "Iteration 532, loss = 0.11238179\n",
      "Iteration 533, loss = 0.11215060\n",
      "Iteration 534, loss = 0.11192050\n",
      "Iteration 535, loss = 0.11169149\n",
      "Iteration 536, loss = 0.11146356\n",
      "Iteration 537, loss = 0.11123671\n",
      "Iteration 538, loss = 0.11101092\n",
      "Iteration 539, loss = 0.11078619\n",
      "Iteration 540, loss = 0.11056251\n",
      "Iteration 541, loss = 0.11033987\n",
      "Iteration 542, loss = 0.11011828\n",
      "Iteration 543, loss = 0.10989772\n",
      "Iteration 544, loss = 0.10967818\n",
      "Iteration 545, loss = 0.10945966\n",
      "Iteration 546, loss = 0.10924216\n",
      "Iteration 547, loss = 0.10902566\n",
      "Iteration 548, loss = 0.10881017\n",
      "Iteration 549, loss = 0.10859567\n",
      "Iteration 550, loss = 0.10838216\n",
      "Iteration 551, loss = 0.10816962\n",
      "Iteration 552, loss = 0.10795807\n",
      "Iteration 553, loss = 0.10774748\n",
      "Iteration 554, loss = 0.10753786\n",
      "Iteration 555, loss = 0.10732920\n",
      "Iteration 556, loss = 0.10712149\n",
      "Iteration 557, loss = 0.10691472\n",
      "Iteration 558, loss = 0.10670890\n",
      "Iteration 559, loss = 0.10650401\n",
      "Iteration 560, loss = 0.10630005\n",
      "Iteration 561, loss = 0.10609701\n",
      "Iteration 562, loss = 0.10589489\n",
      "Iteration 563, loss = 0.10569368\n",
      "Iteration 564, loss = 0.10549337\n",
      "Iteration 565, loss = 0.10529397\n",
      "Iteration 566, loss = 0.10509547\n",
      "Iteration 567, loss = 0.10489785\n",
      "Iteration 568, loss = 0.10470112\n",
      "Iteration 569, loss = 0.10450526\n",
      "Iteration 570, loss = 0.10431029\n",
      "Iteration 571, loss = 0.10411617\n",
      "Iteration 572, loss = 0.10392293\n",
      "Iteration 573, loss = 0.10373054\n",
      "Iteration 574, loss = 0.10353901\n",
      "Iteration 575, loss = 0.10334832\n",
      "Iteration 576, loss = 0.10315847\n",
      "Iteration 577, loss = 0.10296947\n",
      "Iteration 578, loss = 0.10278130\n",
      "Iteration 579, loss = 0.10259395\n",
      "Iteration 580, loss = 0.10240743\n",
      "Iteration 581, loss = 0.10222173\n",
      "Iteration 582, loss = 0.10203684\n",
      "Iteration 583, loss = 0.10185275\n",
      "Iteration 584, loss = 0.10166948\n",
      "Iteration 585, loss = 0.10148700\n",
      "Iteration 586, loss = 0.10130532\n",
      "Iteration 587, loss = 0.10112442\n",
      "Iteration 588, loss = 0.10094431\n",
      "Iteration 589, loss = 0.10076498\n",
      "Iteration 590, loss = 0.10058643\n",
      "Iteration 591, loss = 0.10040865\n",
      "Iteration 592, loss = 0.10023164\n",
      "Iteration 593, loss = 0.10005539\n",
      "Iteration 594, loss = 0.09987990\n",
      "Iteration 595, loss = 0.09970516\n",
      "Iteration 596, loss = 0.09953117\n",
      "Iteration 597, loss = 0.09935793\n",
      "Iteration 598, loss = 0.09918542\n",
      "Iteration 599, loss = 0.09901366\n",
      "Iteration 600, loss = 0.09884262\n",
      "Iteration 601, loss = 0.09867232\n",
      "Iteration 602, loss = 0.09850274\n",
      "Iteration 603, loss = 0.09833388\n",
      "Iteration 604, loss = 0.09816573\n",
      "Iteration 605, loss = 0.09799830\n",
      "Iteration 606, loss = 0.09783157\n",
      "Iteration 607, loss = 0.09766555\n",
      "Iteration 608, loss = 0.09750023\n",
      "Iteration 609, loss = 0.09733560\n",
      "Iteration 610, loss = 0.09717167\n",
      "Iteration 611, loss = 0.09700842\n",
      "Iteration 612, loss = 0.09684586\n",
      "Iteration 613, loss = 0.09668397\n",
      "Iteration 614, loss = 0.09652277\n",
      "Iteration 615, loss = 0.09636223\n",
      "Iteration 616, loss = 0.09620237\n",
      "Iteration 617, loss = 0.09604317\n",
      "Iteration 618, loss = 0.09588463\n",
      "Iteration 619, loss = 0.09572675\n",
      "Iteration 620, loss = 0.09556952\n",
      "Iteration 621, loss = 0.09541295\n",
      "Iteration 622, loss = 0.09525702\n",
      "Iteration 623, loss = 0.09510173\n",
      "Iteration 624, loss = 0.09494709\n",
      "Iteration 625, loss = 0.09479308\n",
      "Iteration 626, loss = 0.09463970\n",
      "Iteration 627, loss = 0.09448695\n",
      "Iteration 628, loss = 0.09433483\n",
      "Iteration 629, loss = 0.09418333\n",
      "Iteration 630, loss = 0.09403245\n",
      "Iteration 631, loss = 0.09388218\n",
      "Iteration 632, loss = 0.09373253\n",
      "Iteration 633, loss = 0.09358349\n",
      "Iteration 634, loss = 0.09343505\n",
      "Iteration 635, loss = 0.09328721\n",
      "Iteration 636, loss = 0.09313998\n",
      "Iteration 637, loss = 0.09299333\n",
      "Iteration 638, loss = 0.09284728\n",
      "Iteration 639, loss = 0.09270183\n",
      "Iteration 640, loss = 0.09255695\n",
      "Iteration 641, loss = 0.09241266\n",
      "Iteration 642, loss = 0.09226895\n",
      "Iteration 643, loss = 0.09212582\n",
      "Iteration 644, loss = 0.09198326\n",
      "Iteration 645, loss = 0.09184127\n",
      "Iteration 646, loss = 0.09169984\n",
      "Iteration 647, loss = 0.09155899\n",
      "Iteration 648, loss = 0.09141869\n",
      "Iteration 649, loss = 0.09127895\n",
      "Iteration 650, loss = 0.09113977\n",
      "Iteration 651, loss = 0.09100114\n",
      "Iteration 652, loss = 0.09086306\n",
      "Iteration 653, loss = 0.09072553\n",
      "Iteration 654, loss = 0.09058854\n",
      "Iteration 655, loss = 0.09045209\n",
      "Iteration 656, loss = 0.09031618\n",
      "Iteration 657, loss = 0.09018081\n",
      "Iteration 658, loss = 0.09004597\n",
      "Iteration 659, loss = 0.08991166\n",
      "Iteration 660, loss = 0.08977787\n",
      "Iteration 661, loss = 0.08964461\n",
      "Iteration 662, loss = 0.08951187\n",
      "Iteration 663, loss = 0.08937965\n",
      "Iteration 664, loss = 0.08924795\n",
      "Iteration 665, loss = 0.08911675\n",
      "Iteration 666, loss = 0.08898607\n",
      "Iteration 667, loss = 0.08885590\n",
      "Iteration 668, loss = 0.08872623\n",
      "Iteration 669, loss = 0.08859707\n",
      "Iteration 670, loss = 0.08846841\n",
      "Iteration 671, loss = 0.08834024\n",
      "Iteration 672, loss = 0.08821257\n",
      "Iteration 673, loss = 0.08808539\n",
      "Iteration 674, loss = 0.08795870\n",
      "Iteration 675, loss = 0.08783250\n",
      "Iteration 676, loss = 0.08770679\n",
      "Iteration 677, loss = 0.08758155\n",
      "Iteration 678, loss = 0.08745680\n",
      "Iteration 679, loss = 0.08733253\n",
      "Iteration 680, loss = 0.08720873\n",
      "Iteration 681, loss = 0.08708540\n",
      "Iteration 682, loss = 0.08696254\n",
      "Iteration 683, loss = 0.08684015\n",
      "Iteration 684, loss = 0.08671823\n",
      "Iteration 685, loss = 0.08659677\n",
      "Iteration 686, loss = 0.08647577\n",
      "Iteration 687, loss = 0.08635523\n",
      "Iteration 688, loss = 0.08623515\n",
      "Iteration 689, loss = 0.08611552\n",
      "Iteration 690, loss = 0.08599634\n",
      "Iteration 691, loss = 0.08587761\n",
      "Iteration 692, loss = 0.08575933\n",
      "Iteration 693, loss = 0.08564149\n",
      "Iteration 694, loss = 0.08552410\n",
      "Iteration 695, loss = 0.08540714\n",
      "Iteration 696, loss = 0.08529063\n",
      "Iteration 697, loss = 0.08517455\n",
      "Iteration 698, loss = 0.08505891\n",
      "Iteration 699, loss = 0.08494369\n",
      "Iteration 700, loss = 0.08482891\n",
      "Iteration 701, loss = 0.08471456\n",
      "Iteration 702, loss = 0.08460063\n",
      "Iteration 703, loss = 0.08448712\n",
      "Iteration 704, loss = 0.08437404\n",
      "Iteration 705, loss = 0.08426137\n",
      "Iteration 706, loss = 0.08414912\n",
      "Iteration 707, loss = 0.08403729\n",
      "Iteration 708, loss = 0.08392587\n",
      "Iteration 709, loss = 0.08381486\n",
      "Iteration 710, loss = 0.08370426\n",
      "Iteration 711, loss = 0.08359407\n",
      "Iteration 712, loss = 0.08348428\n",
      "Iteration 713, loss = 0.08337490\n",
      "Iteration 714, loss = 0.08326591\n",
      "Iteration 715, loss = 0.08315733\n",
      "Iteration 716, loss = 0.08304914\n",
      "Iteration 717, loss = 0.08294135\n",
      "Iteration 718, loss = 0.08283396\n",
      "Iteration 719, loss = 0.08272695\n",
      "Iteration 720, loss = 0.08262033\n",
      "Iteration 721, loss = 0.08251410\n",
      "Iteration 722, loss = 0.08240826\n",
      "Iteration 723, loss = 0.08230280\n",
      "Iteration 724, loss = 0.08219773\n",
      "Iteration 725, loss = 0.08209303\n",
      "Iteration 726, loss = 0.08198871\n",
      "Iteration 727, loss = 0.08188477\n",
      "Iteration 728, loss = 0.08178121\n",
      "Iteration 729, loss = 0.08167802\n",
      "Iteration 730, loss = 0.08157520\n",
      "Iteration 731, loss = 0.08147274\n",
      "Iteration 732, loss = 0.08137066\n",
      "Iteration 733, loss = 0.08126894\n",
      "Iteration 734, loss = 0.08116759\n",
      "Iteration 735, loss = 0.08106660\n",
      "Iteration 736, loss = 0.08096597\n",
      "Iteration 737, loss = 0.08086570\n",
      "Iteration 738, loss = 0.08076579\n",
      "Iteration 739, loss = 0.08066624\n",
      "Iteration 740, loss = 0.08056703\n",
      "Iteration 741, loss = 0.08046818\n",
      "Iteration 742, loss = 0.08036969\n",
      "Iteration 743, loss = 0.08027154\n",
      "Iteration 744, loss = 0.08017374\n",
      "Iteration 745, loss = 0.08007628\n",
      "Iteration 746, loss = 0.07997917\n",
      "Iteration 747, loss = 0.07988240\n",
      "Iteration 748, loss = 0.07978597\n",
      "Iteration 749, loss = 0.07968989\n",
      "Iteration 750, loss = 0.07959414\n",
      "Iteration 751, loss = 0.07949873\n",
      "Iteration 752, loss = 0.07940365\n",
      "Iteration 753, loss = 0.07930890\n",
      "Iteration 754, loss = 0.07921449\n",
      "Iteration 755, loss = 0.07912041\n",
      "Iteration 756, loss = 0.07902665\n",
      "Iteration 757, loss = 0.07893323\n",
      "Iteration 758, loss = 0.07884013\n",
      "Iteration 759, loss = 0.07874735\n",
      "Iteration 760, loss = 0.07865490\n",
      "Iteration 761, loss = 0.07856277\n",
      "Iteration 762, loss = 0.07847095\n",
      "Iteration 763, loss = 0.07837946\n",
      "Iteration 764, loss = 0.07828828\n",
      "Iteration 765, loss = 0.07819742\n",
      "Iteration 766, loss = 0.07810687\n",
      "Iteration 767, loss = 0.07801664\n",
      "Iteration 768, loss = 0.07792672\n",
      "Iteration 769, loss = 0.07783710\n",
      "Iteration 770, loss = 0.07774780\n",
      "Iteration 771, loss = 0.07765880\n",
      "Iteration 772, loss = 0.07757011\n",
      "Iteration 773, loss = 0.07748172\n",
      "Iteration 774, loss = 0.07739363\n",
      "Iteration 775, loss = 0.07730585\n",
      "Iteration 776, loss = 0.07721836\n",
      "Iteration 777, loss = 0.07713118\n",
      "Iteration 778, loss = 0.07704429\n",
      "Iteration 779, loss = 0.07695770\n",
      "Iteration 780, loss = 0.07687140\n",
      "Iteration 781, loss = 0.07678540\n",
      "Iteration 782, loss = 0.07669968\n",
      "Iteration 783, loss = 0.07661426\n",
      "Iteration 784, loss = 0.07652913\n",
      "Iteration 785, loss = 0.07644429\n",
      "Iteration 786, loss = 0.07635973\n",
      "Iteration 787, loss = 0.07627546\n",
      "Iteration 788, loss = 0.07619148\n",
      "Iteration 789, loss = 0.07610777\n",
      "Iteration 790, loss = 0.07602435\n",
      "Iteration 791, loss = 0.07594121\n",
      "Iteration 792, loss = 0.07585835\n",
      "Iteration 793, loss = 0.07577577\n",
      "Iteration 794, loss = 0.07569346\n",
      "Iteration 795, loss = 0.07561143\n",
      "Iteration 796, loss = 0.07552968\n",
      "Iteration 797, loss = 0.07544820\n",
      "Iteration 798, loss = 0.07536699\n",
      "Iteration 799, loss = 0.07528605\n",
      "Iteration 800, loss = 0.07520538\n",
      "Iteration 801, loss = 0.07512498\n",
      "Iteration 802, loss = 0.07504484\n",
      "Iteration 803, loss = 0.07496498\n",
      "Iteration 804, loss = 0.07488537\n",
      "Iteration 805, loss = 0.07480604\n",
      "Iteration 806, loss = 0.07472696\n",
      "Iteration 807, loss = 0.07464815\n",
      "Iteration 808, loss = 0.07456959\n",
      "Iteration 809, loss = 0.07449130\n",
      "Iteration 810, loss = 0.07441326\n",
      "Iteration 811, loss = 0.07433548\n",
      "Iteration 812, loss = 0.07425796\n",
      "Iteration 813, loss = 0.07418069\n",
      "Iteration 814, loss = 0.07410368\n",
      "Iteration 815, loss = 0.07402692\n",
      "Iteration 816, loss = 0.07395041\n",
      "Iteration 817, loss = 0.07387415\n",
      "Iteration 818, loss = 0.07379814\n",
      "Iteration 819, loss = 0.07372238\n",
      "Iteration 820, loss = 0.07364687\n",
      "Iteration 821, loss = 0.07357160\n",
      "Iteration 822, loss = 0.07349658\n",
      "Iteration 823, loss = 0.07342180\n",
      "Iteration 824, loss = 0.07334726\n",
      "Iteration 825, loss = 0.07327297\n",
      "Iteration 826, loss = 0.07319892\n",
      "Iteration 827, loss = 0.07312511\n",
      "Iteration 828, loss = 0.07305153\n",
      "Iteration 829, loss = 0.07297820\n",
      "Iteration 830, loss = 0.07290510\n",
      "Iteration 831, loss = 0.07283224\n",
      "Iteration 832, loss = 0.07275961\n",
      "Iteration 833, loss = 0.07268722\n",
      "Iteration 834, loss = 0.07261506\n",
      "Iteration 835, loss = 0.07254313\n",
      "Iteration 836, loss = 0.07247143\n",
      "Iteration 837, loss = 0.07239997\n",
      "Iteration 838, loss = 0.07232873\n",
      "Iteration 839, loss = 0.07225772\n",
      "Iteration 840, loss = 0.07218694\n",
      "Iteration 841, loss = 0.07211638\n",
      "Iteration 842, loss = 0.07204605\n",
      "Iteration 843, loss = 0.07197595\n",
      "Iteration 844, loss = 0.07190606\n",
      "Iteration 845, loss = 0.07183640\n",
      "Iteration 846, loss = 0.07176696\n",
      "Iteration 847, loss = 0.07169775\n",
      "Iteration 848, loss = 0.07162875\n",
      "Iteration 849, loss = 0.07155997\n",
      "Iteration 850, loss = 0.07149141\n",
      "Iteration 851, loss = 0.07142306\n",
      "Iteration 852, loss = 0.07135493\n",
      "Iteration 853, loss = 0.07128702\n",
      "Iteration 854, loss = 0.07121932\n",
      "Iteration 855, loss = 0.07115183\n",
      "Iteration 856, loss = 0.07108456\n",
      "Iteration 857, loss = 0.07101750\n",
      "Iteration 858, loss = 0.07095065\n",
      "Iteration 859, loss = 0.07088401\n",
      "Iteration 860, loss = 0.07081758\n",
      "Iteration 861, loss = 0.07075135\n",
      "Iteration 862, loss = 0.07068534\n",
      "Iteration 863, loss = 0.07061953\n",
      "Iteration 864, loss = 0.07055393\n",
      "Iteration 865, loss = 0.07048853\n",
      "Iteration 866, loss = 0.07042333\n",
      "Iteration 867, loss = 0.07035834\n",
      "Iteration 868, loss = 0.07029355\n",
      "Iteration 869, loss = 0.07022896\n",
      "Iteration 870, loss = 0.07016457\n",
      "Iteration 871, loss = 0.07010039\n",
      "Iteration 872, loss = 0.07003640\n",
      "Iteration 873, loss = 0.06997261\n",
      "Iteration 874, loss = 0.06990902\n",
      "Iteration 875, loss = 0.06984562\n",
      "Iteration 876, loss = 0.06978242\n",
      "Iteration 877, loss = 0.06971942\n",
      "Iteration 878, loss = 0.06965661\n",
      "Iteration 879, loss = 0.06959399\n",
      "Iteration 880, loss = 0.06953156\n",
      "Iteration 881, loss = 0.06946933\n",
      "Iteration 882, loss = 0.06940729\n",
      "Iteration 883, loss = 0.06934544\n",
      "Iteration 884, loss = 0.06928378\n",
      "Iteration 885, loss = 0.06922231\n",
      "Iteration 886, loss = 0.06916102\n",
      "Iteration 887, loss = 0.06909992\n",
      "Iteration 888, loss = 0.06903901\n",
      "Iteration 889, loss = 0.06897829\n",
      "Iteration 890, loss = 0.06891775\n",
      "Iteration 891, loss = 0.06885740\n",
      "Iteration 892, loss = 0.06879722\n",
      "Iteration 893, loss = 0.06873724\n",
      "Iteration 894, loss = 0.06867743\n",
      "Iteration 895, loss = 0.06861781\n",
      "Iteration 896, loss = 0.06855836\n",
      "Iteration 897, loss = 0.06849910\n",
      "Iteration 898, loss = 0.06844001\n",
      "Iteration 899, loss = 0.06838111\n",
      "Iteration 900, loss = 0.06832238\n",
      "Iteration 901, loss = 0.06826383\n",
      "Iteration 902, loss = 0.06820546\n",
      "Iteration 903, loss = 0.06814726\n",
      "Iteration 904, loss = 0.06808924\n",
      "Iteration 905, loss = 0.06803139\n",
      "Iteration 906, loss = 0.06797371\n",
      "Iteration 907, loss = 0.06791621\n",
      "Iteration 908, loss = 0.06785888\n",
      "Iteration 909, loss = 0.06780172\n",
      "Iteration 910, loss = 0.06774474\n",
      "Iteration 911, loss = 0.06768792\n",
      "Iteration 912, loss = 0.06763127\n",
      "Iteration 913, loss = 0.06757480\n",
      "Iteration 914, loss = 0.06751849\n",
      "Iteration 915, loss = 0.06746235\n",
      "Iteration 916, loss = 0.06740637\n",
      "Iteration 917, loss = 0.06735056\n",
      "Iteration 918, loss = 0.06729492\n",
      "Iteration 919, loss = 0.06723945\n",
      "Iteration 920, loss = 0.06718413\n",
      "Iteration 921, loss = 0.06712899\n",
      "Iteration 922, loss = 0.06707400\n",
      "Iteration 923, loss = 0.06701918\n",
      "Iteration 924, loss = 0.06696452\n",
      "Iteration 925, loss = 0.06691002\n",
      "Iteration 926, loss = 0.06685568\n",
      "Iteration 927, loss = 0.06680150\n",
      "Iteration 928, loss = 0.06674748\n",
      "Iteration 929, loss = 0.06669362\n",
      "Iteration 930, loss = 0.06663992\n",
      "Iteration 931, loss = 0.06658638\n",
      "Iteration 932, loss = 0.06653299\n",
      "Iteration 933, loss = 0.06647976\n",
      "Iteration 934, loss = 0.06642669\n",
      "Iteration 935, loss = 0.06637377\n",
      "Iteration 936, loss = 0.06632100\n",
      "Iteration 937, loss = 0.06626839\n",
      "Iteration 938, loss = 0.06621593\n",
      "Iteration 939, loss = 0.06616363\n",
      "Iteration 940, loss = 0.06611148\n",
      "Iteration 941, loss = 0.06605948\n",
      "Iteration 942, loss = 0.06600763\n",
      "Iteration 943, loss = 0.06595593\n",
      "Iteration 944, loss = 0.06590438\n",
      "Iteration 945, loss = 0.06585298\n",
      "Iteration 946, loss = 0.06580173\n",
      "Iteration 947, loss = 0.06575063\n",
      "Iteration 948, loss = 0.06569968\n",
      "Iteration 949, loss = 0.06564887\n",
      "Iteration 950, loss = 0.06559821\n",
      "Iteration 951, loss = 0.06554770\n",
      "Iteration 952, loss = 0.06549733\n",
      "Iteration 953, loss = 0.06544710\n",
      "Iteration 954, loss = 0.06539703\n",
      "Iteration 955, loss = 0.06534709\n",
      "Iteration 956, loss = 0.06529730\n",
      "Iteration 957, loss = 0.06524765\n",
      "Iteration 958, loss = 0.06519814\n",
      "Iteration 959, loss = 0.06514878\n",
      "Iteration 960, loss = 0.06509956\n",
      "Iteration 961, loss = 0.06505047\n",
      "Iteration 962, loss = 0.06500153\n",
      "Iteration 963, loss = 0.06495273\n",
      "Iteration 964, loss = 0.06490406\n",
      "Iteration 965, loss = 0.06485554\n",
      "Iteration 966, loss = 0.06480715\n",
      "Iteration 967, loss = 0.06475890\n",
      "Iteration 968, loss = 0.06471079\n",
      "Iteration 969, loss = 0.06466281\n",
      "Iteration 970, loss = 0.06461497\n",
      "Iteration 971, loss = 0.06456727\n",
      "Iteration 972, loss = 0.06451970\n",
      "Iteration 973, loss = 0.06447226\n",
      "Iteration 974, loss = 0.06442496\n",
      "Iteration 975, loss = 0.06437780\n",
      "Iteration 976, loss = 0.06433076\n",
      "Iteration 977, loss = 0.06428386\n",
      "Iteration 978, loss = 0.06423709\n",
      "Iteration 979, loss = 0.06419045\n",
      "Iteration 980, loss = 0.06414395\n",
      "Iteration 981, loss = 0.06409757\n",
      "Iteration 982, loss = 0.06405133\n",
      "Iteration 983, loss = 0.06400521\n",
      "Iteration 984, loss = 0.06395922\n",
      "Iteration 985, loss = 0.06391336\n",
      "Iteration 986, loss = 0.06386763\n",
      "Iteration 987, loss = 0.06382203\n",
      "Iteration 988, loss = 0.06377656\n",
      "Iteration 989, loss = 0.06373121\n",
      "Iteration 990, loss = 0.06368599\n",
      "Iteration 991, loss = 0.06364089\n",
      "Iteration 992, loss = 0.06359592\n",
      "Iteration 993, loss = 0.06355108\n",
      "Iteration 994, loss = 0.06350635\n",
      "Iteration 995, loss = 0.06346176\n",
      "Iteration 996, loss = 0.06341729\n",
      "Iteration 997, loss = 0.06337294\n",
      "Iteration 998, loss = 0.06332871\n",
      "Iteration 999, loss = 0.06328460\n",
      "Iteration 1000, loss = 0.06324062\n",
      "Iteration 1001, loss = 0.06319676\n",
      "Iteration 1002, loss = 0.06315302\n",
      "Iteration 1003, loss = 0.06310940\n",
      "Iteration 1004, loss = 0.06306590\n",
      "Iteration 1005, loss = 0.06302252\n",
      "Iteration 1006, loss = 0.06297926\n",
      "Iteration 1007, loss = 0.06293612\n",
      "Iteration 1008, loss = 0.06289309\n",
      "Iteration 1009, loss = 0.06285019\n",
      "Iteration 1010, loss = 0.06280740\n",
      "Iteration 1011, loss = 0.06276473\n",
      "Iteration 1012, loss = 0.06272217\n",
      "Iteration 1013, loss = 0.06267974\n",
      "Iteration 1014, loss = 0.06263742\n",
      "Iteration 1015, loss = 0.06259521\n",
      "Iteration 1016, loss = 0.06255312\n",
      "Iteration 1017, loss = 0.06251114\n",
      "Iteration 1018, loss = 0.06246928\n",
      "Iteration 1019, loss = 0.06242753\n",
      "Iteration 1020, loss = 0.06238589\n",
      "Iteration 1021, loss = 0.06234437\n",
      "Iteration 1022, loss = 0.06230296\n",
      "Iteration 1023, loss = 0.06226166\n",
      "Iteration 1024, loss = 0.06222048\n",
      "Iteration 1025, loss = 0.06217940\n",
      "Iteration 1026, loss = 0.06213844\n",
      "Iteration 1027, loss = 0.06209759\n",
      "Iteration 1028, loss = 0.06205684\n",
      "Iteration 1029, loss = 0.06201621\n",
      "Iteration 1030, loss = 0.06197569\n",
      "Iteration 1031, loss = 0.06193527\n",
      "Iteration 1032, loss = 0.06189497\n",
      "Iteration 1033, loss = 0.06185477\n",
      "Iteration 1034, loss = 0.06181468\n",
      "Iteration 1035, loss = 0.06177470\n",
      "Iteration 1036, loss = 0.06173483\n",
      "Iteration 1037, loss = 0.06169506\n",
      "Iteration 1038, loss = 0.06165540\n",
      "Iteration 1039, loss = 0.06161584\n",
      "Iteration 1040, loss = 0.06157639\n",
      "Iteration 1041, loss = 0.06153705\n",
      "Iteration 1042, loss = 0.06149781\n",
      "Iteration 1043, loss = 0.06145867\n",
      "Iteration 1044, loss = 0.06141964\n",
      "Iteration 1045, loss = 0.06138072\n",
      "Iteration 1046, loss = 0.06134189\n",
      "Iteration 1047, loss = 0.06130317\n",
      "Iteration 1048, loss = 0.06126455\n",
      "Iteration 1049, loss = 0.06122604\n",
      "Iteration 1050, loss = 0.06118763\n",
      "Iteration 1051, loss = 0.06114931\n",
      "Iteration 1052, loss = 0.06111110\n",
      "Iteration 1053, loss = 0.06107299\n",
      "Iteration 1054, loss = 0.06103499\n",
      "Iteration 1055, loss = 0.06099708\n",
      "Iteration 1056, loss = 0.06095927\n",
      "Iteration 1057, loss = 0.06092156\n",
      "Iteration 1058, loss = 0.06088395\n",
      "Iteration 1059, loss = 0.06084644\n",
      "Iteration 1060, loss = 0.06080903\n",
      "Iteration 1061, loss = 0.06077171\n",
      "Iteration 1062, loss = 0.06073450\n",
      "Iteration 1063, loss = 0.06069738\n",
      "Iteration 1064, loss = 0.06066036\n",
      "Iteration 1065, loss = 0.06062343\n",
      "Iteration 1066, loss = 0.06058660\n",
      "Iteration 1067, loss = 0.06054987\n",
      "Iteration 1068, loss = 0.06051324\n",
      "Iteration 1069, loss = 0.06047670\n",
      "Iteration 1070, loss = 0.06044025\n",
      "Iteration 1071, loss = 0.06040390\n",
      "Iteration 1072, loss = 0.06036764\n",
      "Iteration 1073, loss = 0.06033148\n",
      "Iteration 1074, loss = 0.06029541\n",
      "Iteration 1075, loss = 0.06025944\n",
      "Iteration 1076, loss = 0.06022356\n",
      "Iteration 1077, loss = 0.06018777\n",
      "Iteration 1078, loss = 0.06015208\n",
      "Iteration 1079, loss = 0.06011647\n",
      "Iteration 1080, loss = 0.06008096\n",
      "Iteration 1081, loss = 0.06004554\n",
      "Iteration 1082, loss = 0.06001021\n",
      "Iteration 1083, loss = 0.05997498\n",
      "Iteration 1084, loss = 0.05993983\n",
      "Iteration 1085, loss = 0.05990478\n",
      "Iteration 1086, loss = 0.05986981\n",
      "Iteration 1087, loss = 0.05983493\n",
      "Iteration 1088, loss = 0.05980015\n",
      "Iteration 1089, loss = 0.05976545\n",
      "Iteration 1090, loss = 0.05973084\n",
      "Iteration 1091, loss = 0.05969632\n",
      "Iteration 1092, loss = 0.05966189\n",
      "Iteration 1093, loss = 0.05962755\n",
      "Iteration 1094, loss = 0.05959329\n",
      "Iteration 1095, loss = 0.05955912\n",
      "Iteration 1096, loss = 0.05952504\n",
      "Iteration 1097, loss = 0.05949105\n",
      "Iteration 1098, loss = 0.05945714\n",
      "Iteration 1099, loss = 0.05942332\n",
      "Iteration 1100, loss = 0.05938959\n",
      "Iteration 1101, loss = 0.05935594\n",
      "Iteration 1102, loss = 0.05932237\n",
      "Iteration 1103, loss = 0.05928889\n",
      "Iteration 1104, loss = 0.05925550\n",
      "Iteration 1105, loss = 0.05922219\n",
      "Iteration 1106, loss = 0.05918896\n",
      "Iteration 1107, loss = 0.05915582\n",
      "Iteration 1108, loss = 0.05912276\n",
      "Iteration 1109, loss = 0.05908979\n",
      "Iteration 1110, loss = 0.05905690\n",
      "Iteration 1111, loss = 0.05902409\n",
      "Iteration 1112, loss = 0.05899136\n",
      "Iteration 1113, loss = 0.05895872\n",
      "Iteration 1114, loss = 0.05892616\n",
      "Iteration 1115, loss = 0.05889368\n",
      "Iteration 1116, loss = 0.05886128\n",
      "Iteration 1117, loss = 0.05882896\n",
      "Iteration 1118, loss = 0.05879673\n",
      "Iteration 1119, loss = 0.05876457\n",
      "Iteration 1120, loss = 0.05873250\n",
      "Iteration 1121, loss = 0.05870050\n",
      "Iteration 1122, loss = 0.05866859\n",
      "Iteration 1123, loss = 0.05863675\n",
      "Iteration 1124, loss = 0.05860499\n",
      "Iteration 1125, loss = 0.05857332\n",
      "Iteration 1126, loss = 0.05854172\n",
      "Iteration 1127, loss = 0.05851020\n",
      "Iteration 1128, loss = 0.05847876\n",
      "Iteration 1129, loss = 0.05844739\n",
      "Iteration 1130, loss = 0.05841611\n",
      "Iteration 1131, loss = 0.05838490\n",
      "Iteration 1132, loss = 0.05835377\n",
      "Iteration 1133, loss = 0.05832272\n",
      "Iteration 1134, loss = 0.05829174\n",
      "Iteration 1135, loss = 0.05826084\n",
      "Iteration 1136, loss = 0.05823002\n",
      "Iteration 1137, loss = 0.05819927\n",
      "Iteration 1138, loss = 0.05816860\n",
      "Iteration 1139, loss = 0.05813800\n",
      "Iteration 1140, loss = 0.05810748\n",
      "Iteration 1141, loss = 0.05807703\n",
      "Iteration 1142, loss = 0.05804666\n",
      "Iteration 1143, loss = 0.05801636\n",
      "Iteration 1144, loss = 0.05798614\n",
      "Iteration 1145, loss = 0.05795599\n",
      "Iteration 1146, loss = 0.05792592\n",
      "Iteration 1147, loss = 0.05789591\n",
      "Iteration 1148, loss = 0.05786599\n",
      "Iteration 1149, loss = 0.05783613\n",
      "Iteration 1150, loss = 0.05780635\n",
      "Iteration 1151, loss = 0.05777664\n",
      "Iteration 1152, loss = 0.05774700\n",
      "Iteration 1153, loss = 0.05771744\n",
      "Iteration 1154, loss = 0.05768795\n",
      "Iteration 1155, loss = 0.05765852\n",
      "Iteration 1156, loss = 0.05762918\n",
      "Iteration 1157, loss = 0.05759990\n",
      "Iteration 1158, loss = 0.05757069\n",
      "Iteration 1159, loss = 0.05754155\n",
      "Iteration 1160, loss = 0.05751249\n",
      "Iteration 1161, loss = 0.05748349\n",
      "Iteration 1162, loss = 0.05745456\n",
      "Iteration 1163, loss = 0.05742571\n",
      "Iteration 1164, loss = 0.05739692\n",
      "Iteration 1165, loss = 0.05736821\n",
      "Iteration 1166, loss = 0.05733956\n",
      "Iteration 1167, loss = 0.05731098\n",
      "Iteration 1168, loss = 0.05728247\n",
      "Iteration 1169, loss = 0.05725403\n",
      "Iteration 1170, loss = 0.05722565\n",
      "Iteration 1171, loss = 0.05719735\n",
      "Iteration 1172, loss = 0.05716911\n",
      "Iteration 1173, loss = 0.05714094\n",
      "Iteration 1174, loss = 0.05711284\n",
      "Iteration 1175, loss = 0.05708480\n",
      "Iteration 1176, loss = 0.05705684\n",
      "Iteration 1177, loss = 0.05702894\n",
      "Iteration 1178, loss = 0.05700110\n",
      "Iteration 1179, loss = 0.05697333\n",
      "Iteration 1180, loss = 0.05694563\n",
      "Iteration 1181, loss = 0.05691799\n",
      "Iteration 1182, loss = 0.05689042\n",
      "Iteration 1183, loss = 0.05686292\n",
      "Iteration 1184, loss = 0.05683548\n",
      "Iteration 1185, loss = 0.05680810\n",
      "Iteration 1186, loss = 0.05678080\n",
      "Iteration 1187, loss = 0.05675355\n",
      "Iteration 1188, loss = 0.05672637\n",
      "Iteration 1189, loss = 0.05669925\n",
      "Iteration 1190, loss = 0.05667220\n",
      "Iteration 1191, loss = 0.05664521\n",
      "Iteration 1192, loss = 0.05661829\n",
      "Iteration 1193, loss = 0.05659143\n",
      "Iteration 1194, loss = 0.05656463\n",
      "Iteration 1195, loss = 0.05653790\n",
      "Iteration 1196, loss = 0.05651122\n",
      "Iteration 1197, loss = 0.05648461\n",
      "Iteration 1198, loss = 0.05645807\n",
      "Iteration 1199, loss = 0.05643158\n",
      "Iteration 1200, loss = 0.05640516\n",
      "Iteration 1201, loss = 0.05637880\n",
      "Iteration 1202, loss = 0.05635250\n",
      "Iteration 1203, loss = 0.05632626\n",
      "Iteration 1204, loss = 0.05630009\n",
      "Iteration 1205, loss = 0.05627397\n",
      "Iteration 1206, loss = 0.05624792\n",
      "Iteration 1207, loss = 0.05622193\n",
      "Iteration 1208, loss = 0.05619599\n",
      "Iteration 1209, loss = 0.05617012\n",
      "Iteration 1210, loss = 0.05614431\n",
      "Iteration 1211, loss = 0.05611856\n",
      "Iteration 1212, loss = 0.05609287\n",
      "Iteration 1213, loss = 0.05606723\n",
      "Iteration 1214, loss = 0.05604166\n",
      "Iteration 1215, loss = 0.05601615\n",
      "Iteration 1216, loss = 0.05599069\n",
      "Iteration 1217, loss = 0.05596529\n",
      "Iteration 1218, loss = 0.05593996\n",
      "Iteration 1219, loss = 0.05591468\n",
      "Iteration 1220, loss = 0.05588946\n",
      "Iteration 1221, loss = 0.05586429\n",
      "Iteration 1222, loss = 0.05583919\n",
      "Iteration 1223, loss = 0.05581414\n",
      "Iteration 1224, loss = 0.05578915\n",
      "Iteration 1225, loss = 0.05576422\n",
      "Iteration 1226, loss = 0.05573935\n",
      "Iteration 1227, loss = 0.05571453\n",
      "Iteration 1228, loss = 0.05568977\n",
      "Iteration 1229, loss = 0.05566507\n",
      "Iteration 1230, loss = 0.05564042\n",
      "Iteration 1231, loss = 0.05561583\n",
      "Iteration 1232, loss = 0.05559129\n",
      "Iteration 1233, loss = 0.05556681\n",
      "Iteration 1234, loss = 0.05554239\n",
      "Iteration 1235, loss = 0.05551802\n",
      "Iteration 1236, loss = 0.05549371\n",
      "Iteration 1237, loss = 0.05546945\n",
      "Iteration 1238, loss = 0.05544525\n",
      "Iteration 1239, loss = 0.05542111\n",
      "Iteration 1240, loss = 0.05539701\n",
      "Iteration 1241, loss = 0.05537298\n",
      "Iteration 1242, loss = 0.05534899\n",
      "Iteration 1243, loss = 0.05532507\n",
      "Iteration 1244, loss = 0.05530119\n",
      "Iteration 1245, loss = 0.05527737\n",
      "Iteration 1246, loss = 0.05525361\n",
      "Iteration 1247, loss = 0.05522989\n",
      "Iteration 1248, loss = 0.05520623\n",
      "Iteration 1249, loss = 0.05518263\n",
      "Iteration 1250, loss = 0.05515908\n",
      "Iteration 1251, loss = 0.05513558\n",
      "Iteration 1252, loss = 0.05511213\n",
      "Iteration 1253, loss = 0.05508874\n",
      "Iteration 1254, loss = 0.05506539\n",
      "Iteration 1255, loss = 0.05504210\n",
      "Iteration 1256, loss = 0.05501887\n",
      "Iteration 1257, loss = 0.05499568\n",
      "Iteration 1258, loss = 0.05497255\n",
      "Iteration 1259, loss = 0.05494947\n",
      "Iteration 1260, loss = 0.05492644\n",
      "Iteration 1261, loss = 0.05490346\n",
      "Iteration 1262, loss = 0.05488053\n",
      "Iteration 1263, loss = 0.05485765\n",
      "Iteration 1264, loss = 0.05483483\n",
      "Iteration 1265, loss = 0.05481205\n",
      "Iteration 1266, loss = 0.05478933\n",
      "Iteration 1267, loss = 0.05476666\n",
      "Iteration 1268, loss = 0.05474403\n",
      "Iteration 1269, loss = 0.05472146\n",
      "Iteration 1270, loss = 0.05469894\n",
      "Iteration 1271, loss = 0.05467647\n",
      "Iteration 1272, loss = 0.05465404\n",
      "Iteration 1273, loss = 0.05463167\n",
      "Iteration 1274, loss = 0.05460934\n",
      "Iteration 1275, loss = 0.05458707\n",
      "Iteration 1276, loss = 0.05456484\n",
      "Iteration 1277, loss = 0.05454267\n",
      "Iteration 1278, loss = 0.05452054\n",
      "Iteration 1279, loss = 0.05449846\n",
      "Iteration 1280, loss = 0.05447643\n",
      "Iteration 1281, loss = 0.05445444\n",
      "Iteration 1282, loss = 0.05443251\n",
      "Iteration 1283, loss = 0.05441062\n",
      "Iteration 1284, loss = 0.05438879\n",
      "Iteration 1285, loss = 0.05436700\n",
      "Iteration 1286, loss = 0.05434525\n",
      "Iteration 1287, loss = 0.05432356\n",
      "Iteration 1288, loss = 0.05430191\n",
      "Iteration 1289, loss = 0.05428031\n",
      "Iteration 1290, loss = 0.05425875\n",
      "Iteration 1291, loss = 0.05423725\n",
      "Iteration 1292, loss = 0.05421579\n",
      "Iteration 1293, loss = 0.05419438\n",
      "Iteration 1294, loss = 0.05417301\n",
      "Iteration 1295, loss = 0.05415169\n",
      "Iteration 1296, loss = 0.05413041\n",
      "Iteration 1297, loss = 0.05410919\n",
      "Iteration 1298, loss = 0.05408800\n",
      "Iteration 1299, loss = 0.05406687\n",
      "Iteration 1300, loss = 0.05404578\n",
      "Iteration 1301, loss = 0.05402473\n",
      "Iteration 1302, loss = 0.05400373\n",
      "Iteration 1303, loss = 0.05398278\n",
      "Iteration 1304, loss = 0.05396187\n",
      "Iteration 1305, loss = 0.05394101\n",
      "Iteration 1306, loss = 0.05392019\n",
      "Iteration 1307, loss = 0.05389941\n",
      "Iteration 1308, loss = 0.05387868\n",
      "Iteration 1309, loss = 0.05385800\n",
      "Iteration 1310, loss = 0.05383736\n",
      "Iteration 1311, loss = 0.05381676\n",
      "Iteration 1312, loss = 0.05379621\n",
      "Iteration 1313, loss = 0.05377570\n",
      "Iteration 1314, loss = 0.05375524\n",
      "Iteration 1315, loss = 0.05373482\n",
      "Iteration 1316, loss = 0.05371444\n",
      "Iteration 1317, loss = 0.05369411\n",
      "Iteration 1318, loss = 0.05367382\n",
      "Iteration 1319, loss = 0.05365357\n",
      "Iteration 1320, loss = 0.05363337\n",
      "Iteration 1321, loss = 0.05361320\n",
      "Iteration 1322, loss = 0.05359309\n",
      "Iteration 1323, loss = 0.05357301\n",
      "Iteration 1324, loss = 0.05355298\n",
      "Iteration 1325, loss = 0.05353299\n",
      "Iteration 1326, loss = 0.05351304\n",
      "Iteration 1327, loss = 0.05349313\n",
      "Iteration 1328, loss = 0.05347327\n",
      "Iteration 1329, loss = 0.05345345\n",
      "Iteration 1330, loss = 0.05343367\n",
      "Iteration 1331, loss = 0.05341393\n",
      "Iteration 1332, loss = 0.05339423\n",
      "Iteration 1333, loss = 0.05337458\n",
      "Iteration 1334, loss = 0.05335496\n",
      "Iteration 1335, loss = 0.05333539\n",
      "Iteration 1336, loss = 0.05331586\n",
      "Iteration 1337, loss = 0.05329637\n",
      "Iteration 1338, loss = 0.05327692\n",
      "Iteration 1339, loss = 0.05325751\n",
      "Iteration 1340, loss = 0.05323814\n",
      "Iteration 1341, loss = 0.05321881\n",
      "Iteration 1342, loss = 0.05319953\n",
      "Iteration 1343, loss = 0.05318028\n",
      "Iteration 1344, loss = 0.05316107\n",
      "Iteration 1345, loss = 0.05314190\n",
      "Iteration 1346, loss = 0.05312278\n",
      "Iteration 1347, loss = 0.05310369\n",
      "Iteration 1348, loss = 0.05308464\n",
      "Iteration 1349, loss = 0.05306563\n",
      "Iteration 1350, loss = 0.05304667\n",
      "Iteration 1351, loss = 0.05302774\n",
      "Iteration 1352, loss = 0.05300885\n",
      "Iteration 1353, loss = 0.05299000\n",
      "Iteration 1354, loss = 0.05297119\n",
      "Iteration 1355, loss = 0.05295241\n",
      "Iteration 1356, loss = 0.05293368\n",
      "Iteration 1357, loss = 0.05291498\n",
      "Iteration 1358, loss = 0.05289633\n",
      "Iteration 1359, loss = 0.05287771\n",
      "Iteration 1360, loss = 0.05285913\n",
      "Iteration 1361, loss = 0.05284059\n",
      "Iteration 1362, loss = 0.05282208\n",
      "Iteration 1363, loss = 0.05280362\n",
      "Iteration 1364, loss = 0.05278519\n",
      "Iteration 1365, loss = 0.05276680\n",
      "Iteration 1366, loss = 0.05274845\n",
      "Iteration 1367, loss = 0.05273013\n",
      "Iteration 1368, loss = 0.05271186\n",
      "Iteration 1369, loss = 0.05269362\n",
      "Iteration 1370, loss = 0.05267541\n",
      "Iteration 1371, loss = 0.05265725\n",
      "Iteration 1372, loss = 0.05263912\n",
      "Iteration 1373, loss = 0.05262103\n",
      "Iteration 1374, loss = 0.05260297\n",
      "Iteration 1375, loss = 0.05258496\n",
      "Iteration 1376, loss = 0.05256697\n",
      "Iteration 1377, loss = 0.05254903\n",
      "Iteration 1378, loss = 0.05253112\n",
      "Iteration 1379, loss = 0.05251325\n",
      "Iteration 1380, loss = 0.05249541\n",
      "Iteration 1381, loss = 0.05247761\n",
      "Iteration 1382, loss = 0.05245985\n",
      "Iteration 1383, loss = 0.05244212\n",
      "Iteration 1384, loss = 0.05242443\n",
      "Iteration 1385, loss = 0.05240677\n",
      "Iteration 1386, loss = 0.05238915\n",
      "Iteration 1387, loss = 0.05237157\n",
      "Iteration 1388, loss = 0.05235402\n",
      "Iteration 1389, loss = 0.05233650\n",
      "Iteration 1390, loss = 0.05231903\n",
      "Iteration 1391, loss = 0.05230158\n",
      "Iteration 1392, loss = 0.05228417\n",
      "Iteration 1393, loss = 0.05226680\n",
      "Iteration 1394, loss = 0.05224946\n",
      "Iteration 1395, loss = 0.05223215\n",
      "Iteration 1396, loss = 0.05221488\n",
      "Iteration 1397, loss = 0.05219765\n",
      "Iteration 1398, loss = 0.05218045\n",
      "Iteration 1399, loss = 0.05216328\n",
      "Iteration 1400, loss = 0.05214615\n",
      "Iteration 1401, loss = 0.05212905\n",
      "Iteration 1402, loss = 0.05211198\n",
      "Iteration 1403, loss = 0.05209495\n",
      "Iteration 1404, loss = 0.05207796\n",
      "Iteration 1405, loss = 0.05206099\n",
      "Iteration 1406, loss = 0.05204406\n",
      "Iteration 1407, loss = 0.05202717\n",
      "Iteration 1408, loss = 0.05201030\n",
      "Iteration 1409, loss = 0.05199347\n",
      "Iteration 1410, loss = 0.05197668\n",
      "Iteration 1411, loss = 0.05195992\n",
      "Iteration 1412, loss = 0.05194319\n",
      "Iteration 1413, loss = 0.05192649\n",
      "Iteration 1414, loss = 0.05190983\n",
      "Iteration 1415, loss = 0.05189319\n",
      "Iteration 1416, loss = 0.05187660\n",
      "Iteration 1417, loss = 0.05186003\n",
      "Iteration 1418, loss = 0.05184350\n",
      "Iteration 1419, loss = 0.05182700\n",
      "Iteration 1420, loss = 0.05181053\n",
      "Iteration 1421, loss = 0.05179409\n",
      "Iteration 1422, loss = 0.05177769\n",
      "Iteration 1423, loss = 0.05176132\n",
      "Iteration 1424, loss = 0.05174498\n",
      "Iteration 1425, loss = 0.05172867\n",
      "Iteration 1426, loss = 0.05171239\n",
      "Iteration 1427, loss = 0.05169615\n",
      "Iteration 1428, loss = 0.05167993\n",
      "Iteration 1429, loss = 0.05166375\n",
      "Iteration 1430, loss = 0.05164760\n",
      "Iteration 1431, loss = 0.05163148\n",
      "Iteration 1432, loss = 0.05161540\n",
      "Iteration 1433, loss = 0.05159934\n",
      "Iteration 1434, loss = 0.05158332\n",
      "Iteration 1435, loss = 0.05156732\n",
      "Iteration 1436, loss = 0.05155136\n",
      "Iteration 1437, loss = 0.05153543\n",
      "Iteration 1438, loss = 0.05151953\n",
      "Iteration 1439, loss = 0.05150365\n",
      "Iteration 1440, loss = 0.05148781\n",
      "Iteration 1441, loss = 0.05147200\n",
      "Iteration 1442, loss = 0.05145623\n",
      "Iteration 1443, loss = 0.05144048\n",
      "Iteration 1444, loss = 0.05142476\n",
      "Iteration 1445, loss = 0.05140907\n",
      "Iteration 1446, loss = 0.05139341\n",
      "Iteration 1447, loss = 0.05137778\n",
      "Iteration 1448, loss = 0.05136218\n",
      "Iteration 1449, loss = 0.05134661\n",
      "Iteration 1450, loss = 0.05133108\n",
      "Iteration 1451, loss = 0.05131557\n",
      "Iteration 1452, loss = 0.05130009\n",
      "Iteration 1453, loss = 0.05128464\n",
      "Iteration 1454, loss = 0.05126922\n",
      "Iteration 1455, loss = 0.05125382\n",
      "Iteration 1456, loss = 0.05123846\n",
      "Iteration 1457, loss = 0.05122313\n",
      "Iteration 1458, loss = 0.05120783\n",
      "Iteration 1459, loss = 0.05119255\n",
      "Iteration 1460, loss = 0.05117731\n",
      "Iteration 1461, loss = 0.05116209\n",
      "Iteration 1462, loss = 0.05114690\n",
      "Iteration 1463, loss = 0.05113174\n",
      "Iteration 1464, loss = 0.05111661\n",
      "Iteration 1465, loss = 0.05110151\n",
      "Iteration 1466, loss = 0.05108643\n",
      "Iteration 1467, loss = 0.05107139\n",
      "Iteration 1468, loss = 0.05105637\n",
      "Iteration 1469, loss = 0.05104138\n",
      "Iteration 1470, loss = 0.05102642\n",
      "Iteration 1471, loss = 0.05101149\n",
      "Iteration 1472, loss = 0.05099659\n",
      "Iteration 1473, loss = 0.05098171\n",
      "Iteration 1474, loss = 0.05096686\n",
      "Iteration 1475, loss = 0.05095204\n",
      "Iteration 1476, loss = 0.05093725\n",
      "Iteration 1477, loss = 0.05092248\n",
      "Iteration 1478, loss = 0.05090774\n",
      "Iteration 1479, loss = 0.05089303\n",
      "Iteration 1480, loss = 0.05087835\n",
      "Iteration 1481, loss = 0.05086369\n",
      "Iteration 1482, loss = 0.05084906\n",
      "Iteration 1483, loss = 0.05083446\n",
      "Iteration 1484, loss = 0.05081989\n",
      "Iteration 1485, loss = 0.05080534\n",
      "Iteration 1486, loss = 0.05079082\n",
      "Iteration 1487, loss = 0.05077633\n",
      "Iteration 1488, loss = 0.05076186\n",
      "Iteration 1489, loss = 0.05074742\n",
      "Iteration 1490, loss = 0.05073301\n",
      "Iteration 1491, loss = 0.05071862\n",
      "Iteration 1492, loss = 0.05070426\n",
      "Iteration 1493, loss = 0.05068993\n",
      "Iteration 1494, loss = 0.05067562\n",
      "Iteration 1495, loss = 0.05066134\n",
      "Iteration 1496, loss = 0.05064709\n",
      "Iteration 1497, loss = 0.05063286\n",
      "Iteration 1498, loss = 0.05061866\n",
      "Iteration 1499, loss = 0.05060448\n",
      "Iteration 1500, loss = 0.05059033\n",
      "Iteration 1501, loss = 0.05057621\n",
      "Iteration 1502, loss = 0.05056211\n",
      "Iteration 1503, loss = 0.05054803\n",
      "Iteration 1504, loss = 0.05053399\n",
      "Iteration 1505, loss = 0.05051997\n",
      "Iteration 1506, loss = 0.05050597\n",
      "Iteration 1507, loss = 0.05049200\n",
      "Iteration 1508, loss = 0.05047805\n",
      "Iteration 1509, loss = 0.05046414\n",
      "Iteration 1510, loss = 0.05045024\n",
      "Iteration 1511, loss = 0.05043637\n",
      "Iteration 1512, loss = 0.05042253\n",
      "Iteration 1513, loss = 0.05040871\n",
      "Iteration 1514, loss = 0.05039492\n",
      "Iteration 1515, loss = 0.05038115\n",
      "Iteration 1516, loss = 0.05036740\n",
      "Iteration 1517, loss = 0.05035368\n",
      "Iteration 1518, loss = 0.05033999\n",
      "Iteration 1519, loss = 0.05032632\n",
      "Iteration 1520, loss = 0.05031267\n",
      "Iteration 1521, loss = 0.05029905\n",
      "Iteration 1522, loss = 0.05028546\n",
      "Iteration 1523, loss = 0.05027189\n",
      "Iteration 1524, loss = 0.05025834\n",
      "Iteration 1525, loss = 0.05024482\n",
      "Iteration 1526, loss = 0.05023132\n",
      "Iteration 1527, loss = 0.05021784\n",
      "Iteration 1528, loss = 0.05020439\n",
      "Iteration 1529, loss = 0.05019096\n",
      "Iteration 1530, loss = 0.05017756\n",
      "Iteration 1531, loss = 0.05016418\n",
      "Iteration 1532, loss = 0.05015083\n",
      "Iteration 1533, loss = 0.05013750\n",
      "Iteration 1534, loss = 0.05012419\n",
      "Iteration 1535, loss = 0.05011091\n",
      "Iteration 1536, loss = 0.05009765\n",
      "Iteration 1537, loss = 0.05008441\n",
      "Iteration 1538, loss = 0.05007120\n",
      "Iteration 1539, loss = 0.05005801\n",
      "Iteration 1540, loss = 0.05004484\n",
      "Iteration 1541, loss = 0.05003170\n",
      "Iteration 1542, loss = 0.05001858\n",
      "Iteration 1543, loss = 0.05000548\n",
      "Iteration 1544, loss = 0.04999241\n",
      "Iteration 1545, loss = 0.04997936\n",
      "Iteration 1546, loss = 0.04996633\n",
      "Iteration 1547, loss = 0.04995333\n",
      "Iteration 1548, loss = 0.04994034\n",
      "Iteration 1549, loss = 0.04992739\n",
      "Iteration 1550, loss = 0.04991445\n",
      "Iteration 1551, loss = 0.04990154\n",
      "Iteration 1552, loss = 0.04988864\n",
      "Iteration 1553, loss = 0.04987578\n",
      "Iteration 1554, loss = 0.04986293\n",
      "Iteration 1555, loss = 0.04985011\n",
      "Iteration 1556, loss = 0.04983731\n",
      "Iteration 1557, loss = 0.04982453\n",
      "Iteration 1558, loss = 0.04981177\n",
      "Iteration 1559, loss = 0.04979904\n",
      "Iteration 1560, loss = 0.04978632\n",
      "Iteration 1561, loss = 0.04977363\n",
      "Iteration 1562, loss = 0.04976097\n",
      "Iteration 1563, loss = 0.04974832\n",
      "Iteration 1564, loss = 0.04973570\n",
      "Iteration 1565, loss = 0.04972309\n",
      "Iteration 1566, loss = 0.04971051\n",
      "Iteration 1567, loss = 0.04969796\n",
      "Iteration 1568, loss = 0.04968542\n",
      "Iteration 1569, loss = 0.04967290\n",
      "Iteration 1570, loss = 0.04966041\n",
      "Iteration 1571, loss = 0.04964794\n",
      "Iteration 1572, loss = 0.04963549\n",
      "Iteration 1573, loss = 0.04962306\n",
      "Iteration 1574, loss = 0.04961065\n",
      "Iteration 1575, loss = 0.04959826\n",
      "Iteration 1576, loss = 0.04958590\n",
      "Iteration 1577, loss = 0.04957355\n",
      "Iteration 1578, loss = 0.04956123\n",
      "Iteration 1579, loss = 0.04954893\n",
      "Iteration 1580, loss = 0.04953665\n",
      "Iteration 1581, loss = 0.04952439\n",
      "Iteration 1582, loss = 0.04951215\n",
      "Iteration 1583, loss = 0.04949993\n",
      "Iteration 1584, loss = 0.04948773\n",
      "Iteration 1585, loss = 0.04947556\n",
      "Iteration 1586, loss = 0.04946340\n",
      "Iteration 1587, loss = 0.04945126\n",
      "Iteration 1588, loss = 0.04943915\n",
      "Iteration 1589, loss = 0.04942706\n",
      "Iteration 1590, loss = 0.04941498\n",
      "Iteration 1591, loss = 0.04940293\n",
      "Iteration 1592, loss = 0.04939090\n",
      "Iteration 1593, loss = 0.04937888\n",
      "Iteration 1594, loss = 0.04936689\n",
      "Iteration 1595, loss = 0.04935492\n",
      "Iteration 1596, loss = 0.04934297\n",
      "Iteration 1597, loss = 0.04933104\n",
      "Iteration 1598, loss = 0.04931912\n",
      "Iteration 1599, loss = 0.04930723\n",
      "Iteration 1600, loss = 0.04929536\n",
      "Iteration 1601, loss = 0.04928351\n",
      "Iteration 1602, loss = 0.04927168\n",
      "Iteration 1603, loss = 0.04925987\n",
      "Iteration 1604, loss = 0.04924807\n",
      "Iteration 1605, loss = 0.04923630\n",
      "Iteration 1606, loss = 0.04922455\n",
      "Iteration 1607, loss = 0.04921281\n",
      "Iteration 1608, loss = 0.04920110\n",
      "Iteration 1609, loss = 0.04918941\n",
      "Iteration 1610, loss = 0.04917773\n",
      "Iteration 1611, loss = 0.04916608\n",
      "Iteration 1612, loss = 0.04915444\n",
      "Iteration 1613, loss = 0.04914282\n",
      "Iteration 1614, loss = 0.04913123\n",
      "Iteration 1615, loss = 0.04911965\n",
      "Iteration 1616, loss = 0.04910809\n",
      "Iteration 1617, loss = 0.04909655\n",
      "Iteration 1618, loss = 0.04908503\n",
      "Iteration 1619, loss = 0.04907353\n",
      "Iteration 1620, loss = 0.04906204\n",
      "Iteration 1621, loss = 0.04905058\n",
      "Iteration 1622, loss = 0.04903914\n",
      "Iteration 1623, loss = 0.04902771\n",
      "Iteration 1624, loss = 0.04901630\n",
      "Iteration 1625, loss = 0.04900491\n",
      "Iteration 1626, loss = 0.04899354\n",
      "Iteration 1627, loss = 0.04898219\n",
      "Iteration 1628, loss = 0.04897086\n",
      "Iteration 1629, loss = 0.04895955\n",
      "Iteration 1630, loss = 0.04894825\n",
      "Iteration 1631, loss = 0.04893697\n",
      "Iteration 1632, loss = 0.04892571\n",
      "Iteration 1633, loss = 0.04891447\n",
      "Iteration 1634, loss = 0.04890325\n",
      "Iteration 1635, loss = 0.04889205\n",
      "Iteration 1636, loss = 0.04888086\n",
      "Iteration 1637, loss = 0.04886970\n",
      "Iteration 1638, loss = 0.04885855\n",
      "Iteration 1639, loss = 0.04884742\n",
      "Iteration 1640, loss = 0.04883630\n",
      "Iteration 1641, loss = 0.04882521\n",
      "Iteration 1642, loss = 0.04881413\n",
      "Iteration 1643, loss = 0.04880307\n",
      "Iteration 1644, loss = 0.04879203\n",
      "Iteration 1645, loss = 0.04878101\n",
      "Iteration 1646, loss = 0.04877000\n",
      "Iteration 1647, loss = 0.04875901\n",
      "Iteration 1648, loss = 0.04874804\n",
      "Iteration 1649, loss = 0.04873709\n",
      "Iteration 1650, loss = 0.04872615\n",
      "Iteration 1651, loss = 0.04871524\n",
      "Iteration 1652, loss = 0.04870434\n",
      "Iteration 1653, loss = 0.04869345\n",
      "Iteration 1654, loss = 0.04868259\n",
      "Iteration 1655, loss = 0.04867174\n",
      "Iteration 1656, loss = 0.04866091\n",
      "Iteration 1657, loss = 0.04865010\n",
      "Iteration 1658, loss = 0.04863930\n",
      "Iteration 1659, loss = 0.04862852\n",
      "Iteration 1660, loss = 0.04861776\n",
      "Iteration 1661, loss = 0.04860701\n",
      "Iteration 1662, loss = 0.04859629\n",
      "Iteration 1663, loss = 0.04858558\n",
      "Iteration 1664, loss = 0.04857488\n",
      "Iteration 1665, loss = 0.04856421\n",
      "Iteration 1666, loss = 0.04855355\n",
      "Iteration 1667, loss = 0.04854290\n",
      "Iteration 1668, loss = 0.04853228\n",
      "Iteration 1669, loss = 0.04852167\n",
      "Iteration 1670, loss = 0.04851108\n",
      "Iteration 1671, loss = 0.04850050\n",
      "Iteration 1672, loss = 0.04848994\n",
      "Iteration 1673, loss = 0.04847940\n",
      "Iteration 1674, loss = 0.04846887\n",
      "Iteration 1675, loss = 0.04845836\n",
      "Iteration 1676, loss = 0.04844787\n",
      "Iteration 1677, loss = 0.04843740\n",
      "Iteration 1678, loss = 0.04842694\n",
      "Iteration 1679, loss = 0.04841649\n",
      "Iteration 1680, loss = 0.04840606\n",
      "Iteration 1681, loss = 0.04839565\n",
      "Iteration 1682, loss = 0.04838526\n",
      "Iteration 1683, loss = 0.04837488\n",
      "Iteration 1684, loss = 0.04836452\n",
      "Iteration 1685, loss = 0.04835417\n",
      "Iteration 1686, loss = 0.04834384\n",
      "Iteration 1687, loss = 0.04833353\n",
      "Iteration 1688, loss = 0.04832323\n",
      "Iteration 1689, loss = 0.04831295\n",
      "Iteration 1690, loss = 0.04830268\n",
      "Iteration 1691, loss = 0.04829243\n",
      "Iteration 1692, loss = 0.04828220\n",
      "Iteration 1693, loss = 0.04827198\n",
      "Iteration 1694, loss = 0.04826177\n",
      "Iteration 1695, loss = 0.04825159\n",
      "Iteration 1696, loss = 0.04824142\n",
      "Iteration 1697, loss = 0.04823126\n",
      "Iteration 1698, loss = 0.04822112\n",
      "Iteration 1699, loss = 0.04821100\n",
      "Iteration 1700, loss = 0.04820089\n",
      "Iteration 1701, loss = 0.04819079\n",
      "Iteration 1702, loss = 0.04818072\n",
      "Iteration 1703, loss = 0.04817065\n",
      "Iteration 1704, loss = 0.04816061\n",
      "Iteration 1705, loss = 0.04815057\n",
      "Iteration 1706, loss = 0.04814056\n",
      "Iteration 1707, loss = 0.04813056\n",
      "Iteration 1708, loss = 0.04812057\n",
      "Iteration 1709, loss = 0.04811060\n",
      "Iteration 1710, loss = 0.04810064\n",
      "Iteration 1711, loss = 0.04809070\n",
      "Iteration 1712, loss = 0.04808078\n",
      "Iteration 1713, loss = 0.04807087\n",
      "Iteration 1714, loss = 0.04806097\n",
      "Iteration 1715, loss = 0.04805109\n",
      "Iteration 1716, loss = 0.04804123\n",
      "Iteration 1717, loss = 0.04803138\n",
      "Iteration 1718, loss = 0.04802154\n",
      "Iteration 1719, loss = 0.04801172\n",
      "Iteration 1720, loss = 0.04800191\n",
      "Iteration 1721, loss = 0.04799212\n",
      "Iteration 1722, loss = 0.04798235\n",
      "Iteration 1723, loss = 0.04797258\n",
      "Iteration 1724, loss = 0.04796284\n",
      "Iteration 1725, loss = 0.04795310\n",
      "Iteration 1726, loss = 0.04794339\n",
      "Iteration 1727, loss = 0.04793368\n",
      "Iteration 1728, loss = 0.04792399\n",
      "Iteration 1729, loss = 0.04791432\n",
      "Iteration 1730, loss = 0.04790466\n",
      "Iteration 1731, loss = 0.04789501\n",
      "Iteration 1732, loss = 0.04788538\n",
      "Iteration 1733, loss = 0.04787577\n",
      "Iteration 1734, loss = 0.04786616\n",
      "Iteration 1735, loss = 0.04785658\n",
      "Iteration 1736, loss = 0.04784700\n",
      "Iteration 1737, loss = 0.04783744\n",
      "Iteration 1738, loss = 0.04782790\n",
      "Iteration 1739, loss = 0.04781837\n",
      "Iteration 1740, loss = 0.04780885\n",
      "Iteration 1741, loss = 0.04779935\n",
      "Iteration 1742, loss = 0.04778986\n",
      "Iteration 1743, loss = 0.04778038\n",
      "Iteration 1744, loss = 0.04777092\n",
      "Iteration 1745, loss = 0.04776147\n",
      "Iteration 1746, loss = 0.04775204\n",
      "Iteration 1747, loss = 0.04774262\n",
      "Iteration 1748, loss = 0.04773321\n",
      "Iteration 1749, loss = 0.04772382\n",
      "Iteration 1750, loss = 0.04771444\n",
      "Iteration 1751, loss = 0.04770508\n",
      "Iteration 1752, loss = 0.04769573\n",
      "Iteration 1753, loss = 0.04768639\n",
      "Iteration 1754, loss = 0.04767707\n",
      "Iteration 1755, loss = 0.04766776\n",
      "Iteration 1756, loss = 0.04765846\n",
      "Iteration 1757, loss = 0.04764918\n",
      "Iteration 1758, loss = 0.04763991\n",
      "Iteration 1759, loss = 0.04763066\n",
      "Iteration 1760, loss = 0.04762142\n",
      "Iteration 1761, loss = 0.04761219\n",
      "Iteration 1762, loss = 0.04760297\n",
      "Iteration 1763, loss = 0.04759377\n",
      "Iteration 1764, loss = 0.04758458\n",
      "Iteration 1765, loss = 0.04757541\n",
      "Iteration 1766, loss = 0.04756624\n",
      "Iteration 1767, loss = 0.04755710\n",
      "Iteration 1768, loss = 0.04754796\n",
      "Iteration 1769, loss = 0.04753884\n",
      "Iteration 1770, loss = 0.04752973\n",
      "Iteration 1771, loss = 0.04752063\n",
      "Iteration 1772, loss = 0.04751155\n",
      "Iteration 1773, loss = 0.04750248\n",
      "Iteration 1774, loss = 0.04749342\n",
      "Iteration 1775, loss = 0.04748438\n",
      "Iteration 1776, loss = 0.04747535\n",
      "Iteration 1777, loss = 0.04746633\n",
      "Iteration 1778, loss = 0.04745732\n",
      "Iteration 1779, loss = 0.04744833\n",
      "Iteration 1780, loss = 0.04743935\n",
      "Iteration 1781, loss = 0.04743038\n",
      "Iteration 1782, loss = 0.04742143\n",
      "Iteration 1783, loss = 0.04741249\n",
      "Iteration 1784, loss = 0.04740356\n",
      "Iteration 1785, loss = 0.04739465\n",
      "Iteration 1786, loss = 0.04738574\n",
      "Iteration 1787, loss = 0.04737685\n",
      "Iteration 1788, loss = 0.04736797\n",
      "Iteration 1789, loss = 0.04735911\n",
      "Iteration 1790, loss = 0.04735025\n",
      "Iteration 1791, loss = 0.04734141\n",
      "Iteration 1792, loss = 0.04733259\n",
      "Iteration 1793, loss = 0.04732377\n",
      "Iteration 1794, loss = 0.04731497\n",
      "Iteration 1795, loss = 0.04730618\n",
      "Iteration 1796, loss = 0.04729740\n",
      "Iteration 1797, loss = 0.04728863\n",
      "Iteration 1798, loss = 0.04727988\n",
      "Iteration 1799, loss = 0.04727114\n",
      "Iteration 1800, loss = 0.04726241\n",
      "Iteration 1801, loss = 0.04725369\n",
      "Iteration 1802, loss = 0.04724499\n",
      "Iteration 1803, loss = 0.04723629\n",
      "Iteration 1804, loss = 0.04722761\n",
      "Iteration 1805, loss = 0.04721895\n",
      "Iteration 1806, loss = 0.04721029\n",
      "Iteration 1807, loss = 0.04720164\n",
      "Iteration 1808, loss = 0.04719301\n",
      "Iteration 1809, loss = 0.04718439\n",
      "Iteration 1810, loss = 0.04717578\n",
      "Iteration 1811, loss = 0.04716719\n",
      "Iteration 1812, loss = 0.04715860\n",
      "Iteration 1813, loss = 0.04715003\n",
      "Iteration 1814, loss = 0.04714147\n",
      "Iteration 1815, loss = 0.04713292\n",
      "Iteration 1816, loss = 0.04712438\n",
      "Iteration 1817, loss = 0.04711586\n",
      "Iteration 1818, loss = 0.04710734\n",
      "Iteration 1819, loss = 0.04709884\n",
      "Iteration 1820, loss = 0.04709035\n",
      "Iteration 1821, loss = 0.04708187\n",
      "Iteration 1822, loss = 0.04707341\n",
      "Iteration 1823, loss = 0.04706495\n",
      "Iteration 1824, loss = 0.04705651\n",
      "Iteration 1825, loss = 0.04704808\n",
      "Iteration 1826, loss = 0.04703966\n",
      "Iteration 1827, loss = 0.04703125\n",
      "Iteration 1828, loss = 0.04702285\n",
      "Iteration 1829, loss = 0.04701446\n",
      "Iteration 1830, loss = 0.04700609\n",
      "Iteration 1831, loss = 0.04699772\n",
      "Iteration 1832, loss = 0.04698937\n",
      "Iteration 1833, loss = 0.04698103\n",
      "Iteration 1834, loss = 0.04697270\n",
      "Iteration 1835, loss = 0.04696438\n",
      "Iteration 1836, loss = 0.04695608\n",
      "Iteration 1837, loss = 0.04694778\n",
      "Iteration 1838, loss = 0.04693950\n",
      "Iteration 1839, loss = 0.04693122\n",
      "Iteration 1840, loss = 0.04692296\n",
      "Iteration 1841, loss = 0.04691471\n",
      "Iteration 1842, loss = 0.04690647\n",
      "Iteration 1843, loss = 0.04689824\n",
      "Iteration 1844, loss = 0.04689003\n",
      "Iteration 1845, loss = 0.04688182\n",
      "Iteration 1846, loss = 0.04687362\n",
      "Iteration 1847, loss = 0.04686544\n",
      "Iteration 1848, loss = 0.04685727\n",
      "Iteration 1849, loss = 0.04684910\n",
      "Iteration 1850, loss = 0.04684095\n",
      "Iteration 1851, loss = 0.04683281\n",
      "Iteration 1852, loss = 0.04682468\n",
      "Iteration 1853, loss = 0.04681656\n",
      "Iteration 1854, loss = 0.04680845\n",
      "Iteration 1855, loss = 0.04680036\n",
      "Iteration 1856, loss = 0.04679227\n",
      "Iteration 1857, loss = 0.04678419\n",
      "Iteration 1858, loss = 0.04677613\n",
      "Iteration 1859, loss = 0.04676807\n",
      "Iteration 1860, loss = 0.04676003\n",
      "Iteration 1861, loss = 0.04675200\n",
      "Iteration 1862, loss = 0.04674397\n",
      "Iteration 1863, loss = 0.04673596\n",
      "Iteration 1864, loss = 0.04672796\n",
      "Iteration 1865, loss = 0.04671997\n",
      "Iteration 1866, loss = 0.04671199\n",
      "Iteration 1867, loss = 0.04670402\n",
      "Iteration 1868, loss = 0.04669606\n",
      "Iteration 1869, loss = 0.04668811\n",
      "Iteration 1870, loss = 0.04668017\n",
      "Iteration 1871, loss = 0.04667224\n",
      "Iteration 1872, loss = 0.04666432\n",
      "Iteration 1873, loss = 0.04665642\n",
      "Iteration 1874, loss = 0.04664852\n",
      "Iteration 1875, loss = 0.04664063\n",
      "Iteration 1876, loss = 0.04663276\n",
      "Iteration 1877, loss = 0.04662489\n",
      "Iteration 1878, loss = 0.04661704\n",
      "Iteration 1879, loss = 0.04660919\n",
      "Iteration 1880, loss = 0.04660135\n",
      "Iteration 1881, loss = 0.04659353\n",
      "Iteration 1882, loss = 0.04658571\n",
      "Iteration 1883, loss = 0.04657791\n",
      "Iteration 1884, loss = 0.04657011\n",
      "Iteration 1885, loss = 0.04656233\n",
      "Iteration 1886, loss = 0.04655455\n",
      "Iteration 1887, loss = 0.04654679\n",
      "Iteration 1888, loss = 0.04653903\n",
      "Iteration 1889, loss = 0.04653129\n",
      "Iteration 1890, loss = 0.04652355\n",
      "Iteration 1891, loss = 0.04651583\n",
      "Iteration 1892, loss = 0.04650812\n",
      "Iteration 1893, loss = 0.04650041\n",
      "Iteration 1894, loss = 0.04649272\n",
      "Iteration 1895, loss = 0.04648503\n",
      "Iteration 1896, loss = 0.04647735\n",
      "Iteration 1897, loss = 0.04646969\n",
      "Iteration 1898, loss = 0.04646203\n",
      "Iteration 1899, loss = 0.04645439\n",
      "Iteration 1900, loss = 0.04644675\n",
      "Iteration 1901, loss = 0.04643912\n",
      "Iteration 1902, loss = 0.04643151\n",
      "Iteration 1903, loss = 0.04642390\n",
      "Iteration 1904, loss = 0.04641630\n",
      "Iteration 1905, loss = 0.04640872\n",
      "Iteration 1906, loss = 0.04640114\n",
      "Iteration 1907, loss = 0.04639357\n",
      "Iteration 1908, loss = 0.04638601\n",
      "Iteration 1909, loss = 0.04637846\n",
      "Iteration 1910, loss = 0.04637092\n",
      "Iteration 1911, loss = 0.04636339\n",
      "Iteration 1912, loss = 0.04635587\n",
      "Iteration 1913, loss = 0.04634836\n",
      "Iteration 1914, loss = 0.04634086\n",
      "Iteration 1915, loss = 0.04633336\n",
      "Iteration 1916, loss = 0.04632588\n",
      "Iteration 1917, loss = 0.04631841\n",
      "Iteration 1918, loss = 0.04631094\n",
      "Iteration 1919, loss = 0.04630349\n",
      "Iteration 1920, loss = 0.04629604\n",
      "Iteration 1921, loss = 0.04628861\n",
      "Iteration 1922, loss = 0.04628118\n",
      "Iteration 1923, loss = 0.04627376\n",
      "Iteration 1924, loss = 0.04626635\n",
      "Iteration 1925, loss = 0.04625896\n",
      "Iteration 1926, loss = 0.04625157\n",
      "Iteration 1927, loss = 0.04624418\n",
      "Iteration 1928, loss = 0.04623681\n",
      "Iteration 1929, loss = 0.04622945\n",
      "Iteration 1930, loss = 0.04622210\n",
      "Iteration 1931, loss = 0.04621475\n",
      "Iteration 1932, loss = 0.04620742\n",
      "Iteration 1933, loss = 0.04620009\n",
      "Iteration 1934, loss = 0.04619278\n",
      "Iteration 1935, loss = 0.04618547\n",
      "Iteration 1936, loss = 0.04617817\n",
      "Iteration 1937, loss = 0.04617088\n",
      "Iteration 1938, loss = 0.04616360\n",
      "Iteration 1939, loss = 0.04615633\n",
      "Iteration 1940, loss = 0.04614906\n",
      "Iteration 1941, loss = 0.04614181\n",
      "Iteration 1942, loss = 0.04613456\n",
      "Iteration 1943, loss = 0.04612733\n",
      "Iteration 1944, loss = 0.04612010\n",
      "Iteration 1945, loss = 0.04611288\n",
      "Iteration 1946, loss = 0.04610567\n",
      "Iteration 1947, loss = 0.04609847\n",
      "Iteration 1948, loss = 0.04609128\n",
      "Iteration 1949, loss = 0.04608410\n",
      "Iteration 1950, loss = 0.04607692\n",
      "Iteration 1951, loss = 0.04606976\n",
      "Iteration 1952, loss = 0.04606260\n",
      "Iteration 1953, loss = 0.04605545\n",
      "Iteration 1954, loss = 0.04604831\n",
      "Iteration 1955, loss = 0.04604118\n",
      "Iteration 1956, loss = 0.04603406\n",
      "Iteration 1957, loss = 0.04602694\n",
      "Iteration 1958, loss = 0.04601984\n",
      "Iteration 1959, loss = 0.04601274\n",
      "Iteration 1960, loss = 0.04600565\n",
      "Iteration 1961, loss = 0.04599857\n",
      "Iteration 1962, loss = 0.04599150\n",
      "Iteration 1963, loss = 0.04598444\n",
      "Iteration 1964, loss = 0.04597739\n",
      "Iteration 1965, loss = 0.04597034\n",
      "Iteration 1966, loss = 0.04596330\n",
      "Iteration 1967, loss = 0.04595627\n",
      "Iteration 1968, loss = 0.04594925\n",
      "Iteration 1969, loss = 0.04594224\n",
      "Iteration 1970, loss = 0.04593524\n",
      "Iteration 1971, loss = 0.04592824\n",
      "Iteration 1972, loss = 0.04592126\n",
      "Iteration 1973, loss = 0.04591428\n",
      "Iteration 1974, loss = 0.04590731\n",
      "Iteration 1975, loss = 0.04590035\n",
      "Iteration 1976, loss = 0.04589339\n",
      "Iteration 1977, loss = 0.04588645\n",
      "Iteration 1978, loss = 0.04587951\n",
      "Iteration 1979, loss = 0.04587258\n",
      "Iteration 1980, loss = 0.04586566\n",
      "Iteration 1981, loss = 0.04585875\n",
      "Iteration 1982, loss = 0.04585184\n",
      "Iteration 1983, loss = 0.04584495\n",
      "Iteration 1984, loss = 0.04583806\n",
      "Iteration 1985, loss = 0.04583118\n",
      "Iteration 1986, loss = 0.04582431\n",
      "Iteration 1987, loss = 0.04581745\n",
      "Iteration 1988, loss = 0.04581059\n",
      "Iteration 1989, loss = 0.04580374\n",
      "Iteration 1990, loss = 0.04579690\n",
      "Iteration 1991, loss = 0.04579007\n",
      "Iteration 1992, loss = 0.04578325\n",
      "Iteration 1993, loss = 0.04577643\n",
      "Iteration 1994, loss = 0.04576962\n",
      "Iteration 1995, loss = 0.04576283\n",
      "Iteration 1996, loss = 0.04575603\n",
      "Iteration 1997, loss = 0.04574925\n",
      "Iteration 1998, loss = 0.04574247\n",
      "Iteration 1999, loss = 0.04573571\n",
      "Iteration 2000, loss = 0.04572895\n",
      "Iteration 2001, loss = 0.04572219\n",
      "Iteration 2002, loss = 0.04571545\n",
      "Iteration 2003, loss = 0.04570871\n",
      "Iteration 2004, loss = 0.04570198\n",
      "Iteration 2005, loss = 0.04569526\n",
      "Iteration 2006, loss = 0.04568855\n",
      "Iteration 2007, loss = 0.04568185\n",
      "Iteration 2008, loss = 0.04567515\n",
      "Iteration 2009, loss = 0.04566846\n",
      "Iteration 2010, loss = 0.04566178\n",
      "Iteration 2011, loss = 0.04565510\n",
      "Iteration 2012, loss = 0.04564843\n",
      "Iteration 2013, loss = 0.04564177\n",
      "Iteration 2014, loss = 0.04563512\n",
      "Iteration 2015, loss = 0.04562848\n",
      "Iteration 2016, loss = 0.04562184\n",
      "Iteration 2017, loss = 0.04561521\n",
      "Iteration 2018, loss = 0.04560859\n",
      "Iteration 2019, loss = 0.04560198\n",
      "Iteration 2020, loss = 0.04559537\n",
      "Iteration 2021, loss = 0.04558878\n",
      "Iteration 2022, loss = 0.04558218\n",
      "Iteration 2023, loss = 0.04557560\n",
      "Iteration 2024, loss = 0.04556902\n",
      "Iteration 2025, loss = 0.04556246\n",
      "Iteration 2026, loss = 0.04555590\n",
      "Iteration 2027, loss = 0.04554934\n",
      "Iteration 2028, loss = 0.04554280\n",
      "Iteration 2029, loss = 0.04553626\n",
      "Iteration 2030, loss = 0.04552973\n",
      "Iteration 2031, loss = 0.04552320\n",
      "Iteration 2032, loss = 0.04551668\n",
      "Iteration 2033, loss = 0.04551018\n",
      "Iteration 2034, loss = 0.04550367\n",
      "Iteration 2035, loss = 0.04549718\n",
      "Iteration 2036, loss = 0.04549069\n",
      "Iteration 2037, loss = 0.04548421\n",
      "Iteration 2038, loss = 0.04547774\n",
      "Iteration 2039, loss = 0.04547127\n",
      "Iteration 2040, loss = 0.04546482\n",
      "Iteration 2041, loss = 0.04545836\n",
      "Iteration 2042, loss = 0.04545192\n",
      "Iteration 2043, loss = 0.04544548\n",
      "Iteration 2044, loss = 0.04543905\n",
      "Iteration 2045, loss = 0.04543263\n",
      "Iteration 2046, loss = 0.04542622\n",
      "Iteration 2047, loss = 0.04541981\n",
      "Iteration 2048, loss = 0.04541341\n",
      "Iteration 2049, loss = 0.04540702\n",
      "Iteration 2050, loss = 0.04540063\n",
      "Iteration 2051, loss = 0.04539425\n",
      "Iteration 2052, loss = 0.04538788\n",
      "Iteration 2053, loss = 0.04538151\n",
      "Iteration 2054, loss = 0.04537515\n",
      "Iteration 2055, loss = 0.04536880\n",
      "Iteration 2056, loss = 0.04536246\n",
      "Iteration 2057, loss = 0.04535612\n",
      "Iteration 2058, loss = 0.04534979\n",
      "Iteration 2059, loss = 0.04534347\n",
      "Iteration 2060, loss = 0.04533715\n",
      "Iteration 2061, loss = 0.04533084\n",
      "Iteration 2062, loss = 0.04532454\n",
      "Iteration 2063, loss = 0.04531824\n",
      "Iteration 2064, loss = 0.04531195\n",
      "Iteration 2065, loss = 0.04530567\n",
      "Iteration 2066, loss = 0.04529940\n",
      "Iteration 2067, loss = 0.04529313\n",
      "Iteration 2068, loss = 0.04528687\n",
      "Iteration 2069, loss = 0.04528061\n",
      "Iteration 2070, loss = 0.04527437\n",
      "Iteration 2071, loss = 0.04526813\n",
      "Iteration 2072, loss = 0.04526189\n",
      "Iteration 2073, loss = 0.04525567\n",
      "Iteration 2074, loss = 0.04524945\n",
      "Iteration 2075, loss = 0.04524323\n",
      "Iteration 2076, loss = 0.04523703\n",
      "Iteration 2077, loss = 0.04523083\n",
      "Iteration 2078, loss = 0.04522463\n",
      "Iteration 2079, loss = 0.04521844\n",
      "Iteration 2080, loss = 0.04521226\n",
      "Iteration 2081, loss = 0.04520609\n",
      "Iteration 2082, loss = 0.04519992\n",
      "Iteration 2083, loss = 0.04519376\n",
      "Iteration 2084, loss = 0.04518761\n",
      "Iteration 2085, loss = 0.04518146\n",
      "Iteration 2086, loss = 0.04517532\n",
      "Iteration 2087, loss = 0.04516919\n",
      "Iteration 2088, loss = 0.04516306\n",
      "Iteration 2089, loss = 0.04515694\n",
      "Iteration 2090, loss = 0.04515083\n",
      "Iteration 2091, loss = 0.04514472\n",
      "Iteration 2092, loss = 0.04513862\n",
      "Iteration 2093, loss = 0.04513253\n",
      "Iteration 2094, loss = 0.04512644\n",
      "Iteration 2095, loss = 0.04512036\n",
      "Iteration 2096, loss = 0.04511428\n",
      "Iteration 2097, loss = 0.04510822\n",
      "Iteration 2098, loss = 0.04510215\n",
      "Iteration 2099, loss = 0.04509610\n",
      "Iteration 2100, loss = 0.04509005\n",
      "Iteration 2101, loss = 0.04508401\n",
      "Iteration 2102, loss = 0.04507797\n",
      "Iteration 2103, loss = 0.04507194\n",
      "Iteration 2104, loss = 0.04506592\n",
      "Iteration 2105, loss = 0.04505990\n",
      "Iteration 2106, loss = 0.04505389\n",
      "Iteration 2107, loss = 0.04504788\n",
      "Iteration 2108, loss = 0.04504189\n",
      "Iteration 2109, loss = 0.04503589\n",
      "Iteration 2110, loss = 0.04502991\n",
      "Iteration 2111, loss = 0.04502393\n",
      "Iteration 2112, loss = 0.04501796\n",
      "Iteration 2113, loss = 0.04501199\n",
      "Iteration 2114, loss = 0.04500603\n",
      "Iteration 2115, loss = 0.04500008\n",
      "Iteration 2116, loss = 0.04499413\n",
      "Iteration 2117, loss = 0.04498819\n",
      "Iteration 2118, loss = 0.04498225\n",
      "Iteration 2119, loss = 0.04497632\n",
      "Iteration 2120, loss = 0.04497040\n",
      "Iteration 2121, loss = 0.04496448\n",
      "Iteration 2122, loss = 0.04495857\n",
      "Iteration 2123, loss = 0.04495267\n",
      "Iteration 2124, loss = 0.04494677\n",
      "Iteration 2125, loss = 0.04494088\n",
      "Iteration 2126, loss = 0.04493499\n",
      "Iteration 2127, loss = 0.04492911\n",
      "Iteration 2128, loss = 0.04492324\n",
      "Iteration 2129, loss = 0.04491737\n",
      "Iteration 2130, loss = 0.04491151\n",
      "Iteration 2131, loss = 0.04490565\n",
      "Iteration 2132, loss = 0.04489980\n",
      "Iteration 2133, loss = 0.04489396\n",
      "Iteration 2134, loss = 0.04488812\n",
      "Iteration 2135, loss = 0.04488229\n",
      "Iteration 2136, loss = 0.04487646\n",
      "Iteration 2137, loss = 0.04487064\n",
      "Iteration 2138, loss = 0.04486483\n",
      "Iteration 2139, loss = 0.04485902\n",
      "Iteration 2140, loss = 0.04485322\n",
      "Iteration 2141, loss = 0.04484742\n",
      "Iteration 2142, loss = 0.04484163\n",
      "Iteration 2143, loss = 0.04483585\n",
      "Iteration 2144, loss = 0.04483007\n",
      "Iteration 2145, loss = 0.04482430\n",
      "Iteration 2146, loss = 0.04481853\n",
      "Iteration 2147, loss = 0.04481277\n",
      "Iteration 2148, loss = 0.04480702\n",
      "Iteration 2149, loss = 0.04480127\n",
      "Iteration 2150, loss = 0.04479552\n",
      "Iteration 2151, loss = 0.04478979\n",
      "Iteration 2152, loss = 0.04478405\n",
      "Iteration 2153, loss = 0.04477833\n",
      "Iteration 2154, loss = 0.04477261\n",
      "Iteration 2155, loss = 0.04476690\n",
      "Iteration 2156, loss = 0.04476119\n",
      "Iteration 2157, loss = 0.04475548\n",
      "Iteration 2158, loss = 0.04474979\n",
      "Iteration 2159, loss = 0.04474410\n",
      "Iteration 2160, loss = 0.04473841\n",
      "Iteration 2161, loss = 0.04473273\n",
      "Iteration 2162, loss = 0.04472706\n",
      "Iteration 2163, loss = 0.04472139\n",
      "Iteration 2164, loss = 0.04471573\n",
      "Iteration 2165, loss = 0.04471007\n",
      "Iteration 2166, loss = 0.04470442\n",
      "Iteration 2167, loss = 0.04469877\n",
      "Iteration 2168, loss = 0.04469313\n",
      "Iteration 2169, loss = 0.04468750\n",
      "Iteration 2170, loss = 0.04468187\n",
      "Iteration 2171, loss = 0.04467625\n",
      "Iteration 2172, loss = 0.04467063\n",
      "Iteration 2173, loss = 0.04466502\n",
      "Iteration 2174, loss = 0.04465941\n",
      "Iteration 2175, loss = 0.04465381\n",
      "Iteration 2176, loss = 0.04464822\n",
      "Iteration 2177, loss = 0.04464263\n",
      "Iteration 2178, loss = 0.04463704\n",
      "Iteration 2179, loss = 0.04463147\n",
      "Iteration 2180, loss = 0.04462589\n",
      "Iteration 2181, loss = 0.04462032\n",
      "Iteration 2182, loss = 0.04461476\n",
      "Iteration 2183, loss = 0.04460921\n",
      "Iteration 2184, loss = 0.04460366\n",
      "Iteration 2185, loss = 0.04459811\n",
      "Iteration 2186, loss = 0.04459257\n",
      "Iteration 2187, loss = 0.04458704\n",
      "Iteration 2188, loss = 0.04458151\n",
      "Iteration 2189, loss = 0.04457598\n",
      "Iteration 2190, loss = 0.04457046\n",
      "Iteration 2191, loss = 0.04456495\n",
      "Iteration 2192, loss = 0.04455944\n",
      "Iteration 2193, loss = 0.04455394\n",
      "Iteration 2194, loss = 0.04454844\n",
      "Iteration 2195, loss = 0.04454295\n",
      "Iteration 2196, loss = 0.04453747\n",
      "Iteration 2197, loss = 0.04453199\n",
      "Iteration 2198, loss = 0.04452651\n",
      "Iteration 2199, loss = 0.04452104\n",
      "Iteration 2200, loss = 0.04451558\n",
      "Iteration 2201, loss = 0.04451012\n",
      "Iteration 2202, loss = 0.04450466\n",
      "Iteration 2203, loss = 0.04449921\n",
      "Iteration 2204, loss = 0.04449377\n",
      "Iteration 2205, loss = 0.04448833\n",
      "Iteration 2206, loss = 0.04448290\n",
      "Iteration 2207, loss = 0.04447747\n",
      "Iteration 2208, loss = 0.04447205\n",
      "Iteration 2209, loss = 0.04446663\n",
      "Iteration 2210, loss = 0.04446122\n",
      "Iteration 2211, loss = 0.04445581\n",
      "Iteration 2212, loss = 0.04445041\n",
      "Iteration 2213, loss = 0.04444501\n",
      "Iteration 2214, loss = 0.04443962\n",
      "Iteration 2215, loss = 0.04443424\n",
      "Iteration 2216, loss = 0.04442885\n",
      "Iteration 2217, loss = 0.04442348\n",
      "Iteration 2218, loss = 0.04441811\n",
      "Iteration 2219, loss = 0.04441274\n",
      "Iteration 2220, loss = 0.04440738\n",
      "Iteration 2221, loss = 0.04440203\n",
      "Iteration 2222, loss = 0.04439668\n",
      "Iteration 2223, loss = 0.04439133\n",
      "Iteration 2224, loss = 0.04438599\n",
      "Iteration 2225, loss = 0.04438066\n",
      "Iteration 2226, loss = 0.04437533\n",
      "Iteration 2227, loss = 0.04437000\n",
      "Iteration 2228, loss = 0.04436468\n",
      "Iteration 2229, loss = 0.04435937\n",
      "Iteration 2230, loss = 0.04435406\n",
      "Iteration 2231, loss = 0.04434875\n",
      "Iteration 2232, loss = 0.04434345\n",
      "Iteration 2233, loss = 0.04433816\n",
      "Iteration 2234, loss = 0.04433287\n",
      "Iteration 2235, loss = 0.04432758\n",
      "Iteration 2236, loss = 0.04432230\n",
      "Iteration 2237, loss = 0.04431703\n",
      "Iteration 2238, loss = 0.04431176\n",
      "Iteration 2239, loss = 0.04430649\n",
      "Iteration 2240, loss = 0.04430123\n",
      "Iteration 2241, loss = 0.04429598\n",
      "Iteration 2242, loss = 0.04429073\n",
      "Iteration 2243, loss = 0.04428548\n",
      "Iteration 2244, loss = 0.04428024\n",
      "Iteration 2245, loss = 0.04427501\n",
      "Iteration 2246, loss = 0.04426978\n",
      "Iteration 2247, loss = 0.04426455\n",
      "Iteration 2248, loss = 0.04425933\n",
      "Iteration 2249, loss = 0.04425412\n",
      "Iteration 2250, loss = 0.04424891\n",
      "Iteration 2251, loss = 0.04424370\n",
      "Iteration 2252, loss = 0.04423850\n",
      "Iteration 2253, loss = 0.04423330\n",
      "Iteration 2254, loss = 0.04422811\n",
      "Iteration 2255, loss = 0.04422292\n",
      "Iteration 2256, loss = 0.04421774\n",
      "Iteration 2257, loss = 0.04421256\n",
      "Iteration 2258, loss = 0.04420739\n",
      "Iteration 2259, loss = 0.04420222\n",
      "Iteration 2260, loss = 0.04419706\n",
      "Iteration 2261, loss = 0.04419190\n",
      "Iteration 2262, loss = 0.04418675\n",
      "Iteration 2263, loss = 0.04418160\n",
      "Iteration 2264, loss = 0.04417646\n",
      "Iteration 2265, loss = 0.04417132\n",
      "Iteration 2266, loss = 0.04416618\n",
      "Iteration 2267, loss = 0.04416106\n",
      "Iteration 2268, loss = 0.04415593\n",
      "Iteration 2269, loss = 0.04415081\n",
      "Iteration 2270, loss = 0.04414569\n",
      "Iteration 2271, loss = 0.04414058\n",
      "Iteration 2272, loss = 0.04413548\n",
      "Iteration 2273, loss = 0.04413038\n",
      "Iteration 2274, loss = 0.04412528\n",
      "Iteration 2275, loss = 0.04412019\n",
      "Iteration 2276, loss = 0.04411510\n",
      "Iteration 2277, loss = 0.04411002\n",
      "Iteration 2278, loss = 0.04410494\n",
      "Iteration 2279, loss = 0.04409986\n",
      "Iteration 2280, loss = 0.04409479\n",
      "Iteration 2281, loss = 0.04408973\n",
      "Iteration 2282, loss = 0.04408467\n",
      "Iteration 2283, loss = 0.04407962\n",
      "Iteration 2284, loss = 0.04407456\n",
      "Iteration 2285, loss = 0.04406952\n",
      "Iteration 2286, loss = 0.04406448\n",
      "Iteration 2287, loss = 0.04405944\n",
      "Iteration 2288, loss = 0.04405441\n",
      "Iteration 2289, loss = 0.04404938\n",
      "Iteration 2290, loss = 0.04404436\n",
      "Iteration 2291, loss = 0.04403934\n",
      "Iteration 2292, loss = 0.04403432\n",
      "Iteration 2293, loss = 0.04402931\n",
      "Iteration 2294, loss = 0.04402431\n",
      "Iteration 2295, loss = 0.04401931\n",
      "Iteration 2296, loss = 0.04401431\n",
      "Iteration 2297, loss = 0.04400932\n",
      "Iteration 2298, loss = 0.04400433\n",
      "Iteration 2299, loss = 0.04399935\n",
      "Iteration 2300, loss = 0.04399437\n",
      "Iteration 2301, loss = 0.04398939\n",
      "Iteration 2302, loss = 0.04398442\n",
      "Iteration 2303, loss = 0.04397946\n",
      "Iteration 2304, loss = 0.04397450\n",
      "Iteration 2305, loss = 0.04396954\n",
      "Iteration 2306, loss = 0.04396459\n",
      "Iteration 2307, loss = 0.04395964\n",
      "Iteration 2308, loss = 0.04395470\n",
      "Iteration 2309, loss = 0.04394976\n",
      "Iteration 2310, loss = 0.04394483\n",
      "Iteration 2311, loss = 0.04393990\n",
      "Iteration 2312, loss = 0.04393497\n",
      "Iteration 2313, loss = 0.04393005\n",
      "Iteration 2314, loss = 0.04392513\n",
      "Iteration 2315, loss = 0.04392022\n",
      "Iteration 2316, loss = 0.04391531\n",
      "Iteration 2317, loss = 0.04391041\n",
      "Iteration 2318, loss = 0.04390551\n",
      "Iteration 2319, loss = 0.04390061\n",
      "Iteration 2320, loss = 0.04389572\n",
      "Iteration 2321, loss = 0.04389083\n",
      "Iteration 2322, loss = 0.04388595\n",
      "Iteration 2323, loss = 0.04388107\n",
      "Iteration 2324, loss = 0.04387620\n",
      "Iteration 2325, loss = 0.04387133\n",
      "Iteration 2326, loss = 0.04386646\n",
      "Iteration 2327, loss = 0.04386160\n",
      "Iteration 2328, loss = 0.04385674\n",
      "Iteration 2329, loss = 0.04385189\n",
      "Iteration 2330, loss = 0.04384704\n",
      "Iteration 2331, loss = 0.04384220\n",
      "Iteration 2332, loss = 0.04383736\n",
      "Iteration 2333, loss = 0.04383252\n",
      "Iteration 2334, loss = 0.04382769\n",
      "Iteration 2335, loss = 0.04382286\n",
      "Iteration 2336, loss = 0.04381804\n",
      "Iteration 2337, loss = 0.04381322\n",
      "Iteration 2338, loss = 0.04380841\n",
      "Iteration 2339, loss = 0.04380360\n",
      "Iteration 2340, loss = 0.04379879\n",
      "Iteration 2341, loss = 0.04379399\n",
      "Iteration 2342, loss = 0.04378919\n",
      "Iteration 2343, loss = 0.04378439\n",
      "Iteration 2344, loss = 0.04377960\n",
      "Iteration 2345, loss = 0.04377482\n",
      "Iteration 2346, loss = 0.04377004\n",
      "Iteration 2347, loss = 0.04376526\n",
      "Iteration 2348, loss = 0.04376048\n",
      "Iteration 2349, loss = 0.04375572\n",
      "Iteration 2350, loss = 0.04375095\n",
      "Iteration 2351, loss = 0.04374619\n",
      "Iteration 2352, loss = 0.04374143\n",
      "Iteration 2353, loss = 0.04373668\n",
      "Iteration 2354, loss = 0.04373193\n",
      "Iteration 2355, loss = 0.04372718\n",
      "Iteration 2356, loss = 0.04372244\n",
      "Iteration 2357, loss = 0.04371771\n",
      "Iteration 2358, loss = 0.04371297\n",
      "Iteration 2359, loss = 0.04370824\n",
      "Iteration 2360, loss = 0.04370352\n",
      "Iteration 2361, loss = 0.04369880\n",
      "Iteration 2362, loss = 0.04369408\n",
      "Iteration 2363, loss = 0.04368937\n",
      "Iteration 2364, loss = 0.04368466\n",
      "Iteration 2365, loss = 0.04367996\n",
      "Iteration 2366, loss = 0.04367526\n",
      "Iteration 2367, loss = 0.04367056\n",
      "Iteration 2368, loss = 0.04366587\n",
      "Iteration 2369, loss = 0.04366118\n",
      "Iteration 2370, loss = 0.04365649\n",
      "Iteration 2371, loss = 0.04365181\n",
      "Iteration 2372, loss = 0.04364713\n",
      "Iteration 2373, loss = 0.04364246\n",
      "Iteration 2374, loss = 0.04363779\n",
      "Iteration 2375, loss = 0.04363313\n",
      "Iteration 2376, loss = 0.04362846\n",
      "Iteration 2377, loss = 0.04362381\n",
      "Iteration 2378, loss = 0.04361915\n",
      "Iteration 2379, loss = 0.04361450\n",
      "Iteration 2380, loss = 0.04360986\n",
      "Iteration 2381, loss = 0.04360522\n",
      "Iteration 2382, loss = 0.04360058\n",
      "Iteration 2383, loss = 0.04359594\n",
      "Iteration 2384, loss = 0.04359131\n",
      "Iteration 2385, loss = 0.04358669\n",
      "Iteration 2386, loss = 0.04358207\n",
      "Iteration 2387, loss = 0.04357745\n",
      "Iteration 2388, loss = 0.04357283\n",
      "Iteration 2389, loss = 0.04356822\n",
      "Iteration 2390, loss = 0.04356361\n",
      "Iteration 2391, loss = 0.04355901\n",
      "Iteration 2392, loss = 0.04355441\n",
      "Iteration 2393, loss = 0.04354982\n",
      "Iteration 2394, loss = 0.04354522\n",
      "Iteration 2395, loss = 0.04354064\n",
      "Iteration 2396, loss = 0.04353605\n",
      "Iteration 2397, loss = 0.04353147\n",
      "Iteration 2398, loss = 0.04352690\n",
      "Iteration 2399, loss = 0.04352232\n",
      "Iteration 2400, loss = 0.04351775\n",
      "Iteration 2401, loss = 0.04351319\n",
      "Iteration 2402, loss = 0.04350863\n",
      "Iteration 2403, loss = 0.04350407\n",
      "Iteration 2404, loss = 0.04349951\n",
      "Iteration 2405, loss = 0.04349496\n",
      "Iteration 2406, loss = 0.04349042\n",
      "Iteration 2407, loss = 0.04348587\n",
      "Iteration 2408, loss = 0.04348134\n",
      "Iteration 2409, loss = 0.04347680\n",
      "Iteration 2410, loss = 0.04347227\n",
      "Iteration 2411, loss = 0.04346774\n",
      "Iteration 2412, loss = 0.04346322\n",
      "Iteration 2413, loss = 0.04345870\n",
      "Iteration 2414, loss = 0.04345418\n",
      "Iteration 2415, loss = 0.04344967\n",
      "Iteration 2416, loss = 0.04344516\n",
      "Iteration 2417, loss = 0.04344065\n",
      "Iteration 2418, loss = 0.04343615\n",
      "Iteration 2419, loss = 0.04343165\n",
      "Iteration 2420, loss = 0.04342715\n",
      "Iteration 2421, loss = 0.04342266\n",
      "Iteration 2422, loss = 0.04341818\n",
      "Iteration 2423, loss = 0.04341369\n",
      "Iteration 2424, loss = 0.04340921\n",
      "Iteration 2425, loss = 0.04340474\n",
      "Iteration 2426, loss = 0.04340026\n",
      "Iteration 2427, loss = 0.04339579\n",
      "Iteration 2428, loss = 0.04339133\n",
      "Iteration 2429, loss = 0.04338686\n",
      "Iteration 2430, loss = 0.04338241\n",
      "Iteration 2431, loss = 0.04337795\n",
      "Iteration 2432, loss = 0.04337350\n",
      "Iteration 2433, loss = 0.04336905\n",
      "Iteration 2434, loss = 0.04336461\n",
      "Iteration 2435, loss = 0.04336017\n",
      "Iteration 2436, loss = 0.04335573\n",
      "Iteration 2437, loss = 0.04335130\n",
      "Iteration 2438, loss = 0.04334687\n",
      "Iteration 2439, loss = 0.04334244\n",
      "Iteration 2440, loss = 0.04333802\n",
      "Iteration 2441, loss = 0.04333360\n",
      "Iteration 2442, loss = 0.04332918\n",
      "Iteration 2443, loss = 0.04332477\n",
      "Iteration 2444, loss = 0.04332036\n",
      "Iteration 2445, loss = 0.04331595\n",
      "Iteration 2446, loss = 0.04331155\n",
      "Iteration 2447, loss = 0.04330715\n",
      "Iteration 2448, loss = 0.04330276\n",
      "Iteration 2449, loss = 0.04329837\n",
      "Iteration 2450, loss = 0.04329398\n",
      "Iteration 2451, loss = 0.04328960\n",
      "Iteration 2452, loss = 0.04328521\n",
      "Iteration 2453, loss = 0.04328084\n",
      "Iteration 2454, loss = 0.04327646\n",
      "Iteration 2455, loss = 0.04327209\n",
      "Iteration 2456, loss = 0.04326773\n",
      "Iteration 2457, loss = 0.04326336\n",
      "Iteration 2458, loss = 0.04325900\n",
      "Iteration 2459, loss = 0.04325465\n",
      "Iteration 2460, loss = 0.04325029\n",
      "Iteration 2461, loss = 0.04324594\n",
      "Iteration 2462, loss = 0.04324160\n",
      "Iteration 2463, loss = 0.04323725\n",
      "Iteration 2464, loss = 0.04323291\n",
      "Iteration 2465, loss = 0.04322858\n",
      "Iteration 2466, loss = 0.04322424\n",
      "Iteration 2467, loss = 0.04321991\n",
      "Iteration 2468, loss = 0.04321559\n",
      "Iteration 2469, loss = 0.04321127\n",
      "Iteration 2470, loss = 0.04320695\n",
      "Iteration 2471, loss = 0.04320263\n",
      "Iteration 2472, loss = 0.04319832\n",
      "Iteration 2473, loss = 0.04319401\n",
      "Iteration 2474, loss = 0.04318970\n",
      "Iteration 2475, loss = 0.04318540\n",
      "Iteration 2476, loss = 0.04318110\n",
      "Iteration 2477, loss = 0.04317681\n",
      "Iteration 2478, loss = 0.04317251\n",
      "Iteration 2479, loss = 0.04316823\n",
      "Iteration 2480, loss = 0.04316394\n",
      "Iteration 2481, loss = 0.04315966\n",
      "Iteration 2482, loss = 0.04315538\n",
      "Iteration 2483, loss = 0.04315110\n",
      "Iteration 2484, loss = 0.04314683\n",
      "Iteration 2485, loss = 0.04314256\n",
      "Iteration 2486, loss = 0.04313830\n",
      "Iteration 2487, loss = 0.04313403\n",
      "Iteration 2488, loss = 0.04312977\n",
      "Iteration 2489, loss = 0.04312552\n",
      "Iteration 2490, loss = 0.04312127\n",
      "Iteration 2491, loss = 0.04311702\n",
      "Iteration 2492, loss = 0.04311277\n",
      "Iteration 2493, loss = 0.04310853\n",
      "Iteration 2494, loss = 0.04310429\n",
      "Iteration 2495, loss = 0.04310005\n",
      "Iteration 2496, loss = 0.04309582\n",
      "Iteration 2497, loss = 0.04309159\n",
      "Iteration 2498, loss = 0.04308736\n",
      "Iteration 2499, loss = 0.04308314\n",
      "Iteration 2500, loss = 0.04307892\n",
      "Iteration 2501, loss = 0.04307470\n",
      "Iteration 2502, loss = 0.04307049\n",
      "Iteration 2503, loss = 0.04306628\n",
      "Iteration 2504, loss = 0.04306207\n",
      "Iteration 2505, loss = 0.04305787\n",
      "Iteration 2506, loss = 0.04305366\n",
      "Iteration 2507, loss = 0.04304947\n",
      "Iteration 2508, loss = 0.04304527\n",
      "Iteration 2509, loss = 0.04304108\n",
      "Iteration 2510, loss = 0.04303689\n",
      "Iteration 2511, loss = 0.04303271\n",
      "Iteration 2512, loss = 0.04302853\n",
      "Iteration 2513, loss = 0.04302435\n",
      "Iteration 2514, loss = 0.04302017\n",
      "Iteration 2515, loss = 0.04301600\n",
      "Iteration 2516, loss = 0.04301183\n",
      "Iteration 2517, loss = 0.04300767\n",
      "Iteration 2518, loss = 0.04300350\n",
      "Iteration 2519, loss = 0.04299934\n",
      "Iteration 2520, loss = 0.04299519\n",
      "Iteration 2521, loss = 0.04299103\n",
      "Iteration 2522, loss = 0.04298688\n",
      "Iteration 2523, loss = 0.04298274\n",
      "Iteration 2524, loss = 0.04297859\n",
      "Iteration 2525, loss = 0.04297445\n",
      "Iteration 2526, loss = 0.04297031\n",
      "Iteration 2527, loss = 0.04296618\n",
      "Iteration 2528, loss = 0.04296205\n",
      "Iteration 2529, loss = 0.04295792\n",
      "Iteration 2530, loss = 0.04295379\n",
      "Iteration 2531, loss = 0.04294967\n",
      "Iteration 2532, loss = 0.04294555\n",
      "Iteration 2533, loss = 0.04294144\n",
      "Iteration 2534, loss = 0.04293732\n",
      "Iteration 2535, loss = 0.04293321\n",
      "Iteration 2536, loss = 0.04292911\n",
      "Iteration 2537, loss = 0.04292500\n",
      "Iteration 2538, loss = 0.04292090\n",
      "Iteration 2539, loss = 0.04291680\n",
      "Iteration 2540, loss = 0.04291271\n",
      "Iteration 2541, loss = 0.04290862\n",
      "Iteration 2542, loss = 0.04290453\n",
      "Iteration 2543, loss = 0.04290044\n",
      "Iteration 2544, loss = 0.04289636\n",
      "Iteration 2545, loss = 0.04289228\n",
      "Iteration 2546, loss = 0.04288821\n",
      "Iteration 2547, loss = 0.04288413\n",
      "Iteration 2548, loss = 0.04288006\n",
      "Iteration 2549, loss = 0.04287599\n",
      "Iteration 2550, loss = 0.04287193\n",
      "Iteration 2551, loss = 0.04286787\n",
      "Iteration 2552, loss = 0.04286381\n",
      "Iteration 2553, loss = 0.04285975\n",
      "Iteration 2554, loss = 0.04285570\n",
      "Iteration 2555, loss = 0.04285165\n",
      "Iteration 2556, loss = 0.04284761\n",
      "Iteration 2557, loss = 0.04284356\n",
      "Iteration 2558, loss = 0.04283952\n",
      "Iteration 2559, loss = 0.04283548\n",
      "Iteration 2560, loss = 0.04283145\n",
      "Iteration 2561, loss = 0.04282742\n",
      "Iteration 2562, loss = 0.04282339\n",
      "Iteration 2563, loss = 0.04281936\n",
      "Iteration 2564, loss = 0.04281534\n",
      "Iteration 2565, loss = 0.04281132\n",
      "Iteration 2566, loss = 0.04280730\n",
      "Iteration 2567, loss = 0.04280329\n",
      "Iteration 2568, loss = 0.04279928\n",
      "Iteration 2569, loss = 0.04279527\n",
      "Iteration 2570, loss = 0.04279126\n",
      "Iteration 2571, loss = 0.04278726\n",
      "Iteration 2572, loss = 0.04278326\n",
      "Iteration 2573, loss = 0.04277927\n",
      "Iteration 2574, loss = 0.04277527\n",
      "Iteration 2575, loss = 0.04277128\n",
      "Iteration 2576, loss = 0.04276729\n",
      "Iteration 2577, loss = 0.04276331\n",
      "Iteration 2578, loss = 0.04275933\n",
      "Iteration 2579, loss = 0.04275535\n",
      "Iteration 2580, loss = 0.04275137\n",
      "Iteration 2581, loss = 0.04274740\n",
      "Iteration 2582, loss = 0.04274343\n",
      "Iteration 2583, loss = 0.04273946\n",
      "Iteration 2584, loss = 0.04273549\n",
      "Iteration 2585, loss = 0.04273153\n",
      "Iteration 2586, loss = 0.04272757\n",
      "Iteration 2587, loss = 0.04272362\n",
      "Iteration 2588, loss = 0.04271966\n",
      "Iteration 2589, loss = 0.04271571\n",
      "Iteration 2590, loss = 0.04271176\n",
      "Iteration 2591, loss = 0.04270782\n",
      "Iteration 2592, loss = 0.04270388\n",
      "Iteration 2593, loss = 0.04269994\n",
      "Iteration 2594, loss = 0.04269600\n",
      "Iteration 2595, loss = 0.04269207\n",
      "Iteration 2596, loss = 0.04268814\n",
      "Iteration 2597, loss = 0.04268421\n",
      "Iteration 2598, loss = 0.04268028\n",
      "Iteration 2599, loss = 0.04267636\n",
      "Iteration 2600, loss = 0.04267244\n",
      "Iteration 2601, loss = 0.04266852\n",
      "Iteration 2602, loss = 0.04266461\n",
      "Iteration 2603, loss = 0.04266070\n",
      "Iteration 2604, loss = 0.04265679\n",
      "Iteration 2605, loss = 0.04265288\n",
      "Iteration 2606, loss = 0.04264898\n",
      "Iteration 2607, loss = 0.04264508\n",
      "Iteration 2608, loss = 0.04264118\n",
      "Iteration 2609, loss = 0.04263729\n",
      "Iteration 2610, loss = 0.04263340\n",
      "Iteration 2611, loss = 0.04262951\n",
      "Iteration 2612, loss = 0.04262562\n",
      "Iteration 2613, loss = 0.04262174\n",
      "Iteration 2614, loss = 0.04261786\n",
      "Iteration 2615, loss = 0.04261398\n",
      "Iteration 2616, loss = 0.04261010\n",
      "Iteration 2617, loss = 0.04260623\n",
      "Iteration 2618, loss = 0.04260236\n",
      "Iteration 2619, loss = 0.04259849\n",
      "Iteration 2620, loss = 0.04259463\n",
      "Iteration 2621, loss = 0.04259077\n",
      "Iteration 2622, loss = 0.04258691\n",
      "Iteration 2623, loss = 0.04258305\n",
      "Iteration 2624, loss = 0.04257920\n",
      "Iteration 2625, loss = 0.04257535\n",
      "Iteration 2626, loss = 0.04257150\n",
      "Iteration 2627, loss = 0.04256765\n",
      "Iteration 2628, loss = 0.04256381\n",
      "Iteration 2629, loss = 0.04255997\n",
      "Iteration 2630, loss = 0.04255613\n",
      "Iteration 2631, loss = 0.04255230\n",
      "Iteration 2632, loss = 0.04254846\n",
      "Iteration 2633, loss = 0.04254463\n",
      "Iteration 2634, loss = 0.04254081\n",
      "Iteration 2635, loss = 0.04253698\n",
      "Iteration 2636, loss = 0.04253316\n",
      "Iteration 2637, loss = 0.04252934\n",
      "Iteration 2638, loss = 0.04252553\n",
      "Iteration 2639, loss = 0.04252171\n",
      "Iteration 2640, loss = 0.04251790\n",
      "Iteration 2641, loss = 0.04251409\n",
      "Iteration 2642, loss = 0.04251029\n",
      "Iteration 2643, loss = 0.04250649\n",
      "Iteration 2644, loss = 0.04250268\n",
      "Iteration 2645, loss = 0.04249889\n",
      "Iteration 2646, loss = 0.04249509\n",
      "Iteration 2647, loss = 0.04249130\n",
      "Iteration 2648, loss = 0.04248751\n",
      "Iteration 2649, loss = 0.04248372\n",
      "Iteration 2650, loss = 0.04247994\n",
      "Iteration 2651, loss = 0.04247615\n",
      "Iteration 2652, loss = 0.04247238\n",
      "Iteration 2653, loss = 0.04246860\n",
      "Iteration 2654, loss = 0.04246482\n",
      "Iteration 2655, loss = 0.04246105\n",
      "Iteration 2656, loss = 0.04245728\n",
      "Iteration 2657, loss = 0.04245352\n",
      "Iteration 2658, loss = 0.04244975\n",
      "Iteration 2659, loss = 0.04244599\n",
      "Iteration 2660, loss = 0.04244223\n",
      "Iteration 2661, loss = 0.04243848\n",
      "Iteration 2662, loss = 0.04243472\n",
      "Iteration 2663, loss = 0.04243097\n",
      "Iteration 2664, loss = 0.04242722\n",
      "Iteration 2665, loss = 0.04242348\n",
      "Iteration 2666, loss = 0.04241973\n",
      "Iteration 2667, loss = 0.04241599\n",
      "Iteration 2668, loss = 0.04241225\n",
      "Iteration 2669, loss = 0.04240852\n",
      "Iteration 2670, loss = 0.04240478\n",
      "Iteration 2671, loss = 0.04240105\n",
      "Iteration 2672, loss = 0.04239733\n",
      "Iteration 2673, loss = 0.04239360\n",
      "Iteration 2674, loss = 0.04238988\n",
      "Iteration 2675, loss = 0.04238616\n",
      "Iteration 2676, loss = 0.04238244\n",
      "Iteration 2677, loss = 0.04237872\n",
      "Iteration 2678, loss = 0.04237501\n",
      "Iteration 2679, loss = 0.04237130\n",
      "Iteration 2680, loss = 0.04236759\n",
      "Iteration 2681, loss = 0.04236388\n",
      "Iteration 2682, loss = 0.04236018\n",
      "Iteration 2683, loss = 0.04235648\n",
      "Iteration 2684, loss = 0.04235278\n",
      "Iteration 2685, loss = 0.04234909\n",
      "Iteration 2686, loss = 0.04234539\n",
      "Iteration 2687, loss = 0.04234170\n",
      "Iteration 2688, loss = 0.04233801\n",
      "Iteration 2689, loss = 0.04233433\n",
      "Iteration 2690, loss = 0.04233065\n",
      "Iteration 2691, loss = 0.04232696\n",
      "Iteration 2692, loss = 0.04232329\n",
      "Iteration 2693, loss = 0.04231961\n",
      "Iteration 2694, loss = 0.04231594\n",
      "Iteration 2695, loss = 0.04231227\n",
      "Iteration 2696, loss = 0.04230860\n",
      "Iteration 2697, loss = 0.04230493\n",
      "Iteration 2698, loss = 0.04230127\n",
      "Iteration 2699, loss = 0.04229761\n",
      "Iteration 2700, loss = 0.04229395\n",
      "Iteration 2701, loss = 0.04229029\n",
      "Iteration 2702, loss = 0.04228664\n",
      "Iteration 2703, loss = 0.04228299\n",
      "Iteration 2704, loss = 0.04227934\n",
      "Iteration 2705, loss = 0.04227569\n",
      "Iteration 2706, loss = 0.04227205\n",
      "Iteration 2707, loss = 0.04226840\n",
      "Iteration 2708, loss = 0.04226477\n",
      "Iteration 2709, loss = 0.04226113\n",
      "Iteration 2710, loss = 0.04225749\n",
      "Iteration 2711, loss = 0.04225386\n",
      "Iteration 2712, loss = 0.04225023\n",
      "Iteration 2713, loss = 0.04224660\n",
      "Iteration 2714, loss = 0.04224298\n",
      "Iteration 2715, loss = 0.04223936\n",
      "Iteration 2716, loss = 0.04223574\n",
      "Iteration 2717, loss = 0.04223212\n",
      "Iteration 2718, loss = 0.04222850\n",
      "Iteration 2719, loss = 0.04222489\n",
      "Iteration 2720, loss = 0.04222128\n",
      "Iteration 2721, loss = 0.04221767\n",
      "Iteration 2722, loss = 0.04221407\n",
      "Iteration 2723, loss = 0.04221046\n",
      "Iteration 2724, loss = 0.04220686\n",
      "Iteration 2725, loss = 0.04220326\n",
      "Iteration 2726, loss = 0.04219967\n",
      "Iteration 2727, loss = 0.04219607\n",
      "Iteration 2728, loss = 0.04219248\n",
      "Iteration 2729, loss = 0.04218889\n",
      "Iteration 2730, loss = 0.04218530\n",
      "Iteration 2731, loss = 0.04218172\n",
      "Iteration 2732, loss = 0.04217814\n",
      "Iteration 2733, loss = 0.04217456\n",
      "Iteration 2734, loss = 0.04217098\n",
      "Iteration 2735, loss = 0.04216740\n",
      "Iteration 2736, loss = 0.04216383\n",
      "Iteration 2737, loss = 0.04216026\n",
      "Iteration 2738, loss = 0.04215669\n",
      "Iteration 2739, loss = 0.04215312\n",
      "Iteration 2740, loss = 0.04214956\n",
      "Iteration 2741, loss = 0.04214600\n",
      "Iteration 2742, loss = 0.04214244\n",
      "Iteration 2743, loss = 0.04213888\n",
      "Iteration 2744, loss = 0.04213533\n",
      "Iteration 2745, loss = 0.04213178\n",
      "Iteration 2746, loss = 0.04212823\n",
      "Iteration 2747, loss = 0.04212468\n",
      "Iteration 2748, loss = 0.04212113\n",
      "Iteration 2749, loss = 0.04211759\n",
      "Iteration 2750, loss = 0.04211405\n",
      "Iteration 2751, loss = 0.04211051\n",
      "Iteration 2752, loss = 0.04210697\n",
      "Iteration 2753, loss = 0.04210344\n",
      "Iteration 2754, loss = 0.04209991\n",
      "Iteration 2755, loss = 0.04209638\n",
      "Iteration 2756, loss = 0.04209285\n",
      "Iteration 2757, loss = 0.04208933\n",
      "Iteration 2758, loss = 0.04208580\n",
      "Iteration 2759, loss = 0.04208228\n",
      "Iteration 2760, loss = 0.04207876\n",
      "Iteration 2761, loss = 0.04207525\n",
      "Iteration 2762, loss = 0.04207173\n",
      "Iteration 2763, loss = 0.04206822\n",
      "Iteration 2764, loss = 0.04206471\n",
      "Iteration 2765, loss = 0.04206121\n",
      "Iteration 2766, loss = 0.04205770\n",
      "Iteration 2767, loss = 0.04205420\n",
      "Iteration 2768, loss = 0.04205070\n",
      "Iteration 2769, loss = 0.04204720\n",
      "Iteration 2770, loss = 0.04204371\n",
      "Iteration 2771, loss = 0.04204021\n",
      "Iteration 2772, loss = 0.04203672\n",
      "Iteration 2773, loss = 0.04203323\n",
      "Iteration 2774, loss = 0.04202974\n",
      "Iteration 2775, loss = 0.04202626\n",
      "Iteration 2776, loss = 0.04202278\n",
      "Iteration 2777, loss = 0.04201930\n",
      "Iteration 2778, loss = 0.04201582\n",
      "Iteration 2779, loss = 0.04201234\n",
      "Iteration 2780, loss = 0.04200887\n",
      "Iteration 2781, loss = 0.04200540\n",
      "Iteration 2782, loss = 0.04200193\n",
      "Iteration 2783, loss = 0.04199846\n",
      "Iteration 2784, loss = 0.04199500\n",
      "Iteration 2785, loss = 0.04199153\n",
      "Iteration 2786, loss = 0.04198807\n",
      "Iteration 2787, loss = 0.04198461\n",
      "Iteration 2788, loss = 0.04198116\n",
      "Iteration 2789, loss = 0.04197770\n",
      "Iteration 2790, loss = 0.04197425\n",
      "Iteration 2791, loss = 0.04197080\n",
      "Iteration 2792, loss = 0.04196735\n",
      "Iteration 2793, loss = 0.04196391\n",
      "Iteration 2794, loss = 0.04196046\n",
      "Iteration 2795, loss = 0.04195702\n",
      "Iteration 2796, loss = 0.04195358\n",
      "Iteration 2797, loss = 0.04195015\n",
      "Iteration 2798, loss = 0.04194671\n",
      "Iteration 2799, loss = 0.04194328\n",
      "Iteration 2800, loss = 0.04193985\n",
      "Iteration 2801, loss = 0.04193642\n",
      "Iteration 2802, loss = 0.04193300\n",
      "Iteration 2803, loss = 0.04192957\n",
      "Iteration 2804, loss = 0.04192615\n",
      "Iteration 2805, loss = 0.04192273\n",
      "Iteration 2806, loss = 0.04191931\n",
      "Iteration 2807, loss = 0.04191590\n",
      "Iteration 2808, loss = 0.04191248\n",
      "Iteration 2809, loss = 0.04190907\n",
      "Iteration 2810, loss = 0.04190566\n",
      "Iteration 2811, loss = 0.04190225\n",
      "Iteration 2812, loss = 0.04189885\n",
      "Iteration 2813, loss = 0.04189545\n",
      "Iteration 2814, loss = 0.04189205\n",
      "Iteration 2815, loss = 0.04188865\n",
      "Iteration 2816, loss = 0.04188525\n",
      "Iteration 2817, loss = 0.04188186\n",
      "Iteration 2818, loss = 0.04187846\n",
      "Iteration 2819, loss = 0.04187507\n",
      "Iteration 2820, loss = 0.04187169\n",
      "Iteration 2821, loss = 0.04186830\n",
      "Iteration 2822, loss = 0.04186492\n",
      "Iteration 2823, loss = 0.04186153\n",
      "Iteration 2824, loss = 0.04185815\n",
      "Iteration 2825, loss = 0.04185478\n",
      "Iteration 2826, loss = 0.04185140\n",
      "Iteration 2827, loss = 0.04184803\n",
      "Iteration 2828, loss = 0.04184465\n",
      "Iteration 2829, loss = 0.04184128\n",
      "Iteration 2830, loss = 0.04183792\n",
      "Iteration 2831, loss = 0.04183455\n",
      "Iteration 2832, loss = 0.04183119\n",
      "Iteration 2833, loss = 0.04182783\n",
      "Iteration 2834, loss = 0.04182447\n",
      "Iteration 2835, loss = 0.04182111\n",
      "Iteration 2836, loss = 0.04181776\n",
      "Iteration 2837, loss = 0.04181440\n",
      "Iteration 2838, loss = 0.04181105\n",
      "Iteration 2839, loss = 0.04180770\n",
      "Iteration 2840, loss = 0.04180435\n",
      "Iteration 2841, loss = 0.04180101\n",
      "Iteration 2842, loss = 0.04179767\n",
      "Iteration 2843, loss = 0.04179433\n",
      "Iteration 2844, loss = 0.04179099\n",
      "Iteration 2845, loss = 0.04178765\n",
      "Iteration 2846, loss = 0.04178431\n",
      "Iteration 2847, loss = 0.04178098\n",
      "Iteration 2848, loss = 0.04177765\n",
      "Iteration 2849, loss = 0.04177432\n",
      "Iteration 2850, loss = 0.04177099\n",
      "Iteration 2851, loss = 0.04176767\n",
      "Iteration 2852, loss = 0.04176435\n",
      "Iteration 2853, loss = 0.04176103\n",
      "Iteration 2854, loss = 0.04175771\n",
      "Iteration 2855, loss = 0.04175439\n",
      "Iteration 2856, loss = 0.04175108\n",
      "Iteration 2857, loss = 0.04174776\n",
      "Iteration 2858, loss = 0.04174445\n",
      "Iteration 2859, loss = 0.04174114\n",
      "Iteration 2860, loss = 0.04173784\n",
      "Iteration 2861, loss = 0.04173453\n",
      "Iteration 2862, loss = 0.04173123\n",
      "Iteration 2863, loss = 0.04172793\n",
      "Iteration 2864, loss = 0.04172463\n",
      "Iteration 2865, loss = 0.04172133\n",
      "Iteration 2866, loss = 0.04171804\n",
      "Iteration 2867, loss = 0.04171474\n",
      "Iteration 2868, loss = 0.04171145\n",
      "Iteration 2869, loss = 0.04170816\n",
      "Iteration 2870, loss = 0.04170488\n",
      "Iteration 2871, loss = 0.04170159\n",
      "Iteration 2872, loss = 0.04169831\n",
      "Iteration 2873, loss = 0.04169503\n",
      "Iteration 2874, loss = 0.04169175\n",
      "Iteration 2875, loss = 0.04168847\n",
      "Iteration 2876, loss = 0.04168519\n",
      "Iteration 2877, loss = 0.04168192\n",
      "Iteration 2878, loss = 0.04167865\n",
      "Iteration 2879, loss = 0.04167538\n",
      "Iteration 2880, loss = 0.04167211\n",
      "Iteration 2881, loss = 0.04166884\n",
      "Iteration 2882, loss = 0.04166558\n",
      "Iteration 2883, loss = 0.04166232\n",
      "Iteration 2884, loss = 0.04165906\n",
      "Iteration 2885, loss = 0.04165580\n",
      "Iteration 2886, loss = 0.04165254\n",
      "Iteration 2887, loss = 0.04164929\n",
      "Iteration 2888, loss = 0.04164604\n",
      "Iteration 2889, loss = 0.04164279\n",
      "Iteration 2890, loss = 0.04163954\n",
      "Iteration 2891, loss = 0.04163629\n",
      "Iteration 2892, loss = 0.04163305\n",
      "Iteration 2893, loss = 0.04162980\n",
      "Iteration 2894, loss = 0.04162656\n",
      "Iteration 2895, loss = 0.04162332\n",
      "Iteration 2896, loss = 0.04162009\n",
      "Iteration 2897, loss = 0.04161685\n",
      "Iteration 2898, loss = 0.04161362\n",
      "Iteration 2899, loss = 0.04161039\n",
      "Iteration 2900, loss = 0.04160716\n",
      "Iteration 2901, loss = 0.04160393\n",
      "Iteration 2902, loss = 0.04160070\n",
      "Iteration 2903, loss = 0.04159748\n",
      "Iteration 2904, loss = 0.04159426\n",
      "Iteration 2905, loss = 0.04159104\n",
      "Iteration 2906, loss = 0.04158782\n",
      "Iteration 2907, loss = 0.04158460\n",
      "Iteration 2908, loss = 0.04158139\n",
      "Iteration 2909, loss = 0.04157817\n",
      "Iteration 2910, loss = 0.04157496\n",
      "Iteration 2911, loss = 0.04157175\n",
      "Iteration 2912, loss = 0.04156855\n",
      "Iteration 2913, loss = 0.04156534\n",
      "Iteration 2914, loss = 0.04156214\n",
      "Iteration 2915, loss = 0.04155894\n",
      "Iteration 2916, loss = 0.04155574\n",
      "Iteration 2917, loss = 0.04155254\n",
      "Iteration 2918, loss = 0.04154934\n",
      "Iteration 2919, loss = 0.04154615\n",
      "Iteration 2920, loss = 0.04154296\n",
      "Iteration 2921, loss = 0.04153977\n",
      "Iteration 2922, loss = 0.04153658\n",
      "Iteration 2923, loss = 0.04153339\n",
      "Iteration 2924, loss = 0.04153021\n",
      "Iteration 2925, loss = 0.04152702\n",
      "Iteration 2926, loss = 0.04152384\n",
      "Iteration 2927, loss = 0.04152066\n",
      "Iteration 2928, loss = 0.04151748\n",
      "Iteration 2929, loss = 0.04151431\n",
      "Iteration 2930, loss = 0.04151113\n",
      "Iteration 2931, loss = 0.04150796\n",
      "Iteration 2932, loss = 0.04150479\n",
      "Iteration 2933, loss = 0.04150162\n",
      "Iteration 2934, loss = 0.04149846\n",
      "Iteration 2935, loss = 0.04149529\n",
      "Iteration 2936, loss = 0.04149213\n",
      "Iteration 2937, loss = 0.04148897\n",
      "Iteration 2938, loss = 0.04148581\n",
      "Iteration 2939, loss = 0.04148265\n",
      "Iteration 2940, loss = 0.04147949\n",
      "Iteration 2941, loss = 0.04147634\n",
      "Iteration 2942, loss = 0.04147319\n",
      "Iteration 2943, loss = 0.04147004\n",
      "Iteration 2944, loss = 0.04146689\n",
      "Iteration 2945, loss = 0.04146374\n",
      "Iteration 2946, loss = 0.04146059\n",
      "Iteration 2947, loss = 0.04145745\n",
      "Iteration 2948, loss = 0.04145431\n",
      "Iteration 2949, loss = 0.04145117\n",
      "Iteration 2950, loss = 0.04144803\n",
      "Iteration 2951, loss = 0.04144489\n",
      "Iteration 2952, loss = 0.04144176\n",
      "Iteration 2953, loss = 0.04143863\n",
      "Iteration 2954, loss = 0.04143550\n",
      "Iteration 2955, loss = 0.04143237\n",
      "Iteration 2956, loss = 0.04142924\n",
      "Iteration 2957, loss = 0.04142611\n",
      "Iteration 2958, loss = 0.04142299\n",
      "Iteration 2959, loss = 0.04141987\n",
      "Iteration 2960, loss = 0.04141675\n",
      "Iteration 2961, loss = 0.04141363\n",
      "Iteration 2962, loss = 0.04141051\n",
      "Iteration 2963, loss = 0.04140739\n",
      "Iteration 2964, loss = 0.04140428\n",
      "Iteration 2965, loss = 0.04140117\n",
      "Iteration 2966, loss = 0.04139806\n",
      "Iteration 2967, loss = 0.04139495\n",
      "Iteration 2968, loss = 0.04139184\n",
      "Iteration 2969, loss = 0.04138874\n",
      "Iteration 2970, loss = 0.04138564\n",
      "Iteration 2971, loss = 0.04138253\n",
      "Iteration 2972, loss = 0.04137943\n",
      "Iteration 2973, loss = 0.04137634\n",
      "Iteration 2974, loss = 0.04137324\n",
      "Iteration 2975, loss = 0.04137015\n",
      "Iteration 2976, loss = 0.04136705\n",
      "Iteration 2977, loss = 0.04136396\n",
      "Iteration 2978, loss = 0.04136087\n",
      "Iteration 2979, loss = 0.04135779\n",
      "Iteration 2980, loss = 0.04135470\n",
      "Iteration 2981, loss = 0.04135161\n",
      "Iteration 2982, loss = 0.04134853\n",
      "Iteration 2983, loss = 0.04134545\n",
      "Iteration 2984, loss = 0.04134237\n",
      "Iteration 2985, loss = 0.04133929\n",
      "Iteration 2986, loss = 0.04133622\n",
      "Iteration 2987, loss = 0.04133315\n",
      "Iteration 2988, loss = 0.04133007\n",
      "Iteration 2989, loss = 0.04132700\n",
      "Iteration 2990, loss = 0.04132393\n",
      "Iteration 2991, loss = 0.04132087\n",
      "Iteration 2992, loss = 0.04131780\n",
      "Iteration 2993, loss = 0.04131474\n",
      "Iteration 2994, loss = 0.04131167\n",
      "Iteration 2995, loss = 0.04130861\n",
      "Iteration 2996, loss = 0.04130555\n",
      "Iteration 2997, loss = 0.04130250\n",
      "Iteration 2998, loss = 0.04129944\n",
      "Iteration 2999, loss = 0.04129639\n",
      "Iteration 3000, loss = 0.04129334\n",
      "Iteration 3001, loss = 0.04129029\n",
      "Iteration 3002, loss = 0.04128724\n",
      "Iteration 3003, loss = 0.04128419\n",
      "Iteration 3004, loss = 0.04128114\n",
      "Iteration 3005, loss = 0.04127810\n",
      "Iteration 3006, loss = 0.04127506\n",
      "Iteration 3007, loss = 0.04127202\n",
      "Iteration 3008, loss = 0.04126898\n",
      "Iteration 3009, loss = 0.04126594\n",
      "Iteration 3010, loss = 0.04126291\n",
      "Iteration 3011, loss = 0.04125987\n",
      "Iteration 3012, loss = 0.04125684\n",
      "Iteration 3013, loss = 0.04125381\n",
      "Iteration 3014, loss = 0.04125078\n",
      "Iteration 3015, loss = 0.04124775\n",
      "Iteration 3016, loss = 0.04124473\n",
      "Iteration 3017, loss = 0.04124170\n",
      "Iteration 3018, loss = 0.04123868\n",
      "Iteration 3019, loss = 0.04123566\n",
      "Iteration 3020, loss = 0.04123264\n",
      "Iteration 3021, loss = 0.04122962\n",
      "Iteration 3022, loss = 0.04122661\n",
      "Iteration 3023, loss = 0.04122359\n",
      "Iteration 3024, loss = 0.04122058\n",
      "Iteration 3025, loss = 0.04121757\n",
      "Iteration 3026, loss = 0.04121456\n",
      "Iteration 3027, loss = 0.04121155\n",
      "Iteration 3028, loss = 0.04120854\n",
      "Iteration 3029, loss = 0.04120554\n",
      "Iteration 3030, loss = 0.04120254\n",
      "Iteration 3031, loss = 0.04119953\n",
      "Iteration 3032, loss = 0.04119653\n",
      "Iteration 3033, loss = 0.04119354\n",
      "Iteration 3034, loss = 0.04119054\n",
      "Iteration 3035, loss = 0.04118754\n",
      "Iteration 3036, loss = 0.04118455\n",
      "Iteration 3037, loss = 0.04118156\n",
      "Iteration 3038, loss = 0.04117857\n",
      "Iteration 3039, loss = 0.04117558\n",
      "Iteration 3040, loss = 0.04117259\n",
      "Iteration 3041, loss = 0.04116961\n",
      "Iteration 3042, loss = 0.04116662\n",
      "Iteration 3043, loss = 0.04116364\n",
      "Iteration 3044, loss = 0.04116066\n",
      "Iteration 3045, loss = 0.04115768\n",
      "Iteration 3046, loss = 0.04115470\n",
      "Iteration 3047, loss = 0.04115173\n",
      "Iteration 3048, loss = 0.04114875\n",
      "Iteration 3049, loss = 0.04114578\n",
      "Iteration 3050, loss = 0.04114281\n",
      "Iteration 3051, loss = 0.04113984\n",
      "Iteration 3052, loss = 0.04113687\n",
      "Iteration 3053, loss = 0.04113391\n",
      "Iteration 3054, loss = 0.04113094\n",
      "Iteration 3055, loss = 0.04112798\n",
      "Iteration 3056, loss = 0.04112502\n",
      "Iteration 3057, loss = 0.04112205\n",
      "Iteration 3058, loss = 0.04111910\n",
      "Iteration 3059, loss = 0.04111614\n",
      "Iteration 3060, loss = 0.04111318\n",
      "Iteration 3061, loss = 0.04111023\n",
      "Iteration 3062, loss = 0.04110728\n",
      "Iteration 3063, loss = 0.04110433\n",
      "Iteration 3064, loss = 0.04110138\n",
      "Iteration 3065, loss = 0.04109843\n",
      "Iteration 3066, loss = 0.04109548\n",
      "Iteration 3067, loss = 0.04109254\n",
      "Iteration 3068, loss = 0.04108959\n",
      "Iteration 3069, loss = 0.04108665\n",
      "Iteration 3070, loss = 0.04108371\n",
      "Iteration 3071, loss = 0.04108077\n",
      "Iteration 3072, loss = 0.04107784\n",
      "Iteration 3073, loss = 0.04107490\n",
      "Iteration 3074, loss = 0.04107197\n",
      "Iteration 3075, loss = 0.04106903\n",
      "Iteration 3076, loss = 0.04106610\n",
      "Iteration 3077, loss = 0.04106317\n",
      "Iteration 3078, loss = 0.04106024\n",
      "Iteration 3079, loss = 0.04105732\n",
      "Iteration 3080, loss = 0.04105439\n",
      "Iteration 3081, loss = 0.04105147\n",
      "Iteration 3082, loss = 0.04104855\n",
      "Iteration 3083, loss = 0.04104563\n",
      "Iteration 3084, loss = 0.04104271\n",
      "Iteration 3085, loss = 0.04103979\n",
      "Iteration 3086, loss = 0.04103687\n",
      "Iteration 3087, loss = 0.04103396\n",
      "Iteration 3088, loss = 0.04103105\n",
      "Iteration 3089, loss = 0.04102813\n",
      "Iteration 3090, loss = 0.04102522\n",
      "Iteration 3091, loss = 0.04102232\n",
      "Iteration 3092, loss = 0.04101941\n",
      "Iteration 3093, loss = 0.04101650\n",
      "Iteration 3094, loss = 0.04101360\n",
      "Iteration 3095, loss = 0.04101070\n",
      "Iteration 3096, loss = 0.04100780\n",
      "Iteration 3097, loss = 0.04100490\n",
      "Iteration 3098, loss = 0.04100200\n",
      "Iteration 3099, loss = 0.04099910\n",
      "Iteration 3100, loss = 0.04099621\n",
      "Iteration 3101, loss = 0.04099331\n",
      "Iteration 3102, loss = 0.04099042\n",
      "Iteration 3103, loss = 0.04098753\n",
      "Iteration 3104, loss = 0.04098464\n",
      "Iteration 3105, loss = 0.04098175\n",
      "Iteration 3106, loss = 0.04097887\n",
      "Iteration 3107, loss = 0.04097598\n",
      "Iteration 3108, loss = 0.04097310\n",
      "Iteration 3109, loss = 0.04097022\n",
      "Iteration 3110, loss = 0.04096734\n",
      "Iteration 3111, loss = 0.04096446\n",
      "Iteration 3112, loss = 0.04096158\n",
      "Iteration 3113, loss = 0.04095870\n",
      "Iteration 3114, loss = 0.04095583\n",
      "Iteration 3115, loss = 0.04095295\n",
      "Iteration 3116, loss = 0.04095008\n",
      "Iteration 3117, loss = 0.04094721\n",
      "Iteration 3118, loss = 0.04094434\n",
      "Iteration 3119, loss = 0.04094148\n",
      "Iteration 3120, loss = 0.04093861\n",
      "Iteration 3121, loss = 0.04093575\n",
      "Iteration 3122, loss = 0.04093288\n",
      "Iteration 3123, loss = 0.04093002\n",
      "Iteration 3124, loss = 0.04092716\n",
      "Iteration 3125, loss = 0.04092430\n",
      "Iteration 3126, loss = 0.04092144\n",
      "Iteration 3127, loss = 0.04091859\n",
      "Iteration 3128, loss = 0.04091573\n",
      "Iteration 3129, loss = 0.04091288\n",
      "Iteration 3130, loss = 0.04091003\n",
      "Iteration 3131, loss = 0.04090718\n",
      "Iteration 3132, loss = 0.04090433\n",
      "Iteration 3133, loss = 0.04090148\n",
      "Iteration 3134, loss = 0.04089864\n",
      "Iteration 3135, loss = 0.04089579\n",
      "Iteration 3136, loss = 0.04089295\n",
      "Iteration 3137, loss = 0.04089011\n",
      "Iteration 3138, loss = 0.04088727\n",
      "Iteration 3139, loss = 0.04088443\n",
      "Iteration 3140, loss = 0.04088159\n",
      "Iteration 3141, loss = 0.04087875\n",
      "Iteration 3142, loss = 0.04087592\n",
      "Iteration 3143, loss = 0.04087309\n",
      "Iteration 3144, loss = 0.04087025\n",
      "Iteration 3145, loss = 0.04086742\n",
      "Iteration 3146, loss = 0.04086459\n",
      "Iteration 3147, loss = 0.04086177\n",
      "Iteration 3148, loss = 0.04085894\n",
      "Iteration 3149, loss = 0.04085612\n",
      "Iteration 3150, loss = 0.04085329\n",
      "Iteration 3151, loss = 0.04085047\n",
      "Iteration 3152, loss = 0.04084765\n",
      "Iteration 3153, loss = 0.04084483\n",
      "Iteration 3154, loss = 0.04084201\n",
      "Iteration 3155, loss = 0.04083919\n",
      "Iteration 3156, loss = 0.04083638\n",
      "Iteration 3157, loss = 0.04083357\n",
      "Iteration 3158, loss = 0.04083075\n",
      "Iteration 3159, loss = 0.04082794\n",
      "Iteration 3160, loss = 0.04082513\n",
      "Iteration 3161, loss = 0.04082232\n",
      "Iteration 3162, loss = 0.04081952\n",
      "Iteration 3163, loss = 0.04081671\n",
      "Iteration 3164, loss = 0.04081391\n",
      "Iteration 3165, loss = 0.04081110\n",
      "Iteration 3166, loss = 0.04080830\n",
      "Iteration 3167, loss = 0.04080550\n",
      "Iteration 3168, loss = 0.04080270\n",
      "Iteration 3169, loss = 0.04079990\n",
      "Iteration 3170, loss = 0.04079711\n",
      "Iteration 3171, loss = 0.04079431\n",
      "Iteration 3172, loss = 0.04079152\n",
      "Iteration 3173, loss = 0.04078873\n",
      "Iteration 3174, loss = 0.04078594\n",
      "Iteration 3175, loss = 0.04078315\n",
      "Iteration 3176, loss = 0.04078036\n",
      "Iteration 3177, loss = 0.04077757\n",
      "Iteration 3178, loss = 0.04077479\n",
      "Iteration 3179, loss = 0.04077200\n",
      "Iteration 3180, loss = 0.04076922\n",
      "Iteration 3181, loss = 0.04076644\n",
      "Iteration 3182, loss = 0.04076366\n",
      "Iteration 3183, loss = 0.04076088\n",
      "Iteration 3184, loss = 0.04075810\n",
      "Iteration 3185, loss = 0.04075533\n",
      "Iteration 3186, loss = 0.04075255\n",
      "Iteration 3187, loss = 0.04074978\n",
      "Iteration 3188, loss = 0.04074701\n",
      "Iteration 3189, loss = 0.04074424\n",
      "Iteration 3190, loss = 0.04074147\n",
      "Iteration 3191, loss = 0.04073870\n",
      "Iteration 3192, loss = 0.04073593\n",
      "Iteration 3193, loss = 0.04073317\n",
      "Iteration 3194, loss = 0.04073040\n",
      "Iteration 3195, loss = 0.04072764\n",
      "Iteration 3196, loss = 0.04072488\n",
      "Iteration 3197, loss = 0.04072212\n",
      "Iteration 3198, loss = 0.04071936\n",
      "Iteration 3199, loss = 0.04071660\n",
      "Iteration 3200, loss = 0.04071384\n",
      "Iteration 3201, loss = 0.04071109\n",
      "Iteration 3202, loss = 0.04070833\n",
      "Iteration 3203, loss = 0.04070558\n",
      "Iteration 3204, loss = 0.04070283\n",
      "Iteration 3205, loss = 0.04070008\n",
      "Iteration 3206, loss = 0.04069733\n",
      "Iteration 3207, loss = 0.04069459\n",
      "Iteration 3208, loss = 0.04069184\n",
      "Iteration 3209, loss = 0.04068909\n",
      "Iteration 3210, loss = 0.04068635\n",
      "Iteration 3211, loss = 0.04068361\n",
      "Iteration 3212, loss = 0.04068087\n",
      "Iteration 3213, loss = 0.04067813\n",
      "Iteration 3214, loss = 0.04067539\n",
      "Iteration 3215, loss = 0.04067265\n",
      "Iteration 3216, loss = 0.04066992\n",
      "Iteration 3217, loss = 0.04066718\n",
      "Iteration 3218, loss = 0.04066445\n",
      "Iteration 3219, loss = 0.04066172\n",
      "Iteration 3220, loss = 0.04065899\n",
      "Iteration 3221, loss = 0.04065626\n",
      "Iteration 3222, loss = 0.04065353\n",
      "Iteration 3223, loss = 0.04065080\n",
      "Iteration 3224, loss = 0.04064808\n",
      "Iteration 3225, loss = 0.04064535\n",
      "Iteration 3226, loss = 0.04064263\n",
      "Iteration 3227, loss = 0.04063991\n",
      "Iteration 3228, loss = 0.04063719\n",
      "Iteration 3229, loss = 0.04063447\n",
      "Iteration 3230, loss = 0.04063175\n",
      "Iteration 3231, loss = 0.04062903\n",
      "Iteration 3232, loss = 0.04062632\n",
      "Iteration 3233, loss = 0.04062360\n",
      "Iteration 3234, loss = 0.04062089\n",
      "Iteration 3235, loss = 0.04061818\n",
      "Iteration 3236, loss = 0.04061547\n",
      "Iteration 3237, loss = 0.04061276\n",
      "Iteration 3238, loss = 0.04061005\n",
      "Iteration 3239, loss = 0.04060734\n",
      "Iteration 3240, loss = 0.04060464\n",
      "Iteration 3241, loss = 0.04060193\n",
      "Iteration 3242, loss = 0.04059923\n",
      "Iteration 3243, loss = 0.04059653\n",
      "Iteration 3244, loss = 0.04059383\n",
      "Iteration 3245, loss = 0.04059113\n",
      "Iteration 3246, loss = 0.04058843\n",
      "Iteration 3247, loss = 0.04058573\n",
      "Iteration 3248, loss = 0.04058304\n",
      "Iteration 3249, loss = 0.04058034\n",
      "Iteration 3250, loss = 0.04057765\n",
      "Iteration 3251, loss = 0.04057496\n",
      "Iteration 3252, loss = 0.04057226\n",
      "Iteration 3253, loss = 0.04056957\n",
      "Iteration 3254, loss = 0.04056689\n",
      "Iteration 3255, loss = 0.04056420\n",
      "Iteration 3256, loss = 0.04056151\n",
      "Iteration 3257, loss = 0.04055883\n",
      "Iteration 3258, loss = 0.04055614\n",
      "Iteration 3259, loss = 0.04055346\n",
      "Iteration 3260, loss = 0.04055078\n",
      "Iteration 3261, loss = 0.04054810\n",
      "Iteration 3262, loss = 0.04054542\n",
      "Iteration 3263, loss = 0.04054274\n",
      "Iteration 3264, loss = 0.04054007\n",
      "Iteration 3265, loss = 0.04053739\n",
      "Iteration 3266, loss = 0.04053472\n",
      "Iteration 3267, loss = 0.04053204\n",
      "Iteration 3268, loss = 0.04052937\n",
      "Iteration 3269, loss = 0.04052670\n",
      "Iteration 3270, loss = 0.04052403\n",
      "Iteration 3271, loss = 0.04052136\n",
      "Iteration 3272, loss = 0.04051870\n",
      "Iteration 3273, loss = 0.04051603\n",
      "Iteration 3274, loss = 0.04051337\n",
      "Iteration 3275, loss = 0.04051070\n",
      "Iteration 3276, loss = 0.04050804\n",
      "Iteration 3277, loss = 0.04050538\n",
      "Iteration 3278, loss = 0.04050272\n",
      "Iteration 3279, loss = 0.04050006\n",
      "Iteration 3280, loss = 0.04049740\n",
      "Iteration 3281, loss = 0.04049475\n",
      "Iteration 3282, loss = 0.04049209\n",
      "Iteration 3283, loss = 0.04048944\n",
      "Iteration 3284, loss = 0.04048678\n",
      "Iteration 3285, loss = 0.04048413\n",
      "Iteration 3286, loss = 0.04048148\n",
      "Iteration 3287, loss = 0.04047883\n",
      "Iteration 3288, loss = 0.04047618\n",
      "Iteration 3289, loss = 0.04047354\n",
      "Iteration 3290, loss = 0.04047089\n",
      "Iteration 3291, loss = 0.04046824\n",
      "Iteration 3292, loss = 0.04046560\n",
      "Iteration 3293, loss = 0.04046296\n",
      "Iteration 3294, loss = 0.04046032\n",
      "Iteration 3295, loss = 0.04045768\n",
      "Iteration 3296, loss = 0.04045504\n",
      "Iteration 3297, loss = 0.04045240\n",
      "Iteration 3298, loss = 0.04044976\n",
      "Iteration 3299, loss = 0.04044713\n",
      "Iteration 3300, loss = 0.04044449\n",
      "Iteration 3301, loss = 0.04044186\n",
      "Iteration 3302, loss = 0.04043922\n",
      "Iteration 3303, loss = 0.04043659\n",
      "Iteration 3304, loss = 0.04043396\n",
      "Iteration 3305, loss = 0.04043133\n",
      "Iteration 3306, loss = 0.04042871\n",
      "Iteration 3307, loss = 0.04042608\n",
      "Iteration 3308, loss = 0.04042345\n",
      "Iteration 3309, loss = 0.04042083\n",
      "Iteration 3310, loss = 0.04041821\n",
      "Iteration 3311, loss = 0.04041558\n",
      "Iteration 3312, loss = 0.04041296\n",
      "Iteration 3313, loss = 0.04041034\n",
      "Iteration 3314, loss = 0.04040772\n",
      "Iteration 3315, loss = 0.04040510\n",
      "Iteration 3316, loss = 0.04040249\n",
      "Iteration 3317, loss = 0.04039987\n",
      "Iteration 3318, loss = 0.04039726\n",
      "Iteration 3319, loss = 0.04039464\n",
      "Iteration 3320, loss = 0.04039203\n",
      "Iteration 3321, loss = 0.04038942\n",
      "Iteration 3322, loss = 0.04038681\n",
      "Iteration 3323, loss = 0.04038420\n",
      "Iteration 3324, loss = 0.04038159\n",
      "Iteration 3325, loss = 0.04037899\n",
      "Iteration 3326, loss = 0.04037638\n",
      "Iteration 3327, loss = 0.04037378\n",
      "Iteration 3328, loss = 0.04037117\n",
      "Iteration 3329, loss = 0.04036857\n",
      "Iteration 3330, loss = 0.04036597\n",
      "Iteration 3331, loss = 0.04036337\n",
      "Iteration 3332, loss = 0.04036077\n",
      "Iteration 3333, loss = 0.04035817\n",
      "Iteration 3334, loss = 0.04035557\n",
      "Iteration 3335, loss = 0.04035298\n",
      "Iteration 3336, loss = 0.04035038\n",
      "Iteration 3337, loss = 0.04034779\n",
      "Iteration 3338, loss = 0.04034519\n",
      "Iteration 3339, loss = 0.04034260\n",
      "Iteration 3340, loss = 0.04034001\n",
      "Iteration 3341, loss = 0.04033742\n",
      "Iteration 3342, loss = 0.04033483\n",
      "Iteration 3343, loss = 0.04033224\n",
      "Iteration 3344, loss = 0.04032966\n",
      "Iteration 3345, loss = 0.04032707\n",
      "Iteration 3346, loss = 0.04032449\n",
      "Iteration 3347, loss = 0.04032190\n",
      "Iteration 3348, loss = 0.04031932\n",
      "Iteration 3349, loss = 0.04031674\n",
      "Iteration 3350, loss = 0.04031416\n",
      "Iteration 3351, loss = 0.04031158\n",
      "Iteration 3352, loss = 0.04030900\n",
      "Iteration 3353, loss = 0.04030643\n",
      "Iteration 3354, loss = 0.04030385\n",
      "Iteration 3355, loss = 0.04030128\n",
      "Iteration 3356, loss = 0.04029870\n",
      "Iteration 3357, loss = 0.04029613\n",
      "Iteration 3358, loss = 0.04029356\n",
      "Iteration 3359, loss = 0.04029099\n",
      "Iteration 3360, loss = 0.04028842\n",
      "Iteration 3361, loss = 0.04028585\n",
      "Iteration 3362, loss = 0.04028328\n",
      "Iteration 3363, loss = 0.04028071\n",
      "Iteration 3364, loss = 0.04027815\n",
      "Iteration 3365, loss = 0.04027558\n",
      "Iteration 3366, loss = 0.04027302\n",
      "Iteration 3367, loss = 0.04027045\n",
      "Iteration 3368, loss = 0.04026789\n",
      "Iteration 3369, loss = 0.04026533\n",
      "Iteration 3370, loss = 0.04026277\n",
      "Iteration 3371, loss = 0.04026021\n",
      "Iteration 3372, loss = 0.04025766\n",
      "Iteration 3373, loss = 0.04025510\n",
      "Iteration 3374, loss = 0.04025254\n",
      "Iteration 3375, loss = 0.04024999\n",
      "Iteration 3376, loss = 0.04024743\n",
      "Iteration 3377, loss = 0.04024488\n",
      "Iteration 3378, loss = 0.04024233\n",
      "Iteration 3379, loss = 0.04023978\n",
      "Iteration 3380, loss = 0.04023723\n",
      "Iteration 3381, loss = 0.04023468\n",
      "Iteration 3382, loss = 0.04023213\n",
      "Iteration 3383, loss = 0.04022959\n",
      "Iteration 3384, loss = 0.04022704\n",
      "Iteration 3385, loss = 0.04022450\n",
      "Iteration 3386, loss = 0.04022195\n",
      "Iteration 3387, loss = 0.04021941\n",
      "Iteration 3388, loss = 0.04021687\n",
      "Iteration 3389, loss = 0.04021433\n",
      "Iteration 3390, loss = 0.04021179\n",
      "Iteration 3391, loss = 0.04020925\n",
      "Iteration 3392, loss = 0.04020671\n",
      "Iteration 3393, loss = 0.04020417\n",
      "Iteration 3394, loss = 0.04020164\n",
      "Iteration 3395, loss = 0.04019910\n",
      "Iteration 3396, loss = 0.04019657\n",
      "Iteration 3397, loss = 0.04019403\n",
      "Iteration 3398, loss = 0.04019150\n",
      "Iteration 3399, loss = 0.04018897\n",
      "Iteration 3400, loss = 0.04018644\n",
      "Iteration 3401, loss = 0.04018391\n",
      "Iteration 3402, loss = 0.04018138\n",
      "Iteration 3403, loss = 0.04017885\n",
      "Iteration 3404, loss = 0.04017633\n",
      "Iteration 3405, loss = 0.04017380\n",
      "Iteration 3406, loss = 0.04017128\n",
      "Iteration 3407, loss = 0.04016875\n",
      "Iteration 3408, loss = 0.04016623\n",
      "Iteration 3409, loss = 0.04016371\n",
      "Iteration 3410, loss = 0.04016119\n",
      "Iteration 3411, loss = 0.04015867\n",
      "Iteration 3412, loss = 0.04015615\n",
      "Iteration 3413, loss = 0.04015363\n",
      "Iteration 3414, loss = 0.04015111\n",
      "Iteration 3415, loss = 0.04014860\n",
      "Iteration 3416, loss = 0.04014608\n",
      "Iteration 3417, loss = 0.04014357\n",
      "Iteration 3418, loss = 0.04014106\n",
      "Iteration 3419, loss = 0.04013854\n",
      "Iteration 3420, loss = 0.04013603\n",
      "Iteration 3421, loss = 0.04013352\n",
      "Iteration 3422, loss = 0.04013101\n",
      "Iteration 3423, loss = 0.04012850\n",
      "Iteration 3424, loss = 0.04012599\n",
      "Iteration 3425, loss = 0.04012349\n",
      "Iteration 3426, loss = 0.04012098\n",
      "Iteration 3427, loss = 0.04011848\n",
      "Iteration 3428, loss = 0.04011597\n",
      "Iteration 3429, loss = 0.04011347\n",
      "Iteration 3430, loss = 0.04011096\n",
      "Iteration 3431, loss = 0.04010846\n",
      "Iteration 3432, loss = 0.04010596\n",
      "Iteration 3433, loss = 0.04010346\n",
      "Iteration 3434, loss = 0.04010096\n",
      "Iteration 3435, loss = 0.04009847\n",
      "Iteration 3436, loss = 0.04009597\n",
      "Iteration 3437, loss = 0.04009347\n",
      "Iteration 3438, loss = 0.04009098\n",
      "Iteration 3439, loss = 0.04008848\n",
      "Iteration 3440, loss = 0.04008599\n",
      "Iteration 3441, loss = 0.04008350\n",
      "Iteration 3442, loss = 0.04008100\n",
      "Iteration 3443, loss = 0.04007851\n",
      "Iteration 3444, loss = 0.04007602\n",
      "Iteration 3445, loss = 0.04007353\n",
      "Iteration 3446, loss = 0.04007104\n",
      "Iteration 3447, loss = 0.04006856\n",
      "Iteration 3448, loss = 0.04006607\n",
      "Iteration 3449, loss = 0.04006358\n",
      "Iteration 3450, loss = 0.04006110\n",
      "Iteration 3451, loss = 0.04005861\n",
      "Iteration 3452, loss = 0.04005613\n",
      "Iteration 3453, loss = 0.04005365\n",
      "Iteration 3454, loss = 0.04005117\n",
      "Iteration 3455, loss = 0.04004869\n",
      "Iteration 3456, loss = 0.04004621\n",
      "Iteration 3457, loss = 0.04004373\n",
      "Iteration 3458, loss = 0.04004125\n",
      "Iteration 3459, loss = 0.04003877\n",
      "Iteration 3460, loss = 0.04003629\n",
      "Iteration 3461, loss = 0.04003382\n",
      "Iteration 3462, loss = 0.04003134\n",
      "Iteration 3463, loss = 0.04002887\n",
      "Iteration 3464, loss = 0.04002640\n",
      "Iteration 3465, loss = 0.04002392\n",
      "Iteration 3466, loss = 0.04002145\n",
      "Iteration 3467, loss = 0.04001898\n",
      "Iteration 3468, loss = 0.04001651\n",
      "Iteration 3469, loss = 0.04001404\n",
      "Iteration 3470, loss = 0.04001157\n",
      "Iteration 3471, loss = 0.04000911\n",
      "Iteration 3472, loss = 0.04000664\n",
      "Iteration 3473, loss = 0.04000417\n",
      "Iteration 3474, loss = 0.04000171\n",
      "Iteration 3475, loss = 0.03999924\n",
      "Iteration 3476, loss = 0.03999678\n",
      "Iteration 3477, loss = 0.03999432\n",
      "Iteration 3478, loss = 0.03999186\n",
      "Iteration 3479, loss = 0.03998939\n",
      "Iteration 3480, loss = 0.03998693\n",
      "Iteration 3481, loss = 0.03998448\n",
      "Iteration 3482, loss = 0.03998202\n",
      "Iteration 3483, loss = 0.03997956\n",
      "Iteration 3484, loss = 0.03997710\n",
      "Iteration 3485, loss = 0.03997464\n",
      "Iteration 3486, loss = 0.03997219\n",
      "Iteration 3487, loss = 0.03996973\n",
      "Iteration 3488, loss = 0.03996728\n",
      "Iteration 3489, loss = 0.03996483\n",
      "Iteration 3490, loss = 0.03996237\n",
      "Iteration 3491, loss = 0.03995992\n",
      "Iteration 3492, loss = 0.03995747\n",
      "Iteration 3493, loss = 0.03995502\n",
      "Iteration 3494, loss = 0.03995257\n",
      "Iteration 3495, loss = 0.03995012\n",
      "Iteration 3496, loss = 0.03994768\n",
      "Iteration 3497, loss = 0.03994523\n",
      "Iteration 3498, loss = 0.03994278\n",
      "Iteration 3499, loss = 0.03994034\n",
      "Iteration 3500, loss = 0.03993789\n",
      "Iteration 3501, loss = 0.03993545\n",
      "Iteration 3502, loss = 0.03993300\n",
      "Iteration 3503, loss = 0.03993056\n",
      "Iteration 3504, loss = 0.03992812\n",
      "Iteration 3505, loss = 0.03992568\n",
      "Iteration 3506, loss = 0.03992324\n",
      "Iteration 3507, loss = 0.03992080\n",
      "Iteration 3508, loss = 0.03991836\n",
      "Iteration 3509, loss = 0.03991592\n",
      "Iteration 3510, loss = 0.03991348\n",
      "Iteration 3511, loss = 0.03991105\n",
      "Iteration 3512, loss = 0.03990861\n",
      "Iteration 3513, loss = 0.03990618\n",
      "Iteration 3514, loss = 0.03990374\n",
      "Iteration 3515, loss = 0.03990131\n",
      "Iteration 3516, loss = 0.03989888\n",
      "Iteration 3517, loss = 0.03989644\n",
      "Iteration 3518, loss = 0.03989401\n",
      "Iteration 3519, loss = 0.03989158\n",
      "Iteration 3520, loss = 0.03988915\n",
      "Iteration 3521, loss = 0.03988672\n",
      "Iteration 3522, loss = 0.03988429\n",
      "Iteration 3523, loss = 0.03988186\n",
      "Iteration 3524, loss = 0.03987944\n",
      "Iteration 3525, loss = 0.03987701\n",
      "Iteration 3526, loss = 0.03987458\n",
      "Iteration 3527, loss = 0.03987216\n",
      "Iteration 3528, loss = 0.03986973\n",
      "Iteration 3529, loss = 0.03986731\n",
      "Iteration 3530, loss = 0.03986489\n",
      "Iteration 3531, loss = 0.03986246\n",
      "Iteration 3532, loss = 0.03986004\n",
      "Iteration 3533, loss = 0.03985762\n",
      "Iteration 3534, loss = 0.03985520\n",
      "Iteration 3535, loss = 0.03985278\n",
      "Iteration 3536, loss = 0.03985036\n",
      "Iteration 3537, loss = 0.03984794\n",
      "Iteration 3538, loss = 0.03984553\n",
      "Iteration 3539, loss = 0.03984311\n",
      "Iteration 3540, loss = 0.03984069\n",
      "Iteration 3541, loss = 0.03983828\n",
      "Iteration 3542, loss = 0.03983586\n",
      "Iteration 3543, loss = 0.03983345\n",
      "Iteration 3544, loss = 0.03983103\n",
      "Iteration 3545, loss = 0.03982862\n",
      "Iteration 3546, loss = 0.03982621\n",
      "Iteration 3547, loss = 0.03982380\n",
      "Iteration 3548, loss = 0.03982138\n",
      "Iteration 3549, loss = 0.03981897\n",
      "Iteration 3550, loss = 0.03981656\n",
      "Iteration 3551, loss = 0.03981415\n",
      "Iteration 3552, loss = 0.03981175\n",
      "Iteration 3553, loss = 0.03980934\n",
      "Iteration 3554, loss = 0.03980693\n",
      "Iteration 3555, loss = 0.03980452\n",
      "Iteration 3556, loss = 0.03980212\n",
      "Iteration 3557, loss = 0.03979971\n",
      "Iteration 3558, loss = 0.03979731\n",
      "Iteration 3559, loss = 0.03979490\n",
      "Iteration 3560, loss = 0.03979250\n",
      "Iteration 3561, loss = 0.03979010\n",
      "Iteration 3562, loss = 0.03978770\n",
      "Iteration 3563, loss = 0.03978529\n",
      "Iteration 3564, loss = 0.03978289\n",
      "Iteration 3565, loss = 0.03978049\n",
      "Iteration 3566, loss = 0.03977809\n",
      "Iteration 3567, loss = 0.03977569\n",
      "Iteration 3568, loss = 0.03977329\n",
      "Iteration 3569, loss = 0.03977090\n",
      "Iteration 3570, loss = 0.03976850\n",
      "Iteration 3571, loss = 0.03976610\n",
      "Iteration 3572, loss = 0.03976371\n",
      "Iteration 3573, loss = 0.03976131\n",
      "Iteration 3574, loss = 0.03975892\n",
      "Iteration 3575, loss = 0.03975652\n",
      "Iteration 3576, loss = 0.03975413\n",
      "Iteration 3577, loss = 0.03975174\n",
      "Iteration 3578, loss = 0.03974934\n",
      "Iteration 3579, loss = 0.03974695\n",
      "Iteration 3580, loss = 0.03974456\n",
      "Iteration 3581, loss = 0.03974217\n",
      "Iteration 3582, loss = 0.03973978\n",
      "Iteration 3583, loss = 0.03973739\n",
      "Iteration 3584, loss = 0.03973500\n",
      "Iteration 3585, loss = 0.03973261\n",
      "Iteration 3586, loss = 0.03973022\n",
      "Iteration 3587, loss = 0.03972784\n",
      "Iteration 3588, loss = 0.03972545\n",
      "Iteration 3589, loss = 0.03972306\n",
      "Iteration 3590, loss = 0.03972068\n",
      "Iteration 3591, loss = 0.03971829\n",
      "Iteration 3592, loss = 0.03971591\n",
      "Iteration 3593, loss = 0.03971353\n",
      "Iteration 3594, loss = 0.03971114\n",
      "Iteration 3595, loss = 0.03970876\n",
      "Iteration 3596, loss = 0.03970638\n",
      "Iteration 3597, loss = 0.03970400\n",
      "Iteration 3598, loss = 0.03970161\n",
      "Iteration 3599, loss = 0.03969923\n",
      "Iteration 3600, loss = 0.03969685\n",
      "Iteration 3601, loss = 0.03969448\n",
      "Iteration 3602, loss = 0.03969210\n",
      "Iteration 3603, loss = 0.03968972\n",
      "Iteration 3604, loss = 0.03968734\n",
      "Iteration 3605, loss = 0.03968496\n",
      "Iteration 3606, loss = 0.03968259\n",
      "Iteration 3607, loss = 0.03968021\n",
      "Iteration 3608, loss = 0.03967784\n",
      "Iteration 3609, loss = 0.03967546\n",
      "Iteration 3610, loss = 0.03967309\n",
      "Iteration 3611, loss = 0.03967071\n",
      "Iteration 3612, loss = 0.03966834\n",
      "Iteration 3613, loss = 0.03966597\n",
      "Iteration 3614, loss = 0.03966359\n",
      "Iteration 3615, loss = 0.03966122\n",
      "Iteration 3616, loss = 0.03965885\n",
      "Iteration 3617, loss = 0.03965648\n",
      "Iteration 3618, loss = 0.03965411\n",
      "Iteration 3619, loss = 0.03965174\n",
      "Iteration 3620, loss = 0.03964937\n",
      "Iteration 3621, loss = 0.03964700\n",
      "Iteration 3622, loss = 0.03964463\n",
      "Iteration 3623, loss = 0.03964227\n",
      "Iteration 3624, loss = 0.03963990\n",
      "Iteration 3625, loss = 0.03963753\n",
      "Iteration 3626, loss = 0.03963517\n",
      "Iteration 3627, loss = 0.03963280\n",
      "Iteration 3628, loss = 0.03963044\n",
      "Iteration 3629, loss = 0.03962807\n",
      "Iteration 3630, loss = 0.03962571\n",
      "Iteration 3631, loss = 0.03962334\n",
      "Iteration 3632, loss = 0.03962098\n",
      "Iteration 3633, loss = 0.03961862\n",
      "Iteration 3634, loss = 0.03961626\n",
      "Iteration 3635, loss = 0.03961390\n",
      "Iteration 3636, loss = 0.03961153\n",
      "Iteration 3637, loss = 0.03960917\n",
      "Iteration 3638, loss = 0.03960681\n",
      "Iteration 3639, loss = 0.03960446\n",
      "Iteration 3640, loss = 0.03960210\n",
      "Iteration 3641, loss = 0.03959974\n",
      "Iteration 3642, loss = 0.03959738\n",
      "Iteration 3643, loss = 0.03959502\n",
      "Iteration 3644, loss = 0.03959267\n",
      "Iteration 3645, loss = 0.03959031\n",
      "Iteration 3646, loss = 0.03958795\n",
      "Iteration 3647, loss = 0.03958560\n",
      "Iteration 3648, loss = 0.03958324\n",
      "Iteration 3649, loss = 0.03958089\n",
      "Iteration 3650, loss = 0.03957853\n",
      "Iteration 3651, loss = 0.03957618\n",
      "Iteration 3652, loss = 0.03957383\n",
      "Iteration 3653, loss = 0.03957148\n",
      "Iteration 3654, loss = 0.03956912\n",
      "Iteration 3655, loss = 0.03956677\n",
      "Iteration 3656, loss = 0.03956442\n",
      "Iteration 3657, loss = 0.03956207\n",
      "Iteration 3658, loss = 0.03955972\n",
      "Iteration 3659, loss = 0.03955737\n",
      "Iteration 3660, loss = 0.03955502\n",
      "Iteration 3661, loss = 0.03955267\n",
      "Iteration 3662, loss = 0.03955032\n",
      "Iteration 3663, loss = 0.03954798\n",
      "Iteration 3664, loss = 0.03954563\n",
      "Iteration 3665, loss = 0.03954328\n",
      "Iteration 3666, loss = 0.03954094\n",
      "Iteration 3667, loss = 0.03953859\n",
      "Iteration 3668, loss = 0.03953625\n",
      "Iteration 3669, loss = 0.03953390\n",
      "Iteration 3670, loss = 0.03953156\n",
      "Iteration 3671, loss = 0.03952921\n",
      "Iteration 3672, loss = 0.03952687\n",
      "Iteration 3673, loss = 0.03952453\n",
      "Iteration 3674, loss = 0.03952218\n",
      "Iteration 3675, loss = 0.03951984\n",
      "Iteration 3676, loss = 0.03951750\n",
      "Iteration 3677, loss = 0.03951516\n",
      "Iteration 3678, loss = 0.03951282\n",
      "Iteration 3679, loss = 0.03951048\n",
      "Iteration 3680, loss = 0.03950814\n",
      "Iteration 3681, loss = 0.03950580\n",
      "Iteration 3682, loss = 0.03950346\n",
      "Iteration 3683, loss = 0.03950112\n",
      "Iteration 3684, loss = 0.03949879\n",
      "Iteration 3685, loss = 0.03949645\n",
      "Iteration 3686, loss = 0.03949411\n",
      "Iteration 3687, loss = 0.03949178\n",
      "Iteration 3688, loss = 0.03948944\n",
      "Iteration 3689, loss = 0.03948710\n",
      "Iteration 3690, loss = 0.03948477\n",
      "Iteration 3691, loss = 0.03948243\n",
      "Iteration 3692, loss = 0.03948010\n",
      "Iteration 3693, loss = 0.03947777\n",
      "Iteration 3694, loss = 0.03947543\n",
      "Iteration 3695, loss = 0.03947310\n",
      "Iteration 3696, loss = 0.03947077\n",
      "Iteration 3697, loss = 0.03946844\n",
      "Iteration 3698, loss = 0.03946611\n",
      "Iteration 3699, loss = 0.03946378\n",
      "Iteration 3700, loss = 0.03946145\n",
      "Iteration 3701, loss = 0.03945912\n",
      "Iteration 3702, loss = 0.03945679\n",
      "Iteration 3703, loss = 0.03945446\n",
      "Iteration 3704, loss = 0.03945213\n",
      "Iteration 3705, loss = 0.03944980\n",
      "Iteration 3706, loss = 0.03944747\n",
      "Iteration 3707, loss = 0.03944515\n",
      "Iteration 3708, loss = 0.03944282\n",
      "Iteration 3709, loss = 0.03944050\n",
      "Iteration 3710, loss = 0.03943817\n",
      "Iteration 3711, loss = 0.03943585\n",
      "Iteration 3712, loss = 0.03943352\n",
      "Iteration 3713, loss = 0.03943120\n",
      "Iteration 3714, loss = 0.03942887\n",
      "Iteration 3715, loss = 0.03942655\n",
      "Iteration 3716, loss = 0.03942423\n",
      "Iteration 3717, loss = 0.03942191\n",
      "Iteration 3718, loss = 0.03941958\n",
      "Iteration 3719, loss = 0.03941726\n",
      "Iteration 3720, loss = 0.03941494\n",
      "Iteration 3721, loss = 0.03941262\n",
      "Iteration 3722, loss = 0.03941030\n",
      "Iteration 3723, loss = 0.03940798\n",
      "Iteration 3724, loss = 0.03940566\n",
      "Iteration 3725, loss = 0.03940335\n",
      "Iteration 3726, loss = 0.03940103\n",
      "Iteration 3727, loss = 0.03939871\n",
      "Iteration 3728, loss = 0.03939639\n",
      "Iteration 3729, loss = 0.03939408\n",
      "Iteration 3730, loss = 0.03939176\n",
      "Iteration 3731, loss = 0.03938945\n",
      "Iteration 3732, loss = 0.03938713\n",
      "Iteration 3733, loss = 0.03938482\n",
      "Iteration 3734, loss = 0.03938250\n",
      "Iteration 3735, loss = 0.03938019\n",
      "Iteration 3736, loss = 0.03937788\n",
      "Iteration 3737, loss = 0.03937556\n",
      "Iteration 3738, loss = 0.03937325\n",
      "Iteration 3739, loss = 0.03937094\n",
      "Iteration 3740, loss = 0.03936863\n",
      "Iteration 3741, loss = 0.03936632\n",
      "Iteration 3742, loss = 0.03936401\n",
      "Iteration 3743, loss = 0.03936170\n",
      "Iteration 3744, loss = 0.03935939\n",
      "Iteration 3745, loss = 0.03935708\n",
      "Iteration 3746, loss = 0.03935477\n",
      "Iteration 3747, loss = 0.03935247\n",
      "Iteration 3748, loss = 0.03935016\n",
      "Iteration 3749, loss = 0.03934785\n",
      "Iteration 3750, loss = 0.03934555\n",
      "Iteration 3751, loss = 0.03934324\n",
      "Iteration 3752, loss = 0.03934094\n",
      "Iteration 3753, loss = 0.03933863\n",
      "Iteration 3754, loss = 0.03933633\n",
      "Iteration 3755, loss = 0.03933403\n",
      "Iteration 3756, loss = 0.03933172\n",
      "Iteration 3757, loss = 0.03932942\n",
      "Iteration 3758, loss = 0.03932712\n",
      "Iteration 3759, loss = 0.03932482\n",
      "Iteration 3760, loss = 0.03932252\n",
      "Iteration 3761, loss = 0.03932022\n",
      "Iteration 3762, loss = 0.03931792\n",
      "Iteration 3763, loss = 0.03931562\n",
      "Iteration 3764, loss = 0.03931332\n",
      "Iteration 3765, loss = 0.03931102\n",
      "Iteration 3766, loss = 0.03930872\n",
      "Iteration 3767, loss = 0.03930642\n",
      "Iteration 3768, loss = 0.03930413\n",
      "Iteration 3769, loss = 0.03930183\n",
      "Iteration 3770, loss = 0.03929954\n",
      "Iteration 3771, loss = 0.03929724\n",
      "Iteration 3772, loss = 0.03929495\n",
      "Iteration 3773, loss = 0.03929265\n",
      "Iteration 3774, loss = 0.03929036\n",
      "Iteration 3775, loss = 0.03928807\n",
      "Iteration 3776, loss = 0.03928577\n",
      "Iteration 3777, loss = 0.03928348\n",
      "Iteration 3778, loss = 0.03928119\n",
      "Iteration 3779, loss = 0.03927890\n",
      "Iteration 3780, loss = 0.03927661\n",
      "Iteration 3781, loss = 0.03927432\n",
      "Iteration 3782, loss = 0.03927203\n",
      "Iteration 3783, loss = 0.03926974\n",
      "Iteration 3784, loss = 0.03926746\n",
      "Iteration 3785, loss = 0.03926517\n",
      "Iteration 3786, loss = 0.03926288\n",
      "Iteration 3787, loss = 0.03926059\n",
      "Iteration 3788, loss = 0.03925831\n",
      "Iteration 3789, loss = 0.03925602\n",
      "Iteration 3790, loss = 0.03925374\n",
      "Iteration 3791, loss = 0.03925145\n",
      "Iteration 3792, loss = 0.03924917\n",
      "Iteration 3793, loss = 0.03924689\n",
      "Iteration 3794, loss = 0.03924461\n",
      "Iteration 3795, loss = 0.03924232\n",
      "Iteration 3796, loss = 0.03924004\n",
      "Iteration 3797, loss = 0.03923776\n",
      "Iteration 3798, loss = 0.03923548\n",
      "Iteration 3799, loss = 0.03923320\n",
      "Iteration 3800, loss = 0.03923092\n",
      "Iteration 3801, loss = 0.03922865\n",
      "Iteration 3802, loss = 0.03922637\n",
      "Iteration 3803, loss = 0.03922409\n",
      "Iteration 3804, loss = 0.03922181\n",
      "Iteration 3805, loss = 0.03921954\n",
      "Iteration 3806, loss = 0.03921726\n",
      "Iteration 3807, loss = 0.03921499\n",
      "Iteration 3808, loss = 0.03921271\n",
      "Iteration 3809, loss = 0.03921044\n",
      "Iteration 3810, loss = 0.03920817\n",
      "Iteration 3811, loss = 0.03920589\n",
      "Iteration 3812, loss = 0.03920362\n",
      "Iteration 3813, loss = 0.03920135\n",
      "Iteration 3814, loss = 0.03919908\n",
      "Iteration 3815, loss = 0.03919681\n",
      "Iteration 3816, loss = 0.03919454\n",
      "Iteration 3817, loss = 0.03919227\n",
      "Iteration 3818, loss = 0.03919000\n",
      "Iteration 3819, loss = 0.03918773\n",
      "Iteration 3820, loss = 0.03918547\n",
      "Iteration 3821, loss = 0.03918320\n",
      "Iteration 3822, loss = 0.03918094\n",
      "Iteration 3823, loss = 0.03917867\n",
      "Iteration 3824, loss = 0.03917641\n",
      "Iteration 3825, loss = 0.03917414\n",
      "Iteration 3826, loss = 0.03917188\n",
      "Iteration 3827, loss = 0.03916961\n",
      "Iteration 3828, loss = 0.03916735\n",
      "Iteration 3829, loss = 0.03916509\n",
      "Iteration 3830, loss = 0.03916283\n",
      "Iteration 3831, loss = 0.03916057\n",
      "Iteration 3832, loss = 0.03915831\n",
      "Iteration 3833, loss = 0.03915605\n",
      "Iteration 3834, loss = 0.03915379\n",
      "Iteration 3835, loss = 0.03915153\n",
      "Iteration 3836, loss = 0.03914928\n",
      "Iteration 3837, loss = 0.03914702\n",
      "Iteration 3838, loss = 0.03914477\n",
      "Iteration 3839, loss = 0.03914251\n",
      "Iteration 3840, loss = 0.03914026\n",
      "Iteration 3841, loss = 0.03913800\n",
      "Iteration 3842, loss = 0.03913575\n",
      "Iteration 3843, loss = 0.03913350\n",
      "Iteration 3844, loss = 0.03913124\n",
      "Iteration 3845, loss = 0.03912899\n",
      "Iteration 3846, loss = 0.03912674\n",
      "Iteration 3847, loss = 0.03912449\n",
      "Iteration 3848, loss = 0.03912224\n",
      "Iteration 3849, loss = 0.03911999\n",
      "Iteration 3850, loss = 0.03911774\n",
      "Iteration 3851, loss = 0.03911550\n",
      "Iteration 3852, loss = 0.03911325\n",
      "Iteration 3853, loss = 0.03911100\n",
      "Iteration 3854, loss = 0.03910876\n",
      "Iteration 3855, loss = 0.03910651\n",
      "Iteration 3856, loss = 0.03910427\n",
      "Iteration 3857, loss = 0.03910203\n",
      "Iteration 3858, loss = 0.03909978\n",
      "Iteration 3859, loss = 0.03909754\n",
      "Iteration 3860, loss = 0.03909530\n",
      "Iteration 3861, loss = 0.03909306\n",
      "Iteration 3862, loss = 0.03909082\n",
      "Iteration 3863, loss = 0.03908858\n",
      "Iteration 3864, loss = 0.03908634\n",
      "Iteration 3865, loss = 0.03908410\n",
      "Iteration 3866, loss = 0.03908186\n",
      "Iteration 3867, loss = 0.03907963\n",
      "Iteration 3868, loss = 0.03907739\n",
      "Iteration 3869, loss = 0.03907515\n",
      "Iteration 3870, loss = 0.03907292\n",
      "Iteration 3871, loss = 0.03907069\n",
      "Iteration 3872, loss = 0.03906845\n",
      "Iteration 3873, loss = 0.03906622\n",
      "Iteration 3874, loss = 0.03906399\n",
      "Iteration 3875, loss = 0.03906176\n",
      "Iteration 3876, loss = 0.03905952\n",
      "Iteration 3877, loss = 0.03905729\n",
      "Iteration 3878, loss = 0.03905507\n",
      "Iteration 3879, loss = 0.03905284\n",
      "Iteration 3880, loss = 0.03905061\n",
      "Iteration 3881, loss = 0.03904838\n",
      "Iteration 3882, loss = 0.03904615\n",
      "Iteration 3883, loss = 0.03904393\n",
      "Iteration 3884, loss = 0.03904170\n",
      "Iteration 3885, loss = 0.03903948\n",
      "Iteration 3886, loss = 0.03903726\n",
      "Iteration 3887, loss = 0.03903503\n",
      "Iteration 3888, loss = 0.03903281\n",
      "Iteration 3889, loss = 0.03903059\n",
      "Iteration 3890, loss = 0.03902837\n",
      "Iteration 3891, loss = 0.03902615\n",
      "Iteration 3892, loss = 0.03902393\n",
      "Iteration 3893, loss = 0.03902171\n",
      "Iteration 3894, loss = 0.03901949\n",
      "Iteration 3895, loss = 0.03901727\n",
      "Iteration 3896, loss = 0.03901506\n",
      "Iteration 3897, loss = 0.03901284\n",
      "Iteration 3898, loss = 0.03901062\n",
      "Iteration 3899, loss = 0.03900841\n",
      "Iteration 3900, loss = 0.03900620\n",
      "Iteration 3901, loss = 0.03900398\n",
      "Iteration 3902, loss = 0.03900177\n",
      "Iteration 3903, loss = 0.03899956\n",
      "Iteration 3904, loss = 0.03899735\n",
      "Iteration 3905, loss = 0.03899514\n",
      "Iteration 3906, loss = 0.03899293\n",
      "Iteration 3907, loss = 0.03899072\n",
      "Iteration 3908, loss = 0.03898851\n",
      "Iteration 3909, loss = 0.03898630\n",
      "Iteration 3910, loss = 0.03898409\n",
      "Iteration 3911, loss = 0.03898189\n",
      "Iteration 3912, loss = 0.03897968\n",
      "Iteration 3913, loss = 0.03897748\n",
      "Iteration 3914, loss = 0.03897527\n",
      "Iteration 3915, loss = 0.03897307\n",
      "Iteration 3916, loss = 0.03897087\n",
      "Iteration 3917, loss = 0.03896867\n",
      "Iteration 3918, loss = 0.03896647\n",
      "Iteration 3919, loss = 0.03896427\n",
      "Iteration 3920, loss = 0.03896207\n",
      "Iteration 3921, loss = 0.03895987\n",
      "Iteration 3922, loss = 0.03895767\n",
      "Iteration 3923, loss = 0.03895547\n",
      "Iteration 3924, loss = 0.03895327\n",
      "Iteration 3925, loss = 0.03895108\n",
      "Iteration 3926, loss = 0.03894888\n",
      "Iteration 3927, loss = 0.03894669\n",
      "Iteration 3928, loss = 0.03894450\n",
      "Iteration 3929, loss = 0.03894230\n",
      "Iteration 3930, loss = 0.03894011\n",
      "Iteration 3931, loss = 0.03893792\n",
      "Iteration 3932, loss = 0.03893573\n",
      "Iteration 3933, loss = 0.03893354\n",
      "Iteration 3934, loss = 0.03893135\n",
      "Iteration 3935, loss = 0.03892916\n",
      "Iteration 3936, loss = 0.03892697\n",
      "Iteration 3937, loss = 0.03892478\n",
      "Iteration 3938, loss = 0.03892260\n",
      "Iteration 3939, loss = 0.03892041\n",
      "Iteration 3940, loss = 0.03891823\n",
      "Iteration 3941, loss = 0.03891604\n",
      "Iteration 3942, loss = 0.03891386\n",
      "Iteration 3943, loss = 0.03891168\n",
      "Iteration 3944, loss = 0.03890950\n",
      "Iteration 3945, loss = 0.03890731\n",
      "Iteration 3946, loss = 0.03890513\n",
      "Iteration 3947, loss = 0.03890295\n",
      "Iteration 3948, loss = 0.03890077\n",
      "Iteration 3949, loss = 0.03889860\n",
      "Iteration 3950, loss = 0.03889642\n",
      "Iteration 3951, loss = 0.03889424\n",
      "Iteration 3952, loss = 0.03889207\n",
      "Iteration 3953, loss = 0.03888989\n",
      "Iteration 3954, loss = 0.03888772\n",
      "Iteration 3955, loss = 0.03888554\n",
      "Iteration 3956, loss = 0.03888337\n",
      "Iteration 3957, loss = 0.03888120\n",
      "Iteration 3958, loss = 0.03887903\n",
      "Iteration 3959, loss = 0.03887685\n",
      "Iteration 3960, loss = 0.03887468\n",
      "Iteration 3961, loss = 0.03887251\n",
      "Iteration 3962, loss = 0.03887035\n",
      "Iteration 3963, loss = 0.03886818\n",
      "Iteration 3964, loss = 0.03886601\n",
      "Iteration 3965, loss = 0.03886384\n",
      "Iteration 3966, loss = 0.03886168\n",
      "Iteration 3967, loss = 0.03885951\n",
      "Iteration 3968, loss = 0.03885735\n",
      "Iteration 3969, loss = 0.03885519\n",
      "Iteration 3970, loss = 0.03885302\n",
      "Iteration 3971, loss = 0.03885086\n",
      "Iteration 3972, loss = 0.03884870\n",
      "Iteration 3973, loss = 0.03884654\n",
      "Iteration 3974, loss = 0.03884438\n",
      "Iteration 3975, loss = 0.03884222\n",
      "Iteration 3976, loss = 0.03884006\n",
      "Iteration 3977, loss = 0.03883791\n",
      "Iteration 3978, loss = 0.03883575\n",
      "Iteration 3979, loss = 0.03883359\n",
      "Iteration 3980, loss = 0.03883144\n",
      "Iteration 3981, loss = 0.03882928\n",
      "Iteration 3982, loss = 0.03882713\n",
      "Iteration 3983, loss = 0.03882498\n",
      "Iteration 3984, loss = 0.03882283\n",
      "Iteration 3985, loss = 0.03882067\n",
      "Iteration 3986, loss = 0.03881852\n",
      "Iteration 3987, loss = 0.03881637\n",
      "Iteration 3988, loss = 0.03881422\n",
      "Iteration 3989, loss = 0.03881208\n",
      "Iteration 3990, loss = 0.03880993\n",
      "Iteration 3991, loss = 0.03880778\n",
      "Iteration 3992, loss = 0.03880563\n",
      "Iteration 3993, loss = 0.03880349\n",
      "Iteration 3994, loss = 0.03880134\n",
      "Iteration 3995, loss = 0.03879920\n",
      "Iteration 3996, loss = 0.03879706\n",
      "Iteration 3997, loss = 0.03879492\n",
      "Iteration 3998, loss = 0.03879277\n",
      "Iteration 3999, loss = 0.03879063\n",
      "Iteration 4000, loss = 0.03878849\n",
      "Iteration 4001, loss = 0.03878635\n",
      "Iteration 4002, loss = 0.03878421\n",
      "Iteration 4003, loss = 0.03878208\n",
      "Iteration 4004, loss = 0.03877994\n",
      "Iteration 4005, loss = 0.03877780\n",
      "Iteration 4006, loss = 0.03877567\n",
      "Iteration 4007, loss = 0.03877353\n",
      "Iteration 4008, loss = 0.03877140\n",
      "Iteration 4009, loss = 0.03876926\n",
      "Iteration 4010, loss = 0.03876713\n",
      "Iteration 4011, loss = 0.03876500\n",
      "Iteration 4012, loss = 0.03876287\n",
      "Iteration 4013, loss = 0.03876074\n",
      "Iteration 4014, loss = 0.03875861\n",
      "Iteration 4015, loss = 0.03875648\n",
      "Iteration 4016, loss = 0.03875435\n",
      "Iteration 4017, loss = 0.03875222\n",
      "Iteration 4018, loss = 0.03875010\n",
      "Iteration 4019, loss = 0.03874797\n",
      "Iteration 4020, loss = 0.03874585\n",
      "Iteration 4021, loss = 0.03874372\n",
      "Iteration 4022, loss = 0.03874160\n",
      "Iteration 4023, loss = 0.03873947\n",
      "Iteration 4024, loss = 0.03873735\n",
      "Iteration 4025, loss = 0.03873523\n",
      "Iteration 4026, loss = 0.03873311\n",
      "Iteration 4027, loss = 0.03873099\n",
      "Iteration 4028, loss = 0.03872887\n",
      "Iteration 4029, loss = 0.03872675\n",
      "Iteration 4030, loss = 0.03872463\n",
      "Iteration 4031, loss = 0.03872252\n",
      "Iteration 4032, loss = 0.03872040\n",
      "Iteration 4033, loss = 0.03871828\n",
      "Iteration 4034, loss = 0.03871617\n",
      "Iteration 4035, loss = 0.03871406\n",
      "Iteration 4036, loss = 0.03871194\n",
      "Iteration 4037, loss = 0.03870983\n",
      "Iteration 4038, loss = 0.03870772\n",
      "Iteration 4039, loss = 0.03870561\n",
      "Iteration 4040, loss = 0.03870350\n",
      "Iteration 4041, loss = 0.03870139\n",
      "Iteration 4042, loss = 0.03869928\n",
      "Iteration 4043, loss = 0.03869717\n",
      "Iteration 4044, loss = 0.03869506\n",
      "Iteration 4045, loss = 0.03869296\n",
      "Iteration 4046, loss = 0.03869085\n",
      "Iteration 4047, loss = 0.03868874\n",
      "Iteration 4048, loss = 0.03868664\n",
      "Iteration 4049, loss = 0.03868454\n",
      "Iteration 4050, loss = 0.03868243\n",
      "Iteration 4051, loss = 0.03868033\n",
      "Iteration 4052, loss = 0.03867823\n",
      "Iteration 4053, loss = 0.03867613\n",
      "Iteration 4054, loss = 0.03867403\n",
      "Iteration 4055, loss = 0.03867193\n",
      "Iteration 4056, loss = 0.03866983\n",
      "Iteration 4057, loss = 0.03866773\n",
      "Iteration 4058, loss = 0.03866564\n",
      "Iteration 4059, loss = 0.03866354\n",
      "Iteration 4060, loss = 0.03866145\n",
      "Iteration 4061, loss = 0.03865935\n",
      "Iteration 4062, loss = 0.03865726\n",
      "Iteration 4063, loss = 0.03865516\n",
      "Iteration 4064, loss = 0.03865307\n",
      "Iteration 4065, loss = 0.03865098\n",
      "Iteration 4066, loss = 0.03864889\n",
      "Iteration 4067, loss = 0.03864680\n",
      "Iteration 4068, loss = 0.03864471\n",
      "Iteration 4069, loss = 0.03864262\n",
      "Iteration 4070, loss = 0.03864053\n",
      "Iteration 4071, loss = 0.03863844\n",
      "Iteration 4072, loss = 0.03863636\n",
      "Iteration 4073, loss = 0.03863427\n",
      "Iteration 4074, loss = 0.03863218\n",
      "Iteration 4075, loss = 0.03863010\n",
      "Iteration 4076, loss = 0.03862802\n",
      "Iteration 4077, loss = 0.03862593\n",
      "Iteration 4078, loss = 0.03862385\n",
      "Iteration 4079, loss = 0.03862177\n",
      "Iteration 4080, loss = 0.03861969\n",
      "Iteration 4081, loss = 0.03861761\n",
      "Iteration 4082, loss = 0.03861553\n",
      "Iteration 4083, loss = 0.03861345\n",
      "Iteration 4084, loss = 0.03861137\n",
      "Iteration 4085, loss = 0.03860929\n",
      "Iteration 4086, loss = 0.03860722\n",
      "Iteration 4087, loss = 0.03860514\n",
      "Iteration 4088, loss = 0.03860307\n",
      "Iteration 4089, loss = 0.03860099\n",
      "Iteration 4090, loss = 0.03859892\n",
      "Iteration 4091, loss = 0.03859685\n",
      "Iteration 4092, loss = 0.03859477\n",
      "Iteration 4093, loss = 0.03859270\n",
      "Iteration 4094, loss = 0.03859063\n",
      "Iteration 4095, loss = 0.03858856\n",
      "Iteration 4096, loss = 0.03858649\n",
      "Iteration 4097, loss = 0.03858442\n",
      "Iteration 4098, loss = 0.03858236\n",
      "Iteration 4099, loss = 0.03858029\n",
      "Iteration 4100, loss = 0.03857822\n",
      "Iteration 4101, loss = 0.03857616\n",
      "Iteration 4102, loss = 0.03857409\n",
      "Iteration 4103, loss = 0.03857203\n",
      "Iteration 4104, loss = 0.03856997\n",
      "Iteration 4105, loss = 0.03856790\n",
      "Iteration 4106, loss = 0.03856584\n",
      "Iteration 4107, loss = 0.03856378\n",
      "Iteration 4108, loss = 0.03856172\n",
      "Iteration 4109, loss = 0.03855966\n",
      "Iteration 4110, loss = 0.03855760\n",
      "Iteration 4111, loss = 0.03855554\n",
      "Iteration 4112, loss = 0.03855348\n",
      "Iteration 4113, loss = 0.03855143\n",
      "Iteration 4114, loss = 0.03854937\n",
      "Iteration 4115, loss = 0.03854731\n",
      "Iteration 4116, loss = 0.03854526\n",
      "Iteration 4117, loss = 0.03854321\n",
      "Iteration 4118, loss = 0.03854115\n",
      "Iteration 4119, loss = 0.03853910\n",
      "Iteration 4120, loss = 0.03853705\n",
      "Iteration 4121, loss = 0.03853500\n",
      "Iteration 4122, loss = 0.03853295\n",
      "Iteration 4123, loss = 0.03853090\n",
      "Iteration 4124, loss = 0.03852885\n",
      "Iteration 4125, loss = 0.03852680\n",
      "Iteration 4126, loss = 0.03852475\n",
      "Iteration 4127, loss = 0.03852270\n",
      "Iteration 4128, loss = 0.03852066\n",
      "Iteration 4129, loss = 0.03851861\n",
      "Iteration 4130, loss = 0.03851657\n",
      "Iteration 4131, loss = 0.03851452\n",
      "Iteration 4132, loss = 0.03851248\n",
      "Iteration 4133, loss = 0.03851044\n",
      "Iteration 4134, loss = 0.03850840\n",
      "Iteration 4135, loss = 0.03850635\n",
      "Iteration 4136, loss = 0.03850431\n",
      "Iteration 4137, loss = 0.03850227\n",
      "Iteration 4138, loss = 0.03850023\n",
      "Iteration 4139, loss = 0.03849820\n",
      "Iteration 4140, loss = 0.03849616\n",
      "Iteration 4141, loss = 0.03849412\n",
      "Iteration 4142, loss = 0.03849209\n",
      "Iteration 4143, loss = 0.03849005\n",
      "Iteration 4144, loss = 0.03848801\n",
      "Iteration 4145, loss = 0.03848598\n",
      "Iteration 4146, loss = 0.03848395\n",
      "Iteration 4147, loss = 0.03848191\n",
      "Iteration 4148, loss = 0.03847988\n",
      "Iteration 4149, loss = 0.03847785\n",
      "Iteration 4150, loss = 0.03847582\n",
      "Iteration 4151, loss = 0.03847379\n",
      "Iteration 4152, loss = 0.03847176\n",
      "Iteration 4153, loss = 0.03846973\n",
      "Iteration 4154, loss = 0.03846770\n",
      "Iteration 4155, loss = 0.03846568\n",
      "Iteration 4156, loss = 0.03846365\n",
      "Iteration 4157, loss = 0.03846162\n",
      "Iteration 4158, loss = 0.03845960\n",
      "Iteration 4159, loss = 0.03845758\n",
      "Iteration 4160, loss = 0.03845555\n",
      "Iteration 4161, loss = 0.03845353\n",
      "Iteration 4162, loss = 0.03845151\n",
      "Iteration 4163, loss = 0.03844948\n",
      "Iteration 4164, loss = 0.03844746\n",
      "Iteration 4165, loss = 0.03844544\n",
      "Iteration 4166, loss = 0.03844342\n",
      "Iteration 4167, loss = 0.03844141\n",
      "Iteration 4168, loss = 0.03843939\n",
      "Iteration 4169, loss = 0.03843737\n",
      "Iteration 4170, loss = 0.03843535\n",
      "Iteration 4171, loss = 0.03843334\n",
      "Iteration 4172, loss = 0.03843132\n",
      "Iteration 4173, loss = 0.03842931\n",
      "Iteration 4174, loss = 0.03842729\n",
      "Iteration 4175, loss = 0.03842528\n",
      "Iteration 4176, loss = 0.03842327\n",
      "Iteration 4177, loss = 0.03842126\n",
      "Iteration 4178, loss = 0.03841924\n",
      "Iteration 4179, loss = 0.03841723\n",
      "Iteration 4180, loss = 0.03841522\n",
      "Iteration 4181, loss = 0.03841322\n",
      "Iteration 4182, loss = 0.03841121\n",
      "Iteration 4183, loss = 0.03840920\n",
      "Iteration 4184, loss = 0.03840719\n",
      "Iteration 4185, loss = 0.03840519\n",
      "Iteration 4186, loss = 0.03840318\n",
      "Iteration 4187, loss = 0.03840118\n",
      "Iteration 4188, loss = 0.03839917\n",
      "Iteration 4189, loss = 0.03839717\n",
      "Iteration 4190, loss = 0.03839516\n",
      "Iteration 4191, loss = 0.03839316\n",
      "Iteration 4192, loss = 0.03839116\n",
      "Iteration 4193, loss = 0.03838916\n",
      "Iteration 4194, loss = 0.03838716\n",
      "Iteration 4195, loss = 0.03838516\n",
      "Iteration 4196, loss = 0.03838316\n",
      "Iteration 4197, loss = 0.03838116\n",
      "Iteration 4198, loss = 0.03837917\n",
      "Iteration 4199, loss = 0.03837717\n",
      "Iteration 4200, loss = 0.03837517\n",
      "Iteration 4201, loss = 0.03837318\n",
      "Iteration 4202, loss = 0.03837118\n",
      "Iteration 4203, loss = 0.03836919\n",
      "Iteration 4204, loss = 0.03836720\n",
      "Iteration 4205, loss = 0.03836520\n",
      "Iteration 4206, loss = 0.03836321\n",
      "Iteration 4207, loss = 0.03836122\n",
      "Iteration 4208, loss = 0.03835923\n",
      "Iteration 4209, loss = 0.03835724\n",
      "Iteration 4210, loss = 0.03835525\n",
      "Iteration 4211, loss = 0.03835326\n",
      "Iteration 4212, loss = 0.03835127\n",
      "Iteration 4213, loss = 0.03834929\n",
      "Iteration 4214, loss = 0.03834730\n",
      "Iteration 4215, loss = 0.03834531\n",
      "Iteration 4216, loss = 0.03834333\n",
      "Iteration 4217, loss = 0.03834134\n",
      "Iteration 4218, loss = 0.03833936\n",
      "Iteration 4219, loss = 0.03833738\n",
      "Iteration 4220, loss = 0.03833539\n",
      "Iteration 4221, loss = 0.03833341\n",
      "Iteration 4222, loss = 0.03833143\n",
      "Iteration 4223, loss = 0.03832945\n",
      "Iteration 4224, loss = 0.03832747\n",
      "Iteration 4225, loss = 0.03832549\n",
      "Iteration 4226, loss = 0.03832351\n",
      "Iteration 4227, loss = 0.03832153\n",
      "Iteration 4228, loss = 0.03831956\n",
      "Iteration 4229, loss = 0.03831758\n",
      "Iteration 4230, loss = 0.03831561\n",
      "Iteration 4231, loss = 0.03831363\n",
      "Iteration 4232, loss = 0.03831166\n",
      "Iteration 4233, loss = 0.03830968\n",
      "Iteration 4234, loss = 0.03830771\n",
      "Iteration 4235, loss = 0.03830574\n",
      "Iteration 4236, loss = 0.03830376\n",
      "Iteration 4237, loss = 0.03830179\n",
      "Iteration 4238, loss = 0.03829982\n",
      "Iteration 4239, loss = 0.03829785\n",
      "Iteration 4240, loss = 0.03829588\n",
      "Iteration 4241, loss = 0.03829392\n",
      "Iteration 4242, loss = 0.03829195\n",
      "Iteration 4243, loss = 0.03828998\n",
      "Iteration 4244, loss = 0.03828801\n",
      "Iteration 4245, loss = 0.03828605\n",
      "Iteration 4246, loss = 0.03828408\n",
      "Iteration 4247, loss = 0.03828212\n",
      "Iteration 4248, loss = 0.03828015\n",
      "Iteration 4249, loss = 0.03827819\n",
      "Iteration 4250, loss = 0.03827623\n",
      "Iteration 4251, loss = 0.03827427\n",
      "Iteration 4252, loss = 0.03827231\n",
      "Iteration 4253, loss = 0.03827035\n",
      "Iteration 4254, loss = 0.03826839\n",
      "Iteration 4255, loss = 0.03826643\n",
      "Iteration 4256, loss = 0.03826447\n",
      "Iteration 4257, loss = 0.03826251\n",
      "Iteration 4258, loss = 0.03826055\n",
      "Iteration 4259, loss = 0.03825860\n",
      "Iteration 4260, loss = 0.03825664\n",
      "Iteration 4261, loss = 0.03825468\n",
      "Iteration 4262, loss = 0.03825273\n",
      "Iteration 4263, loss = 0.03825078\n",
      "Iteration 4264, loss = 0.03824882\n",
      "Iteration 4265, loss = 0.03824687\n",
      "Iteration 4266, loss = 0.03824492\n",
      "Iteration 4267, loss = 0.03824297\n",
      "Iteration 4268, loss = 0.03824102\n",
      "Iteration 4269, loss = 0.03823907\n",
      "Iteration 4270, loss = 0.03823712\n",
      "Iteration 4271, loss = 0.03823517\n",
      "Iteration 4272, loss = 0.03823322\n",
      "Iteration 4273, loss = 0.03823127\n",
      "Iteration 4274, loss = 0.03822932\n",
      "Iteration 4275, loss = 0.03822738\n",
      "Iteration 4276, loss = 0.03822543\n",
      "Iteration 4277, loss = 0.03822349\n",
      "Iteration 4278, loss = 0.03822154\n",
      "Iteration 4279, loss = 0.03821960\n",
      "Iteration 4280, loss = 0.03821766\n",
      "Iteration 4281, loss = 0.03821571\n",
      "Iteration 4282, loss = 0.03821377\n",
      "Iteration 4283, loss = 0.03821183\n",
      "Iteration 4284, loss = 0.03820989\n",
      "Iteration 4285, loss = 0.03820795\n",
      "Iteration 4286, loss = 0.03820601\n",
      "Iteration 4287, loss = 0.03820407\n",
      "Iteration 4288, loss = 0.03820214\n",
      "Iteration 4289, loss = 0.03820020\n",
      "Iteration 4290, loss = 0.03819826\n",
      "Iteration 4291, loss = 0.03819633\n",
      "Iteration 4292, loss = 0.03819439\n",
      "Iteration 4293, loss = 0.03819246\n",
      "Iteration 4294, loss = 0.03819052\n",
      "Iteration 4295, loss = 0.03818859\n",
      "Iteration 4296, loss = 0.03818666\n",
      "Iteration 4297, loss = 0.03818473\n",
      "Iteration 4298, loss = 0.03818279\n",
      "Iteration 4299, loss = 0.03818086\n",
      "Iteration 4300, loss = 0.03817893\n",
      "Iteration 4301, loss = 0.03817700\n",
      "Iteration 4302, loss = 0.03817508\n",
      "Iteration 4303, loss = 0.03817315\n",
      "Iteration 4304, loss = 0.03817122\n",
      "Iteration 4305, loss = 0.03816929\n",
      "Iteration 4306, loss = 0.03816737\n",
      "Iteration 4307, loss = 0.03816544\n",
      "Iteration 4308, loss = 0.03816352\n",
      "Iteration 4309, loss = 0.03816159\n",
      "Iteration 4310, loss = 0.03815967\n",
      "Iteration 4311, loss = 0.03815774\n",
      "Iteration 4312, loss = 0.03815582\n",
      "Iteration 4313, loss = 0.03815390\n",
      "Iteration 4314, loss = 0.03815198\n",
      "Iteration 4315, loss = 0.03815006\n",
      "Iteration 4316, loss = 0.03814814\n",
      "Iteration 4317, loss = 0.03814622\n",
      "Iteration 4318, loss = 0.03814430\n",
      "Iteration 4319, loss = 0.03814238\n",
      "Iteration 4320, loss = 0.03814046\n",
      "Iteration 4321, loss = 0.03813855\n",
      "Iteration 4322, loss = 0.03813663\n",
      "Iteration 4323, loss = 0.03813472\n",
      "Iteration 4324, loss = 0.03813280\n",
      "Iteration 4325, loss = 0.03813089\n",
      "Iteration 4326, loss = 0.03812897\n",
      "Iteration 4327, loss = 0.03812706\n",
      "Iteration 4328, loss = 0.03812515\n",
      "Iteration 4329, loss = 0.03812324\n",
      "Iteration 4330, loss = 0.03812132\n",
      "Iteration 4331, loss = 0.03811941\n",
      "Iteration 4332, loss = 0.03811750\n",
      "Iteration 4333, loss = 0.03811560\n",
      "Iteration 4334, loss = 0.03811369\n",
      "Iteration 4335, loss = 0.03811178\n",
      "Iteration 4336, loss = 0.03810987\n",
      "Iteration 4337, loss = 0.03810796\n",
      "Iteration 4338, loss = 0.03810606\n",
      "Iteration 4339, loss = 0.03810415\n",
      "Iteration 4340, loss = 0.03810225\n",
      "Iteration 4341, loss = 0.03810034\n",
      "Iteration 4342, loss = 0.03809844\n",
      "Iteration 4343, loss = 0.03809654\n",
      "Iteration 4344, loss = 0.03809463\n",
      "Iteration 4345, loss = 0.03809273\n",
      "Iteration 4346, loss = 0.03809083\n",
      "Iteration 4347, loss = 0.03808893\n",
      "Iteration 4348, loss = 0.03808703\n",
      "Iteration 4349, loss = 0.03808513\n",
      "Iteration 4350, loss = 0.03808323\n",
      "Iteration 4351, loss = 0.03808133\n",
      "Iteration 4352, loss = 0.03807944\n",
      "Iteration 4353, loss = 0.03807754\n",
      "Iteration 4354, loss = 0.03807564\n",
      "Iteration 4355, loss = 0.03807375\n",
      "Iteration 4356, loss = 0.03807185\n",
      "Iteration 4357, loss = 0.03806996\n",
      "Iteration 4358, loss = 0.03806806\n",
      "Iteration 4359, loss = 0.03806617\n",
      "Iteration 4360, loss = 0.03806428\n",
      "Iteration 4361, loss = 0.03806239\n",
      "Iteration 4362, loss = 0.03806050\n",
      "Iteration 4363, loss = 0.03805860\n",
      "Iteration 4364, loss = 0.03805671\n",
      "Iteration 4365, loss = 0.03805483\n",
      "Iteration 4366, loss = 0.03805294\n",
      "Iteration 4367, loss = 0.03805105\n",
      "Iteration 4368, loss = 0.03804916\n",
      "Iteration 4369, loss = 0.03804727\n",
      "Iteration 4370, loss = 0.03804539\n",
      "Iteration 4371, loss = 0.03804350\n",
      "Iteration 4372, loss = 0.03804162\n",
      "Iteration 4373, loss = 0.03803973\n",
      "Iteration 4374, loss = 0.03803785\n",
      "Iteration 4375, loss = 0.03803596\n",
      "Iteration 4376, loss = 0.03803408\n",
      "Iteration 4377, loss = 0.03803220\n",
      "Iteration 4378, loss = 0.03803032\n",
      "Iteration 4379, loss = 0.03802844\n",
      "Iteration 4380, loss = 0.03802656\n",
      "Iteration 4381, loss = 0.03802468\n",
      "Iteration 4382, loss = 0.03802280\n",
      "Iteration 4383, loss = 0.03802092\n",
      "Iteration 4384, loss = 0.03801904\n",
      "Iteration 4385, loss = 0.03801716\n",
      "Iteration 4386, loss = 0.03801529\n",
      "Iteration 4387, loss = 0.03801341\n",
      "Iteration 4388, loss = 0.03801153\n",
      "Iteration 4389, loss = 0.03800966\n",
      "Iteration 4390, loss = 0.03800778\n",
      "Iteration 4391, loss = 0.03800591\n",
      "Iteration 4392, loss = 0.03800404\n",
      "Iteration 4393, loss = 0.03800216\n",
      "Iteration 4394, loss = 0.03800029\n",
      "Iteration 4395, loss = 0.03799842\n",
      "Iteration 4396, loss = 0.03799655\n",
      "Iteration 4397, loss = 0.03799468\n",
      "Iteration 4398, loss = 0.03799281\n",
      "Iteration 4399, loss = 0.03799094\n",
      "Iteration 4400, loss = 0.03798907\n",
      "Iteration 4401, loss = 0.03798720\n",
      "Iteration 4402, loss = 0.03798534\n",
      "Iteration 4403, loss = 0.03798347\n",
      "Iteration 4404, loss = 0.03798160\n",
      "Iteration 4405, loss = 0.03797974\n",
      "Iteration 4406, loss = 0.03797787\n",
      "Iteration 4407, loss = 0.03797601\n",
      "Iteration 4408, loss = 0.03797415\n",
      "Iteration 4409, loss = 0.03797228\n",
      "Iteration 4410, loss = 0.03797042\n",
      "Iteration 4411, loss = 0.03796856\n",
      "Iteration 4412, loss = 0.03796670\n",
      "Iteration 4413, loss = 0.03796484\n",
      "Iteration 4414, loss = 0.03796298\n",
      "Iteration 4415, loss = 0.03796112\n",
      "Iteration 4416, loss = 0.03795926\n",
      "Iteration 4417, loss = 0.03795740\n",
      "Iteration 4418, loss = 0.03795554\n",
      "Iteration 4419, loss = 0.03795368\n",
      "Iteration 4420, loss = 0.03795183\n",
      "Iteration 4421, loss = 0.03794997\n",
      "Iteration 4422, loss = 0.03794812\n",
      "Iteration 4423, loss = 0.03794626\n",
      "Iteration 4424, loss = 0.03794441\n",
      "Iteration 4425, loss = 0.03794255\n",
      "Iteration 4426, loss = 0.03794070\n",
      "Iteration 4427, loss = 0.03793885\n",
      "Iteration 4428, loss = 0.03793700\n",
      "Iteration 4429, loss = 0.03793514\n",
      "Iteration 4430, loss = 0.03793329\n",
      "Iteration 4431, loss = 0.03793144\n",
      "Iteration 4432, loss = 0.03792959\n",
      "Iteration 4433, loss = 0.03792774\n",
      "Iteration 4434, loss = 0.03792590\n",
      "Iteration 4435, loss = 0.03792405\n",
      "Iteration 4436, loss = 0.03792220\n",
      "Iteration 4437, loss = 0.03792035\n",
      "Iteration 4438, loss = 0.03791851\n",
      "Iteration 4439, loss = 0.03791666\n",
      "Iteration 4440, loss = 0.03791482\n",
      "Iteration 4441, loss = 0.03791297\n",
      "Iteration 4442, loss = 0.03791113\n",
      "Iteration 4443, loss = 0.03790929\n",
      "Iteration 4444, loss = 0.03790744\n",
      "Iteration 4445, loss = 0.03790560\n",
      "Iteration 4446, loss = 0.03790376\n",
      "Iteration 4447, loss = 0.03790192\n",
      "Iteration 4448, loss = 0.03790008\n",
      "Iteration 4449, loss = 0.03789824\n",
      "Iteration 4450, loss = 0.03789640\n",
      "Iteration 4451, loss = 0.03789456\n",
      "Iteration 4452, loss = 0.03789272\n",
      "Iteration 4453, loss = 0.03789088\n",
      "Iteration 4454, loss = 0.03788905\n",
      "Iteration 4455, loss = 0.03788721\n",
      "Iteration 4456, loss = 0.03788537\n",
      "Iteration 4457, loss = 0.03788354\n",
      "Iteration 4458, loss = 0.03788170\n",
      "Iteration 4459, loss = 0.03787987\n",
      "Iteration 4460, loss = 0.03787804\n",
      "Iteration 4461, loss = 0.03787620\n",
      "Iteration 4462, loss = 0.03787437\n",
      "Iteration 4463, loss = 0.03787254\n",
      "Iteration 4464, loss = 0.03787071\n",
      "Iteration 4465, loss = 0.03786888\n",
      "Iteration 4466, loss = 0.03786705\n",
      "Iteration 4467, loss = 0.03786522\n",
      "Iteration 4468, loss = 0.03786339\n",
      "Iteration 4469, loss = 0.03786156\n",
      "Iteration 4470, loss = 0.03785973\n",
      "Iteration 4471, loss = 0.03785791\n",
      "Iteration 4472, loss = 0.03785608\n",
      "Iteration 4473, loss = 0.03785425\n",
      "Iteration 4474, loss = 0.03785243\n",
      "Iteration 4475, loss = 0.03785060\n",
      "Iteration 4476, loss = 0.03784878\n",
      "Iteration 4477, loss = 0.03784695\n",
      "Iteration 4478, loss = 0.03784513\n",
      "Iteration 4479, loss = 0.03784331\n",
      "Iteration 4480, loss = 0.03784148\n",
      "Iteration 4481, loss = 0.03783966\n",
      "Iteration 4482, loss = 0.03783784\n",
      "Iteration 4483, loss = 0.03783602\n",
      "Iteration 4484, loss = 0.03783420\n",
      "Iteration 4485, loss = 0.03783238\n",
      "Iteration 4486, loss = 0.03783056\n",
      "Iteration 4487, loss = 0.03782874\n",
      "Iteration 4488, loss = 0.03782693\n",
      "Iteration 4489, loss = 0.03782511\n",
      "Iteration 4490, loss = 0.03782329\n",
      "Iteration 4491, loss = 0.03782148\n",
      "Iteration 4492, loss = 0.03781966\n",
      "Iteration 4493, loss = 0.03781785\n",
      "Iteration 4494, loss = 0.03781603\n",
      "Iteration 4495, loss = 0.03781422\n",
      "Iteration 4496, loss = 0.03781240\n",
      "Iteration 4497, loss = 0.03781059\n",
      "Iteration 4498, loss = 0.03780878\n",
      "Iteration 4499, loss = 0.03780697\n",
      "Iteration 4500, loss = 0.03780516\n",
      "Iteration 4501, loss = 0.03780334\n",
      "Iteration 4502, loss = 0.03780153\n",
      "Iteration 4503, loss = 0.03779973\n",
      "Iteration 4504, loss = 0.03779792\n",
      "Iteration 4505, loss = 0.03779611\n",
      "Iteration 4506, loss = 0.03779430\n",
      "Iteration 4507, loss = 0.03779249\n",
      "Iteration 4508, loss = 0.03779069\n",
      "Iteration 4509, loss = 0.03778888\n",
      "Iteration 4510, loss = 0.03778707\n",
      "Iteration 4511, loss = 0.03778527\n",
      "Iteration 4512, loss = 0.03778346\n",
      "Iteration 4513, loss = 0.03778166\n",
      "Iteration 4514, loss = 0.03777986\n",
      "Iteration 4515, loss = 0.03777805\n",
      "Iteration 4516, loss = 0.03777625\n",
      "Iteration 4517, loss = 0.03777445\n",
      "Iteration 4518, loss = 0.03777265\n",
      "Iteration 4519, loss = 0.03777085\n",
      "Iteration 4520, loss = 0.03776905\n",
      "Iteration 4521, loss = 0.03776725\n",
      "Iteration 4522, loss = 0.03776545\n",
      "Iteration 4523, loss = 0.03776365\n",
      "Iteration 4524, loss = 0.03776185\n",
      "Iteration 4525, loss = 0.03776005\n",
      "Iteration 4526, loss = 0.03775826\n",
      "Iteration 4527, loss = 0.03775646\n",
      "Iteration 4528, loss = 0.03775466\n",
      "Iteration 4529, loss = 0.03775287\n",
      "Iteration 4530, loss = 0.03775107\n",
      "Iteration 4531, loss = 0.03774928\n",
      "Iteration 4532, loss = 0.03774749\n",
      "Iteration 4533, loss = 0.03774569\n",
      "Iteration 4534, loss = 0.03774390\n",
      "Iteration 4535, loss = 0.03774211\n",
      "Iteration 4536, loss = 0.03774032\n",
      "Iteration 4537, loss = 0.03773852\n",
      "Iteration 4538, loss = 0.03773673\n",
      "Iteration 4539, loss = 0.03773494\n",
      "Iteration 4540, loss = 0.03773315\n",
      "Iteration 4541, loss = 0.03773136\n",
      "Iteration 4542, loss = 0.03772958\n",
      "Iteration 4543, loss = 0.03772779\n",
      "Iteration 4544, loss = 0.03772600\n",
      "Iteration 4545, loss = 0.03772421\n",
      "Iteration 4546, loss = 0.03772243\n",
      "Iteration 4547, loss = 0.03772064\n",
      "Iteration 4548, loss = 0.03771886\n",
      "Iteration 4549, loss = 0.03771707\n",
      "Iteration 4550, loss = 0.03771529\n",
      "Iteration 4551, loss = 0.03771350\n",
      "Iteration 4552, loss = 0.03771172\n",
      "Iteration 4553, loss = 0.03770994\n",
      "Iteration 4554, loss = 0.03770816\n",
      "Iteration 4555, loss = 0.03770637\n",
      "Iteration 4556, loss = 0.03770459\n",
      "Iteration 4557, loss = 0.03770281\n",
      "Iteration 4558, loss = 0.03770103\n",
      "Iteration 4559, loss = 0.03769925\n",
      "Iteration 4560, loss = 0.03769747\n",
      "Iteration 4561, loss = 0.03769570\n",
      "Iteration 4562, loss = 0.03769392\n",
      "Iteration 4563, loss = 0.03769214\n",
      "Iteration 4564, loss = 0.03769036\n",
      "Iteration 4565, loss = 0.03768859\n",
      "Iteration 4566, loss = 0.03768681\n",
      "Iteration 4567, loss = 0.03768504\n",
      "Iteration 4568, loss = 0.03768326\n",
      "Iteration 4569, loss = 0.03768149\n",
      "Iteration 4570, loss = 0.03767971\n",
      "Iteration 4571, loss = 0.03767794\n",
      "Iteration 4572, loss = 0.03767617\n",
      "Iteration 4573, loss = 0.03767439\n",
      "Iteration 4574, loss = 0.03767262\n",
      "Iteration 4575, loss = 0.03767085\n",
      "Iteration 4576, loss = 0.03766908\n",
      "Iteration 4577, loss = 0.03766731\n",
      "Iteration 4578, loss = 0.03766554\n",
      "Iteration 4579, loss = 0.03766377\n",
      "Iteration 4580, loss = 0.03766200\n",
      "Iteration 4581, loss = 0.03766023\n",
      "Iteration 4582, loss = 0.03765847\n",
      "Iteration 4583, loss = 0.03765670\n",
      "Iteration 4584, loss = 0.03765493\n",
      "Iteration 4585, loss = 0.03765317\n",
      "Iteration 4586, loss = 0.03765140\n",
      "Iteration 4587, loss = 0.03764964\n",
      "Iteration 4588, loss = 0.03764787\n",
      "Iteration 4589, loss = 0.03764611\n",
      "Iteration 4590, loss = 0.03764434\n",
      "Iteration 4591, loss = 0.03764258\n",
      "Iteration 4592, loss = 0.03764082\n",
      "Iteration 4593, loss = 0.03763906\n",
      "Iteration 4594, loss = 0.03763729\n",
      "Iteration 4595, loss = 0.03763553\n",
      "Iteration 4596, loss = 0.03763377\n",
      "Iteration 4597, loss = 0.03763201\n",
      "Iteration 4598, loss = 0.03763025\n",
      "Iteration 4599, loss = 0.03762849\n",
      "Iteration 4600, loss = 0.03762674\n",
      "Iteration 4601, loss = 0.03762498\n",
      "Iteration 4602, loss = 0.03762322\n",
      "Iteration 4603, loss = 0.03762146\n",
      "Iteration 4604, loss = 0.03761971\n",
      "Iteration 4605, loss = 0.03761795\n",
      "Iteration 4606, loss = 0.03761620\n",
      "Iteration 4607, loss = 0.03761444\n",
      "Iteration 4608, loss = 0.03761269\n",
      "Iteration 4609, loss = 0.03761093\n",
      "Iteration 4610, loss = 0.03760918\n",
      "Iteration 4611, loss = 0.03760743\n",
      "Iteration 4612, loss = 0.03760567\n",
      "Iteration 4613, loss = 0.03760392\n",
      "Iteration 4614, loss = 0.03760217\n",
      "Iteration 4615, loss = 0.03760042\n",
      "Iteration 4616, loss = 0.03759867\n",
      "Iteration 4617, loss = 0.03759692\n",
      "Iteration 4618, loss = 0.03759517\n",
      "Iteration 4619, loss = 0.03759342\n",
      "Iteration 4620, loss = 0.03759167\n",
      "Iteration 4621, loss = 0.03758992\n",
      "Iteration 4622, loss = 0.03758817\n",
      "Iteration 4623, loss = 0.03758643\n",
      "Iteration 4624, loss = 0.03758468\n",
      "Iteration 4625, loss = 0.03758294\n",
      "Iteration 4626, loss = 0.03758119\n",
      "Iteration 4627, loss = 0.03757944\n",
      "Iteration 4628, loss = 0.03757770\n",
      "Iteration 4629, loss = 0.03757596\n",
      "Iteration 4630, loss = 0.03757421\n",
      "Iteration 4631, loss = 0.03757247\n",
      "Iteration 4632, loss = 0.03757073\n",
      "Iteration 4633, loss = 0.03756898\n",
      "Iteration 4634, loss = 0.03756724\n",
      "Iteration 4635, loss = 0.03756550\n",
      "Iteration 4636, loss = 0.03756376\n",
      "Iteration 4637, loss = 0.03756202\n",
      "Iteration 4638, loss = 0.03756028\n",
      "Iteration 4639, loss = 0.03755854\n",
      "Iteration 4640, loss = 0.03755680\n",
      "Iteration 4641, loss = 0.03755506\n",
      "Iteration 4642, loss = 0.03755333\n",
      "Iteration 4643, loss = 0.03755159\n",
      "Iteration 4644, loss = 0.03754985\n",
      "Iteration 4645, loss = 0.03754812\n",
      "Iteration 4646, loss = 0.03754638\n",
      "Iteration 4647, loss = 0.03754464\n",
      "Iteration 4648, loss = 0.03754291\n",
      "Iteration 4649, loss = 0.03754117\n",
      "Iteration 4650, loss = 0.03753944\n",
      "Iteration 4651, loss = 0.03753771\n",
      "Iteration 4652, loss = 0.03753597\n",
      "Iteration 4653, loss = 0.03753424\n",
      "Iteration 4654, loss = 0.03753251\n",
      "Iteration 4655, loss = 0.03753078\n",
      "Iteration 4656, loss = 0.03752905\n",
      "Iteration 4657, loss = 0.03752732\n",
      "Iteration 4658, loss = 0.03752559\n",
      "Iteration 4659, loss = 0.03752386\n",
      "Iteration 4660, loss = 0.03752213\n",
      "Iteration 4661, loss = 0.03752040\n",
      "Iteration 4662, loss = 0.03751867\n",
      "Iteration 4663, loss = 0.03751694\n",
      "Iteration 4664, loss = 0.03751522\n",
      "Iteration 4665, loss = 0.03751349\n",
      "Iteration 4666, loss = 0.03751176\n",
      "Iteration 4667, loss = 0.03751004\n",
      "Iteration 4668, loss = 0.03750831\n",
      "Iteration 4669, loss = 0.03750659\n",
      "Iteration 4670, loss = 0.03750486\n",
      "Iteration 4671, loss = 0.03750314\n",
      "Iteration 4672, loss = 0.03750141\n",
      "Iteration 4673, loss = 0.03749969\n",
      "Iteration 4674, loss = 0.03749797\n",
      "Iteration 4675, loss = 0.03749625\n",
      "Iteration 4676, loss = 0.03749452\n",
      "Iteration 4677, loss = 0.03749280\n",
      "Iteration 4678, loss = 0.03749108\n",
      "Iteration 4679, loss = 0.03748936\n",
      "Iteration 4680, loss = 0.03748764\n",
      "Iteration 4681, loss = 0.03748592\n",
      "Iteration 4682, loss = 0.03748420\n",
      "Iteration 4683, loss = 0.03748248\n",
      "Iteration 4684, loss = 0.03748077\n",
      "Iteration 4685, loss = 0.03747905\n",
      "Iteration 4686, loss = 0.03747733\n",
      "Iteration 4687, loss = 0.03747562\n",
      "Iteration 4688, loss = 0.03747390\n",
      "Iteration 4689, loss = 0.03747218\n",
      "Iteration 4690, loss = 0.03747047\n",
      "Iteration 4691, loss = 0.03746875\n",
      "Iteration 4692, loss = 0.03746704\n",
      "Iteration 4693, loss = 0.03746532\n",
      "Iteration 4694, loss = 0.03746361\n",
      "Iteration 4695, loss = 0.03746190\n",
      "Iteration 4696, loss = 0.03746019\n",
      "Iteration 4697, loss = 0.03745847\n",
      "Iteration 4698, loss = 0.03745676\n",
      "Iteration 4699, loss = 0.03745505\n",
      "Iteration 4700, loss = 0.03745334\n",
      "Iteration 4701, loss = 0.03745163\n",
      "Iteration 4702, loss = 0.03744992\n",
      "Iteration 4703, loss = 0.03744821\n",
      "Iteration 4704, loss = 0.03744650\n",
      "Iteration 4705, loss = 0.03744479\n",
      "Iteration 4706, loss = 0.03744309\n",
      "Iteration 4707, loss = 0.03744138\n",
      "Iteration 4708, loss = 0.03743967\n",
      "Iteration 4709, loss = 0.03743796\n",
      "Iteration 4710, loss = 0.03743626\n",
      "Iteration 4711, loss = 0.03743455\n",
      "Iteration 4712, loss = 0.03743285\n",
      "Iteration 4713, loss = 0.03743114\n",
      "Iteration 4714, loss = 0.03742944\n",
      "Iteration 4715, loss = 0.03742773\n",
      "Iteration 4716, loss = 0.03742603\n",
      "Iteration 4717, loss = 0.03742433\n",
      "Iteration 4718, loss = 0.03742262\n",
      "Iteration 4719, loss = 0.03742092\n",
      "Iteration 4720, loss = 0.03741922\n",
      "Iteration 4721, loss = 0.03741752\n",
      "Iteration 4722, loss = 0.03741582\n",
      "Iteration 4723, loss = 0.03741412\n",
      "Iteration 4724, loss = 0.03741242\n",
      "Iteration 4725, loss = 0.03741072\n",
      "Iteration 4726, loss = 0.03740902\n",
      "Iteration 4727, loss = 0.03740732\n",
      "Iteration 4728, loss = 0.03740562\n",
      "Iteration 4729, loss = 0.03740392\n",
      "Iteration 4730, loss = 0.03740223\n",
      "Iteration 4731, loss = 0.03740053\n",
      "Iteration 4732, loss = 0.03739883\n",
      "Iteration 4733, loss = 0.03739714\n",
      "Iteration 4734, loss = 0.03739544\n",
      "Iteration 4735, loss = 0.03739375\n",
      "Iteration 4736, loss = 0.03739205\n",
      "Iteration 4737, loss = 0.03739036\n",
      "Iteration 4738, loss = 0.03738866\n",
      "Iteration 4739, loss = 0.03738697\n",
      "Iteration 4740, loss = 0.03738528\n",
      "Iteration 4741, loss = 0.03738358\n",
      "Iteration 4742, loss = 0.03738189\n",
      "Iteration 4743, loss = 0.03738020\n",
      "Iteration 4744, loss = 0.03737851\n",
      "Iteration 4745, loss = 0.03737682\n",
      "Iteration 4746, loss = 0.03737513\n",
      "Iteration 4747, loss = 0.03737344\n",
      "Iteration 4748, loss = 0.03737175\n",
      "Iteration 4749, loss = 0.03737006\n",
      "Iteration 4750, loss = 0.03736837\n",
      "Iteration 4751, loss = 0.03736668\n",
      "Iteration 4752, loss = 0.03736499\n",
      "Iteration 4753, loss = 0.03736330\n",
      "Iteration 4754, loss = 0.03736162\n",
      "Iteration 4755, loss = 0.03735993\n",
      "Iteration 4756, loss = 0.03735824\n",
      "Iteration 4757, loss = 0.03735656\n",
      "Iteration 4758, loss = 0.03735487\n",
      "Iteration 4759, loss = 0.03735319\n",
      "Iteration 4760, loss = 0.03735150\n",
      "Iteration 4761, loss = 0.03734982\n",
      "Iteration 4762, loss = 0.03734814\n",
      "Iteration 4763, loss = 0.03734645\n",
      "Iteration 4764, loss = 0.03734477\n",
      "Iteration 4765, loss = 0.03734309\n",
      "Iteration 4766, loss = 0.03734140\n",
      "Iteration 4767, loss = 0.03733972\n",
      "Iteration 4768, loss = 0.03733804\n",
      "Iteration 4769, loss = 0.03733636\n",
      "Iteration 4770, loss = 0.03733468\n",
      "Iteration 4771, loss = 0.03733300\n",
      "Iteration 4772, loss = 0.03733132\n",
      "Iteration 4773, loss = 0.03732964\n",
      "Iteration 4774, loss = 0.03732796\n",
      "Iteration 4775, loss = 0.03732628\n",
      "Iteration 4776, loss = 0.03732461\n",
      "Iteration 4777, loss = 0.03732293\n",
      "Iteration 4778, loss = 0.03732125\n",
      "Iteration 4779, loss = 0.03731957\n",
      "Iteration 4780, loss = 0.03731790\n",
      "Iteration 4781, loss = 0.03731622\n",
      "Iteration 4782, loss = 0.03731455\n",
      "Iteration 4783, loss = 0.03731287\n",
      "Iteration 4784, loss = 0.03731120\n",
      "Iteration 4785, loss = 0.03730952\n",
      "Iteration 4786, loss = 0.03730785\n",
      "Iteration 4787, loss = 0.03730617\n",
      "Iteration 4788, loss = 0.03730450\n",
      "Iteration 4789, loss = 0.03730283\n",
      "Iteration 4790, loss = 0.03730116\n",
      "Iteration 4791, loss = 0.03729948\n",
      "Iteration 4792, loss = 0.03729781\n",
      "Iteration 4793, loss = 0.03729614\n",
      "Iteration 4794, loss = 0.03729447\n",
      "Iteration 4795, loss = 0.03729280\n",
      "Iteration 4796, loss = 0.03729113\n",
      "Iteration 4797, loss = 0.03728946\n",
      "Iteration 4798, loss = 0.03728779\n",
      "Iteration 4799, loss = 0.03728612\n",
      "Iteration 4800, loss = 0.03728446\n",
      "Iteration 4801, loss = 0.03728279\n",
      "Iteration 4802, loss = 0.03728112\n",
      "Iteration 4803, loss = 0.03727945\n",
      "Iteration 4804, loss = 0.03727779\n",
      "Iteration 4805, loss = 0.03727612\n",
      "Iteration 4806, loss = 0.03727445\n",
      "Iteration 4807, loss = 0.03727279\n",
      "Iteration 4808, loss = 0.03727112\n",
      "Iteration 4809, loss = 0.03726946\n",
      "Iteration 4810, loss = 0.03726779\n",
      "Iteration 4811, loss = 0.03726613\n",
      "Iteration 4812, loss = 0.03726447\n",
      "Iteration 4813, loss = 0.03726280\n",
      "Iteration 4814, loss = 0.03726114\n",
      "Iteration 4815, loss = 0.03725948\n",
      "Iteration 4816, loss = 0.03725782\n",
      "Iteration 4817, loss = 0.03725615\n",
      "Iteration 4818, loss = 0.03725449\n",
      "Iteration 4819, loss = 0.03725283\n",
      "Iteration 4820, loss = 0.03725117\n",
      "Iteration 4821, loss = 0.03724951\n",
      "Iteration 4822, loss = 0.03724785\n",
      "Iteration 4823, loss = 0.03724619\n",
      "Iteration 4824, loss = 0.03724453\n",
      "Iteration 4825, loss = 0.03724287\n",
      "Iteration 4826, loss = 0.03724122\n",
      "Iteration 4827, loss = 0.03723956\n",
      "Iteration 4828, loss = 0.03723790\n",
      "Iteration 4829, loss = 0.03723624\n",
      "Iteration 4830, loss = 0.03723459\n",
      "Iteration 4831, loss = 0.03723293\n",
      "Iteration 4832, loss = 0.03723127\n",
      "Iteration 4833, loss = 0.03722962\n",
      "Iteration 4834, loss = 0.03722796\n",
      "Iteration 4835, loss = 0.03722631\n",
      "Iteration 4836, loss = 0.03722465\n",
      "Iteration 4837, loss = 0.03722300\n",
      "Iteration 4838, loss = 0.03722135\n",
      "Iteration 4839, loss = 0.03721969\n",
      "Iteration 4840, loss = 0.03721804\n",
      "Iteration 4841, loss = 0.03721639\n",
      "Iteration 4842, loss = 0.03721473\n",
      "Iteration 4843, loss = 0.03721308\n",
      "Iteration 4844, loss = 0.03721143\n",
      "Iteration 4845, loss = 0.03720978\n",
      "Iteration 4846, loss = 0.03720813\n",
      "Iteration 4847, loss = 0.03720648\n",
      "Iteration 4848, loss = 0.03720483\n",
      "Iteration 4849, loss = 0.03720318\n",
      "Iteration 4850, loss = 0.03720153\n",
      "Iteration 4851, loss = 0.03719988\n",
      "Iteration 4852, loss = 0.03719823\n",
      "Iteration 4853, loss = 0.03719658\n",
      "Iteration 4854, loss = 0.03719494\n",
      "Iteration 4855, loss = 0.03719329\n",
      "Iteration 4856, loss = 0.03719164\n",
      "Iteration 4857, loss = 0.03718999\n",
      "Iteration 4858, loss = 0.03718835\n",
      "Iteration 4859, loss = 0.03718670\n",
      "Iteration 4860, loss = 0.03718506\n",
      "Iteration 4861, loss = 0.03718341\n",
      "Iteration 4862, loss = 0.03718177\n",
      "Iteration 4863, loss = 0.03718012\n",
      "Iteration 4864, loss = 0.03717848\n",
      "Iteration 4865, loss = 0.03717683\n",
      "Iteration 4866, loss = 0.03717519\n",
      "Iteration 4867, loss = 0.03717355\n",
      "Iteration 4868, loss = 0.03717190\n",
      "Iteration 4869, loss = 0.03717026\n",
      "Iteration 4870, loss = 0.03716862\n",
      "Iteration 4871, loss = 0.03716698\n",
      "Iteration 4872, loss = 0.03716533\n",
      "Iteration 4873, loss = 0.03716369\n",
      "Iteration 4874, loss = 0.03716205\n",
      "Iteration 4875, loss = 0.03716041\n",
      "Iteration 4876, loss = 0.03715877\n",
      "Iteration 4877, loss = 0.03715713\n",
      "Iteration 4878, loss = 0.03715549\n",
      "Iteration 4879, loss = 0.03715385\n",
      "Iteration 4880, loss = 0.03715222\n",
      "Iteration 4881, loss = 0.03715058\n",
      "Iteration 4882, loss = 0.03714894\n",
      "Iteration 4883, loss = 0.03714730\n",
      "Iteration 4884, loss = 0.03714566\n",
      "Iteration 4885, loss = 0.03714403\n",
      "Iteration 4886, loss = 0.03714239\n",
      "Iteration 4887, loss = 0.03714075\n",
      "Iteration 4888, loss = 0.03713912\n",
      "Iteration 4889, loss = 0.03713748\n",
      "Iteration 4890, loss = 0.03713585\n",
      "Iteration 4891, loss = 0.03713421\n",
      "Iteration 4892, loss = 0.03713258\n",
      "Iteration 4893, loss = 0.03713094\n",
      "Iteration 4894, loss = 0.03712931\n",
      "Iteration 4895, loss = 0.03712768\n",
      "Iteration 4896, loss = 0.03712604\n",
      "Iteration 4897, loss = 0.03712441\n",
      "Iteration 4898, loss = 0.03712278\n",
      "Iteration 4899, loss = 0.03712114\n",
      "Iteration 4900, loss = 0.03711951\n",
      "Iteration 4901, loss = 0.03711788\n",
      "Iteration 4902, loss = 0.03711625\n",
      "Iteration 4903, loss = 0.03711462\n",
      "Iteration 4904, loss = 0.03711299\n",
      "Iteration 4905, loss = 0.03711136\n",
      "Iteration 4906, loss = 0.03710973\n",
      "Iteration 4907, loss = 0.03710810\n",
      "Iteration 4908, loss = 0.03710647\n",
      "Iteration 4909, loss = 0.03710484\n",
      "Iteration 4910, loss = 0.03710321\n",
      "Iteration 4911, loss = 0.03710158\n",
      "Iteration 4912, loss = 0.03709995\n",
      "Iteration 4913, loss = 0.03709833\n",
      "Iteration 4914, loss = 0.03709670\n",
      "Iteration 4915, loss = 0.03709507\n",
      "Iteration 4916, loss = 0.03709344\n",
      "Iteration 4917, loss = 0.03709182\n",
      "Iteration 4918, loss = 0.03709019\n",
      "Iteration 4919, loss = 0.03708857\n",
      "Iteration 4920, loss = 0.03708694\n",
      "Iteration 4921, loss = 0.03708532\n",
      "Iteration 4922, loss = 0.03708369\n",
      "Iteration 4923, loss = 0.03708207\n",
      "Iteration 4924, loss = 0.03708044\n",
      "Iteration 4925, loss = 0.03707882\n",
      "Iteration 4926, loss = 0.03707719\n",
      "Iteration 4927, loss = 0.03707557\n",
      "Iteration 4928, loss = 0.03707395\n",
      "Iteration 4929, loss = 0.03707233\n",
      "Iteration 4930, loss = 0.03707070\n",
      "Iteration 4931, loss = 0.03706908\n",
      "Iteration 4932, loss = 0.03706746\n",
      "Iteration 4933, loss = 0.03706584\n",
      "Iteration 4934, loss = 0.03706422\n",
      "Iteration 4935, loss = 0.03706260\n",
      "Iteration 4936, loss = 0.03706098\n",
      "Iteration 4937, loss = 0.03705936\n",
      "Iteration 4938, loss = 0.03705774\n",
      "Iteration 4939, loss = 0.03705612\n",
      "Iteration 4940, loss = 0.03705450\n",
      "Iteration 4941, loss = 0.03705288\n",
      "Iteration 4942, loss = 0.03705126\n",
      "Iteration 4943, loss = 0.03704964\n",
      "Iteration 4944, loss = 0.03704802\n",
      "Iteration 4945, loss = 0.03704640\n",
      "Iteration 4946, loss = 0.03704479\n",
      "Iteration 4947, loss = 0.03704317\n",
      "Iteration 4948, loss = 0.03704155\n",
      "Iteration 4949, loss = 0.03703994\n",
      "Iteration 4950, loss = 0.03703832\n",
      "Iteration 4951, loss = 0.03703670\n",
      "Iteration 4952, loss = 0.03703509\n",
      "Iteration 4953, loss = 0.03703347\n",
      "Iteration 4954, loss = 0.03703186\n",
      "Iteration 4955, loss = 0.03703024\n",
      "Iteration 4956, loss = 0.03702863\n",
      "Iteration 4957, loss = 0.03702701\n",
      "Iteration 4958, loss = 0.03702540\n",
      "Iteration 4959, loss = 0.03702379\n",
      "Iteration 4960, loss = 0.03702217\n",
      "Iteration 4961, loss = 0.03702056\n",
      "Iteration 4962, loss = 0.03701895\n",
      "Iteration 4963, loss = 0.03701733\n",
      "Iteration 4964, loss = 0.03701572\n",
      "Iteration 4965, loss = 0.03701411\n",
      "Iteration 4966, loss = 0.03701250\n",
      "Iteration 4967, loss = 0.03701089\n",
      "Iteration 4968, loss = 0.03700928\n",
      "Iteration 4969, loss = 0.03700766\n",
      "Iteration 4970, loss = 0.03700605\n",
      "Iteration 4971, loss = 0.03700444\n",
      "Iteration 4972, loss = 0.03700283\n",
      "Iteration 4973, loss = 0.03700122\n",
      "Iteration 4974, loss = 0.03699961\n",
      "Iteration 4975, loss = 0.03699801\n",
      "Iteration 4976, loss = 0.03699640\n",
      "Iteration 4977, loss = 0.03699479\n",
      "Iteration 4978, loss = 0.03699318\n",
      "Iteration 4979, loss = 0.03699157\n",
      "Iteration 4980, loss = 0.03698996\n",
      "Iteration 4981, loss = 0.03698836\n",
      "Iteration 4982, loss = 0.03698675\n",
      "Iteration 4983, loss = 0.03698514\n",
      "Iteration 4984, loss = 0.03698353\n",
      "Iteration 4985, loss = 0.03698193\n",
      "Iteration 4986, loss = 0.03698032\n",
      "Iteration 4987, loss = 0.03697872\n",
      "Iteration 4988, loss = 0.03697711\n",
      "Iteration 4989, loss = 0.03697551\n",
      "Iteration 4990, loss = 0.03697390\n",
      "Iteration 4991, loss = 0.03697230\n",
      "Iteration 4992, loss = 0.03697069\n",
      "Iteration 4993, loss = 0.03696909\n",
      "Iteration 4994, loss = 0.03696748\n",
      "Iteration 4995, loss = 0.03696588\n",
      "Iteration 4996, loss = 0.03696427\n",
      "Iteration 4997, loss = 0.03696267\n",
      "Iteration 4998, loss = 0.03696107\n",
      "Iteration 4999, loss = 0.03695947\n",
      "Iteration 5000, loss = 0.03695786\n",
      "Iteration 5001, loss = 0.03695626\n",
      "Iteration 5002, loss = 0.03695466\n",
      "Iteration 5003, loss = 0.03695306\n",
      "Iteration 5004, loss = 0.03695146\n",
      "Iteration 5005, loss = 0.03694985\n",
      "Iteration 5006, loss = 0.03694825\n",
      "Iteration 5007, loss = 0.03694665\n",
      "Iteration 5008, loss = 0.03694505\n",
      "Iteration 5009, loss = 0.03694345\n",
      "Iteration 5010, loss = 0.03694185\n",
      "Iteration 5011, loss = 0.03694025\n",
      "Iteration 5012, loss = 0.03693865\n",
      "Iteration 5013, loss = 0.03693705\n",
      "Iteration 5014, loss = 0.03693545\n",
      "Iteration 5015, loss = 0.03693386\n",
      "Iteration 5016, loss = 0.03693226\n",
      "Iteration 5017, loss = 0.03693066\n",
      "Iteration 5018, loss = 0.03692906\n",
      "Iteration 5019, loss = 0.03692746\n",
      "Iteration 5020, loss = 0.03692587\n",
      "Iteration 5021, loss = 0.03692427\n",
      "Iteration 5022, loss = 0.03692267\n",
      "Iteration 5023, loss = 0.03692107\n",
      "Iteration 5024, loss = 0.03691948\n",
      "Iteration 5025, loss = 0.03691788\n",
      "Iteration 5026, loss = 0.03691628\n",
      "Iteration 5027, loss = 0.03691469\n",
      "Iteration 5028, loss = 0.03691309\n",
      "Iteration 5029, loss = 0.03691150\n",
      "Iteration 5030, loss = 0.03690990\n",
      "Iteration 5031, loss = 0.03690831\n",
      "Iteration 5032, loss = 0.03690671\n",
      "Iteration 5033, loss = 0.03690512\n",
      "Iteration 5034, loss = 0.03690352\n",
      "Iteration 5035, loss = 0.03690193\n",
      "Iteration 5036, loss = 0.03690034\n",
      "Iteration 5037, loss = 0.03689874\n",
      "Iteration 5038, loss = 0.03689715\n",
      "Iteration 5039, loss = 0.03689556\n",
      "Iteration 5040, loss = 0.03689396\n",
      "Iteration 5041, loss = 0.03689237\n",
      "Iteration 5042, loss = 0.03689078\n",
      "Iteration 5043, loss = 0.03688919\n",
      "Iteration 5044, loss = 0.03688759\n",
      "Iteration 5045, loss = 0.03688600\n",
      "Iteration 5046, loss = 0.03688441\n",
      "Iteration 5047, loss = 0.03688282\n",
      "Iteration 5048, loss = 0.03688123\n",
      "Iteration 5049, loss = 0.03687964\n",
      "Iteration 5050, loss = 0.03687805\n",
      "Iteration 5051, loss = 0.03687646\n",
      "Iteration 5052, loss = 0.03687487\n",
      "Iteration 5053, loss = 0.03687328\n",
      "Iteration 5054, loss = 0.03687169\n",
      "Iteration 5055, loss = 0.03687010\n",
      "Iteration 5056, loss = 0.03686851\n",
      "Iteration 5057, loss = 0.03686692\n",
      "Iteration 5058, loss = 0.03686533\n",
      "Iteration 5059, loss = 0.03686374\n",
      "Iteration 5060, loss = 0.03686215\n",
      "Iteration 5061, loss = 0.03686056\n",
      "Iteration 5062, loss = 0.03685897\n",
      "Iteration 5063, loss = 0.03685739\n",
      "Iteration 5064, loss = 0.03685580\n",
      "Iteration 5065, loss = 0.03685421\n",
      "Iteration 5066, loss = 0.03685262\n",
      "Iteration 5067, loss = 0.03685104\n",
      "Iteration 5068, loss = 0.03684945\n",
      "Iteration 5069, loss = 0.03684786\n",
      "Iteration 5070, loss = 0.03684628\n",
      "Iteration 5071, loss = 0.03684469\n",
      "Iteration 5072, loss = 0.03684310\n",
      "Iteration 5073, loss = 0.03684152\n",
      "Iteration 5074, loss = 0.03683993\n",
      "Iteration 5075, loss = 0.03683835\n",
      "Iteration 5076, loss = 0.03683676\n",
      "Iteration 5077, loss = 0.03683517\n",
      "Iteration 5078, loss = 0.03683359\n",
      "Iteration 5079, loss = 0.03683200\n",
      "Iteration 5080, loss = 0.03683042\n",
      "Iteration 5081, loss = 0.03682884\n",
      "Iteration 5082, loss = 0.03682725\n",
      "Iteration 5083, loss = 0.03682567\n",
      "Iteration 5084, loss = 0.03682408\n",
      "Iteration 5085, loss = 0.03682250\n",
      "Iteration 5086, loss = 0.03682092\n",
      "Iteration 5087, loss = 0.03681933\n",
      "Iteration 5088, loss = 0.03681775\n",
      "Iteration 5089, loss = 0.03681617\n",
      "Iteration 5090, loss = 0.03681458\n",
      "Iteration 5091, loss = 0.03681300\n",
      "Iteration 5092, loss = 0.03681142\n",
      "Iteration 5093, loss = 0.03680984\n",
      "Iteration 5094, loss = 0.03680825\n",
      "Iteration 5095, loss = 0.03680667\n",
      "Iteration 5096, loss = 0.03680509\n",
      "Iteration 5097, loss = 0.03680351\n",
      "Iteration 5098, loss = 0.03680193\n",
      "Iteration 5099, loss = 0.03680035\n",
      "Iteration 5100, loss = 0.03679877\n",
      "Iteration 5101, loss = 0.03679718\n",
      "Iteration 5102, loss = 0.03679560\n",
      "Iteration 5103, loss = 0.03679402\n",
      "Iteration 5104, loss = 0.03679244\n",
      "Iteration 5105, loss = 0.03679086\n",
      "Iteration 5106, loss = 0.03678928\n",
      "Iteration 5107, loss = 0.03678770\n",
      "Iteration 5108, loss = 0.03678612\n",
      "Iteration 5109, loss = 0.03678454\n",
      "Iteration 5110, loss = 0.03678296\n",
      "Iteration 5111, loss = 0.03678138\n",
      "Iteration 5112, loss = 0.03677981\n",
      "Iteration 5113, loss = 0.03677823\n",
      "Iteration 5114, loss = 0.03677665\n",
      "Iteration 5115, loss = 0.03677507\n",
      "Iteration 5116, loss = 0.03677349\n",
      "Iteration 5117, loss = 0.03677191\n",
      "Iteration 5118, loss = 0.03677033\n",
      "Iteration 5119, loss = 0.03676876\n",
      "Iteration 5120, loss = 0.03676718\n",
      "Iteration 5121, loss = 0.03676560\n",
      "Iteration 5122, loss = 0.03676402\n",
      "Iteration 5123, loss = 0.03676245\n",
      "Iteration 5124, loss = 0.03676087\n",
      "Iteration 5125, loss = 0.03675929\n",
      "Iteration 5126, loss = 0.03675771\n",
      "Iteration 5127, loss = 0.03675614\n",
      "Iteration 5128, loss = 0.03675456\n",
      "Iteration 5129, loss = 0.03675298\n",
      "Iteration 5130, loss = 0.03675141\n",
      "Iteration 5131, loss = 0.03674983\n",
      "Iteration 5132, loss = 0.03674826\n",
      "Iteration 5133, loss = 0.03674668\n",
      "Iteration 5134, loss = 0.03674510\n",
      "Iteration 5135, loss = 0.03674353\n",
      "Iteration 5136, loss = 0.03674195\n",
      "Iteration 5137, loss = 0.03674038\n",
      "Iteration 5138, loss = 0.03673880\n",
      "Iteration 5139, loss = 0.03673723\n",
      "Iteration 5140, loss = 0.03673565\n",
      "Iteration 5141, loss = 0.03673408\n",
      "Iteration 5142, loss = 0.03673250\n",
      "Iteration 5143, loss = 0.03673093\n",
      "Iteration 5144, loss = 0.03672935\n",
      "Iteration 5145, loss = 0.03672778\n",
      "Iteration 5146, loss = 0.03672620\n",
      "Iteration 5147, loss = 0.03672463\n",
      "Iteration 5148, loss = 0.03672306\n",
      "Iteration 5149, loss = 0.03672148\n",
      "Iteration 5150, loss = 0.03671991\n",
      "Iteration 5151, loss = 0.03671833\n",
      "Iteration 5152, loss = 0.03671676\n",
      "Iteration 5153, loss = 0.03671519\n",
      "Iteration 5154, loss = 0.03671361\n",
      "Iteration 5155, loss = 0.03671204\n",
      "Iteration 5156, loss = 0.03671047\n",
      "Iteration 5157, loss = 0.03670890\n",
      "Iteration 5158, loss = 0.03670732\n",
      "Iteration 5159, loss = 0.03670575\n",
      "Iteration 5160, loss = 0.03670418\n",
      "Iteration 5161, loss = 0.03670260\n",
      "Iteration 5162, loss = 0.03670103\n",
      "Iteration 5163, loss = 0.03669946\n",
      "Iteration 5164, loss = 0.03669789\n",
      "Iteration 5165, loss = 0.03669632\n",
      "Iteration 5166, loss = 0.03669474\n",
      "Iteration 5167, loss = 0.03669317\n",
      "Iteration 5168, loss = 0.03669160\n",
      "Iteration 5169, loss = 0.03669003\n",
      "Iteration 5170, loss = 0.03668846\n",
      "Iteration 5171, loss = 0.03668689\n",
      "Iteration 5172, loss = 0.03668531\n",
      "Iteration 5173, loss = 0.03668374\n",
      "Iteration 5174, loss = 0.03668217\n",
      "Iteration 5175, loss = 0.03668060\n",
      "Iteration 5176, loss = 0.03667903\n",
      "Iteration 5177, loss = 0.03667746\n",
      "Iteration 5178, loss = 0.03667589\n",
      "Iteration 5179, loss = 0.03667432\n",
      "Iteration 5180, loss = 0.03667275\n",
      "Iteration 5181, loss = 0.03667118\n",
      "Iteration 5182, loss = 0.03666961\n",
      "Iteration 5183, loss = 0.03666804\n",
      "Iteration 5184, loss = 0.03666647\n",
      "Iteration 5185, loss = 0.03666490\n",
      "Iteration 5186, loss = 0.03666333\n",
      "Iteration 5187, loss = 0.03666176\n",
      "Iteration 5188, loss = 0.03666019\n",
      "Iteration 5189, loss = 0.03665862\n",
      "Iteration 5190, loss = 0.03665705\n",
      "Iteration 5191, loss = 0.03665548\n",
      "Iteration 5192, loss = 0.03665391\n",
      "Iteration 5193, loss = 0.03665234\n",
      "Iteration 5194, loss = 0.03665077\n",
      "Iteration 5195, loss = 0.03664920\n",
      "Iteration 5196, loss = 0.03664763\n",
      "Iteration 5197, loss = 0.03664606\n",
      "Iteration 5198, loss = 0.03664449\n",
      "Iteration 5199, loss = 0.03664292\n",
      "Iteration 5200, loss = 0.03664135\n",
      "Iteration 5201, loss = 0.03663979\n",
      "Iteration 5202, loss = 0.03663822\n",
      "Iteration 5203, loss = 0.03663665\n",
      "Iteration 5204, loss = 0.03663508\n",
      "Iteration 5205, loss = 0.03663351\n",
      "Iteration 5206, loss = 0.03663194\n",
      "Iteration 5207, loss = 0.03663037\n",
      "Iteration 5208, loss = 0.03662881\n",
      "Iteration 5209, loss = 0.03662724\n",
      "Iteration 5210, loss = 0.03662567\n",
      "Iteration 5211, loss = 0.03662410\n",
      "Iteration 5212, loss = 0.03662253\n",
      "Iteration 5213, loss = 0.03662096\n",
      "Iteration 5214, loss = 0.03661940\n",
      "Iteration 5215, loss = 0.03661783\n",
      "Iteration 5216, loss = 0.03661626\n",
      "Iteration 5217, loss = 0.03661469\n",
      "Iteration 5218, loss = 0.03661312\n",
      "Iteration 5219, loss = 0.03661156\n",
      "Iteration 5220, loss = 0.03660999\n",
      "Iteration 5221, loss = 0.03660842\n",
      "Iteration 5222, loss = 0.03660685\n",
      "Iteration 5223, loss = 0.03660529\n",
      "Iteration 5224, loss = 0.03660372\n",
      "Iteration 5225, loss = 0.03660215\n",
      "Iteration 5226, loss = 0.03660058\n",
      "Iteration 5227, loss = 0.03659902\n",
      "Iteration 5228, loss = 0.03659745\n",
      "Iteration 5229, loss = 0.03659588\n",
      "Iteration 5230, loss = 0.03659431\n",
      "Iteration 5231, loss = 0.03659275\n",
      "Iteration 5232, loss = 0.03659118\n",
      "Iteration 5233, loss = 0.03658961\n",
      "Iteration 5234, loss = 0.03658804\n",
      "Iteration 5235, loss = 0.03658648\n",
      "Iteration 5236, loss = 0.03658491\n",
      "Iteration 5237, loss = 0.03658334\n",
      "Iteration 5238, loss = 0.03658178\n",
      "Iteration 5239, loss = 0.03658021\n",
      "Iteration 5240, loss = 0.03657864\n",
      "Iteration 5241, loss = 0.03657708\n",
      "Iteration 5242, loss = 0.03657551\n",
      "Iteration 5243, loss = 0.03657394\n",
      "Iteration 5244, loss = 0.03657237\n",
      "Iteration 5245, loss = 0.03657081\n",
      "Iteration 5246, loss = 0.03656924\n",
      "Iteration 5247, loss = 0.03656767\n",
      "Iteration 5248, loss = 0.03656611\n",
      "Iteration 5249, loss = 0.03656454\n",
      "Iteration 5250, loss = 0.03656297\n",
      "Iteration 5251, loss = 0.03656141\n",
      "Iteration 5252, loss = 0.03655984\n",
      "Iteration 5253, loss = 0.03655827\n",
      "Iteration 5254, loss = 0.03655671\n",
      "Iteration 5255, loss = 0.03655514\n",
      "Iteration 5256, loss = 0.03655357\n",
      "Iteration 5257, loss = 0.03655201\n",
      "Iteration 5258, loss = 0.03655044\n",
      "Iteration 5259, loss = 0.03654887\n",
      "Iteration 5260, loss = 0.03654731\n",
      "Iteration 5261, loss = 0.03654574\n",
      "Iteration 5262, loss = 0.03654417\n",
      "Iteration 5263, loss = 0.03654261\n",
      "Iteration 5264, loss = 0.03654104\n",
      "Iteration 5265, loss = 0.03653947\n",
      "Iteration 5266, loss = 0.03653791\n",
      "Iteration 5267, loss = 0.03653634\n",
      "Iteration 5268, loss = 0.03653477\n",
      "Iteration 5269, loss = 0.03653320\n",
      "Iteration 5270, loss = 0.03653164\n",
      "Iteration 5271, loss = 0.03653007\n",
      "Iteration 5272, loss = 0.03652850\n",
      "Iteration 5273, loss = 0.03652694\n",
      "Iteration 5274, loss = 0.03652537\n",
      "Iteration 5275, loss = 0.03652380\n",
      "Iteration 5276, loss = 0.03652224\n",
      "Iteration 5277, loss = 0.03652067\n",
      "Iteration 5278, loss = 0.03651910\n",
      "Iteration 5279, loss = 0.03651754\n",
      "Iteration 5280, loss = 0.03651597\n",
      "Iteration 5281, loss = 0.03651440\n",
      "Iteration 5282, loss = 0.03651284\n",
      "Iteration 5283, loss = 0.03651127\n",
      "Iteration 5284, loss = 0.03650970\n",
      "Iteration 5285, loss = 0.03650813\n",
      "Iteration 5286, loss = 0.03650657\n",
      "Iteration 5287, loss = 0.03650500\n",
      "Iteration 5288, loss = 0.03650343\n",
      "Iteration 5289, loss = 0.03650187\n",
      "Iteration 5290, loss = 0.03650030\n",
      "Iteration 5291, loss = 0.03649873\n",
      "Iteration 5292, loss = 0.03649716\n",
      "Iteration 5293, loss = 0.03649560\n",
      "Iteration 5294, loss = 0.03649403\n",
      "Iteration 5295, loss = 0.03649246\n",
      "Iteration 5296, loss = 0.03649089\n",
      "Iteration 5297, loss = 0.03648933\n",
      "Iteration 5298, loss = 0.03648776\n",
      "Iteration 5299, loss = 0.03648619\n",
      "Iteration 5300, loss = 0.03648462\n",
      "Iteration 5301, loss = 0.03648306\n",
      "Iteration 5302, loss = 0.03648149\n",
      "Iteration 5303, loss = 0.03647992\n",
      "Iteration 5304, loss = 0.03647835\n",
      "Iteration 5305, loss = 0.03647679\n",
      "Iteration 5306, loss = 0.03647522\n",
      "Iteration 5307, loss = 0.03647365\n",
      "Iteration 5308, loss = 0.03647208\n",
      "Iteration 5309, loss = 0.03647051\n",
      "Iteration 5310, loss = 0.03646894\n",
      "Iteration 5311, loss = 0.03646738\n",
      "Iteration 5312, loss = 0.03646581\n",
      "Iteration 5313, loss = 0.03646424\n",
      "Iteration 5314, loss = 0.03646267\n",
      "Iteration 5315, loss = 0.03646110\n",
      "Iteration 5316, loss = 0.03645953\n",
      "Iteration 5317, loss = 0.03645797\n",
      "Iteration 5318, loss = 0.03645640\n",
      "Iteration 5319, loss = 0.03645483\n",
      "Iteration 5320, loss = 0.03645326\n",
      "Iteration 5321, loss = 0.03645169\n",
      "Iteration 5322, loss = 0.03645012\n",
      "Iteration 5323, loss = 0.03644855\n",
      "Iteration 5324, loss = 0.03644698\n",
      "Iteration 5325, loss = 0.03644541\n",
      "Iteration 5326, loss = 0.03644384\n",
      "Iteration 5327, loss = 0.03644228\n",
      "Iteration 5328, loss = 0.03644071\n",
      "Iteration 5329, loss = 0.03643914\n",
      "Iteration 5330, loss = 0.03643757\n",
      "Iteration 5331, loss = 0.03643600\n",
      "Iteration 5332, loss = 0.03643443\n",
      "Iteration 5333, loss = 0.03643286\n",
      "Iteration 5334, loss = 0.03643129\n",
      "Iteration 5335, loss = 0.03642972\n",
      "Iteration 5336, loss = 0.03642815\n",
      "Iteration 5337, loss = 0.03642658\n",
      "Iteration 5338, loss = 0.03642501\n",
      "Iteration 5339, loss = 0.03642344\n",
      "Iteration 5340, loss = 0.03642186\n",
      "Iteration 5341, loss = 0.03642029\n",
      "Iteration 5342, loss = 0.03641872\n",
      "Iteration 5343, loss = 0.03641715\n",
      "Iteration 5344, loss = 0.03641558\n",
      "Iteration 5345, loss = 0.03641401\n",
      "Iteration 5346, loss = 0.03641244\n",
      "Iteration 5347, loss = 0.03641087\n",
      "Iteration 5348, loss = 0.03640930\n",
      "Iteration 5349, loss = 0.03640772\n",
      "Iteration 5350, loss = 0.03640615\n",
      "Iteration 5351, loss = 0.03640458\n",
      "Iteration 5352, loss = 0.03640301\n",
      "Iteration 5353, loss = 0.03640144\n",
      "Iteration 5354, loss = 0.03639987\n",
      "Iteration 5355, loss = 0.03639829\n",
      "Iteration 5356, loss = 0.03639672\n",
      "Iteration 5357, loss = 0.03639515\n",
      "Iteration 5358, loss = 0.03639358\n",
      "Iteration 5359, loss = 0.03639200\n",
      "Iteration 5360, loss = 0.03639043\n",
      "Iteration 5361, loss = 0.03638886\n",
      "Iteration 5362, loss = 0.03638728\n",
      "Iteration 5363, loss = 0.03638571\n",
      "Iteration 5364, loss = 0.03638414\n",
      "Iteration 5365, loss = 0.03638256\n",
      "Iteration 5366, loss = 0.03638099\n",
      "Iteration 5367, loss = 0.03637942\n",
      "Iteration 5368, loss = 0.03637784\n",
      "Iteration 5369, loss = 0.03637627\n",
      "Iteration 5370, loss = 0.03637469\n",
      "Iteration 5371, loss = 0.03637312\n",
      "Iteration 5372, loss = 0.03637154\n",
      "Iteration 5373, loss = 0.03636997\n",
      "Iteration 5374, loss = 0.03636840\n",
      "Iteration 5375, loss = 0.03636682\n",
      "Iteration 5376, loss = 0.03636525\n",
      "Iteration 5377, loss = 0.03636367\n",
      "Iteration 5378, loss = 0.03636209\n",
      "Iteration 5379, loss = 0.03636052\n",
      "Iteration 5380, loss = 0.03635894\n",
      "Iteration 5381, loss = 0.03635737\n",
      "Iteration 5382, loss = 0.03635579\n",
      "Iteration 5383, loss = 0.03635422\n",
      "Iteration 5384, loss = 0.03635264\n",
      "Iteration 5385, loss = 0.03635106\n",
      "Iteration 5386, loss = 0.03634949\n",
      "Iteration 5387, loss = 0.03634791\n",
      "Iteration 5388, loss = 0.03634633\n",
      "Iteration 5389, loss = 0.03634475\n",
      "Iteration 5390, loss = 0.03634318\n",
      "Iteration 5391, loss = 0.03634160\n",
      "Iteration 5392, loss = 0.03634002\n",
      "Iteration 5393, loss = 0.03633844\n",
      "Iteration 5394, loss = 0.03633687\n",
      "Iteration 5395, loss = 0.03633529\n",
      "Iteration 5396, loss = 0.03633371\n",
      "Iteration 5397, loss = 0.03633213\n",
      "Iteration 5398, loss = 0.03633055\n",
      "Iteration 5399, loss = 0.03632897\n",
      "Iteration 5400, loss = 0.03632739\n",
      "Iteration 5401, loss = 0.03632581\n",
      "Iteration 5402, loss = 0.03632423\n",
      "Iteration 5403, loss = 0.03632265\n",
      "Iteration 5404, loss = 0.03632107\n",
      "Iteration 5405, loss = 0.03631949\n",
      "Iteration 5406, loss = 0.03631791\n",
      "Iteration 5407, loss = 0.03631633\n",
      "Iteration 5408, loss = 0.03631475\n",
      "Iteration 5409, loss = 0.03631317\n",
      "Iteration 5410, loss = 0.03631159\n",
      "Iteration 5411, loss = 0.03631001\n",
      "Iteration 5412, loss = 0.03630843\n",
      "Iteration 5413, loss = 0.03630685\n",
      "Iteration 5414, loss = 0.03630526\n",
      "Iteration 5415, loss = 0.03630368\n",
      "Iteration 5416, loss = 0.03630210\n",
      "Iteration 5417, loss = 0.03630052\n",
      "Iteration 5418, loss = 0.03629893\n",
      "Iteration 5419, loss = 0.03629735\n",
      "Iteration 5420, loss = 0.03629577\n",
      "Iteration 5421, loss = 0.03629418\n",
      "Iteration 5422, loss = 0.03629260\n",
      "Iteration 5423, loss = 0.03629102\n",
      "Iteration 5424, loss = 0.03628943\n",
      "Iteration 5425, loss = 0.03628785\n",
      "Iteration 5426, loss = 0.03628626\n",
      "Iteration 5427, loss = 0.03628468\n",
      "Iteration 5428, loss = 0.03628309\n",
      "Iteration 5429, loss = 0.03628151\n",
      "Iteration 5430, loss = 0.03627992\n",
      "Iteration 5431, loss = 0.03627834\n",
      "Iteration 5432, loss = 0.03627675\n",
      "Iteration 5433, loss = 0.03627516\n",
      "Iteration 5434, loss = 0.03627358\n",
      "Iteration 5435, loss = 0.03627199\n",
      "Iteration 5436, loss = 0.03627040\n",
      "Iteration 5437, loss = 0.03626882\n",
      "Iteration 5438, loss = 0.03626723\n",
      "Iteration 5439, loss = 0.03626564\n",
      "Iteration 5440, loss = 0.03626405\n",
      "Iteration 5441, loss = 0.03626247\n",
      "Iteration 5442, loss = 0.03626088\n",
      "Iteration 5443, loss = 0.03625929\n",
      "Iteration 5444, loss = 0.03625770\n",
      "Iteration 5445, loss = 0.03625611\n",
      "Iteration 5446, loss = 0.03625452\n",
      "Iteration 5447, loss = 0.03625293\n",
      "Iteration 5448, loss = 0.03625134\n",
      "Iteration 5449, loss = 0.03624975\n",
      "Iteration 5450, loss = 0.03624816\n",
      "Iteration 5451, loss = 0.03624657\n",
      "Iteration 5452, loss = 0.03624498\n",
      "Iteration 5453, loss = 0.03624339\n",
      "Iteration 5454, loss = 0.03624180\n",
      "Iteration 5455, loss = 0.03624020\n",
      "Iteration 5456, loss = 0.03623861\n",
      "Iteration 5457, loss = 0.03623702\n",
      "Iteration 5458, loss = 0.03623543\n",
      "Iteration 5459, loss = 0.03623383\n",
      "Iteration 5460, loss = 0.03623224\n",
      "Iteration 5461, loss = 0.03623065\n",
      "Iteration 5462, loss = 0.03622905\n",
      "Iteration 5463, loss = 0.03622746\n",
      "Iteration 5464, loss = 0.03622586\n",
      "Iteration 5465, loss = 0.03622427\n",
      "Iteration 5466, loss = 0.03622267\n",
      "Iteration 5467, loss = 0.03622108\n",
      "Iteration 5468, loss = 0.03621948\n",
      "Iteration 5469, loss = 0.03621789\n",
      "Iteration 5470, loss = 0.03621629\n",
      "Iteration 5471, loss = 0.03621469\n",
      "Iteration 5472, loss = 0.03621310\n",
      "Iteration 5473, loss = 0.03621150\n",
      "Iteration 5474, loss = 0.03620990\n",
      "Iteration 5475, loss = 0.03620831\n",
      "Iteration 5476, loss = 0.03620671\n",
      "Iteration 5477, loss = 0.03620511\n",
      "Iteration 5478, loss = 0.03620351\n",
      "Iteration 5479, loss = 0.03620191\n",
      "Iteration 5480, loss = 0.03620031\n",
      "Iteration 5481, loss = 0.03619871\n",
      "Iteration 5482, loss = 0.03619711\n",
      "Iteration 5483, loss = 0.03619551\n",
      "Iteration 5484, loss = 0.03619391\n",
      "Iteration 5485, loss = 0.03619231\n",
      "Iteration 5486, loss = 0.03619071\n",
      "Iteration 5487, loss = 0.03618911\n",
      "Iteration 5488, loss = 0.03618750\n",
      "Iteration 5489, loss = 0.03618590\n",
      "Iteration 5490, loss = 0.03618430\n",
      "Iteration 5491, loss = 0.03618270\n",
      "Iteration 5492, loss = 0.03618109\n",
      "Iteration 5493, loss = 0.03617949\n",
      "Iteration 5494, loss = 0.03617789\n",
      "Iteration 5495, loss = 0.03617628\n",
      "Iteration 5496, loss = 0.03617468\n",
      "Iteration 5497, loss = 0.03617307\n",
      "Iteration 5498, loss = 0.03617147\n",
      "Iteration 5499, loss = 0.03616986\n",
      "Iteration 5500, loss = 0.03616825\n",
      "Iteration 5501, loss = 0.03616665\n",
      "Iteration 5502, loss = 0.03616504\n",
      "Iteration 5503, loss = 0.03616343\n",
      "Iteration 5504, loss = 0.03616183\n",
      "Iteration 5505, loss = 0.03616022\n",
      "Iteration 5506, loss = 0.03615861\n",
      "Iteration 5507, loss = 0.03615700\n",
      "Iteration 5508, loss = 0.03615539\n",
      "Iteration 5509, loss = 0.03615378\n",
      "Iteration 5510, loss = 0.03615217\n",
      "Iteration 5511, loss = 0.03615056\n",
      "Iteration 5512, loss = 0.03614895\n",
      "Iteration 5513, loss = 0.03614734\n",
      "Iteration 5514, loss = 0.03614573\n",
      "Iteration 5515, loss = 0.03614412\n",
      "Iteration 5516, loss = 0.03614251\n",
      "Iteration 5517, loss = 0.03614089\n",
      "Iteration 5518, loss = 0.03613928\n",
      "Iteration 5519, loss = 0.03613767\n",
      "Iteration 5520, loss = 0.03613605\n",
      "Iteration 5521, loss = 0.03613444\n",
      "Iteration 5522, loss = 0.03613283\n",
      "Iteration 5523, loss = 0.03613121\n",
      "Iteration 5524, loss = 0.03612960\n",
      "Iteration 5525, loss = 0.03612798\n",
      "Iteration 5526, loss = 0.03612636\n",
      "Iteration 5527, loss = 0.03612475\n",
      "Iteration 5528, loss = 0.03612313\n",
      "Iteration 5529, loss = 0.03612151\n",
      "Iteration 5530, loss = 0.03611990\n",
      "Iteration 5531, loss = 0.03611828\n",
      "Iteration 5532, loss = 0.03611666\n",
      "Iteration 5533, loss = 0.03611504\n",
      "Iteration 5534, loss = 0.03611342\n",
      "Iteration 5535, loss = 0.03611180\n",
      "Iteration 5536, loss = 0.03611018\n",
      "Iteration 5537, loss = 0.03610856\n",
      "Iteration 5538, loss = 0.03610694\n",
      "Iteration 5539, loss = 0.03610532\n",
      "Iteration 5540, loss = 0.03610370\n",
      "Iteration 5541, loss = 0.03610207\n",
      "Iteration 5542, loss = 0.03610045\n",
      "Iteration 5543, loss = 0.03609883\n",
      "Iteration 5544, loss = 0.03609720\n",
      "Iteration 5545, loss = 0.03609558\n",
      "Iteration 5546, loss = 0.03609395\n",
      "Iteration 5547, loss = 0.03609233\n",
      "Iteration 5548, loss = 0.03609070\n",
      "Iteration 5549, loss = 0.03608908\n",
      "Iteration 5550, loss = 0.03608745\n",
      "Iteration 5551, loss = 0.03608582\n",
      "Iteration 5552, loss = 0.03608420\n",
      "Iteration 5553, loss = 0.03608257\n",
      "Iteration 5554, loss = 0.03608094\n",
      "Iteration 5555, loss = 0.03607931\n",
      "Iteration 5556, loss = 0.03607768\n",
      "Iteration 5557, loss = 0.03607605\n",
      "Iteration 5558, loss = 0.03607442\n",
      "Iteration 5559, loss = 0.03607279\n",
      "Iteration 5560, loss = 0.03607116\n",
      "Iteration 5561, loss = 0.03606953\n",
      "Iteration 5562, loss = 0.03606790\n",
      "Iteration 5563, loss = 0.03606627\n",
      "Iteration 5564, loss = 0.03606463\n",
      "Iteration 5565, loss = 0.03606300\n",
      "Iteration 5566, loss = 0.03606137\n",
      "Iteration 5567, loss = 0.03605973\n",
      "Iteration 5568, loss = 0.03605810\n",
      "Iteration 5569, loss = 0.03605646\n",
      "Iteration 5570, loss = 0.03605483\n",
      "Iteration 5571, loss = 0.03605319\n",
      "Iteration 5572, loss = 0.03605155\n",
      "Iteration 5573, loss = 0.03604992\n",
      "Iteration 5574, loss = 0.03604828\n",
      "Iteration 5575, loss = 0.03604664\n",
      "Iteration 5576, loss = 0.03604500\n",
      "Iteration 5577, loss = 0.03604336\n",
      "Iteration 5578, loss = 0.03604172\n",
      "Iteration 5579, loss = 0.03604008\n",
      "Iteration 5580, loss = 0.03603844\n",
      "Iteration 5581, loss = 0.03603680\n",
      "Iteration 5582, loss = 0.03603516\n",
      "Iteration 5583, loss = 0.03603351\n",
      "Iteration 5584, loss = 0.03603187\n",
      "Iteration 5585, loss = 0.03603023\n",
      "Iteration 5586, loss = 0.03602858\n",
      "Iteration 5587, loss = 0.03602694\n",
      "Iteration 5588, loss = 0.03602529\n",
      "Iteration 5589, loss = 0.03602365\n",
      "Iteration 5590, loss = 0.03602200\n",
      "Iteration 5591, loss = 0.03602036\n",
      "Iteration 5592, loss = 0.03601871\n",
      "Iteration 5593, loss = 0.03601706\n",
      "Iteration 5594, loss = 0.03601541\n",
      "Iteration 5595, loss = 0.03601376\n",
      "Iteration 5596, loss = 0.03601211\n",
      "Iteration 5597, loss = 0.03601047\n",
      "Iteration 5598, loss = 0.03600881\n",
      "Iteration 5599, loss = 0.03600716\n",
      "Iteration 5600, loss = 0.03600551\n",
      "Iteration 5601, loss = 0.03600386\n",
      "Iteration 5602, loss = 0.03600221\n",
      "Iteration 5603, loss = 0.03600055\n",
      "Iteration 5604, loss = 0.03599890\n",
      "Iteration 5605, loss = 0.03599725\n",
      "Iteration 5606, loss = 0.03599559\n",
      "Iteration 5607, loss = 0.03599394\n",
      "Iteration 5608, loss = 0.03599228\n",
      "Iteration 5609, loss = 0.03599062\n",
      "Iteration 5610, loss = 0.03598897\n",
      "Iteration 5611, loss = 0.03598731\n",
      "Iteration 5612, loss = 0.03598565\n",
      "Iteration 5613, loss = 0.03598399\n",
      "Iteration 5614, loss = 0.03598233\n",
      "Iteration 5615, loss = 0.03598067\n",
      "Iteration 5616, loss = 0.03597901\n",
      "Iteration 5617, loss = 0.03597735\n",
      "Iteration 5618, loss = 0.03597569\n",
      "Iteration 5619, loss = 0.03597402\n",
      "Iteration 5620, loss = 0.03597236\n",
      "Iteration 5621, loss = 0.03597070\n",
      "Iteration 5622, loss = 0.03596903\n",
      "Iteration 5623, loss = 0.03596737\n",
      "Iteration 5624, loss = 0.03596570\n",
      "Iteration 5625, loss = 0.03596404\n",
      "Iteration 5626, loss = 0.03596237\n",
      "Iteration 5627, loss = 0.03596070\n",
      "Iteration 5628, loss = 0.03595903\n",
      "Iteration 5629, loss = 0.03595736\n",
      "Iteration 5630, loss = 0.03595570\n",
      "Iteration 5631, loss = 0.03595403\n",
      "Iteration 5632, loss = 0.03595235\n",
      "Iteration 5633, loss = 0.03595068\n",
      "Iteration 5634, loss = 0.03594901\n",
      "Iteration 5635, loss = 0.03594734\n",
      "Iteration 5636, loss = 0.03594567\n",
      "Iteration 5637, loss = 0.03594399\n",
      "Iteration 5638, loss = 0.03594232\n",
      "Iteration 5639, loss = 0.03594064\n",
      "Iteration 5640, loss = 0.03593897\n",
      "Iteration 5641, loss = 0.03593729\n",
      "Iteration 5642, loss = 0.03593561\n",
      "Iteration 5643, loss = 0.03593394\n",
      "Iteration 5644, loss = 0.03593226\n",
      "Iteration 5645, loss = 0.03593058\n",
      "Iteration 5646, loss = 0.03592890\n",
      "Iteration 5647, loss = 0.03592722\n",
      "Iteration 5648, loss = 0.03592554\n",
      "Iteration 5649, loss = 0.03592386\n",
      "Iteration 5650, loss = 0.03592218\n",
      "Iteration 5651, loss = 0.03592049\n",
      "Iteration 5652, loss = 0.03591881\n",
      "Iteration 5653, loss = 0.03591713\n",
      "Iteration 5654, loss = 0.03591544\n",
      "Iteration 5655, loss = 0.03591376\n",
      "Iteration 5656, loss = 0.03591207\n",
      "Iteration 5657, loss = 0.03591038\n",
      "Iteration 5658, loss = 0.03590869\n",
      "Iteration 5659, loss = 0.03590701\n",
      "Iteration 5660, loss = 0.03590532\n",
      "Iteration 5661, loss = 0.03590363\n",
      "Iteration 5662, loss = 0.03590194\n",
      "Iteration 5663, loss = 0.03590025\n",
      "Iteration 5664, loss = 0.03589856\n",
      "Iteration 5665, loss = 0.03589686\n",
      "Iteration 5666, loss = 0.03589517\n",
      "Iteration 5667, loss = 0.03589348\n",
      "Iteration 5668, loss = 0.03589178\n",
      "Iteration 5669, loss = 0.03589009\n",
      "Iteration 5670, loss = 0.03588839\n",
      "Iteration 5671, loss = 0.03588669\n",
      "Iteration 5672, loss = 0.03588500\n",
      "Iteration 5673, loss = 0.03588330\n",
      "Iteration 5674, loss = 0.03588160\n",
      "Iteration 5675, loss = 0.03587990\n",
      "Iteration 5676, loss = 0.03587820\n",
      "Iteration 5677, loss = 0.03587650\n",
      "Iteration 5678, loss = 0.03587480\n",
      "Iteration 5679, loss = 0.03587310\n",
      "Iteration 5680, loss = 0.03587139\n",
      "Iteration 5681, loss = 0.03586969\n",
      "Iteration 5682, loss = 0.03586798\n",
      "Iteration 5683, loss = 0.03586628\n",
      "Iteration 5684, loss = 0.03586457\n",
      "Iteration 5685, loss = 0.03586287\n",
      "Iteration 5686, loss = 0.03586116\n",
      "Iteration 5687, loss = 0.03585945\n",
      "Iteration 5688, loss = 0.03585774\n",
      "Iteration 5689, loss = 0.03585603\n",
      "Iteration 5690, loss = 0.03585432\n",
      "Iteration 5691, loss = 0.03585261\n",
      "Iteration 5692, loss = 0.03585090\n",
      "Iteration 5693, loss = 0.03584918\n",
      "Iteration 5694, loss = 0.03584747\n",
      "Iteration 5695, loss = 0.03584576\n",
      "Iteration 5696, loss = 0.03584404\n",
      "Iteration 5697, loss = 0.03584233\n",
      "Iteration 5698, loss = 0.03584061\n",
      "Iteration 5699, loss = 0.03583889\n",
      "Iteration 5700, loss = 0.03583717\n",
      "Iteration 5701, loss = 0.03583545\n",
      "Iteration 5702, loss = 0.03583373\n",
      "Iteration 5703, loss = 0.03583201\n",
      "Iteration 5704, loss = 0.03583029\n",
      "Iteration 5705, loss = 0.03582857\n",
      "Iteration 5706, loss = 0.03582685\n",
      "Iteration 5707, loss = 0.03582512\n",
      "Iteration 5708, loss = 0.03582340\n",
      "Iteration 5709, loss = 0.03582167\n",
      "Iteration 5710, loss = 0.03581995\n",
      "Iteration 5711, loss = 0.03581822\n",
      "Iteration 5712, loss = 0.03581649\n",
      "Iteration 5713, loss = 0.03581477\n",
      "Iteration 5714, loss = 0.03581304\n",
      "Iteration 5715, loss = 0.03581131\n",
      "Iteration 5716, loss = 0.03580957\n",
      "Iteration 5717, loss = 0.03580784\n",
      "Iteration 5718, loss = 0.03580611\n",
      "Iteration 5719, loss = 0.03580438\n",
      "Iteration 5720, loss = 0.03580264\n",
      "Iteration 5721, loss = 0.03580091\n",
      "Iteration 5722, loss = 0.03579917\n",
      "Iteration 5723, loss = 0.03579744\n",
      "Iteration 5724, loss = 0.03579570\n",
      "Iteration 5725, loss = 0.03579396\n",
      "Iteration 5726, loss = 0.03579222\n",
      "Iteration 5727, loss = 0.03579048\n",
      "Iteration 5728, loss = 0.03578874\n",
      "Iteration 5729, loss = 0.03578700\n",
      "Iteration 5730, loss = 0.03578526\n",
      "Iteration 5731, loss = 0.03578351\n",
      "Iteration 5732, loss = 0.03578177\n",
      "Iteration 5733, loss = 0.03578002\n",
      "Iteration 5734, loss = 0.03577828\n",
      "Iteration 5735, loss = 0.03577653\n",
      "Iteration 5736, loss = 0.03577478\n",
      "Iteration 5737, loss = 0.03577304\n",
      "Iteration 5738, loss = 0.03577129\n",
      "Iteration 5739, loss = 0.03576954\n",
      "Iteration 5740, loss = 0.03576779\n",
      "Iteration 5741, loss = 0.03576603\n",
      "Iteration 5742, loss = 0.03576428\n",
      "Iteration 5743, loss = 0.03576253\n",
      "Iteration 5744, loss = 0.03576077\n",
      "Iteration 5745, loss = 0.03575902\n",
      "Iteration 5746, loss = 0.03575726\n",
      "Iteration 5747, loss = 0.03575550\n",
      "Iteration 5748, loss = 0.03575375\n",
      "Iteration 5749, loss = 0.03575199\n",
      "Iteration 5750, loss = 0.03575023\n",
      "Iteration 5751, loss = 0.03574847\n",
      "Iteration 5752, loss = 0.03574671\n",
      "Iteration 5753, loss = 0.03574494\n",
      "Iteration 5754, loss = 0.03574318\n",
      "Iteration 5755, loss = 0.03574142\n",
      "Iteration 5756, loss = 0.03573965\n",
      "Iteration 5757, loss = 0.03573789\n",
      "Iteration 5758, loss = 0.03573612\n",
      "Iteration 5759, loss = 0.03573435\n",
      "Iteration 5760, loss = 0.03573258\n",
      "Iteration 5761, loss = 0.03573081\n",
      "Iteration 5762, loss = 0.03572904\n",
      "Iteration 5763, loss = 0.03572727\n",
      "Iteration 5764, loss = 0.03572550\n",
      "Iteration 5765, loss = 0.03572373\n",
      "Iteration 5766, loss = 0.03572195\n",
      "Iteration 5767, loss = 0.03572018\n",
      "Iteration 5768, loss = 0.03571840\n",
      "Iteration 5769, loss = 0.03571662\n",
      "Iteration 5770, loss = 0.03571485\n",
      "Iteration 5771, loss = 0.03571307\n",
      "Iteration 5772, loss = 0.03571129\n",
      "Iteration 5773, loss = 0.03570951\n",
      "Iteration 5774, loss = 0.03570773\n",
      "Iteration 5775, loss = 0.03570594\n",
      "Iteration 5776, loss = 0.03570416\n",
      "Iteration 5777, loss = 0.03570238\n",
      "Iteration 5778, loss = 0.03570059\n",
      "Iteration 5779, loss = 0.03569881\n",
      "Iteration 5780, loss = 0.03569702\n",
      "Iteration 5781, loss = 0.03569523\n",
      "Iteration 5782, loss = 0.03569344\n",
      "Iteration 5783, loss = 0.03569165\n",
      "Iteration 5784, loss = 0.03568986\n",
      "Iteration 5785, loss = 0.03568807\n",
      "Iteration 5786, loss = 0.03568628\n",
      "Iteration 5787, loss = 0.03568448\n",
      "Iteration 5788, loss = 0.03568269\n",
      "Iteration 5789, loss = 0.03568089\n",
      "Iteration 5790, loss = 0.03567910\n",
      "Iteration 5791, loss = 0.03567730\n",
      "Iteration 5792, loss = 0.03567550\n",
      "Iteration 5793, loss = 0.03567370\n",
      "Iteration 5794, loss = 0.03567190\n",
      "Iteration 5795, loss = 0.03567010\n",
      "Iteration 5796, loss = 0.03566830\n",
      "Iteration 5797, loss = 0.03566649\n",
      "Iteration 5798, loss = 0.03566469\n",
      "Iteration 5799, loss = 0.03566288\n",
      "Iteration 5800, loss = 0.03566108\n",
      "Iteration 5801, loss = 0.03565927\n",
      "Iteration 5802, loss = 0.03565746\n",
      "Iteration 5803, loss = 0.03565565\n",
      "Iteration 5804, loss = 0.03565384\n",
      "Iteration 5805, loss = 0.03565203\n",
      "Iteration 5806, loss = 0.03565022\n",
      "Iteration 5807, loss = 0.03564840\n",
      "Iteration 5808, loss = 0.03564659\n",
      "Iteration 5809, loss = 0.03564478\n",
      "Iteration 5810, loss = 0.03564296\n",
      "Iteration 5811, loss = 0.03564114\n",
      "Iteration 5812, loss = 0.03563932\n",
      "Iteration 5813, loss = 0.03563750\n",
      "Iteration 5814, loss = 0.03563568\n",
      "Iteration 5815, loss = 0.03563386\n",
      "Iteration 5816, loss = 0.03563204\n",
      "Iteration 5817, loss = 0.03563022\n",
      "Iteration 5818, loss = 0.03562839\n",
      "Iteration 5819, loss = 0.03562657\n",
      "Iteration 5820, loss = 0.03562474\n",
      "Iteration 5821, loss = 0.03562291\n",
      "Iteration 5822, loss = 0.03562109\n",
      "Iteration 5823, loss = 0.03561926\n",
      "Iteration 5824, loss = 0.03561743\n",
      "Iteration 5825, loss = 0.03561559\n",
      "Iteration 5826, loss = 0.03561376\n",
      "Iteration 5827, loss = 0.03561193\n",
      "Iteration 5828, loss = 0.03561009\n",
      "Iteration 5829, loss = 0.03560826\n",
      "Iteration 5830, loss = 0.03560642\n",
      "Iteration 5831, loss = 0.03560458\n",
      "Iteration 5832, loss = 0.03560275\n",
      "Iteration 5833, loss = 0.03560091\n",
      "Iteration 5834, loss = 0.03559907\n",
      "Iteration 5835, loss = 0.03559722\n",
      "Iteration 5836, loss = 0.03559538\n",
      "Iteration 5837, loss = 0.03559354\n",
      "Iteration 5838, loss = 0.03559169\n",
      "Iteration 5839, loss = 0.03558985\n",
      "Iteration 5840, loss = 0.03558800\n",
      "Iteration 5841, loss = 0.03558615\n",
      "Iteration 5842, loss = 0.03558430\n",
      "Iteration 5843, loss = 0.03558245\n",
      "Iteration 5844, loss = 0.03558060\n",
      "Iteration 5845, loss = 0.03557875\n",
      "Iteration 5846, loss = 0.03557690\n",
      "Iteration 5847, loss = 0.03557504\n",
      "Iteration 5848, loss = 0.03557319\n",
      "Iteration 5849, loss = 0.03557133\n",
      "Iteration 5850, loss = 0.03556947\n",
      "Iteration 5851, loss = 0.03556761\n",
      "Iteration 5852, loss = 0.03556575\n",
      "Iteration 5853, loss = 0.03556389\n",
      "Iteration 5854, loss = 0.03556203\n",
      "Iteration 5855, loss = 0.03556017\n",
      "Iteration 5856, loss = 0.03555830\n",
      "Iteration 5857, loss = 0.03555644\n",
      "Iteration 5858, loss = 0.03555457\n",
      "Iteration 5859, loss = 0.03555271\n",
      "Iteration 5860, loss = 0.03555084\n",
      "Iteration 5861, loss = 0.03554897\n",
      "Iteration 5862, loss = 0.03554710\n",
      "Iteration 5863, loss = 0.03554523\n",
      "Iteration 5864, loss = 0.03554335\n",
      "Iteration 5865, loss = 0.03554148\n",
      "Iteration 5866, loss = 0.03553960\n",
      "Iteration 5867, loss = 0.03553773\n",
      "Iteration 5868, loss = 0.03553585\n",
      "Iteration 5869, loss = 0.03553397\n",
      "Iteration 5870, loss = 0.03553209\n",
      "Iteration 5871, loss = 0.03553021\n",
      "Iteration 5872, loss = 0.03552833\n",
      "Iteration 5873, loss = 0.03552645\n",
      "Iteration 5874, loss = 0.03552457\n",
      "Iteration 5875, loss = 0.03552268\n",
      "Iteration 5876, loss = 0.03552080\n",
      "Iteration 5877, loss = 0.03551891\n",
      "Iteration 5878, loss = 0.03551702\n",
      "Iteration 5879, loss = 0.03551513\n",
      "Iteration 5880, loss = 0.03551324\n",
      "Iteration 5881, loss = 0.03551135\n",
      "Iteration 5882, loss = 0.03550946\n",
      "Iteration 5883, loss = 0.03550756\n",
      "Iteration 5884, loss = 0.03550567\n",
      "Iteration 5885, loss = 0.03550377\n",
      "Iteration 5886, loss = 0.03550188\n",
      "Iteration 5887, loss = 0.03549998\n",
      "Iteration 5888, loss = 0.03549808\n",
      "Iteration 5889, loss = 0.03549618\n",
      "Iteration 5890, loss = 0.03549428\n",
      "Iteration 5891, loss = 0.03549237\n",
      "Iteration 5892, loss = 0.03549047\n",
      "Iteration 5893, loss = 0.03548857\n",
      "Iteration 5894, loss = 0.03548666\n",
      "Iteration 5895, loss = 0.03548475\n",
      "Iteration 5896, loss = 0.03548284\n",
      "Iteration 5897, loss = 0.03548094\n",
      "Iteration 5898, loss = 0.03547902\n",
      "Iteration 5899, loss = 0.03547711\n",
      "Iteration 5900, loss = 0.03547520\n",
      "Iteration 5901, loss = 0.03547329\n",
      "Iteration 5902, loss = 0.03547137\n",
      "Iteration 5903, loss = 0.03546945\n",
      "Iteration 5904, loss = 0.03546754\n",
      "Iteration 5905, loss = 0.03546562\n",
      "Iteration 5906, loss = 0.03546370\n",
      "Iteration 5907, loss = 0.03546178\n",
      "Iteration 5908, loss = 0.03545986\n",
      "Iteration 5909, loss = 0.03545793\n",
      "Iteration 5910, loss = 0.03545601\n",
      "Iteration 5911, loss = 0.03545408\n",
      "Iteration 5912, loss = 0.03545216\n",
      "Iteration 5913, loss = 0.03545023\n",
      "Iteration 5914, loss = 0.03544830\n",
      "Iteration 5915, loss = 0.03544637\n",
      "Iteration 5916, loss = 0.03544444\n",
      "Iteration 5917, loss = 0.03544251\n",
      "Iteration 5918, loss = 0.03544057\n",
      "Iteration 5919, loss = 0.03543864\n",
      "Iteration 5920, loss = 0.03543670\n",
      "Iteration 5921, loss = 0.03543477\n",
      "Iteration 5922, loss = 0.03543283\n",
      "Iteration 5923, loss = 0.03543089\n",
      "Iteration 5924, loss = 0.03542895\n",
      "Iteration 5925, loss = 0.03542701\n",
      "Iteration 5926, loss = 0.03542506\n",
      "Iteration 5927, loss = 0.03542312\n",
      "Iteration 5928, loss = 0.03542117\n",
      "Iteration 5929, loss = 0.03541923\n",
      "Iteration 5930, loss = 0.03541728\n",
      "Iteration 5931, loss = 0.03541533\n",
      "Iteration 5932, loss = 0.03541338\n",
      "Iteration 5933, loss = 0.03541143\n",
      "Iteration 5934, loss = 0.03540948\n",
      "Iteration 5935, loss = 0.03540752\n",
      "Iteration 5936, loss = 0.03540557\n",
      "Iteration 5937, loss = 0.03540361\n",
      "Iteration 5938, loss = 0.03540166\n",
      "Iteration 5939, loss = 0.03539970\n",
      "Iteration 5940, loss = 0.03539774\n",
      "Iteration 5941, loss = 0.03539578\n",
      "Iteration 5942, loss = 0.03539382\n",
      "Iteration 5943, loss = 0.03539185\n",
      "Iteration 5944, loss = 0.03538989\n",
      "Iteration 5945, loss = 0.03538792\n",
      "Iteration 5946, loss = 0.03538596\n",
      "Iteration 5947, loss = 0.03538399\n",
      "Iteration 5948, loss = 0.03538202\n",
      "Iteration 5949, loss = 0.03538005\n",
      "Iteration 5950, loss = 0.03537808\n",
      "Iteration 5951, loss = 0.03537610\n",
      "Iteration 5952, loss = 0.03537413\n",
      "Iteration 5953, loss = 0.03537216\n",
      "Iteration 5954, loss = 0.03537018\n",
      "Iteration 5955, loss = 0.03536820\n",
      "Iteration 5956, loss = 0.03536622\n",
      "Iteration 5957, loss = 0.03536424\n",
      "Iteration 5958, loss = 0.03536226\n",
      "Iteration 5959, loss = 0.03536028\n",
      "Iteration 5960, loss = 0.03535830\n",
      "Iteration 5961, loss = 0.03535631\n",
      "Iteration 5962, loss = 0.03535433\n",
      "Iteration 5963, loss = 0.03535234\n",
      "Iteration 5964, loss = 0.03535035\n",
      "Iteration 5965, loss = 0.03534836\n",
      "Iteration 5966, loss = 0.03534637\n",
      "Iteration 5967, loss = 0.03534438\n",
      "Iteration 5968, loss = 0.03534238\n",
      "Iteration 5969, loss = 0.03534039\n",
      "Iteration 5970, loss = 0.03533839\n",
      "Iteration 5971, loss = 0.03533640\n",
      "Iteration 5972, loss = 0.03533440\n",
      "Iteration 5973, loss = 0.03533240\n",
      "Iteration 5974, loss = 0.03533040\n",
      "Iteration 5975, loss = 0.03532840\n",
      "Iteration 5976, loss = 0.03532639\n",
      "Iteration 5977, loss = 0.03532439\n",
      "Iteration 5978, loss = 0.03532238\n",
      "Iteration 5979, loss = 0.03532038\n",
      "Iteration 5980, loss = 0.03531837\n",
      "Iteration 5981, loss = 0.03531636\n",
      "Iteration 5982, loss = 0.03531435\n",
      "Iteration 5983, loss = 0.03531234\n",
      "Iteration 5984, loss = 0.03531032\n",
      "Iteration 5985, loss = 0.03530831\n",
      "Iteration 5986, loss = 0.03530629\n",
      "Iteration 5987, loss = 0.03530428\n",
      "Iteration 5988, loss = 0.03530226\n",
      "Iteration 5989, loss = 0.03530024\n",
      "Iteration 5990, loss = 0.03529822\n",
      "Iteration 5991, loss = 0.03529620\n",
      "Iteration 5992, loss = 0.03529417\n",
      "Iteration 5993, loss = 0.03529215\n",
      "Iteration 5994, loss = 0.03529012\n",
      "Iteration 5995, loss = 0.03528810\n",
      "Iteration 5996, loss = 0.03528607\n",
      "Iteration 5997, loss = 0.03528404\n",
      "Iteration 5998, loss = 0.03528201\n",
      "Iteration 5999, loss = 0.03527998\n",
      "Iteration 6000, loss = 0.03527794\n",
      "Iteration 6001, loss = 0.03527591\n",
      "Iteration 6002, loss = 0.03527387\n",
      "Iteration 6003, loss = 0.03527184\n",
      "Iteration 6004, loss = 0.03526980\n",
      "Iteration 6005, loss = 0.03526776\n",
      "Iteration 6006, loss = 0.03526572\n",
      "Iteration 6007, loss = 0.03526367\n",
      "Iteration 6008, loss = 0.03526163\n",
      "Iteration 6009, loss = 0.03525959\n",
      "Iteration 6010, loss = 0.03525754\n",
      "Iteration 6011, loss = 0.03525549\n",
      "Iteration 6012, loss = 0.03525345\n",
      "Iteration 6013, loss = 0.03525140\n",
      "Iteration 6014, loss = 0.03524934\n",
      "Iteration 6015, loss = 0.03524729\n",
      "Iteration 6016, loss = 0.03524524\n",
      "Iteration 6017, loss = 0.03524318\n",
      "Iteration 6018, loss = 0.03524113\n",
      "Iteration 6019, loss = 0.03523907\n",
      "Iteration 6020, loss = 0.03523701\n",
      "Iteration 6021, loss = 0.03523495\n",
      "Iteration 6022, loss = 0.03523289\n",
      "Iteration 6023, loss = 0.03523083\n",
      "Iteration 6024, loss = 0.03522876\n",
      "Iteration 6025, loss = 0.03522670\n",
      "Iteration 6026, loss = 0.03522463\n",
      "Iteration 6027, loss = 0.03522256\n",
      "Iteration 6028, loss = 0.03522049\n",
      "Iteration 6029, loss = 0.03521842\n",
      "Iteration 6030, loss = 0.03521635\n",
      "Iteration 6031, loss = 0.03521428\n",
      "Iteration 6032, loss = 0.03521220\n",
      "Iteration 6033, loss = 0.03521013\n",
      "Iteration 6034, loss = 0.03520805\n",
      "Iteration 6035, loss = 0.03520597\n",
      "Iteration 6036, loss = 0.03520389\n",
      "Iteration 6037, loss = 0.03520181\n",
      "Iteration 6038, loss = 0.03519973\n",
      "Iteration 6039, loss = 0.03519765\n",
      "Iteration 6040, loss = 0.03519556\n",
      "Iteration 6041, loss = 0.03519348\n",
      "Iteration 6042, loss = 0.03519139\n",
      "Iteration 6043, loss = 0.03518930\n",
      "Iteration 6044, loss = 0.03518721\n",
      "Iteration 6045, loss = 0.03518512\n",
      "Iteration 6046, loss = 0.03518302\n",
      "Iteration 6047, loss = 0.03518093\n",
      "Iteration 6048, loss = 0.03517883\n",
      "Iteration 6049, loss = 0.03517674\n",
      "Iteration 6050, loss = 0.03517464\n",
      "Iteration 6051, loss = 0.03517254\n",
      "Iteration 6052, loss = 0.03517044\n",
      "Iteration 6053, loss = 0.03516834\n",
      "Iteration 6054, loss = 0.03516623\n",
      "Iteration 6055, loss = 0.03516413\n",
      "Iteration 6056, loss = 0.03516202\n",
      "Iteration 6057, loss = 0.03515991\n",
      "Iteration 6058, loss = 0.03515781\n",
      "Iteration 6059, loss = 0.03515569\n",
      "Iteration 6060, loss = 0.03515358\n",
      "Iteration 6061, loss = 0.03515147\n",
      "Iteration 6062, loss = 0.03514936\n",
      "Iteration 6063, loss = 0.03514724\n",
      "Iteration 6064, loss = 0.03514512\n",
      "Iteration 6065, loss = 0.03514300\n",
      "Iteration 6066, loss = 0.03514088\n",
      "Iteration 6067, loss = 0.03513876\n",
      "Iteration 6068, loss = 0.03513664\n",
      "Iteration 6069, loss = 0.03513452\n",
      "Iteration 6070, loss = 0.03513239\n",
      "Iteration 6071, loss = 0.03513027\n",
      "Iteration 6072, loss = 0.03512814\n",
      "Iteration 6073, loss = 0.03512601\n",
      "Iteration 6074, loss = 0.03512388\n",
      "Iteration 6075, loss = 0.03512174\n",
      "Iteration 6076, loss = 0.03511961\n",
      "Iteration 6077, loss = 0.03511748\n",
      "Iteration 6078, loss = 0.03511534\n",
      "Iteration 6079, loss = 0.03511320\n",
      "Iteration 6080, loss = 0.03511106\n",
      "Iteration 6081, loss = 0.03510892\n",
      "Iteration 6082, loss = 0.03510678\n",
      "Iteration 6083, loss = 0.03510464\n",
      "Iteration 6084, loss = 0.03510249\n",
      "Iteration 6085, loss = 0.03510035\n",
      "Iteration 6086, loss = 0.03509820\n",
      "Iteration 6087, loss = 0.03509605\n",
      "Iteration 6088, loss = 0.03509390\n",
      "Iteration 6089, loss = 0.03509175\n",
      "Iteration 6090, loss = 0.03508960\n",
      "Iteration 6091, loss = 0.03508744\n",
      "Iteration 6092, loss = 0.03508529\n",
      "Iteration 6093, loss = 0.03508313\n",
      "Iteration 6094, loss = 0.03508097\n",
      "Iteration 6095, loss = 0.03507881\n",
      "Iteration 6096, loss = 0.03507665\n",
      "Iteration 6097, loss = 0.03507449\n",
      "Iteration 6098, loss = 0.03507232\n",
      "Iteration 6099, loss = 0.03507016\n",
      "Iteration 6100, loss = 0.03506799\n",
      "Iteration 6101, loss = 0.03506582\n",
      "Iteration 6102, loss = 0.03506365\n",
      "Iteration 6103, loss = 0.03506148\n",
      "Iteration 6104, loss = 0.03505931\n",
      "Iteration 6105, loss = 0.03505713\n",
      "Iteration 6106, loss = 0.03505495\n",
      "Iteration 6107, loss = 0.03505278\n",
      "Iteration 6108, loss = 0.03505060\n",
      "Iteration 6109, loss = 0.03504842\n",
      "Iteration 6110, loss = 0.03504624\n",
      "Iteration 6111, loss = 0.03504405\n",
      "Iteration 6112, loss = 0.03504187\n",
      "Iteration 6113, loss = 0.03503968\n",
      "Iteration 6114, loss = 0.03503750\n",
      "Iteration 6115, loss = 0.03503531\n",
      "Iteration 6116, loss = 0.03503312\n",
      "Iteration 6117, loss = 0.03503093\n",
      "Iteration 6118, loss = 0.03502873\n",
      "Iteration 6119, loss = 0.03502654\n",
      "Iteration 6120, loss = 0.03502434\n",
      "Iteration 6121, loss = 0.03502214\n",
      "Iteration 6122, loss = 0.03501995\n",
      "Iteration 6123, loss = 0.03501774\n",
      "Iteration 6124, loss = 0.03501554\n",
      "Iteration 6125, loss = 0.03501334\n",
      "Iteration 6126, loss = 0.03501113\n",
      "Iteration 6127, loss = 0.03500893\n",
      "Iteration 6128, loss = 0.03500672\n",
      "Iteration 6129, loss = 0.03500451\n",
      "Iteration 6130, loss = 0.03500230\n",
      "Iteration 6131, loss = 0.03500009\n",
      "Iteration 6132, loss = 0.03499787\n",
      "Iteration 6133, loss = 0.03499566\n",
      "Iteration 6134, loss = 0.03499344\n",
      "Iteration 6135, loss = 0.03499122\n",
      "Iteration 6136, loss = 0.03498900\n",
      "Iteration 6137, loss = 0.03498678\n",
      "Iteration 6138, loss = 0.03498456\n",
      "Iteration 6139, loss = 0.03498233\n",
      "Iteration 6140, loss = 0.03498011\n",
      "Iteration 6141, loss = 0.03497788\n",
      "Iteration 6142, loss = 0.03497565\n",
      "Iteration 6143, loss = 0.03497342\n",
      "Iteration 6144, loss = 0.03497119\n",
      "Iteration 6145, loss = 0.03496896\n",
      "Iteration 6146, loss = 0.03496672\n",
      "Iteration 6147, loss = 0.03496448\n",
      "Iteration 6148, loss = 0.03496225\n",
      "Iteration 6149, loss = 0.03496001\n",
      "Iteration 6150, loss = 0.03495776\n",
      "Iteration 6151, loss = 0.03495552\n",
      "Iteration 6152, loss = 0.03495328\n",
      "Iteration 6153, loss = 0.03495103\n",
      "Iteration 6154, loss = 0.03494878\n",
      "Iteration 6155, loss = 0.03494654\n",
      "Iteration 6156, loss = 0.03494428\n",
      "Iteration 6157, loss = 0.03494203\n",
      "Iteration 6158, loss = 0.03493978\n",
      "Iteration 6159, loss = 0.03493752\n",
      "Iteration 6160, loss = 0.03493527\n",
      "Iteration 6161, loss = 0.03493301\n",
      "Iteration 6162, loss = 0.03493075\n",
      "Iteration 6163, loss = 0.03492849\n",
      "Iteration 6164, loss = 0.03492622\n",
      "Iteration 6165, loss = 0.03492396\n",
      "Iteration 6166, loss = 0.03492169\n",
      "Iteration 6167, loss = 0.03491943\n",
      "Iteration 6168, loss = 0.03491716\n",
      "Iteration 6169, loss = 0.03491489\n",
      "Iteration 6170, loss = 0.03491261\n",
      "Iteration 6171, loss = 0.03491034\n",
      "Iteration 6172, loss = 0.03490806\n",
      "Iteration 6173, loss = 0.03490579\n",
      "Iteration 6174, loss = 0.03490351\n",
      "Iteration 6175, loss = 0.03490123\n",
      "Iteration 6176, loss = 0.03489894\n",
      "Iteration 6177, loss = 0.03489666\n",
      "Iteration 6178, loss = 0.03489438\n",
      "Iteration 6179, loss = 0.03489209\n",
      "Iteration 6180, loss = 0.03488980\n",
      "Iteration 6181, loss = 0.03488751\n",
      "Iteration 6182, loss = 0.03488522\n",
      "Iteration 6183, loss = 0.03488292\n",
      "Iteration 6184, loss = 0.03488063\n",
      "Iteration 6185, loss = 0.03487833\n",
      "Iteration 6186, loss = 0.03487603\n",
      "Iteration 6187, loss = 0.03487373\n",
      "Iteration 6188, loss = 0.03487143\n",
      "Iteration 6189, loss = 0.03486913\n",
      "Iteration 6190, loss = 0.03486682\n",
      "Iteration 6191, loss = 0.03486452\n",
      "Iteration 6192, loss = 0.03486221\n",
      "Iteration 6193, loss = 0.03485990\n",
      "Iteration 6194, loss = 0.03485759\n",
      "Iteration 6195, loss = 0.03485527\n",
      "Iteration 6196, loss = 0.03485296\n",
      "Iteration 6197, loss = 0.03485064\n",
      "Iteration 6198, loss = 0.03484832\n",
      "Iteration 6199, loss = 0.03484600\n",
      "Iteration 6200, loss = 0.03484368\n",
      "Iteration 6201, loss = 0.03484136\n",
      "Iteration 6202, loss = 0.03483903\n",
      "Iteration 6203, loss = 0.03483671\n",
      "Iteration 6204, loss = 0.03483438\n",
      "Iteration 6205, loss = 0.03483205\n",
      "Iteration 6206, loss = 0.03482972\n",
      "Iteration 6207, loss = 0.03482738\n",
      "Iteration 6208, loss = 0.03482505\n",
      "Iteration 6209, loss = 0.03482271\n",
      "Iteration 6210, loss = 0.03482037\n",
      "Iteration 6211, loss = 0.03481803\n",
      "Iteration 6212, loss = 0.03481569\n",
      "Iteration 6213, loss = 0.03481335\n",
      "Iteration 6214, loss = 0.03481100\n",
      "Iteration 6215, loss = 0.03480865\n",
      "Iteration 6216, loss = 0.03480630\n",
      "Iteration 6217, loss = 0.03480395\n",
      "Iteration 6218, loss = 0.03480160\n",
      "Iteration 6219, loss = 0.03479925\n",
      "Iteration 6220, loss = 0.03479689\n",
      "Iteration 6221, loss = 0.03479453\n",
      "Iteration 6222, loss = 0.03479217\n",
      "Iteration 6223, loss = 0.03478981\n",
      "Iteration 6224, loss = 0.03478745\n",
      "Iteration 6225, loss = 0.03478509\n",
      "Iteration 6226, loss = 0.03478272\n",
      "Iteration 6227, loss = 0.03478035\n",
      "Iteration 6228, loss = 0.03477798\n",
      "Iteration 6229, loss = 0.03477561\n",
      "Iteration 6230, loss = 0.03477324\n",
      "Iteration 6231, loss = 0.03477086\n",
      "Iteration 6232, loss = 0.03476848\n",
      "Iteration 6233, loss = 0.03476610\n",
      "Iteration 6234, loss = 0.03476372\n",
      "Iteration 6235, loss = 0.03476134\n",
      "Iteration 6236, loss = 0.03475896\n",
      "Iteration 6237, loss = 0.03475657\n",
      "Iteration 6238, loss = 0.03475418\n",
      "Iteration 6239, loss = 0.03475179\n",
      "Iteration 6240, loss = 0.03474940\n",
      "Iteration 6241, loss = 0.03474701\n",
      "Iteration 6242, loss = 0.03474461\n",
      "Iteration 6243, loss = 0.03474222\n",
      "Iteration 6244, loss = 0.03473982\n",
      "Iteration 6245, loss = 0.03473742\n",
      "Iteration 6246, loss = 0.03473501\n",
      "Iteration 6247, loss = 0.03473261\n",
      "Iteration 6248, loss = 0.03473020\n",
      "Iteration 6249, loss = 0.03472779\n",
      "Iteration 6250, loss = 0.03472538\n",
      "Iteration 6251, loss = 0.03472297\n",
      "Iteration 6252, loss = 0.03472056\n",
      "Iteration 6253, loss = 0.03471814\n",
      "Iteration 6254, loss = 0.03471573\n",
      "Iteration 6255, loss = 0.03471331\n",
      "Iteration 6256, loss = 0.03471089\n",
      "Iteration 6257, loss = 0.03470846\n",
      "Iteration 6258, loss = 0.03470604\n",
      "Iteration 6259, loss = 0.03470361\n",
      "Iteration 6260, loss = 0.03470118\n",
      "Iteration 6261, loss = 0.03469875\n",
      "Iteration 6262, loss = 0.03469632\n",
      "Iteration 6263, loss = 0.03469388\n",
      "Iteration 6264, loss = 0.03469145\n",
      "Iteration 6265, loss = 0.03468901\n",
      "Iteration 6266, loss = 0.03468657\n",
      "Iteration 6267, loss = 0.03468413\n",
      "Iteration 6268, loss = 0.03468168\n",
      "Iteration 6269, loss = 0.03467924\n",
      "Iteration 6270, loss = 0.03467679\n",
      "Iteration 6271, loss = 0.03467434\n",
      "Iteration 6272, loss = 0.03467189\n",
      "Iteration 6273, loss = 0.03466944\n",
      "Iteration 6274, loss = 0.03466698\n",
      "Iteration 6275, loss = 0.03466452\n",
      "Iteration 6276, loss = 0.03466206\n",
      "Iteration 6277, loss = 0.03465960\n",
      "Iteration 6278, loss = 0.03465714\n",
      "Iteration 6279, loss = 0.03465467\n",
      "Iteration 6280, loss = 0.03465220\n",
      "Iteration 6281, loss = 0.03464974\n",
      "Iteration 6282, loss = 0.03464726\n",
      "Iteration 6283, loss = 0.03464479\n",
      "Iteration 6284, loss = 0.03464232\n",
      "Iteration 6285, loss = 0.03463984\n",
      "Iteration 6286, loss = 0.03463736\n",
      "Iteration 6287, loss = 0.03463488\n",
      "Iteration 6288, loss = 0.03463239\n",
      "Iteration 6289, loss = 0.03462991\n",
      "Iteration 6290, loss = 0.03462742\n",
      "Iteration 6291, loss = 0.03462493\n",
      "Iteration 6292, loss = 0.03462244\n",
      "Iteration 6293, loss = 0.03461995\n",
      "Iteration 6294, loss = 0.03461745\n",
      "Iteration 6295, loss = 0.03461496\n",
      "Iteration 6296, loss = 0.03461246\n",
      "Iteration 6297, loss = 0.03460995\n",
      "Iteration 6298, loss = 0.03460745\n",
      "Iteration 6299, loss = 0.03460494\n",
      "Iteration 6300, loss = 0.03460244\n",
      "Iteration 6301, loss = 0.03459993\n",
      "Iteration 6302, loss = 0.03459742\n",
      "Iteration 6303, loss = 0.03459490\n",
      "Iteration 6304, loss = 0.03459239\n",
      "Iteration 6305, loss = 0.03458987\n",
      "Iteration 6306, loss = 0.03458735\n",
      "Iteration 6307, loss = 0.03458483\n",
      "Iteration 6308, loss = 0.03458230\n",
      "Iteration 6309, loss = 0.03457977\n",
      "Iteration 6310, loss = 0.03457725\n",
      "Iteration 6311, loss = 0.03457472\n",
      "Iteration 6312, loss = 0.03457218\n",
      "Iteration 6313, loss = 0.03456965\n",
      "Iteration 6314, loss = 0.03456711\n",
      "Iteration 6315, loss = 0.03456457\n",
      "Iteration 6316, loss = 0.03456203\n",
      "Iteration 6317, loss = 0.03455949\n",
      "Iteration 6318, loss = 0.03455694\n",
      "Iteration 6319, loss = 0.03455439\n",
      "Iteration 6320, loss = 0.03455184\n",
      "Iteration 6321, loss = 0.03454929\n",
      "Iteration 6322, loss = 0.03454674\n",
      "Iteration 6323, loss = 0.03454418\n",
      "Iteration 6324, loss = 0.03454162\n",
      "Iteration 6325, loss = 0.03453906\n",
      "Iteration 6326, loss = 0.03453650\n",
      "Iteration 6327, loss = 0.03453394\n",
      "Iteration 6328, loss = 0.03453137\n",
      "Iteration 6329, loss = 0.03452880\n",
      "Iteration 6330, loss = 0.03452623\n",
      "Iteration 6331, loss = 0.03452365\n",
      "Iteration 6332, loss = 0.03452108\n",
      "Iteration 6333, loss = 0.03451850\n",
      "Iteration 6334, loss = 0.03451592\n",
      "Iteration 6335, loss = 0.03451334\n",
      "Iteration 6336, loss = 0.03451075\n",
      "Iteration 6337, loss = 0.03450816\n",
      "Iteration 6338, loss = 0.03450557\n",
      "Iteration 6339, loss = 0.03450298\n",
      "Iteration 6340, loss = 0.03450039\n",
      "Iteration 6341, loss = 0.03449779\n",
      "Iteration 6342, loss = 0.03449520\n",
      "Iteration 6343, loss = 0.03449259\n",
      "Iteration 6344, loss = 0.03448999\n",
      "Iteration 6345, loss = 0.03448739\n",
      "Iteration 6346, loss = 0.03448478\n",
      "Iteration 6347, loss = 0.03448217\n",
      "Iteration 6348, loss = 0.03447956\n",
      "Iteration 6349, loss = 0.03447694\n",
      "Iteration 6350, loss = 0.03447433\n",
      "Iteration 6351, loss = 0.03447171\n",
      "Iteration 6352, loss = 0.03446909\n",
      "Iteration 6353, loss = 0.03446646\n",
      "Iteration 6354, loss = 0.03446384\n",
      "Iteration 6355, loss = 0.03446121\n",
      "Iteration 6356, loss = 0.03445858\n",
      "Iteration 6357, loss = 0.03445595\n",
      "Iteration 6358, loss = 0.03445331\n",
      "Iteration 6359, loss = 0.03445067\n",
      "Iteration 6360, loss = 0.03444803\n",
      "Iteration 6361, loss = 0.03444539\n",
      "Iteration 6362, loss = 0.03444275\n",
      "Iteration 6363, loss = 0.03444010\n",
      "Iteration 6364, loss = 0.03443745\n",
      "Iteration 6365, loss = 0.03443480\n",
      "Iteration 6366, loss = 0.03443214\n",
      "Iteration 6367, loss = 0.03442949\n",
      "Iteration 6368, loss = 0.03442683\n",
      "Iteration 6369, loss = 0.03442417\n",
      "Iteration 6370, loss = 0.03442150\n",
      "Iteration 6371, loss = 0.03441884\n",
      "Iteration 6372, loss = 0.03441617\n",
      "Iteration 6373, loss = 0.03441350\n",
      "Iteration 6374, loss = 0.03441082\n",
      "Iteration 6375, loss = 0.03440815\n",
      "Iteration 6376, loss = 0.03440547\n",
      "Iteration 6377, loss = 0.03440279\n",
      "Iteration 6378, loss = 0.03440010\n",
      "Iteration 6379, loss = 0.03439742\n",
      "Iteration 6380, loss = 0.03439473\n",
      "Iteration 6381, loss = 0.03439204\n",
      "Iteration 6382, loss = 0.03438935\n",
      "Iteration 6383, loss = 0.03438665\n",
      "Iteration 6384, loss = 0.03438395\n",
      "Iteration 6385, loss = 0.03438125\n",
      "Iteration 6386, loss = 0.03437855\n",
      "Iteration 6387, loss = 0.03437584\n",
      "Iteration 6388, loss = 0.03437314\n",
      "Iteration 6389, loss = 0.03437042\n",
      "Iteration 6390, loss = 0.03436771\n",
      "Iteration 6391, loss = 0.03436500\n",
      "Iteration 6392, loss = 0.03436228\n",
      "Iteration 6393, loss = 0.03435956\n",
      "Iteration 6394, loss = 0.03435683\n",
      "Iteration 6395, loss = 0.03435411\n",
      "Iteration 6396, loss = 0.03435138\n",
      "Iteration 6397, loss = 0.03434865\n",
      "Iteration 6398, loss = 0.03434591\n",
      "Iteration 6399, loss = 0.03434318\n",
      "Iteration 6400, loss = 0.03434044\n",
      "Iteration 6401, loss = 0.03433770\n",
      "Iteration 6402, loss = 0.03433495\n",
      "Iteration 6403, loss = 0.03433221\n",
      "Iteration 6404, loss = 0.03432946\n",
      "Iteration 6405, loss = 0.03432670\n",
      "Iteration 6406, loss = 0.03432395\n",
      "Iteration 6407, loss = 0.03432119\n",
      "Iteration 6408, loss = 0.03431843\n",
      "Iteration 6409, loss = 0.03431567\n",
      "Iteration 6410, loss = 0.03431291\n",
      "Iteration 6411, loss = 0.03431014\n",
      "Iteration 6412, loss = 0.03430737\n",
      "Iteration 6413, loss = 0.03430459\n",
      "Iteration 6414, loss = 0.03430182\n",
      "Iteration 6415, loss = 0.03429904\n",
      "Iteration 6416, loss = 0.03429626\n",
      "Iteration 6417, loss = 0.03429347\n",
      "Iteration 6418, loss = 0.03429069\n",
      "Iteration 6419, loss = 0.03428790\n",
      "Iteration 6420, loss = 0.03428511\n",
      "Iteration 6421, loss = 0.03428231\n",
      "Iteration 6422, loss = 0.03427951\n",
      "Iteration 6423, loss = 0.03427671\n",
      "Iteration 6424, loss = 0.03427391\n",
      "Iteration 6425, loss = 0.03427110\n",
      "Iteration 6426, loss = 0.03426829\n",
      "Iteration 6427, loss = 0.03426548\n",
      "Iteration 6428, loss = 0.03426267\n",
      "Iteration 6429, loss = 0.03425985\n",
      "Iteration 6430, loss = 0.03425703\n",
      "Iteration 6431, loss = 0.03425421\n",
      "Iteration 6432, loss = 0.03425138\n",
      "Iteration 6433, loss = 0.03424855\n",
      "Iteration 6434, loss = 0.03424572\n",
      "Iteration 6435, loss = 0.03424289\n",
      "Iteration 6436, loss = 0.03424005\n",
      "Iteration 6437, loss = 0.03423721\n",
      "Iteration 6438, loss = 0.03423437\n",
      "Iteration 6439, loss = 0.03423153\n",
      "Iteration 6440, loss = 0.03422868\n",
      "Iteration 6441, loss = 0.03422583\n",
      "Iteration 6442, loss = 0.03422297\n",
      "Iteration 6443, loss = 0.03422011\n",
      "Iteration 6444, loss = 0.03421725\n",
      "Iteration 6445, loss = 0.03421439\n",
      "Iteration 6446, loss = 0.03421153\n",
      "Iteration 6447, loss = 0.03420866\n",
      "Iteration 6448, loss = 0.03420579\n",
      "Iteration 6449, loss = 0.03420291\n",
      "Iteration 6450, loss = 0.03420003\n",
      "Iteration 6451, loss = 0.03419715\n",
      "Iteration 6452, loss = 0.03419427\n",
      "Iteration 6453, loss = 0.03419138\n",
      "Iteration 6454, loss = 0.03418849\n",
      "Iteration 6455, loss = 0.03418560\n",
      "Iteration 6456, loss = 0.03418271\n",
      "Iteration 6457, loss = 0.03417981\n",
      "Iteration 6458, loss = 0.03417691\n",
      "Iteration 6459, loss = 0.03417400\n",
      "Iteration 6460, loss = 0.03417110\n",
      "Iteration 6461, loss = 0.03416818\n",
      "Iteration 6462, loss = 0.03416527\n",
      "Iteration 6463, loss = 0.03416235\n",
      "Iteration 6464, loss = 0.03415944\n",
      "Iteration 6465, loss = 0.03415651\n",
      "Iteration 6466, loss = 0.03415359\n",
      "Iteration 6467, loss = 0.03415066\n",
      "Iteration 6468, loss = 0.03414773\n",
      "Iteration 6469, loss = 0.03414479\n",
      "Iteration 6470, loss = 0.03414185\n",
      "Iteration 6471, loss = 0.03413891\n",
      "Iteration 6472, loss = 0.03413597\n",
      "Iteration 6473, loss = 0.03413302\n",
      "Iteration 6474, loss = 0.03413007\n",
      "Iteration 6475, loss = 0.03412711\n",
      "Iteration 6476, loss = 0.03412416\n",
      "Iteration 6477, loss = 0.03412120\n",
      "Iteration 6478, loss = 0.03411823\n",
      "Iteration 6479, loss = 0.03411527\n",
      "Iteration 6480, loss = 0.03411230\n",
      "Iteration 6481, loss = 0.03410932\n",
      "Iteration 6482, loss = 0.03410635\n",
      "Iteration 6483, loss = 0.03410337\n",
      "Iteration 6484, loss = 0.03410039\n",
      "Iteration 6485, loss = 0.03409740\n",
      "Iteration 6486, loss = 0.03409441\n",
      "Iteration 6487, loss = 0.03409142\n",
      "Iteration 6488, loss = 0.03408842\n",
      "Iteration 6489, loss = 0.03408542\n",
      "Iteration 6490, loss = 0.03408242\n",
      "Iteration 6491, loss = 0.03407941\n",
      "Iteration 6492, loss = 0.03407641\n",
      "Iteration 6493, loss = 0.03407339\n",
      "Iteration 6494, loss = 0.03407038\n",
      "Iteration 6495, loss = 0.03406736\n",
      "Iteration 6496, loss = 0.03406434\n",
      "Iteration 6497, loss = 0.03406131\n",
      "Iteration 6498, loss = 0.03405828\n",
      "Iteration 6499, loss = 0.03405525\n",
      "Iteration 6500, loss = 0.03405221\n",
      "Iteration 6501, loss = 0.03404917\n",
      "Iteration 6502, loss = 0.03404613\n",
      "Iteration 6503, loss = 0.03404309\n",
      "Iteration 6504, loss = 0.03404004\n",
      "Iteration 6505, loss = 0.03403698\n",
      "Iteration 6506, loss = 0.03403393\n",
      "Iteration 6507, loss = 0.03403087\n",
      "Iteration 6508, loss = 0.03402780\n",
      "Iteration 6509, loss = 0.03402474\n",
      "Iteration 6510, loss = 0.03402167\n",
      "Iteration 6511, loss = 0.03401859\n",
      "Iteration 6512, loss = 0.03401552\n",
      "Iteration 6513, loss = 0.03401244\n",
      "Iteration 6514, loss = 0.03400935\n",
      "Iteration 6515, loss = 0.03400627\n",
      "Iteration 6516, loss = 0.03400317\n",
      "Iteration 6517, loss = 0.03400008\n",
      "Iteration 6518, loss = 0.03399698\n",
      "Iteration 6519, loss = 0.03399388\n",
      "Iteration 6520, loss = 0.03399077\n",
      "Iteration 6521, loss = 0.03398766\n",
      "Iteration 6522, loss = 0.03398455\n",
      "Iteration 6523, loss = 0.03398144\n",
      "Iteration 6524, loss = 0.03397832\n",
      "Iteration 6525, loss = 0.03397519\n",
      "Iteration 6526, loss = 0.03397207\n",
      "Iteration 6527, loss = 0.03396894\n",
      "Iteration 6528, loss = 0.03396580\n",
      "Iteration 6529, loss = 0.03396266\n",
      "Iteration 6530, loss = 0.03395952\n",
      "Iteration 6531, loss = 0.03395638\n",
      "Iteration 6532, loss = 0.03395323\n",
      "Iteration 6533, loss = 0.03395007\n",
      "Iteration 6534, loss = 0.03394692\n",
      "Iteration 6535, loss = 0.03394376\n",
      "Iteration 6536, loss = 0.03394059\n",
      "Iteration 6537, loss = 0.03393743\n",
      "Iteration 6538, loss = 0.03393426\n",
      "Iteration 6539, loss = 0.03393108\n",
      "Iteration 6540, loss = 0.03392790\n",
      "Iteration 6541, loss = 0.03392472\n",
      "Iteration 6542, loss = 0.03392153\n",
      "Iteration 6543, loss = 0.03391834\n",
      "Iteration 6544, loss = 0.03391515\n",
      "Iteration 6545, loss = 0.03391195\n",
      "Iteration 6546, loss = 0.03390875\n",
      "Iteration 6547, loss = 0.03390554\n",
      "Iteration 6548, loss = 0.03390233\n",
      "Iteration 6549, loss = 0.03389912\n",
      "Iteration 6550, loss = 0.03389590\n",
      "Iteration 6551, loss = 0.03389268\n",
      "Iteration 6552, loss = 0.03388946\n",
      "Iteration 6553, loss = 0.03388623\n",
      "Iteration 6554, loss = 0.03388300\n",
      "Iteration 6555, loss = 0.03387976\n",
      "Iteration 6556, loss = 0.03387652\n",
      "Iteration 6557, loss = 0.03387328\n",
      "Iteration 6558, loss = 0.03387003\n",
      "Iteration 6559, loss = 0.03386678\n",
      "Iteration 6560, loss = 0.03386352\n",
      "Iteration 6561, loss = 0.03386026\n",
      "Iteration 6562, loss = 0.03385700\n",
      "Iteration 6563, loss = 0.03385373\n",
      "Iteration 6564, loss = 0.03385046\n",
      "Iteration 6565, loss = 0.03384718\n",
      "Iteration 6566, loss = 0.03384390\n",
      "Iteration 6567, loss = 0.03384062\n",
      "Iteration 6568, loss = 0.03383733\n",
      "Iteration 6569, loss = 0.03383404\n",
      "Iteration 6570, loss = 0.03383074\n",
      "Iteration 6571, loss = 0.03382744\n",
      "Iteration 6572, loss = 0.03382414\n",
      "Iteration 6573, loss = 0.03382083\n",
      "Iteration 6574, loss = 0.03381752\n",
      "Iteration 6575, loss = 0.03381420\n",
      "Iteration 6576, loss = 0.03381088\n",
      "Iteration 6577, loss = 0.03380756\n",
      "Iteration 6578, loss = 0.03380423\n",
      "Iteration 6579, loss = 0.03380089\n",
      "Iteration 6580, loss = 0.03379756\n",
      "Iteration 6581, loss = 0.03379422\n",
      "Iteration 6582, loss = 0.03379087\n",
      "Iteration 6583, loss = 0.03378752\n",
      "Iteration 6584, loss = 0.03378417\n",
      "Iteration 6585, loss = 0.03378081\n",
      "Iteration 6586, loss = 0.03377745\n",
      "Iteration 6587, loss = 0.03377408\n",
      "Iteration 6588, loss = 0.03377071\n",
      "Iteration 6589, loss = 0.03376734\n",
      "Iteration 6590, loss = 0.03376396\n",
      "Iteration 6591, loss = 0.03376058\n",
      "Iteration 6592, loss = 0.03375719\n",
      "Iteration 6593, loss = 0.03375380\n",
      "Iteration 6594, loss = 0.03375040\n",
      "Iteration 6595, loss = 0.03374700\n",
      "Iteration 6596, loss = 0.03374360\n",
      "Iteration 6597, loss = 0.03374019\n",
      "Iteration 6598, loss = 0.03373678\n",
      "Iteration 6599, loss = 0.03373336\n",
      "Iteration 6600, loss = 0.03372994\n",
      "Iteration 6601, loss = 0.03372652\n",
      "Iteration 6602, loss = 0.03372309\n",
      "Iteration 6603, loss = 0.03371965\n",
      "Iteration 6604, loss = 0.03371621\n",
      "Iteration 6605, loss = 0.03371277\n",
      "Iteration 6606, loss = 0.03370933\n",
      "Iteration 6607, loss = 0.03370587\n",
      "Iteration 6608, loss = 0.03370242\n",
      "Iteration 6609, loss = 0.03369896\n",
      "Iteration 6610, loss = 0.03369549\n",
      "Iteration 6611, loss = 0.03369203\n",
      "Iteration 6612, loss = 0.03368855\n",
      "Iteration 6613, loss = 0.03368508\n",
      "Iteration 6614, loss = 0.03368159\n",
      "Iteration 6615, loss = 0.03367811\n",
      "Iteration 6616, loss = 0.03367462\n",
      "Iteration 6617, loss = 0.03367112\n",
      "Iteration 6618, loss = 0.03366762\n",
      "Iteration 6619, loss = 0.03366412\n",
      "Iteration 6620, loss = 0.03366061\n",
      "Iteration 6621, loss = 0.03365710\n",
      "Iteration 6622, loss = 0.03365358\n",
      "Iteration 6623, loss = 0.03365006\n",
      "Iteration 6624, loss = 0.03364653\n",
      "Iteration 6625, loss = 0.03364300\n",
      "Iteration 6626, loss = 0.03363947\n",
      "Iteration 6627, loss = 0.03363593\n",
      "Iteration 6628, loss = 0.03363239\n",
      "Iteration 6629, loss = 0.03362884\n",
      "Iteration 6630, loss = 0.03362529\n",
      "Iteration 6631, loss = 0.03362173\n",
      "Iteration 6632, loss = 0.03361817\n",
      "Iteration 6633, loss = 0.03361460\n",
      "Iteration 6634, loss = 0.03361103\n",
      "Iteration 6635, loss = 0.03360746\n",
      "Iteration 6636, loss = 0.03360388\n",
      "Iteration 6637, loss = 0.03360029\n",
      "Iteration 6638, loss = 0.03359670\n",
      "Iteration 6639, loss = 0.03359311\n",
      "Iteration 6640, loss = 0.03358951\n",
      "Iteration 6641, loss = 0.03358591\n",
      "Iteration 6642, loss = 0.03358231\n",
      "Iteration 6643, loss = 0.03357870\n",
      "Iteration 6644, loss = 0.03357508\n",
      "Iteration 6645, loss = 0.03357146\n",
      "Iteration 6646, loss = 0.03356784\n",
      "Iteration 6647, loss = 0.03356421\n",
      "Iteration 6648, loss = 0.03356057\n",
      "Iteration 6649, loss = 0.03355693\n",
      "Iteration 6650, loss = 0.03355329\n",
      "Iteration 6651, loss = 0.03354964\n",
      "Iteration 6652, loss = 0.03354599\n",
      "Iteration 6653, loss = 0.03354234\n",
      "Iteration 6654, loss = 0.03353868\n",
      "Iteration 6655, loss = 0.03353501\n",
      "Iteration 6656, loss = 0.03353134\n",
      "Iteration 6657, loss = 0.03352767\n",
      "Iteration 6658, loss = 0.03352399\n",
      "Iteration 6659, loss = 0.03352030\n",
      "Iteration 6660, loss = 0.03351661\n",
      "Iteration 6661, loss = 0.03351292\n",
      "Iteration 6662, loss = 0.03350922\n",
      "Iteration 6663, loss = 0.03350552\n",
      "Iteration 6664, loss = 0.03350182\n",
      "Iteration 6665, loss = 0.03349810\n",
      "Iteration 6666, loss = 0.03349439\n",
      "Iteration 6667, loss = 0.03349067\n",
      "Iteration 6668, loss = 0.03348694\n",
      "Iteration 6669, loss = 0.03348321\n",
      "Iteration 6670, loss = 0.03347948\n",
      "Iteration 6671, loss = 0.03347574\n",
      "Iteration 6672, loss = 0.03347200\n",
      "Iteration 6673, loss = 0.03346825\n",
      "Iteration 6674, loss = 0.03346450\n",
      "Iteration 6675, loss = 0.03346074\n",
      "Iteration 6676, loss = 0.03345698\n",
      "Iteration 6677, loss = 0.03345321\n",
      "Iteration 6678, loss = 0.03344944\n",
      "Iteration 6679, loss = 0.03344567\n",
      "Iteration 6680, loss = 0.03344189\n",
      "Iteration 6681, loss = 0.03343810\n",
      "Iteration 6682, loss = 0.03343431\n",
      "Iteration 6683, loss = 0.03343052\n",
      "Iteration 6684, loss = 0.03342672\n",
      "Iteration 6685, loss = 0.03342292\n",
      "Iteration 6686, loss = 0.03341911\n",
      "Iteration 6687, loss = 0.03341530\n",
      "Iteration 6688, loss = 0.03341148\n",
      "Iteration 6689, loss = 0.03340766\n",
      "Iteration 6690, loss = 0.03340383\n",
      "Iteration 6691, loss = 0.03340000\n",
      "Iteration 6692, loss = 0.03339616\n",
      "Iteration 6693, loss = 0.03339232\n",
      "Iteration 6694, loss = 0.03338848\n",
      "Iteration 6695, loss = 0.03338463\n",
      "Iteration 6696, loss = 0.03338078\n",
      "Iteration 6697, loss = 0.03337692\n",
      "Iteration 6698, loss = 0.03337305\n",
      "Iteration 6699, loss = 0.03336919\n",
      "Iteration 6700, loss = 0.03336531\n",
      "Iteration 6701, loss = 0.03336144\n",
      "Iteration 6702, loss = 0.03335755\n",
      "Iteration 6703, loss = 0.03335367\n",
      "Iteration 6704, loss = 0.03334978\n",
      "Iteration 6705, loss = 0.03334588\n",
      "Iteration 6706, loss = 0.03334198\n",
      "Iteration 6707, loss = 0.03333807\n",
      "Iteration 6708, loss = 0.03333416\n",
      "Iteration 6709, loss = 0.03333025\n",
      "Iteration 6710, loss = 0.03332633\n",
      "Iteration 6711, loss = 0.03332241\n",
      "Iteration 6712, loss = 0.03331848\n",
      "Iteration 6713, loss = 0.03331455\n",
      "Iteration 6714, loss = 0.03331061\n",
      "Iteration 6715, loss = 0.03330667\n",
      "Iteration 6716, loss = 0.03330272\n",
      "Iteration 6717, loss = 0.03329877\n",
      "Iteration 6718, loss = 0.03329481\n",
      "Iteration 6719, loss = 0.03329085\n",
      "Iteration 6720, loss = 0.03328689\n",
      "Iteration 6721, loss = 0.03328292\n",
      "Iteration 6722, loss = 0.03327894\n",
      "Iteration 6723, loss = 0.03327496\n",
      "Iteration 6724, loss = 0.03327098\n",
      "Iteration 6725, loss = 0.03326699\n",
      "Iteration 6726, loss = 0.03326300\n",
      "Iteration 6727, loss = 0.03325900\n",
      "Iteration 6728, loss = 0.03325500\n",
      "Iteration 6729, loss = 0.03325099\n",
      "Iteration 6730, loss = 0.03324698\n",
      "Iteration 6731, loss = 0.03324296\n",
      "Iteration 6732, loss = 0.03323894\n",
      "Iteration 6733, loss = 0.03323491\n",
      "Iteration 6734, loss = 0.03323088\n",
      "Iteration 6735, loss = 0.03322685\n",
      "Iteration 6736, loss = 0.03322281\n",
      "Iteration 6737, loss = 0.03321876\n",
      "Iteration 6738, loss = 0.03321472\n",
      "Iteration 6739, loss = 0.03321066\n",
      "Iteration 6740, loss = 0.03320660\n",
      "Iteration 6741, loss = 0.03320254\n",
      "Iteration 6742, loss = 0.03319847\n",
      "Iteration 6743, loss = 0.03319440\n",
      "Iteration 6744, loss = 0.03319032\n",
      "Iteration 6745, loss = 0.03318624\n",
      "Iteration 6746, loss = 0.03318216\n",
      "Iteration 6747, loss = 0.03317807\n",
      "Iteration 6748, loss = 0.03317397\n",
      "Iteration 6749, loss = 0.03316987\n",
      "Iteration 6750, loss = 0.03316576\n",
      "Iteration 6751, loss = 0.03316166\n",
      "Iteration 6752, loss = 0.03315754\n",
      "Iteration 6753, loss = 0.03315342\n",
      "Iteration 6754, loss = 0.03314930\n",
      "Iteration 6755, loss = 0.03314517\n",
      "Iteration 6756, loss = 0.03314104\n",
      "Iteration 6757, loss = 0.03313690\n",
      "Iteration 6758, loss = 0.03313276\n",
      "Iteration 6759, loss = 0.03312861\n",
      "Iteration 6760, loss = 0.03312446\n",
      "Iteration 6761, loss = 0.03312030\n",
      "Iteration 6762, loss = 0.03311614\n",
      "Iteration 6763, loss = 0.03311198\n",
      "Iteration 6764, loss = 0.03310781\n",
      "Iteration 6765, loss = 0.03310363\n",
      "Iteration 6766, loss = 0.03309945\n",
      "Iteration 6767, loss = 0.03309527\n",
      "Iteration 6768, loss = 0.03309108\n",
      "Iteration 6769, loss = 0.03308689\n",
      "Iteration 6770, loss = 0.03308269\n",
      "Iteration 6771, loss = 0.03307848\n",
      "Iteration 6772, loss = 0.03307428\n",
      "Iteration 6773, loss = 0.03307006\n",
      "Iteration 6774, loss = 0.03306585\n",
      "Iteration 6775, loss = 0.03306163\n",
      "Iteration 6776, loss = 0.03305740\n",
      "Iteration 6777, loss = 0.03305317\n",
      "Iteration 6778, loss = 0.03304893\n",
      "Iteration 6779, loss = 0.03304469\n",
      "Iteration 6780, loss = 0.03304045\n",
      "Iteration 6781, loss = 0.03303620\n",
      "Iteration 6782, loss = 0.03303194\n",
      "Iteration 6783, loss = 0.03302768\n",
      "Iteration 6784, loss = 0.03302342\n",
      "Iteration 6785, loss = 0.03301915\n",
      "Iteration 6786, loss = 0.03301488\n",
      "Iteration 6787, loss = 0.03301060\n",
      "Iteration 6788, loss = 0.03300632\n",
      "Iteration 6789, loss = 0.03300203\n",
      "Iteration 6790, loss = 0.03299774\n",
      "Iteration 6791, loss = 0.03299344\n",
      "Iteration 6792, loss = 0.03298914\n",
      "Iteration 6793, loss = 0.03298483\n",
      "Iteration 6794, loss = 0.03298052\n",
      "Iteration 6795, loss = 0.03297620\n",
      "Iteration 6796, loss = 0.03297188\n",
      "Iteration 6797, loss = 0.03296756\n",
      "Iteration 6798, loss = 0.03296323\n",
      "Iteration 6799, loss = 0.03295889\n",
      "Iteration 6800, loss = 0.03295455\n",
      "Iteration 6801, loss = 0.03295021\n",
      "Iteration 6802, loss = 0.03294586\n",
      "Iteration 6803, loss = 0.03294151\n",
      "Iteration 6804, loss = 0.03293715\n",
      "Iteration 6805, loss = 0.03293278\n",
      "Iteration 6806, loss = 0.03292842\n",
      "Iteration 6807, loss = 0.03292404\n",
      "Iteration 6808, loss = 0.03291967\n",
      "Iteration 6809, loss = 0.03291528\n",
      "Iteration 6810, loss = 0.03291090\n",
      "Iteration 6811, loss = 0.03290650\n",
      "Iteration 6812, loss = 0.03290211\n",
      "Iteration 6813, loss = 0.03289771\n",
      "Iteration 6814, loss = 0.03289330\n",
      "Iteration 6815, loss = 0.03288889\n",
      "Iteration 6816, loss = 0.03288447\n",
      "Iteration 6817, loss = 0.03288005\n",
      "Iteration 6818, loss = 0.03287563\n",
      "Iteration 6819, loss = 0.03287120\n",
      "Iteration 6820, loss = 0.03286676\n",
      "Iteration 6821, loss = 0.03286232\n",
      "Iteration 6822, loss = 0.03285788\n",
      "Iteration 6823, loss = 0.03285343\n",
      "Iteration 6824, loss = 0.03284897\n",
      "Iteration 6825, loss = 0.03284451\n",
      "Iteration 6826, loss = 0.03284005\n",
      "Iteration 6827, loss = 0.03283558\n",
      "Iteration 6828, loss = 0.03283111\n",
      "Iteration 6829, loss = 0.03282663\n",
      "Iteration 6830, loss = 0.03282214\n",
      "Iteration 6831, loss = 0.03281766\n",
      "Iteration 6832, loss = 0.03281316\n",
      "Iteration 6833, loss = 0.03280867\n",
      "Iteration 6834, loss = 0.03280416\n",
      "Iteration 6835, loss = 0.03279965\n",
      "Iteration 6836, loss = 0.03279514\n",
      "Iteration 6837, loss = 0.03279062\n",
      "Iteration 6838, loss = 0.03278610\n",
      "Iteration 6839, loss = 0.03278158\n",
      "Iteration 6840, loss = 0.03277704\n",
      "Iteration 6841, loss = 0.03277251\n",
      "Iteration 6842, loss = 0.03276796\n",
      "Iteration 6843, loss = 0.03276342\n",
      "Iteration 6844, loss = 0.03275887\n",
      "Iteration 6845, loss = 0.03275431\n",
      "Iteration 6846, loss = 0.03274975\n",
      "Iteration 6847, loss = 0.03274518\n",
      "Iteration 6848, loss = 0.03274061\n",
      "Iteration 6849, loss = 0.03273604\n",
      "Iteration 6850, loss = 0.03273145\n",
      "Iteration 6851, loss = 0.03272687\n",
      "Iteration 6852, loss = 0.03272228\n",
      "Iteration 6853, loss = 0.03271768\n",
      "Iteration 6854, loss = 0.03271308\n",
      "Iteration 6855, loss = 0.03270848\n",
      "Iteration 6856, loss = 0.03270386\n",
      "Iteration 6857, loss = 0.03269925\n",
      "Iteration 6858, loss = 0.03269463\n",
      "Iteration 6859, loss = 0.03269000\n",
      "Iteration 6860, loss = 0.03268537\n",
      "Iteration 6861, loss = 0.03268074\n",
      "Iteration 6862, loss = 0.03267610\n",
      "Iteration 6863, loss = 0.03267145\n",
      "Iteration 6864, loss = 0.03266680\n",
      "Iteration 6865, loss = 0.03266215\n",
      "Iteration 6866, loss = 0.03265748\n",
      "Iteration 6867, loss = 0.03265282\n",
      "Iteration 6868, loss = 0.03264815\n",
      "Iteration 6869, loss = 0.03264347\n",
      "Iteration 6870, loss = 0.03263879\n",
      "Iteration 6871, loss = 0.03263411\n",
      "Iteration 6872, loss = 0.03262942\n",
      "Iteration 6873, loss = 0.03262472\n",
      "Iteration 6874, loss = 0.03262002\n",
      "Iteration 6875, loss = 0.03261532\n",
      "Iteration 6876, loss = 0.03261060\n",
      "Iteration 6877, loss = 0.03260589\n",
      "Iteration 6878, loss = 0.03260117\n",
      "Iteration 6879, loss = 0.03259644\n",
      "Iteration 6880, loss = 0.03259171\n",
      "Iteration 6881, loss = 0.03258698\n",
      "Iteration 6882, loss = 0.03258223\n",
      "Iteration 6883, loss = 0.03257749\n",
      "Iteration 6884, loss = 0.03257274\n",
      "Iteration 6885, loss = 0.03256798\n",
      "Iteration 6886, loss = 0.03256322\n",
      "Iteration 6887, loss = 0.03255845\n",
      "Iteration 6888, loss = 0.03255368\n",
      "Iteration 6889, loss = 0.03254890\n",
      "Iteration 6890, loss = 0.03254412\n",
      "Iteration 6891, loss = 0.03253934\n",
      "Iteration 6892, loss = 0.03253454\n",
      "Iteration 6893, loss = 0.03252975\n",
      "Iteration 6894, loss = 0.03252494\n",
      "Iteration 6895, loss = 0.03252014\n",
      "Iteration 6896, loss = 0.03251532\n",
      "Iteration 6897, loss = 0.03251051\n",
      "Iteration 6898, loss = 0.03250568\n",
      "Iteration 6899, loss = 0.03250086\n",
      "Iteration 6900, loss = 0.03249602\n",
      "Iteration 6901, loss = 0.03249118\n",
      "Iteration 6902, loss = 0.03248634\n",
      "Iteration 6903, loss = 0.03248149\n",
      "Iteration 6904, loss = 0.03247664\n",
      "Iteration 6905, loss = 0.03247178\n",
      "Iteration 6906, loss = 0.03246691\n",
      "Iteration 6907, loss = 0.03246204\n",
      "Iteration 6908, loss = 0.03245717\n",
      "Iteration 6909, loss = 0.03245229\n",
      "Iteration 6910, loss = 0.03244740\n",
      "Iteration 6911, loss = 0.03244251\n",
      "Iteration 6912, loss = 0.03243762\n",
      "Iteration 6913, loss = 0.03243272\n",
      "Iteration 6914, loss = 0.03242781\n",
      "Iteration 6915, loss = 0.03242290\n",
      "Iteration 6916, loss = 0.03241798\n",
      "Iteration 6917, loss = 0.03241306\n",
      "Iteration 6918, loss = 0.03240813\n",
      "Iteration 6919, loss = 0.03240320\n",
      "Iteration 6920, loss = 0.03239826\n",
      "Iteration 6921, loss = 0.03239332\n",
      "Iteration 6922, loss = 0.03238837\n",
      "Iteration 6923, loss = 0.03238341\n",
      "Iteration 6924, loss = 0.03237846\n",
      "Iteration 6925, loss = 0.03237349\n",
      "Iteration 6926, loss = 0.03236852\n",
      "Iteration 6927, loss = 0.03236355\n",
      "Iteration 6928, loss = 0.03235857\n",
      "Iteration 6929, loss = 0.03235358\n",
      "Iteration 6930, loss = 0.03234859\n",
      "Iteration 6931, loss = 0.03234359\n",
      "Iteration 6932, loss = 0.03233859\n",
      "Iteration 6933, loss = 0.03233358\n",
      "Iteration 6934, loss = 0.03232857\n",
      "Iteration 6935, loss = 0.03232355\n",
      "Iteration 6936, loss = 0.03231853\n",
      "Iteration 6937, loss = 0.03231350\n",
      "Iteration 6938, loss = 0.03230847\n",
      "Iteration 6939, loss = 0.03230343\n",
      "Iteration 6940, loss = 0.03229838\n",
      "Iteration 6941, loss = 0.03229333\n",
      "Iteration 6942, loss = 0.03228828\n",
      "Iteration 6943, loss = 0.03228322\n",
      "Iteration 6944, loss = 0.03227815\n",
      "Iteration 6945, loss = 0.03227308\n",
      "Iteration 6946, loss = 0.03226800\n",
      "Iteration 6947, loss = 0.03226292\n",
      "Iteration 6948, loss = 0.03225783\n",
      "Iteration 6949, loss = 0.03225274\n",
      "Iteration 6950, loss = 0.03224764\n",
      "Iteration 6951, loss = 0.03224253\n",
      "Iteration 6952, loss = 0.03223742\n",
      "Iteration 6953, loss = 0.03223231\n",
      "Iteration 6954, loss = 0.03222719\n",
      "Iteration 6955, loss = 0.03222206\n",
      "Iteration 6956, loss = 0.03221693\n",
      "Iteration 6957, loss = 0.03221179\n",
      "Iteration 6958, loss = 0.03220665\n",
      "Iteration 6959, loss = 0.03220150\n",
      "Iteration 6960, loss = 0.03219635\n",
      "Iteration 6961, loss = 0.03219119\n",
      "Iteration 6962, loss = 0.03218602\n",
      "Iteration 6963, loss = 0.03218085\n",
      "Iteration 6964, loss = 0.03217568\n",
      "Iteration 6965, loss = 0.03217049\n",
      "Iteration 6966, loss = 0.03216531\n",
      "Iteration 6967, loss = 0.03216012\n",
      "Iteration 6968, loss = 0.03215492\n",
      "Iteration 6969, loss = 0.03214971\n",
      "Iteration 6970, loss = 0.03214450\n",
      "Iteration 6971, loss = 0.03213929\n",
      "Iteration 6972, loss = 0.03213407\n",
      "Iteration 6973, loss = 0.03212884\n",
      "Iteration 6974, loss = 0.03212361\n",
      "Iteration 6975, loss = 0.03211838\n",
      "Iteration 6976, loss = 0.03211313\n",
      "Iteration 6977, loss = 0.03210788\n",
      "Iteration 6978, loss = 0.03210263\n",
      "Iteration 6979, loss = 0.03209737\n",
      "Iteration 6980, loss = 0.03209211\n",
      "Iteration 6981, loss = 0.03208684\n",
      "Iteration 6982, loss = 0.03208156\n",
      "Iteration 6983, loss = 0.03207628\n",
      "Iteration 6984, loss = 0.03207099\n",
      "Iteration 6985, loss = 0.03206570\n",
      "Iteration 6986, loss = 0.03206040\n",
      "Iteration 6987, loss = 0.03205510\n",
      "Iteration 6988, loss = 0.03204979\n",
      "Iteration 6989, loss = 0.03204447\n",
      "Iteration 6990, loss = 0.03203915\n",
      "Iteration 6991, loss = 0.03203382\n",
      "Iteration 6992, loss = 0.03202849\n",
      "Iteration 6993, loss = 0.03202315\n",
      "Iteration 6994, loss = 0.03201781\n",
      "Iteration 6995, loss = 0.03201246\n",
      "Iteration 6996, loss = 0.03200710\n",
      "Iteration 6997, loss = 0.03200174\n",
      "Iteration 6998, loss = 0.03199637\n",
      "Iteration 6999, loss = 0.03199100\n",
      "Iteration 7000, loss = 0.03198562\n",
      "Iteration 7001, loss = 0.03198024\n",
      "Iteration 7002, loss = 0.03197485\n",
      "Iteration 7003, loss = 0.03196946\n",
      "Iteration 7004, loss = 0.03196406\n",
      "Iteration 7005, loss = 0.03195865\n",
      "Iteration 7006, loss = 0.03195324\n",
      "Iteration 7007, loss = 0.03194782\n",
      "Iteration 7008, loss = 0.03194239\n",
      "Iteration 7009, loss = 0.03193697\n",
      "Iteration 7010, loss = 0.03193153\n",
      "Iteration 7011, loss = 0.03192609\n",
      "Iteration 7012, loss = 0.03192064\n",
      "Iteration 7013, loss = 0.03191519\n",
      "Iteration 7014, loss = 0.03190973\n",
      "Iteration 7015, loss = 0.03190427\n",
      "Iteration 7016, loss = 0.03189880\n",
      "Iteration 7017, loss = 0.03189332\n",
      "Iteration 7018, loss = 0.03188784\n",
      "Iteration 7019, loss = 0.03188235\n",
      "Iteration 7020, loss = 0.03187686\n",
      "Iteration 7021, loss = 0.03187136\n",
      "Iteration 7022, loss = 0.03186586\n",
      "Iteration 7023, loss = 0.03186035\n",
      "Iteration 7024, loss = 0.03185483\n",
      "Iteration 7025, loss = 0.03184931\n",
      "Iteration 7026, loss = 0.03184378\n",
      "Iteration 7027, loss = 0.03183825\n",
      "Iteration 7028, loss = 0.03183271\n",
      "Iteration 7029, loss = 0.03182717\n",
      "Iteration 7030, loss = 0.03182162\n",
      "Iteration 7031, loss = 0.03181606\n",
      "Iteration 7032, loss = 0.03181050\n",
      "Iteration 7033, loss = 0.03180493\n",
      "Iteration 7034, loss = 0.03179935\n",
      "Iteration 7035, loss = 0.03179377\n",
      "Iteration 7036, loss = 0.03178819\n",
      "Iteration 7037, loss = 0.03178260\n",
      "Iteration 7038, loss = 0.03177700\n",
      "Iteration 7039, loss = 0.03177140\n",
      "Iteration 7040, loss = 0.03176579\n",
      "Iteration 7041, loss = 0.03176017\n",
      "Iteration 7042, loss = 0.03175455\n",
      "Iteration 7043, loss = 0.03174892\n",
      "Iteration 7044, loss = 0.03174329\n",
      "Iteration 7045, loss = 0.03173765\n",
      "Iteration 7046, loss = 0.03173201\n",
      "Iteration 7047, loss = 0.03172636\n",
      "Iteration 7048, loss = 0.03172070\n",
      "Iteration 7049, loss = 0.03171504\n",
      "Iteration 7050, loss = 0.03170937\n",
      "Iteration 7051, loss = 0.03170370\n",
      "Iteration 7052, loss = 0.03169802\n",
      "Iteration 7053, loss = 0.03169233\n",
      "Iteration 7054, loss = 0.03168664\n",
      "Iteration 7055, loss = 0.03168095\n",
      "Iteration 7056, loss = 0.03167524\n",
      "Iteration 7057, loss = 0.03166953\n",
      "Iteration 7058, loss = 0.03166382\n",
      "Iteration 7059, loss = 0.03165810\n",
      "Iteration 7060, loss = 0.03165237\n",
      "Iteration 7061, loss = 0.03164664\n",
      "Iteration 7062, loss = 0.03164090\n",
      "Iteration 7063, loss = 0.03163516\n",
      "Iteration 7064, loss = 0.03162940\n",
      "Iteration 7065, loss = 0.03162365\n",
      "Iteration 7066, loss = 0.03161789\n",
      "Iteration 7067, loss = 0.03161212\n",
      "Iteration 7068, loss = 0.03160634\n",
      "Iteration 7069, loss = 0.03160056\n",
      "Iteration 7070, loss = 0.03159478\n",
      "Iteration 7071, loss = 0.03158899\n",
      "Iteration 7072, loss = 0.03158319\n",
      "Iteration 7073, loss = 0.03157739\n",
      "Iteration 7074, loss = 0.03157158\n",
      "Iteration 7075, loss = 0.03156576\n",
      "Iteration 7076, loss = 0.03155994\n",
      "Iteration 7077, loss = 0.03155411\n",
      "Iteration 7078, loss = 0.03154828\n",
      "Iteration 7079, loss = 0.03154244\n",
      "Iteration 7080, loss = 0.03153659\n",
      "Iteration 7081, loss = 0.03153074\n",
      "Iteration 7082, loss = 0.03152489\n",
      "Iteration 7083, loss = 0.03151902\n",
      "Iteration 7084, loss = 0.03151315\n",
      "Iteration 7085, loss = 0.03150728\n",
      "Iteration 7086, loss = 0.03150140\n",
      "Iteration 7087, loss = 0.03149551\n",
      "Iteration 7088, loss = 0.03148962\n",
      "Iteration 7089, loss = 0.03148372\n",
      "Iteration 7090, loss = 0.03147781\n",
      "Iteration 7091, loss = 0.03147190\n",
      "Iteration 7092, loss = 0.03146599\n",
      "Iteration 7093, loss = 0.03146006\n",
      "Iteration 7094, loss = 0.03145414\n",
      "Iteration 7095, loss = 0.03144820\n",
      "Iteration 7096, loss = 0.03144226\n",
      "Iteration 7097, loss = 0.03143632\n",
      "Iteration 7098, loss = 0.03143036\n",
      "Iteration 7099, loss = 0.03142440\n",
      "Iteration 7100, loss = 0.03141844\n",
      "Iteration 7101, loss = 0.03141247\n",
      "Iteration 7102, loss = 0.03140649\n",
      "Iteration 7103, loss = 0.03140051\n",
      "Iteration 7104, loss = 0.03139452\n",
      "Iteration 7105, loss = 0.03138853\n",
      "Iteration 7106, loss = 0.03138253\n",
      "Iteration 7107, loss = 0.03137652\n",
      "Iteration 7108, loss = 0.03137051\n",
      "Iteration 7109, loss = 0.03136449\n",
      "Iteration 7110, loss = 0.03135847\n",
      "Iteration 7111, loss = 0.03135244\n",
      "Iteration 7112, loss = 0.03134640\n",
      "Iteration 7113, loss = 0.03134036\n",
      "Iteration 7114, loss = 0.03133431\n",
      "Iteration 7115, loss = 0.03132826\n",
      "Iteration 7116, loss = 0.03132220\n",
      "Iteration 7117, loss = 0.03131613\n",
      "Iteration 7118, loss = 0.03131006\n",
      "Iteration 7119, loss = 0.03130398\n",
      "Iteration 7120, loss = 0.03129790\n",
      "Iteration 7121, loss = 0.03129181\n",
      "Iteration 7122, loss = 0.03128572\n",
      "Iteration 7123, loss = 0.03127961\n",
      "Iteration 7124, loss = 0.03127351\n",
      "Iteration 7125, loss = 0.03126739\n",
      "Iteration 7126, loss = 0.03126127\n",
      "Iteration 7127, loss = 0.03125515\n",
      "Iteration 7128, loss = 0.03124902\n",
      "Iteration 7129, loss = 0.03124288\n",
      "Iteration 7130, loss = 0.03123674\n",
      "Iteration 7131, loss = 0.03123059\n",
      "Iteration 7132, loss = 0.03122443\n",
      "Iteration 7133, loss = 0.03121827\n",
      "Iteration 7134, loss = 0.03121210\n",
      "Iteration 7135, loss = 0.03120593\n",
      "Iteration 7136, loss = 0.03119975\n",
      "Iteration 7137, loss = 0.03119357\n",
      "Iteration 7138, loss = 0.03118737\n",
      "Iteration 7139, loss = 0.03118118\n",
      "Iteration 7140, loss = 0.03117497\n",
      "Iteration 7141, loss = 0.03116877\n",
      "Iteration 7142, loss = 0.03116255\n",
      "Iteration 7143, loss = 0.03115633\n",
      "Iteration 7144, loss = 0.03115010\n",
      "Iteration 7145, loss = 0.03114387\n",
      "Iteration 7146, loss = 0.03113763\n",
      "Iteration 7147, loss = 0.03113139\n",
      "Iteration 7148, loss = 0.03112514\n",
      "Iteration 7149, loss = 0.03111888\n",
      "Iteration 7150, loss = 0.03111262\n",
      "Iteration 7151, loss = 0.03110635\n",
      "Iteration 7152, loss = 0.03110007\n",
      "Iteration 7153, loss = 0.03109379\n",
      "Iteration 7154, loss = 0.03108751\n",
      "Iteration 7155, loss = 0.03108122\n",
      "Iteration 7156, loss = 0.03107492\n",
      "Iteration 7157, loss = 0.03106861\n",
      "Iteration 7158, loss = 0.03106230\n",
      "Iteration 7159, loss = 0.03105599\n",
      "Iteration 7160, loss = 0.03104966\n",
      "Iteration 7161, loss = 0.03104334\n",
      "Iteration 7162, loss = 0.03103700\n",
      "Iteration 7163, loss = 0.03103066\n",
      "Iteration 7164, loss = 0.03102432\n",
      "Iteration 7165, loss = 0.03101797\n",
      "Iteration 7166, loss = 0.03101161\n",
      "Iteration 7167, loss = 0.03100524\n",
      "Iteration 7168, loss = 0.03099888\n",
      "Iteration 7169, loss = 0.03099250\n",
      "Iteration 7170, loss = 0.03098612\n",
      "Iteration 7171, loss = 0.03097973\n",
      "Iteration 7172, loss = 0.03097334\n",
      "Iteration 7173, loss = 0.03096694\n",
      "Iteration 7174, loss = 0.03096053\n",
      "Iteration 7175, loss = 0.03095412\n",
      "Iteration 7176, loss = 0.03094771\n",
      "Iteration 7177, loss = 0.03094128\n",
      "Iteration 7178, loss = 0.03093486\n",
      "Iteration 7179, loss = 0.03092842\n",
      "Iteration 7180, loss = 0.03092198\n",
      "Iteration 7181, loss = 0.03091553\n",
      "Iteration 7182, loss = 0.03090908\n",
      "Iteration 7183, loss = 0.03090262\n",
      "Iteration 7184, loss = 0.03089616\n",
      "Iteration 7185, loss = 0.03088969\n",
      "Iteration 7186, loss = 0.03088321\n",
      "Iteration 7187, loss = 0.03087673\n",
      "Iteration 7188, loss = 0.03087024\n",
      "Iteration 7189, loss = 0.03086375\n",
      "Iteration 7190, loss = 0.03085725\n",
      "Iteration 7191, loss = 0.03085075\n",
      "Iteration 7192, loss = 0.03084424\n",
      "Iteration 7193, loss = 0.03083772\n",
      "Iteration 7194, loss = 0.03083120\n",
      "Iteration 7195, loss = 0.03082467\n",
      "Iteration 7196, loss = 0.03081813\n",
      "Iteration 7197, loss = 0.03081159\n",
      "Iteration 7198, loss = 0.03080505\n",
      "Iteration 7199, loss = 0.03079849\n",
      "Iteration 7200, loss = 0.03079194\n",
      "Iteration 7201, loss = 0.03078537\n",
      "Iteration 7202, loss = 0.03077880\n",
      "Iteration 7203, loss = 0.03077223\n",
      "Iteration 7204, loss = 0.03076565\n",
      "Iteration 7205, loss = 0.03075906\n",
      "Iteration 7206, loss = 0.03075247\n",
      "Iteration 7207, loss = 0.03074587\n",
      "Iteration 7208, loss = 0.03073926\n",
      "Iteration 7209, loss = 0.03073265\n",
      "Iteration 7210, loss = 0.03072604\n",
      "Iteration 7211, loss = 0.03071941\n",
      "Iteration 7212, loss = 0.03071279\n",
      "Iteration 7213, loss = 0.03070615\n",
      "Iteration 7214, loss = 0.03069951\n",
      "Iteration 7215, loss = 0.03069287\n",
      "Iteration 7216, loss = 0.03068621\n",
      "Iteration 7217, loss = 0.03067956\n",
      "Iteration 7218, loss = 0.03067289\n",
      "Iteration 7219, loss = 0.03066623\n",
      "Iteration 7220, loss = 0.03065955\n",
      "Iteration 7221, loss = 0.03065287\n",
      "Iteration 7222, loss = 0.03064619\n",
      "Iteration 7223, loss = 0.03063949\n",
      "Iteration 7224, loss = 0.03063280\n",
      "Iteration 7225, loss = 0.03062609\n",
      "Iteration 7226, loss = 0.03061938\n",
      "Iteration 7227, loss = 0.03061267\n",
      "Iteration 7228, loss = 0.03060595\n",
      "Iteration 7229, loss = 0.03059922\n",
      "Iteration 7230, loss = 0.03059249\n",
      "Iteration 7231, loss = 0.03058575\n",
      "Iteration 7232, loss = 0.03057901\n",
      "Iteration 7233, loss = 0.03057226\n",
      "Iteration 7234, loss = 0.03056550\n",
      "Iteration 7235, loss = 0.03055874\n",
      "Iteration 7236, loss = 0.03055197\n",
      "Iteration 7237, loss = 0.03054520\n",
      "Iteration 7238, loss = 0.03053842\n",
      "Iteration 7239, loss = 0.03053164\n",
      "Iteration 7240, loss = 0.03052485\n",
      "Iteration 7241, loss = 0.03051805\n",
      "Iteration 7242, loss = 0.03051125\n",
      "Iteration 7243, loss = 0.03050445\n",
      "Iteration 7244, loss = 0.03049763\n",
      "Iteration 7245, loss = 0.03049081\n",
      "Iteration 7246, loss = 0.03048399\n",
      "Iteration 7247, loss = 0.03047716\n",
      "Iteration 7248, loss = 0.03047033\n",
      "Iteration 7249, loss = 0.03046348\n",
      "Iteration 7250, loss = 0.03045664\n",
      "Iteration 7251, loss = 0.03044978\n",
      "Iteration 7252, loss = 0.03044293\n",
      "Iteration 7253, loss = 0.03043606\n",
      "Iteration 7254, loss = 0.03042919\n",
      "Iteration 7255, loss = 0.03042232\n",
      "Iteration 7256, loss = 0.03041544\n",
      "Iteration 7257, loss = 0.03040855\n",
      "Iteration 7258, loss = 0.03040166\n",
      "Iteration 7259, loss = 0.03039476\n",
      "Iteration 7260, loss = 0.03038785\n",
      "Iteration 7261, loss = 0.03038095\n",
      "Iteration 7262, loss = 0.03037403\n",
      "Iteration 7263, loss = 0.03036711\n",
      "Iteration 7264, loss = 0.03036018\n",
      "Iteration 7265, loss = 0.03035325\n",
      "Iteration 7266, loss = 0.03034631\n",
      "Iteration 7267, loss = 0.03033937\n",
      "Iteration 7268, loss = 0.03033242\n",
      "Iteration 7269, loss = 0.03032547\n",
      "Iteration 7270, loss = 0.03031851\n",
      "Iteration 7271, loss = 0.03031154\n",
      "Iteration 7272, loss = 0.03030457\n",
      "Iteration 7273, loss = 0.03029759\n",
      "Iteration 7274, loss = 0.03029061\n",
      "Iteration 7275, loss = 0.03028362\n",
      "Iteration 7276, loss = 0.03027663\n",
      "Iteration 7277, loss = 0.03026963\n",
      "Iteration 7278, loss = 0.03026262\n",
      "Iteration 7279, loss = 0.03025561\n",
      "Iteration 7280, loss = 0.03024860\n",
      "Iteration 7281, loss = 0.03024158\n",
      "Iteration 7282, loss = 0.03023455\n",
      "Iteration 7283, loss = 0.03022751\n",
      "Iteration 7284, loss = 0.03022048\n",
      "Iteration 7285, loss = 0.03021343\n",
      "Iteration 7286, loss = 0.03020638\n",
      "Iteration 7287, loss = 0.03019933\n",
      "Iteration 7288, loss = 0.03019227\n",
      "Iteration 7289, loss = 0.03018520\n",
      "Iteration 7290, loss = 0.03017813\n",
      "Iteration 7291, loss = 0.03017105\n",
      "Iteration 7292, loss = 0.03016397\n",
      "Iteration 7293, loss = 0.03015688\n",
      "Iteration 7294, loss = 0.03014979\n",
      "Iteration 7295, loss = 0.03014269\n",
      "Iteration 7296, loss = 0.03013558\n",
      "Iteration 7297, loss = 0.03012847\n",
      "Iteration 7298, loss = 0.03012135\n",
      "Iteration 7299, loss = 0.03011423\n",
      "Iteration 7300, loss = 0.03010711\n",
      "Iteration 7301, loss = 0.03009997\n",
      "Iteration 7302, loss = 0.03009283\n",
      "Iteration 7303, loss = 0.03008569\n",
      "Iteration 7304, loss = 0.03007854\n",
      "Iteration 7305, loss = 0.03007139\n",
      "Iteration 7306, loss = 0.03006423\n",
      "Iteration 7307, loss = 0.03005706\n",
      "Iteration 7308, loss = 0.03004989\n",
      "Iteration 7309, loss = 0.03004271\n",
      "Iteration 7310, loss = 0.03003553\n",
      "Iteration 7311, loss = 0.03002834\n",
      "Iteration 7312, loss = 0.03002115\n",
      "Iteration 7313, loss = 0.03001395\n",
      "Iteration 7314, loss = 0.03000675\n",
      "Iteration 7315, loss = 0.02999954\n",
      "Iteration 7316, loss = 0.02999233\n",
      "Iteration 7317, loss = 0.02998511\n",
      "Iteration 7318, loss = 0.02997788\n",
      "Iteration 7319, loss = 0.02997065\n",
      "Iteration 7320, loss = 0.02996341\n",
      "Iteration 7321, loss = 0.02995617\n",
      "Iteration 7322, loss = 0.02994892\n",
      "Iteration 7323, loss = 0.02994167\n",
      "Iteration 7324, loss = 0.02993441\n",
      "Iteration 7325, loss = 0.02992715\n",
      "Iteration 7326, loss = 0.02991988\n",
      "Iteration 7327, loss = 0.02991261\n",
      "Iteration 7328, loss = 0.02990533\n",
      "Iteration 7329, loss = 0.02989804\n",
      "Iteration 7330, loss = 0.02989075\n",
      "Iteration 7331, loss = 0.02988345\n",
      "Iteration 7332, loss = 0.02987615\n",
      "Iteration 7333, loss = 0.02986885\n",
      "Iteration 7334, loss = 0.02986153\n",
      "Iteration 7335, loss = 0.02985422\n",
      "Iteration 7336, loss = 0.02984689\n",
      "Iteration 7337, loss = 0.02983957\n",
      "Iteration 7338, loss = 0.02983223\n",
      "Iteration 7339, loss = 0.02982489\n",
      "Iteration 7340, loss = 0.02981755\n",
      "Iteration 7341, loss = 0.02981020\n",
      "Iteration 7342, loss = 0.02980285\n",
      "Iteration 7343, loss = 0.02979549\n",
      "Iteration 7344, loss = 0.02978812\n",
      "Iteration 7345, loss = 0.02978075\n",
      "Iteration 7346, loss = 0.02977337\n",
      "Iteration 7347, loss = 0.02976599\n",
      "Iteration 7348, loss = 0.02975861\n",
      "Iteration 7349, loss = 0.02975121\n",
      "Iteration 7350, loss = 0.02974382\n",
      "Iteration 7351, loss = 0.02973641\n",
      "Iteration 7352, loss = 0.02972901\n",
      "Iteration 7353, loss = 0.02972159\n",
      "Iteration 7354, loss = 0.02971417\n",
      "Iteration 7355, loss = 0.02970675\n",
      "Iteration 7356, loss = 0.02969932\n",
      "Iteration 7357, loss = 0.02969189\n",
      "Iteration 7358, loss = 0.02968445\n",
      "Iteration 7359, loss = 0.02967700\n",
      "Iteration 7360, loss = 0.02966955\n",
      "Iteration 7361, loss = 0.02966210\n",
      "Iteration 7362, loss = 0.02965464\n",
      "Iteration 7363, loss = 0.02964717\n",
      "Iteration 7364, loss = 0.02963970\n",
      "Iteration 7365, loss = 0.02963222\n",
      "Iteration 7366, loss = 0.02962474\n",
      "Iteration 7367, loss = 0.02961726\n",
      "Iteration 7368, loss = 0.02960976\n",
      "Iteration 7369, loss = 0.02960227\n",
      "Iteration 7370, loss = 0.02959476\n",
      "Iteration 7371, loss = 0.02958726\n",
      "Iteration 7372, loss = 0.02957974\n",
      "Iteration 7373, loss = 0.02957223\n",
      "Iteration 7374, loss = 0.02956470\n",
      "Iteration 7375, loss = 0.02955717\n",
      "Iteration 7376, loss = 0.02954964\n",
      "Iteration 7377, loss = 0.02954210\n",
      "Iteration 7378, loss = 0.02953456\n",
      "Iteration 7379, loss = 0.02952701\n",
      "Iteration 7380, loss = 0.02951946\n",
      "Iteration 7381, loss = 0.02951190\n",
      "Iteration 7382, loss = 0.02950433\n",
      "Iteration 7383, loss = 0.02949676\n",
      "Iteration 7384, loss = 0.02948919\n",
      "Iteration 7385, loss = 0.02948161\n",
      "Iteration 7386, loss = 0.02947402\n",
      "Iteration 7387, loss = 0.02946643\n",
      "Iteration 7388, loss = 0.02945884\n",
      "Iteration 7389, loss = 0.02945124\n",
      "Iteration 7390, loss = 0.02944363\n",
      "Iteration 7391, loss = 0.02943602\n",
      "Iteration 7392, loss = 0.02942840\n",
      "Iteration 7393, loss = 0.02942078\n",
      "Iteration 7394, loss = 0.02941316\n",
      "Iteration 7395, loss = 0.02940553\n",
      "Iteration 7396, loss = 0.02939789\n",
      "Iteration 7397, loss = 0.02939025\n",
      "Iteration 7398, loss = 0.02938260\n",
      "Iteration 7399, loss = 0.02937495\n",
      "Iteration 7400, loss = 0.02936729\n",
      "Iteration 7401, loss = 0.02935963\n",
      "Iteration 7402, loss = 0.02935197\n",
      "Iteration 7403, loss = 0.02934429\n",
      "Iteration 7404, loss = 0.02933662\n",
      "Iteration 7405, loss = 0.02932894\n",
      "Iteration 7406, loss = 0.02932125\n",
      "Iteration 7407, loss = 0.02931356\n",
      "Iteration 7408, loss = 0.02930586\n",
      "Iteration 7409, loss = 0.02929816\n",
      "Iteration 7410, loss = 0.02929045\n",
      "Iteration 7411, loss = 0.02928274\n",
      "Iteration 7412, loss = 0.02927502\n",
      "Iteration 7413, loss = 0.02926730\n",
      "Iteration 7414, loss = 0.02925957\n",
      "Iteration 7415, loss = 0.02925184\n",
      "Iteration 7416, loss = 0.02924410\n",
      "Iteration 7417, loss = 0.02923636\n",
      "Iteration 7418, loss = 0.02922862\n",
      "Iteration 7419, loss = 0.02922086\n",
      "Iteration 7420, loss = 0.02921311\n",
      "Iteration 7421, loss = 0.02920534\n",
      "Iteration 7422, loss = 0.02919758\n",
      "Iteration 7423, loss = 0.02918981\n",
      "Iteration 7424, loss = 0.02918203\n",
      "Iteration 7425, loss = 0.02917425\n",
      "Iteration 7426, loss = 0.02916646\n",
      "Iteration 7427, loss = 0.02915867\n",
      "Iteration 7428, loss = 0.02915087\n",
      "Iteration 7429, loss = 0.02914307\n",
      "Iteration 7430, loss = 0.02913526\n",
      "Iteration 7431, loss = 0.02912745\n",
      "Iteration 7432, loss = 0.02911964\n",
      "Iteration 7433, loss = 0.02911181\n",
      "Iteration 7434, loss = 0.02910399\n",
      "Iteration 7435, loss = 0.02909616\n",
      "Iteration 7436, loss = 0.02908832\n",
      "Iteration 7437, loss = 0.02908048\n",
      "Iteration 7438, loss = 0.02907263\n",
      "Iteration 7439, loss = 0.02906478\n",
      "Iteration 7440, loss = 0.02905693\n",
      "Iteration 7441, loss = 0.02904907\n",
      "Iteration 7442, loss = 0.02904120\n",
      "Iteration 7443, loss = 0.02903333\n",
      "Iteration 7444, loss = 0.02902546\n",
      "Iteration 7445, loss = 0.02901758\n",
      "Iteration 7446, loss = 0.02900969\n",
      "Iteration 7447, loss = 0.02900180\n",
      "Iteration 7448, loss = 0.02899391\n",
      "Iteration 7449, loss = 0.02898601\n",
      "Iteration 7450, loss = 0.02897811\n",
      "Iteration 7451, loss = 0.02897020\n",
      "Iteration 7452, loss = 0.02896228\n",
      "Iteration 7453, loss = 0.02895437\n",
      "Iteration 7454, loss = 0.02894644\n",
      "Iteration 7455, loss = 0.02893851\n",
      "Iteration 7456, loss = 0.02893058\n",
      "Iteration 7457, loss = 0.02892264\n",
      "Iteration 7458, loss = 0.02891470\n",
      "Iteration 7459, loss = 0.02890675\n",
      "Iteration 7460, loss = 0.02889880\n",
      "Iteration 7461, loss = 0.02889085\n",
      "Iteration 7462, loss = 0.02888288\n",
      "Iteration 7463, loss = 0.02887492\n",
      "Iteration 7464, loss = 0.02886695\n",
      "Iteration 7465, loss = 0.02885897\n",
      "Iteration 7466, loss = 0.02885099\n",
      "Iteration 7467, loss = 0.02884300\n",
      "Iteration 7468, loss = 0.02883501\n",
      "Iteration 7469, loss = 0.02882702\n",
      "Iteration 7470, loss = 0.02881902\n",
      "Iteration 7471, loss = 0.02881102\n",
      "Iteration 7472, loss = 0.02880301\n",
      "Iteration 7473, loss = 0.02879499\n",
      "Iteration 7474, loss = 0.02878698\n",
      "Iteration 7475, loss = 0.02877895\n",
      "Iteration 7476, loss = 0.02877093\n",
      "Iteration 7477, loss = 0.02876289\n",
      "Iteration 7478, loss = 0.02875486\n",
      "Iteration 7479, loss = 0.02874681\n",
      "Iteration 7480, loss = 0.02873877\n",
      "Iteration 7481, loss = 0.02873072\n",
      "Iteration 7482, loss = 0.02872266\n",
      "Iteration 7483, loss = 0.02871460\n",
      "Iteration 7484, loss = 0.02870654\n",
      "Iteration 7485, loss = 0.02869847\n",
      "Iteration 7486, loss = 0.02869039\n",
      "Iteration 7487, loss = 0.02868231\n",
      "Iteration 7488, loss = 0.02867423\n",
      "Iteration 7489, loss = 0.02866614\n",
      "Iteration 7490, loss = 0.02865805\n",
      "Iteration 7491, loss = 0.02864995\n",
      "Iteration 7492, loss = 0.02864185\n",
      "Iteration 7493, loss = 0.02863374\n",
      "Iteration 7494, loss = 0.02862563\n",
      "Iteration 7495, loss = 0.02861752\n",
      "Iteration 7496, loss = 0.02860939\n",
      "Iteration 7497, loss = 0.02860127\n",
      "Iteration 7498, loss = 0.02859314\n",
      "Iteration 7499, loss = 0.02858501\n",
      "Iteration 7500, loss = 0.02857687\n",
      "Iteration 7501, loss = 0.02856872\n",
      "Iteration 7502, loss = 0.02856058\n",
      "Iteration 7503, loss = 0.02855242\n",
      "Iteration 7504, loss = 0.02854427\n",
      "Iteration 7505, loss = 0.02853611\n",
      "Iteration 7506, loss = 0.02852794\n",
      "Iteration 7507, loss = 0.02851977\n",
      "Iteration 7508, loss = 0.02851160\n",
      "Iteration 7509, loss = 0.02850342\n",
      "Iteration 7510, loss = 0.02849523\n",
      "Iteration 7511, loss = 0.02848704\n",
      "Iteration 7512, loss = 0.02847885\n",
      "Iteration 7513, loss = 0.02847065\n",
      "Iteration 7514, loss = 0.02846245\n",
      "Iteration 7515, loss = 0.02845425\n",
      "Iteration 7516, loss = 0.02844603\n",
      "Iteration 7517, loss = 0.02843782\n",
      "Iteration 7518, loss = 0.02842960\n",
      "Iteration 7519, loss = 0.02842138\n",
      "Iteration 7520, loss = 0.02841315\n",
      "Iteration 7521, loss = 0.02840491\n",
      "Iteration 7522, loss = 0.02839668\n",
      "Iteration 7523, loss = 0.02838843\n",
      "Iteration 7524, loss = 0.02838019\n",
      "Iteration 7525, loss = 0.02837194\n",
      "Iteration 7526, loss = 0.02836368\n",
      "Iteration 7527, loss = 0.02835542\n",
      "Iteration 7528, loss = 0.02834716\n",
      "Iteration 7529, loss = 0.02833889\n",
      "Iteration 7530, loss = 0.02833062\n",
      "Iteration 7531, loss = 0.02832234\n",
      "Iteration 7532, loss = 0.02831406\n",
      "Iteration 7533, loss = 0.02830577\n",
      "Iteration 7534, loss = 0.02829748\n",
      "Iteration 7535, loss = 0.02828919\n",
      "Iteration 7536, loss = 0.02828089\n",
      "Iteration 7537, loss = 0.02827259\n",
      "Iteration 7538, loss = 0.02826428\n",
      "Iteration 7539, loss = 0.02825597\n",
      "Iteration 7540, loss = 0.02824765\n",
      "Iteration 7541, loss = 0.02823933\n",
      "Iteration 7542, loss = 0.02823101\n",
      "Iteration 7543, loss = 0.02822268\n",
      "Iteration 7544, loss = 0.02821434\n",
      "Iteration 7545, loss = 0.02820601\n",
      "Iteration 7546, loss = 0.02819766\n",
      "Iteration 7547, loss = 0.02818932\n",
      "Iteration 7548, loss = 0.02818097\n",
      "Iteration 7549, loss = 0.02817261\n",
      "Iteration 7550, loss = 0.02816425\n",
      "Iteration 7551, loss = 0.02815589\n",
      "Iteration 7552, loss = 0.02814752\n",
      "Iteration 7553, loss = 0.02813915\n",
      "Iteration 7554, loss = 0.02813077\n",
      "Iteration 7555, loss = 0.02812239\n",
      "Iteration 7556, loss = 0.02811401\n",
      "Iteration 7557, loss = 0.02810562\n",
      "Iteration 7558, loss = 0.02809723\n",
      "Iteration 7559, loss = 0.02808883\n",
      "Iteration 7560, loss = 0.02808043\n",
      "Iteration 7561, loss = 0.02807202\n",
      "Iteration 7562, loss = 0.02806361\n",
      "Iteration 7563, loss = 0.02805520\n",
      "Iteration 7564, loss = 0.02804678\n",
      "Iteration 7565, loss = 0.02803836\n",
      "Iteration 7566, loss = 0.02802993\n",
      "Iteration 7567, loss = 0.02802150\n",
      "Iteration 7568, loss = 0.02801307\n",
      "Iteration 7569, loss = 0.02800463\n",
      "Iteration 7570, loss = 0.02799619\n",
      "Iteration 7571, loss = 0.02798774\n",
      "Iteration 7572, loss = 0.02797929\n",
      "Iteration 7573, loss = 0.02797083\n",
      "Iteration 7574, loss = 0.02796237\n",
      "Iteration 7575, loss = 0.02795391\n",
      "Iteration 7576, loss = 0.02794544\n",
      "Iteration 7577, loss = 0.02793697\n",
      "Iteration 7578, loss = 0.02792849\n",
      "Iteration 7579, loss = 0.02792001\n",
      "Iteration 7580, loss = 0.02791153\n",
      "Iteration 7581, loss = 0.02790304\n",
      "Iteration 7582, loss = 0.02789455\n",
      "Iteration 7583, loss = 0.02788605\n",
      "Iteration 7584, loss = 0.02787755\n",
      "Iteration 7585, loss = 0.02786905\n",
      "Iteration 7586, loss = 0.02786054\n",
      "Iteration 7587, loss = 0.02785203\n",
      "Iteration 7588, loss = 0.02784351\n",
      "Iteration 7589, loss = 0.02783499\n",
      "Iteration 7590, loss = 0.02782647\n",
      "Iteration 7591, loss = 0.02781794\n",
      "Iteration 7592, loss = 0.02780941\n",
      "Iteration 7593, loss = 0.02780087\n",
      "Iteration 7594, loss = 0.02779233\n",
      "Iteration 7595, loss = 0.02778379\n",
      "Iteration 7596, loss = 0.02777524\n",
      "Iteration 7597, loss = 0.02776668\n",
      "Iteration 7598, loss = 0.02775813\n",
      "Iteration 7599, loss = 0.02774957\n",
      "Iteration 7600, loss = 0.02774100\n",
      "Iteration 7601, loss = 0.02773244\n",
      "Iteration 7602, loss = 0.02772386\n",
      "Iteration 7603, loss = 0.02771529\n",
      "Iteration 7604, loss = 0.02770671\n",
      "Iteration 7605, loss = 0.02769812\n",
      "Iteration 7606, loss = 0.02768954\n",
      "Iteration 7607, loss = 0.02768094\n",
      "Iteration 7608, loss = 0.02767235\n",
      "Iteration 7609, loss = 0.02766375\n",
      "Iteration 7610, loss = 0.02765515\n",
      "Iteration 7611, loss = 0.02764654\n",
      "Iteration 7612, loss = 0.02763793\n",
      "Iteration 7613, loss = 0.02762931\n",
      "Iteration 7614, loss = 0.02762070\n",
      "Iteration 7615, loss = 0.02761207\n",
      "Iteration 7616, loss = 0.02760345\n",
      "Iteration 7617, loss = 0.02759482\n",
      "Iteration 7618, loss = 0.02758618\n",
      "Iteration 7619, loss = 0.02757754\n",
      "Iteration 7620, loss = 0.02756890\n",
      "Iteration 7621, loss = 0.02756026\n",
      "Iteration 7622, loss = 0.02755161\n",
      "Iteration 7623, loss = 0.02754295\n",
      "Iteration 7624, loss = 0.02753430\n",
      "Iteration 7625, loss = 0.02752564\n",
      "Iteration 7626, loss = 0.02751697\n",
      "Iteration 7627, loss = 0.02750830\n",
      "Iteration 7628, loss = 0.02749963\n",
      "Iteration 7629, loss = 0.02749096\n",
      "Iteration 7630, loss = 0.02748228\n",
      "Iteration 7631, loss = 0.02747359\n",
      "Iteration 7632, loss = 0.02746491\n",
      "Iteration 7633, loss = 0.02745622\n",
      "Iteration 7634, loss = 0.02744752\n",
      "Iteration 7635, loss = 0.02743882\n",
      "Iteration 7636, loss = 0.02743012\n",
      "Iteration 7637, loss = 0.02742142\n",
      "Iteration 7638, loss = 0.02741271\n",
      "Iteration 7639, loss = 0.02740400\n",
      "Iteration 7640, loss = 0.02739528\n",
      "Iteration 7641, loss = 0.02738656\n",
      "Iteration 7642, loss = 0.02737783\n",
      "Iteration 7643, loss = 0.02736911\n",
      "Iteration 7644, loss = 0.02736038\n",
      "Iteration 7645, loss = 0.02735164\n",
      "Iteration 7646, loss = 0.02734290\n",
      "Iteration 7647, loss = 0.02733416\n",
      "Iteration 7648, loss = 0.02732541\n",
      "Iteration 7649, loss = 0.02731667\n",
      "Iteration 7650, loss = 0.02730791\n",
      "Iteration 7651, loss = 0.02729916\n",
      "Iteration 7652, loss = 0.02729040\n",
      "Iteration 7653, loss = 0.02728163\n",
      "Iteration 7654, loss = 0.02727286\n",
      "Iteration 7655, loss = 0.02726409\n",
      "Iteration 7656, loss = 0.02725532\n",
      "Iteration 7657, loss = 0.02724654\n",
      "Iteration 7658, loss = 0.02723776\n",
      "Iteration 7659, loss = 0.02722897\n",
      "Iteration 7660, loss = 0.02722018\n",
      "Iteration 7661, loss = 0.02721139\n",
      "Iteration 7662, loss = 0.02720260\n",
      "Iteration 7663, loss = 0.02719380\n",
      "Iteration 7664, loss = 0.02718499\n",
      "Iteration 7665, loss = 0.02717619\n",
      "Iteration 7666, loss = 0.02716738\n",
      "Iteration 7667, loss = 0.02715856\n",
      "Iteration 7668, loss = 0.02714975\n",
      "Iteration 7669, loss = 0.02714093\n",
      "Iteration 7670, loss = 0.02713210\n",
      "Iteration 7671, loss = 0.02712328\n",
      "Iteration 7672, loss = 0.02711444\n",
      "Iteration 7673, loss = 0.02710561\n",
      "Iteration 7674, loss = 0.02709677\n",
      "Iteration 7675, loss = 0.02708793\n",
      "Iteration 7676, loss = 0.02707909\n",
      "Iteration 7677, loss = 0.02707024\n",
      "Iteration 7678, loss = 0.02706139\n",
      "Iteration 7679, loss = 0.02705253\n",
      "Iteration 7680, loss = 0.02704367\n",
      "Iteration 7681, loss = 0.02703481\n",
      "Iteration 7682, loss = 0.02702595\n",
      "Iteration 7683, loss = 0.02701708\n",
      "Iteration 7684, loss = 0.02700820\n",
      "Iteration 7685, loss = 0.02699933\n",
      "Iteration 7686, loss = 0.02699045\n",
      "Iteration 7687, loss = 0.02698157\n",
      "Iteration 7688, loss = 0.02697268\n",
      "Iteration 7689, loss = 0.02696379\n",
      "Iteration 7690, loss = 0.02695490\n",
      "Iteration 7691, loss = 0.02694601\n",
      "Iteration 7692, loss = 0.02693711\n",
      "Iteration 7693, loss = 0.02692820\n",
      "Iteration 7694, loss = 0.02691930\n",
      "Iteration 7695, loss = 0.02691039\n",
      "Iteration 7696, loss = 0.02690148\n",
      "Iteration 7697, loss = 0.02689256\n",
      "Iteration 7698, loss = 0.02688364\n",
      "Iteration 7699, loss = 0.02687472\n",
      "Iteration 7700, loss = 0.02686579\n",
      "Iteration 7701, loss = 0.02685687\n",
      "Iteration 7702, loss = 0.02684793\n",
      "Iteration 7703, loss = 0.02683900\n",
      "Iteration 7704, loss = 0.02683006\n",
      "Iteration 7705, loss = 0.02682112\n",
      "Iteration 7706, loss = 0.02681217\n",
      "Iteration 7707, loss = 0.02680322\n",
      "Iteration 7708, loss = 0.02679427\n",
      "Iteration 7709, loss = 0.02678532\n",
      "Iteration 7710, loss = 0.02677636\n",
      "Iteration 7711, loss = 0.02676740\n",
      "Iteration 7712, loss = 0.02675843\n",
      "Iteration 7713, loss = 0.02674947\n",
      "Iteration 7714, loss = 0.02674049\n",
      "Iteration 7715, loss = 0.02673152\n",
      "Iteration 7716, loss = 0.02672254\n",
      "Iteration 7717, loss = 0.02671356\n",
      "Iteration 7718, loss = 0.02670458\n",
      "Iteration 7719, loss = 0.02669559\n",
      "Iteration 7720, loss = 0.02668660\n",
      "Iteration 7721, loss = 0.02667761\n",
      "Iteration 7722, loss = 0.02666861\n",
      "Iteration 7723, loss = 0.02665961\n",
      "Iteration 7724, loss = 0.02665061\n",
      "Iteration 7725, loss = 0.02664160\n",
      "Iteration 7726, loss = 0.02663259\n",
      "Iteration 7727, loss = 0.02662358\n",
      "Iteration 7728, loss = 0.02661457\n",
      "Iteration 7729, loss = 0.02660555\n",
      "Iteration 7730, loss = 0.02659653\n",
      "Iteration 7731, loss = 0.02658750\n",
      "Iteration 7732, loss = 0.02657847\n",
      "Iteration 7733, loss = 0.02656944\n",
      "Iteration 7734, loss = 0.02656041\n",
      "Iteration 7735, loss = 0.02655137\n",
      "Iteration 7736, loss = 0.02654233\n",
      "Iteration 7737, loss = 0.02653329\n",
      "Iteration 7738, loss = 0.02652424\n",
      "Iteration 7739, loss = 0.02651519\n",
      "Iteration 7740, loss = 0.02650614\n",
      "Iteration 7741, loss = 0.02649709\n",
      "Iteration 7742, loss = 0.02648803\n",
      "Iteration 7743, loss = 0.02647897\n",
      "Iteration 7744, loss = 0.02646990\n",
      "Iteration 7745, loss = 0.02646084\n",
      "Iteration 7746, loss = 0.02645176\n",
      "Iteration 7747, loss = 0.02644269\n",
      "Iteration 7748, loss = 0.02643361\n",
      "Iteration 7749, loss = 0.02642454\n",
      "Iteration 7750, loss = 0.02641545\n",
      "Iteration 7751, loss = 0.02640637\n",
      "Iteration 7752, loss = 0.02639728\n",
      "Iteration 7753, loss = 0.02638819\n",
      "Iteration 7754, loss = 0.02637909\n",
      "Iteration 7755, loss = 0.02637000\n",
      "Iteration 7756, loss = 0.02636090\n",
      "Iteration 7757, loss = 0.02635179\n",
      "Iteration 7758, loss = 0.02634269\n",
      "Iteration 7759, loss = 0.02633358\n",
      "Iteration 7760, loss = 0.02632446\n",
      "Iteration 7761, loss = 0.02631535\n",
      "Iteration 7762, loss = 0.02630623\n",
      "Iteration 7763, loss = 0.02629711\n",
      "Iteration 7764, loss = 0.02628799\n",
      "Iteration 7765, loss = 0.02627886\n",
      "Iteration 7766, loss = 0.02626973\n",
      "Iteration 7767, loss = 0.02626060\n",
      "Iteration 7768, loss = 0.02625146\n",
      "Iteration 7769, loss = 0.02624232\n",
      "Iteration 7770, loss = 0.02623318\n",
      "Iteration 7771, loss = 0.02622404\n",
      "Iteration 7772, loss = 0.02621489\n",
      "Iteration 7773, loss = 0.02620574\n",
      "Iteration 7774, loss = 0.02619659\n",
      "Iteration 7775, loss = 0.02618743\n",
      "Iteration 7776, loss = 0.02617828\n",
      "Iteration 7777, loss = 0.02616911\n",
      "Iteration 7778, loss = 0.02615995\n",
      "Iteration 7779, loss = 0.02615078\n",
      "Iteration 7780, loss = 0.02614161\n",
      "Iteration 7781, loss = 0.02613244\n",
      "Iteration 7782, loss = 0.02612327\n",
      "Iteration 7783, loss = 0.02611409\n",
      "Iteration 7784, loss = 0.02610491\n",
      "Iteration 7785, loss = 0.02609572\n",
      "Iteration 7786, loss = 0.02608654\n",
      "Iteration 7787, loss = 0.02607735\n",
      "Iteration 7788, loss = 0.02606816\n",
      "Iteration 7789, loss = 0.02605896\n",
      "Iteration 7790, loss = 0.02604976\n",
      "Iteration 7791, loss = 0.02604056\n",
      "Iteration 7792, loss = 0.02603136\n",
      "Iteration 7793, loss = 0.02602215\n",
      "Iteration 7794, loss = 0.02601294\n",
      "Iteration 7795, loss = 0.02600373\n",
      "Iteration 7796, loss = 0.02599452\n",
      "Iteration 7797, loss = 0.02598530\n",
      "Iteration 7798, loss = 0.02597608\n",
      "Iteration 7799, loss = 0.02596686\n",
      "Iteration 7800, loss = 0.02595763\n",
      "Iteration 7801, loss = 0.02594841\n",
      "Iteration 7802, loss = 0.02593918\n",
      "Iteration 7803, loss = 0.02592994\n",
      "Iteration 7804, loss = 0.02592071\n",
      "Iteration 7805, loss = 0.02591147\n",
      "Iteration 7806, loss = 0.02590223\n",
      "Iteration 7807, loss = 0.02589298\n",
      "Iteration 7808, loss = 0.02588373\n",
      "Iteration 7809, loss = 0.02587448\n",
      "Iteration 7810, loss = 0.02586523\n",
      "Iteration 7811, loss = 0.02585598\n",
      "Iteration 7812, loss = 0.02584672\n",
      "Iteration 7813, loss = 0.02583746\n",
      "Iteration 7814, loss = 0.02582820\n",
      "Iteration 7815, loss = 0.02581893\n",
      "Iteration 7816, loss = 0.02580966\n",
      "Iteration 7817, loss = 0.02580039\n",
      "Iteration 7818, loss = 0.02579112\n",
      "Iteration 7819, loss = 0.02578184\n",
      "Iteration 7820, loss = 0.02577256\n",
      "Iteration 7821, loss = 0.02576328\n",
      "Iteration 7822, loss = 0.02575400\n",
      "Iteration 7823, loss = 0.02574471\n",
      "Iteration 7824, loss = 0.02573542\n",
      "Iteration 7825, loss = 0.02572613\n",
      "Iteration 7826, loss = 0.02571684\n",
      "Iteration 7827, loss = 0.02570754\n",
      "Iteration 7828, loss = 0.02569824\n",
      "Iteration 7829, loss = 0.02568894\n",
      "Iteration 7830, loss = 0.02567963\n",
      "Iteration 7831, loss = 0.02567032\n",
      "Iteration 7832, loss = 0.02566101\n",
      "Iteration 7833, loss = 0.02565170\n",
      "Iteration 7834, loss = 0.02564239\n",
      "Iteration 7835, loss = 0.02563307\n",
      "Iteration 7836, loss = 0.02562375\n",
      "Iteration 7837, loss = 0.02561443\n",
      "Iteration 7838, loss = 0.02560510\n",
      "Iteration 7839, loss = 0.02559577\n",
      "Iteration 7840, loss = 0.02558644\n",
      "Iteration 7841, loss = 0.02557711\n",
      "Iteration 7842, loss = 0.02556777\n",
      "Iteration 7843, loss = 0.02555843\n",
      "Iteration 7844, loss = 0.02554909\n",
      "Iteration 7845, loss = 0.02553975\n",
      "Iteration 7846, loss = 0.02553041\n",
      "Iteration 7847, loss = 0.02552106\n",
      "Iteration 7848, loss = 0.02551171\n",
      "Iteration 7849, loss = 0.02550235\n",
      "Iteration 7850, loss = 0.02549300\n",
      "Iteration 7851, loss = 0.02548364\n",
      "Iteration 7852, loss = 0.02547428\n",
      "Iteration 7853, loss = 0.02546491\n",
      "Iteration 7854, loss = 0.02545555\n",
      "Iteration 7855, loss = 0.02544618\n",
      "Iteration 7856, loss = 0.02543681\n",
      "Iteration 7857, loss = 0.02542744\n",
      "Iteration 7858, loss = 0.02541806\n",
      "Iteration 7859, loss = 0.02540868\n",
      "Iteration 7860, loss = 0.02539930\n",
      "Iteration 7861, loss = 0.02538992\n",
      "Iteration 7862, loss = 0.02538053\n",
      "Iteration 7863, loss = 0.02537115\n",
      "Iteration 7864, loss = 0.02536176\n",
      "Iteration 7865, loss = 0.02535236\n",
      "Iteration 7866, loss = 0.02534297\n",
      "Iteration 7867, loss = 0.02533357\n",
      "Iteration 7868, loss = 0.02532417\n",
      "Iteration 7869, loss = 0.02531477\n",
      "Iteration 7870, loss = 0.02530536\n",
      "Iteration 7871, loss = 0.02529595\n",
      "Iteration 7872, loss = 0.02528654\n",
      "Iteration 7873, loss = 0.02527713\n",
      "Iteration 7874, loss = 0.02526772\n",
      "Iteration 7875, loss = 0.02525830\n",
      "Iteration 7876, loss = 0.02524888\n",
      "Iteration 7877, loss = 0.02523946\n",
      "Iteration 7878, loss = 0.02523003\n",
      "Iteration 7879, loss = 0.02522061\n",
      "Iteration 7880, loss = 0.02521118\n",
      "Iteration 7881, loss = 0.02520175\n",
      "Iteration 7882, loss = 0.02519231\n",
      "Iteration 7883, loss = 0.02518288\n",
      "Iteration 7884, loss = 0.02517344\n",
      "Iteration 7885, loss = 0.02516400\n",
      "Iteration 7886, loss = 0.02515455\n",
      "Iteration 7887, loss = 0.02514511\n",
      "Iteration 7888, loss = 0.02513566\n",
      "Iteration 7889, loss = 0.02512621\n",
      "Iteration 7890, loss = 0.02511676\n",
      "Iteration 7891, loss = 0.02510730\n",
      "Iteration 7892, loss = 0.02509784\n",
      "Iteration 7893, loss = 0.02508838\n",
      "Iteration 7894, loss = 0.02507892\n",
      "Iteration 7895, loss = 0.02506946\n",
      "Iteration 7896, loss = 0.02505999\n",
      "Iteration 7897, loss = 0.02505052\n",
      "Iteration 7898, loss = 0.02504105\n",
      "Iteration 7899, loss = 0.02503158\n",
      "Iteration 7900, loss = 0.02502210\n",
      "Iteration 7901, loss = 0.02501262\n",
      "Iteration 7902, loss = 0.02500314\n",
      "Iteration 7903, loss = 0.02499366\n",
      "Iteration 7904, loss = 0.02498417\n",
      "Iteration 7905, loss = 0.02497469\n",
      "Iteration 7906, loss = 0.02496520\n",
      "Iteration 7907, loss = 0.02495570\n",
      "Iteration 7908, loss = 0.02494621\n",
      "Iteration 7909, loss = 0.02493671\n",
      "Iteration 7910, loss = 0.02492721\n",
      "Iteration 7911, loss = 0.02491771\n",
      "Iteration 7912, loss = 0.02490821\n",
      "Iteration 7913, loss = 0.02489870\n",
      "Iteration 7914, loss = 0.02488919\n",
      "Iteration 7915, loss = 0.02487968\n",
      "Iteration 7916, loss = 0.02487017\n",
      "Iteration 7917, loss = 0.02486066\n",
      "Iteration 7918, loss = 0.02485114\n",
      "Iteration 7919, loss = 0.02484162\n",
      "Iteration 7920, loss = 0.02483210\n",
      "Iteration 7921, loss = 0.02482257\n",
      "Iteration 7922, loss = 0.02481305\n",
      "Iteration 7923, loss = 0.02480352\n",
      "Iteration 7924, loss = 0.02479399\n",
      "Iteration 7925, loss = 0.02478445\n",
      "Iteration 7926, loss = 0.02477492\n",
      "Iteration 7927, loss = 0.02476538\n",
      "Iteration 7928, loss = 0.02475584\n",
      "Iteration 7929, loss = 0.02474630\n",
      "Iteration 7930, loss = 0.02473676\n",
      "Iteration 7931, loss = 0.02472721\n",
      "Iteration 7932, loss = 0.02471766\n",
      "Iteration 7933, loss = 0.02470811\n",
      "Iteration 7934, loss = 0.02469856\n",
      "Iteration 7935, loss = 0.02468900\n",
      "Iteration 7936, loss = 0.02467944\n",
      "Iteration 7937, loss = 0.02466989\n",
      "Iteration 7938, loss = 0.02466032\n",
      "Iteration 7939, loss = 0.02465076\n",
      "Iteration 7940, loss = 0.02464119\n",
      "Iteration 7941, loss = 0.02463163\n",
      "Iteration 7942, loss = 0.02462205\n",
      "Iteration 7943, loss = 0.02461248\n",
      "Iteration 7944, loss = 0.02460291\n",
      "Iteration 7945, loss = 0.02459333\n",
      "Iteration 7946, loss = 0.02458375\n",
      "Iteration 7947, loss = 0.02457417\n",
      "Iteration 7948, loss = 0.02456459\n",
      "Iteration 7949, loss = 0.02455500\n",
      "Iteration 7950, loss = 0.02454541\n",
      "Iteration 7951, loss = 0.02453582\n",
      "Iteration 7952, loss = 0.02452623\n",
      "Iteration 7953, loss = 0.02451664\n",
      "Iteration 7954, loss = 0.02450704\n",
      "Iteration 7955, loss = 0.02449744\n",
      "Iteration 7956, loss = 0.02448784\n",
      "Iteration 7957, loss = 0.02447824\n",
      "Iteration 7958, loss = 0.02446863\n",
      "Iteration 7959, loss = 0.02445903\n",
      "Iteration 7960, loss = 0.02444942\n",
      "Iteration 7961, loss = 0.02443981\n",
      "Iteration 7962, loss = 0.02443019\n",
      "Iteration 7963, loss = 0.02442058\n",
      "Iteration 7964, loss = 0.02441096\n",
      "Iteration 7965, loss = 0.02440134\n",
      "Iteration 7966, loss = 0.02439172\n",
      "Iteration 7967, loss = 0.02438210\n",
      "Iteration 7968, loss = 0.02437247\n",
      "Iteration 7969, loss = 0.02436284\n",
      "Iteration 7970, loss = 0.02435321\n",
      "Iteration 7971, loss = 0.02434358\n",
      "Iteration 7972, loss = 0.02433395\n",
      "Iteration 7973, loss = 0.02432431\n",
      "Iteration 7974, loss = 0.02431467\n",
      "Iteration 7975, loss = 0.02430503\n",
      "Iteration 7976, loss = 0.02429539\n",
      "Iteration 7977, loss = 0.02428574\n",
      "Iteration 7978, loss = 0.02427610\n",
      "Iteration 7979, loss = 0.02426645\n",
      "Iteration 7980, loss = 0.02425680\n",
      "Iteration 7981, loss = 0.02424714\n",
      "Iteration 7982, loss = 0.02423749\n",
      "Iteration 7983, loss = 0.02422783\n",
      "Iteration 7984, loss = 0.02421817\n",
      "Iteration 7985, loss = 0.02420851\n",
      "Iteration 7986, loss = 0.02419885\n",
      "Iteration 7987, loss = 0.02418919\n",
      "Iteration 7988, loss = 0.02417952\n",
      "Iteration 7989, loss = 0.02416985\n",
      "Iteration 7990, loss = 0.02416018\n",
      "Iteration 7991, loss = 0.02415051\n",
      "Iteration 7992, loss = 0.02414083\n",
      "Iteration 7993, loss = 0.02413115\n",
      "Iteration 7994, loss = 0.02412148\n",
      "Iteration 7995, loss = 0.02411179\n",
      "Iteration 7996, loss = 0.02410211\n",
      "Iteration 7997, loss = 0.02409243\n",
      "Iteration 7998, loss = 0.02408274\n",
      "Iteration 7999, loss = 0.02407305\n",
      "Iteration 8000, loss = 0.02406336\n",
      "Iteration 8001, loss = 0.02405367\n",
      "Iteration 8002, loss = 0.02404397\n",
      "Iteration 8003, loss = 0.02403428\n",
      "Iteration 8004, loss = 0.02402458\n",
      "Iteration 8005, loss = 0.02401488\n",
      "Iteration 8006, loss = 0.02400518\n",
      "Iteration 8007, loss = 0.02399547\n",
      "Iteration 8008, loss = 0.02398577\n",
      "Iteration 8009, loss = 0.02397606\n",
      "Iteration 8010, loss = 0.02396635\n",
      "Iteration 8011, loss = 0.02395663\n",
      "Iteration 8012, loss = 0.02394692\n",
      "Iteration 8013, loss = 0.02393721\n",
      "Iteration 8014, loss = 0.02392749\n",
      "Iteration 8015, loss = 0.02391777\n",
      "Iteration 8016, loss = 0.02390805\n",
      "Iteration 8017, loss = 0.02389832\n",
      "Iteration 8018, loss = 0.02388860\n",
      "Iteration 8019, loss = 0.02387887\n",
      "Iteration 8020, loss = 0.02386914\n",
      "Iteration 8021, loss = 0.02385941\n",
      "Iteration 8022, loss = 0.02384968\n",
      "Iteration 8023, loss = 0.02383994\n",
      "Iteration 8024, loss = 0.02383021\n",
      "Iteration 8025, loss = 0.02382047\n",
      "Iteration 8026, loss = 0.02381073\n",
      "Iteration 8027, loss = 0.02380099\n",
      "Iteration 8028, loss = 0.02379124\n",
      "Iteration 8029, loss = 0.02378150\n",
      "Iteration 8030, loss = 0.02377175\n",
      "Iteration 8031, loss = 0.02376200\n",
      "Iteration 8032, loss = 0.02375225\n",
      "Iteration 8033, loss = 0.02374250\n",
      "Iteration 8034, loss = 0.02373274\n",
      "Iteration 8035, loss = 0.02372298\n",
      "Iteration 8036, loss = 0.02371323\n",
      "Iteration 8037, loss = 0.02370347\n",
      "Iteration 8038, loss = 0.02369370\n",
      "Iteration 8039, loss = 0.02368394\n",
      "Iteration 8040, loss = 0.02367418\n",
      "Iteration 8041, loss = 0.02366441\n",
      "Iteration 8042, loss = 0.02365464\n",
      "Iteration 8043, loss = 0.02364487\n",
      "Iteration 8044, loss = 0.02363510\n",
      "Iteration 8045, loss = 0.02362532\n",
      "Iteration 8046, loss = 0.02361555\n",
      "Iteration 8047, loss = 0.02360577\n",
      "Iteration 8048, loss = 0.02359599\n",
      "Iteration 8049, loss = 0.02358621\n",
      "Iteration 8050, loss = 0.02357643\n",
      "Iteration 8051, loss = 0.02356664\n",
      "Iteration 8052, loss = 0.02355686\n",
      "Iteration 8053, loss = 0.02354707\n",
      "Iteration 8054, loss = 0.02353728\n",
      "Iteration 8055, loss = 0.02352749\n",
      "Iteration 8056, loss = 0.02351770\n",
      "Iteration 8057, loss = 0.02350790\n",
      "Iteration 8058, loss = 0.02349810\n",
      "Iteration 8059, loss = 0.02348831\n",
      "Iteration 8060, loss = 0.02347851\n",
      "Iteration 8061, loss = 0.02346871\n",
      "Iteration 8062, loss = 0.02345890\n",
      "Iteration 8063, loss = 0.02344910\n",
      "Iteration 8064, loss = 0.02343929\n",
      "Iteration 8065, loss = 0.02342949\n",
      "Iteration 8066, loss = 0.02341968\n",
      "Iteration 8067, loss = 0.02340987\n",
      "Iteration 8068, loss = 0.02340006\n",
      "Iteration 8069, loss = 0.02339024\n",
      "Iteration 8070, loss = 0.02338043\n",
      "Iteration 8071, loss = 0.02337061\n",
      "Iteration 8072, loss = 0.02336079\n",
      "Iteration 8073, loss = 0.02335097\n",
      "Iteration 8074, loss = 0.02334115\n",
      "Iteration 8075, loss = 0.02333133\n",
      "Iteration 8076, loss = 0.02332150\n",
      "Iteration 8077, loss = 0.02331168\n",
      "Iteration 8078, loss = 0.02330185\n",
      "Iteration 8079, loss = 0.02329202\n",
      "Iteration 8080, loss = 0.02328219\n",
      "Iteration 8081, loss = 0.02327236\n",
      "Iteration 8082, loss = 0.02326252\n",
      "Iteration 8083, loss = 0.02325269\n",
      "Iteration 8084, loss = 0.02324285\n",
      "Iteration 8085, loss = 0.02323301\n",
      "Iteration 8086, loss = 0.02322318\n",
      "Iteration 8087, loss = 0.02321333\n",
      "Iteration 8088, loss = 0.02320349\n",
      "Iteration 8089, loss = 0.02319365\n",
      "Iteration 8090, loss = 0.02318380\n",
      "Iteration 8091, loss = 0.02317396\n",
      "Iteration 8092, loss = 0.02316411\n",
      "Iteration 8093, loss = 0.02315426\n",
      "Iteration 8094, loss = 0.02314441\n",
      "Iteration 8095, loss = 0.02313456\n",
      "Iteration 8096, loss = 0.02312471\n",
      "Iteration 8097, loss = 0.02311485\n",
      "Iteration 8098, loss = 0.02310500\n",
      "Iteration 8099, loss = 0.02309514\n",
      "Iteration 8100, loss = 0.02308528\n",
      "Iteration 8101, loss = 0.02307542\n",
      "Iteration 8102, loss = 0.02306556\n",
      "Iteration 8103, loss = 0.02305570\n",
      "Iteration 8104, loss = 0.02304583\n",
      "Iteration 8105, loss = 0.02303597\n",
      "Iteration 8106, loss = 0.02302610\n",
      "Iteration 8107, loss = 0.02301624\n",
      "Iteration 8108, loss = 0.02300637\n",
      "Iteration 8109, loss = 0.02299650\n",
      "Iteration 8110, loss = 0.02298663\n",
      "Iteration 8111, loss = 0.02297675\n",
      "Iteration 8112, loss = 0.02296688\n",
      "Iteration 8113, loss = 0.02295701\n",
      "Iteration 8114, loss = 0.02294713\n",
      "Iteration 8115, loss = 0.02293725\n",
      "Iteration 8116, loss = 0.02292738\n",
      "Iteration 8117, loss = 0.02291750\n",
      "Iteration 8118, loss = 0.02290762\n",
      "Iteration 8119, loss = 0.02289773\n",
      "Iteration 8120, loss = 0.02288785\n",
      "Iteration 8121, loss = 0.02287797\n",
      "Iteration 8122, loss = 0.02286808\n",
      "Iteration 8123, loss = 0.02285820\n",
      "Iteration 8124, loss = 0.02284831\n",
      "Iteration 8125, loss = 0.02283842\n",
      "Iteration 8126, loss = 0.02282853\n",
      "Iteration 8127, loss = 0.02281864\n",
      "Iteration 8128, loss = 0.02280875\n",
      "Iteration 8129, loss = 0.02279886\n",
      "Iteration 8130, loss = 0.02278897\n",
      "Iteration 8131, loss = 0.02277907\n",
      "Iteration 8132, loss = 0.02276918\n",
      "Iteration 8133, loss = 0.02275928\n",
      "Iteration 8134, loss = 0.02274939\n",
      "Iteration 8135, loss = 0.02273949\n",
      "Iteration 8136, loss = 0.02272959\n",
      "Iteration 8137, loss = 0.02271969\n",
      "Iteration 8138, loss = 0.02270979\n",
      "Iteration 8139, loss = 0.02269989\n",
      "Iteration 8140, loss = 0.02268999\n",
      "Iteration 8141, loss = 0.02268008\n",
      "Iteration 8142, loss = 0.02267018\n",
      "Iteration 8143, loss = 0.02266027\n",
      "Iteration 8144, loss = 0.02265037\n",
      "Iteration 8145, loss = 0.02264046\n",
      "Iteration 8146, loss = 0.02263055\n",
      "Iteration 8147, loss = 0.02262065\n",
      "Iteration 8148, loss = 0.02261074\n",
      "Iteration 8149, loss = 0.02260083\n",
      "Iteration 8150, loss = 0.02259092\n",
      "Iteration 8151, loss = 0.02258100\n",
      "Iteration 8152, loss = 0.02257109\n",
      "Iteration 8153, loss = 0.02256118\n",
      "Iteration 8154, loss = 0.02255127\n",
      "Iteration 8155, loss = 0.02254135\n",
      "Iteration 8156, loss = 0.02253144\n",
      "Iteration 8157, loss = 0.02252152\n",
      "Iteration 8158, loss = 0.02251161\n",
      "Iteration 8159, loss = 0.02250169\n",
      "Iteration 8160, loss = 0.02249177\n",
      "Iteration 8161, loss = 0.02248185\n",
      "Iteration 8162, loss = 0.02247194\n",
      "Iteration 8163, loss = 0.02246202\n",
      "Iteration 8164, loss = 0.02245210\n",
      "Iteration 8165, loss = 0.02244218\n",
      "Iteration 8166, loss = 0.02243226\n",
      "Iteration 8167, loss = 0.02242233\n",
      "Iteration 8168, loss = 0.02241241\n",
      "Iteration 8169, loss = 0.02240249\n",
      "Iteration 8170, loss = 0.02239257\n",
      "Iteration 8171, loss = 0.02238264\n",
      "Iteration 8172, loss = 0.02237272\n",
      "Iteration 8173, loss = 0.02236279\n",
      "Iteration 8174, loss = 0.02235287\n",
      "Iteration 8175, loss = 0.02234294\n",
      "Iteration 8176, loss = 0.02233302\n",
      "Iteration 8177, loss = 0.02232309\n",
      "Iteration 8178, loss = 0.02231317\n",
      "Iteration 8179, loss = 0.02230324\n",
      "Iteration 8180, loss = 0.02229331\n",
      "Iteration 8181, loss = 0.02228339\n",
      "Iteration 8182, loss = 0.02227346\n",
      "Iteration 8183, loss = 0.02226353\n",
      "Iteration 8184, loss = 0.02225360\n",
      "Iteration 8185, loss = 0.02224367\n",
      "Iteration 8186, loss = 0.02223374\n",
      "Iteration 8187, loss = 0.02222381\n",
      "Iteration 8188, loss = 0.02221388\n",
      "Iteration 8189, loss = 0.02220395\n",
      "Iteration 8190, loss = 0.02219402\n",
      "Iteration 8191, loss = 0.02218409\n",
      "Iteration 8192, loss = 0.02217416\n",
      "Iteration 8193, loss = 0.02216423\n",
      "Iteration 8194, loss = 0.02215430\n",
      "Iteration 8195, loss = 0.02214437\n",
      "Iteration 8196, loss = 0.02213444\n",
      "Iteration 8197, loss = 0.02212451\n",
      "Iteration 8198, loss = 0.02211458\n",
      "Iteration 8199, loss = 0.02210465\n",
      "Iteration 8200, loss = 0.02209472\n",
      "Iteration 8201, loss = 0.02208478\n",
      "Iteration 8202, loss = 0.02207485\n",
      "Iteration 8203, loss = 0.02206492\n",
      "Iteration 8204, loss = 0.02205499\n",
      "Iteration 8205, loss = 0.02204506\n",
      "Iteration 8206, loss = 0.02203513\n",
      "Iteration 8207, loss = 0.02202520\n",
      "Iteration 8208, loss = 0.02201526\n",
      "Iteration 8209, loss = 0.02200533\n",
      "Iteration 8210, loss = 0.02199540\n",
      "Iteration 8211, loss = 0.02198547\n",
      "Iteration 8212, loss = 0.02197554\n",
      "Iteration 8213, loss = 0.02196561\n",
      "Iteration 8214, loss = 0.02195568\n",
      "Iteration 8215, loss = 0.02194575\n",
      "Iteration 8216, loss = 0.02193582\n",
      "Iteration 8217, loss = 0.02192589\n",
      "Iteration 8218, loss = 0.02191596\n",
      "Iteration 8219, loss = 0.02190603\n",
      "Iteration 8220, loss = 0.02189610\n",
      "Iteration 8221, loss = 0.02188617\n",
      "Iteration 8222, loss = 0.02187624\n",
      "Iteration 8223, loss = 0.02186631\n",
      "Iteration 8224, loss = 0.02185638\n",
      "Iteration 8225, loss = 0.02184645\n",
      "Iteration 8226, loss = 0.02183652\n",
      "Iteration 8227, loss = 0.02182660\n",
      "Iteration 8228, loss = 0.02181667\n",
      "Iteration 8229, loss = 0.02180674\n",
      "Iteration 8230, loss = 0.02179681\n",
      "Iteration 8231, loss = 0.02178689\n",
      "Iteration 8232, loss = 0.02177696\n",
      "Iteration 8233, loss = 0.02176704\n",
      "Iteration 8234, loss = 0.02175711\n",
      "Iteration 8235, loss = 0.02174719\n",
      "Iteration 8236, loss = 0.02173726\n",
      "Iteration 8237, loss = 0.02172734\n",
      "Iteration 8238, loss = 0.02171742\n",
      "Iteration 8239, loss = 0.02170749\n",
      "Iteration 8240, loss = 0.02169757\n",
      "Iteration 8241, loss = 0.02168765\n",
      "Iteration 8242, loss = 0.02167773\n",
      "Iteration 8243, loss = 0.02166781\n",
      "Iteration 8244, loss = 0.02165789\n",
      "Iteration 8245, loss = 0.02164797\n",
      "Iteration 8246, loss = 0.02163805\n",
      "Iteration 8247, loss = 0.02162813\n",
      "Iteration 8248, loss = 0.02161822\n",
      "Iteration 8249, loss = 0.02160830\n",
      "Iteration 8250, loss = 0.02159838\n",
      "Iteration 8251, loss = 0.02158847\n",
      "Iteration 8252, loss = 0.02157855\n",
      "Iteration 8253, loss = 0.02156864\n",
      "Iteration 8254, loss = 0.02155873\n",
      "Iteration 8255, loss = 0.02154881\n",
      "Iteration 8256, loss = 0.02153890\n",
      "Iteration 8257, loss = 0.02152899\n",
      "Iteration 8258, loss = 0.02151908\n",
      "Iteration 8259, loss = 0.02150917\n",
      "Iteration 8260, loss = 0.02149926\n",
      "Iteration 8261, loss = 0.02148936\n",
      "Iteration 8262, loss = 0.02147945\n",
      "Iteration 8263, loss = 0.02146954\n",
      "Iteration 8264, loss = 0.02145964\n",
      "Iteration 8265, loss = 0.02144974\n",
      "Iteration 8266, loss = 0.02143983\n",
      "Iteration 8267, loss = 0.02142993\n",
      "Iteration 8268, loss = 0.02142003\n",
      "Iteration 8269, loss = 0.02141013\n",
      "Iteration 8270, loss = 0.02140023\n",
      "Iteration 8271, loss = 0.02139033\n",
      "Iteration 8272, loss = 0.02138043\n",
      "Iteration 8273, loss = 0.02137054\n",
      "Iteration 8274, loss = 0.02136064\n",
      "Iteration 8275, loss = 0.02135075\n",
      "Iteration 8276, loss = 0.02134085\n",
      "Iteration 8277, loss = 0.02133096\n",
      "Iteration 8278, loss = 0.02132107\n",
      "Iteration 8279, loss = 0.02131118\n",
      "Iteration 8280, loss = 0.02130129\n",
      "Iteration 8281, loss = 0.02129140\n",
      "Iteration 8282, loss = 0.02128152\n",
      "Iteration 8283, loss = 0.02127163\n",
      "Iteration 8284, loss = 0.02126175\n",
      "Iteration 8285, loss = 0.02125187\n",
      "Iteration 8286, loss = 0.02124198\n",
      "Iteration 8287, loss = 0.02123210\n",
      "Iteration 8288, loss = 0.02122222\n",
      "Iteration 8289, loss = 0.02121235\n",
      "Iteration 8290, loss = 0.02120247\n",
      "Iteration 8291, loss = 0.02119259\n",
      "Iteration 8292, loss = 0.02118272\n",
      "Iteration 8293, loss = 0.02117285\n",
      "Iteration 8294, loss = 0.02116297\n",
      "Iteration 8295, loss = 0.02115310\n",
      "Iteration 8296, loss = 0.02114323\n",
      "Iteration 8297, loss = 0.02113337\n",
      "Iteration 8298, loss = 0.02112350\n",
      "Iteration 8299, loss = 0.02111364\n",
      "Iteration 8300, loss = 0.02110377\n",
      "Iteration 8301, loss = 0.02109391\n",
      "Iteration 8302, loss = 0.02108405\n",
      "Iteration 8303, loss = 0.02107419\n",
      "Iteration 8304, loss = 0.02106433\n",
      "Iteration 8305, loss = 0.02105448\n",
      "Iteration 8306, loss = 0.02104462\n",
      "Iteration 8307, loss = 0.02103477\n",
      "Iteration 8308, loss = 0.02102492\n",
      "Iteration 8309, loss = 0.02101506\n",
      "Iteration 8310, loss = 0.02100522\n",
      "Iteration 8311, loss = 0.02099537\n",
      "Iteration 8312, loss = 0.02098552\n",
      "Iteration 8313, loss = 0.02097568\n",
      "Iteration 8314, loss = 0.02096584\n",
      "Iteration 8315, loss = 0.02095600\n",
      "Iteration 8316, loss = 0.02094616\n",
      "Iteration 8317, loss = 0.02093632\n",
      "Iteration 8318, loss = 0.02092648\n",
      "Iteration 8319, loss = 0.02091665\n",
      "Iteration 8320, loss = 0.02090681\n",
      "Iteration 8321, loss = 0.02089698\n",
      "Iteration 8322, loss = 0.02088715\n",
      "Iteration 8323, loss = 0.02087733\n",
      "Iteration 8324, loss = 0.02086750\n",
      "Iteration 8325, loss = 0.02085768\n",
      "Iteration 8326, loss = 0.02084785\n",
      "Iteration 8327, loss = 0.02083803\n",
      "Iteration 8328, loss = 0.02082821\n",
      "Iteration 8329, loss = 0.02081840\n",
      "Iteration 8330, loss = 0.02080858\n",
      "Iteration 8331, loss = 0.02079877\n",
      "Iteration 8332, loss = 0.02078896\n",
      "Iteration 8333, loss = 0.02077915\n",
      "Iteration 8334, loss = 0.02076934\n",
      "Iteration 8335, loss = 0.02075953\n",
      "Iteration 8336, loss = 0.02074973\n",
      "Iteration 8337, loss = 0.02073992\n",
      "Iteration 8338, loss = 0.02073012\n",
      "Iteration 8339, loss = 0.02072033\n",
      "Iteration 8340, loss = 0.02071053\n",
      "Iteration 8341, loss = 0.02070073\n",
      "Iteration 8342, loss = 0.02069094\n",
      "Iteration 8343, loss = 0.02068115\n",
      "Iteration 8344, loss = 0.02067136\n",
      "Iteration 8345, loss = 0.02066157\n",
      "Iteration 8346, loss = 0.02065179\n",
      "Iteration 8347, loss = 0.02064201\n",
      "Iteration 8348, loss = 0.02063222\n",
      "Iteration 8349, loss = 0.02062244\n",
      "Iteration 8350, loss = 0.02061267\n",
      "Iteration 8351, loss = 0.02060289\n",
      "Iteration 8352, loss = 0.02059312\n",
      "Iteration 8353, loss = 0.02058335\n",
      "Iteration 8354, loss = 0.02057358\n",
      "Iteration 8355, loss = 0.02056381\n",
      "Iteration 8356, loss = 0.02055405\n",
      "Iteration 8357, loss = 0.02054429\n",
      "Iteration 8358, loss = 0.02053453\n",
      "Iteration 8359, loss = 0.02052477\n",
      "Iteration 8360, loss = 0.02051501\n",
      "Iteration 8361, loss = 0.02050526\n",
      "Iteration 8362, loss = 0.02049551\n",
      "Iteration 8363, loss = 0.02048576\n",
      "Iteration 8364, loss = 0.02047601\n",
      "Iteration 8365, loss = 0.02046626\n",
      "Iteration 8366, loss = 0.02045652\n",
      "Iteration 8367, loss = 0.02044678\n",
      "Iteration 8368, loss = 0.02043704\n",
      "Iteration 8369, loss = 0.02042730\n",
      "Iteration 8370, loss = 0.02041757\n",
      "Iteration 8371, loss = 0.02040784\n",
      "Iteration 8372, loss = 0.02039811\n",
      "Iteration 8373, loss = 0.02038838\n",
      "Iteration 8374, loss = 0.02037866\n",
      "Iteration 8375, loss = 0.02036893\n",
      "Iteration 8376, loss = 0.02035921\n",
      "Iteration 8377, loss = 0.02034950\n",
      "Iteration 8378, loss = 0.02033978\n",
      "Iteration 8379, loss = 0.02033007\n",
      "Iteration 8380, loss = 0.02032036\n",
      "Iteration 8381, loss = 0.02031065\n",
      "Iteration 8382, loss = 0.02030094\n",
      "Iteration 8383, loss = 0.02029124\n",
      "Iteration 8384, loss = 0.02028154\n",
      "Iteration 8385, loss = 0.02027184\n",
      "Iteration 8386, loss = 0.02026214\n",
      "Iteration 8387, loss = 0.02025245\n",
      "Iteration 8388, loss = 0.02024275\n",
      "Iteration 8389, loss = 0.02023306\n",
      "Iteration 8390, loss = 0.02022338\n",
      "Iteration 8391, loss = 0.02021369\n",
      "Iteration 8392, loss = 0.02020401\n",
      "Iteration 8393, loss = 0.02019433\n",
      "Iteration 8394, loss = 0.02018466\n",
      "Iteration 8395, loss = 0.02017498\n",
      "Iteration 8396, loss = 0.02016531\n",
      "Iteration 8397, loss = 0.02015564\n",
      "Iteration 8398, loss = 0.02014597\n",
      "Iteration 8399, loss = 0.02013631\n",
      "Iteration 8400, loss = 0.02012665\n",
      "Iteration 8401, loss = 0.02011699\n",
      "Iteration 8402, loss = 0.02010733\n",
      "Iteration 8403, loss = 0.02009768\n",
      "Iteration 8404, loss = 0.02008802\n",
      "Iteration 8405, loss = 0.02007838\n",
      "Iteration 8406, loss = 0.02006873\n",
      "Iteration 8407, loss = 0.02005909\n",
      "Iteration 8408, loss = 0.02004944\n",
      "Iteration 8409, loss = 0.02003981\n",
      "Iteration 8410, loss = 0.02003017\n",
      "Iteration 8411, loss = 0.02002054\n",
      "Iteration 8412, loss = 0.02001091\n",
      "Iteration 8413, loss = 0.02000128\n",
      "Iteration 8414, loss = 0.01999165\n",
      "Iteration 8415, loss = 0.01998203\n",
      "Iteration 8416, loss = 0.01997241\n",
      "Iteration 8417, loss = 0.01996279\n",
      "Iteration 8418, loss = 0.01995318\n",
      "Iteration 8419, loss = 0.01994357\n",
      "Iteration 8420, loss = 0.01993396\n",
      "Iteration 8421, loss = 0.01992435\n",
      "Iteration 8422, loss = 0.01991475\n",
      "Iteration 8423, loss = 0.01990515\n",
      "Iteration 8424, loss = 0.01989555\n",
      "Iteration 8425, loss = 0.01988595\n",
      "Iteration 8426, loss = 0.01987636\n",
      "Iteration 8427, loss = 0.01986677\n",
      "Iteration 8428, loss = 0.01985719\n",
      "Iteration 8429, loss = 0.01984760\n",
      "Iteration 8430, loss = 0.01983802\n",
      "Iteration 8431, loss = 0.01982844\n",
      "Iteration 8432, loss = 0.01981887\n",
      "Iteration 8433, loss = 0.01980929\n",
      "Iteration 8434, loss = 0.01979972\n",
      "Iteration 8435, loss = 0.01979016\n",
      "Iteration 8436, loss = 0.01978059\n",
      "Iteration 8437, loss = 0.01977103\n",
      "Iteration 8438, loss = 0.01976147\n",
      "Iteration 8439, loss = 0.01975192\n",
      "Iteration 8440, loss = 0.01974236\n",
      "Iteration 8441, loss = 0.01973281\n",
      "Iteration 8442, loss = 0.01972327\n",
      "Iteration 8443, loss = 0.01971372\n",
      "Iteration 8444, loss = 0.01970418\n",
      "Iteration 8445, loss = 0.01969464\n",
      "Iteration 8446, loss = 0.01968511\n",
      "Iteration 8447, loss = 0.01967557\n",
      "Iteration 8448, loss = 0.01966604\n",
      "Iteration 8449, loss = 0.01965652\n",
      "Iteration 8450, loss = 0.01964699\n",
      "Iteration 8451, loss = 0.01963747\n",
      "Iteration 8452, loss = 0.01962795\n",
      "Iteration 8453, loss = 0.01961844\n",
      "Iteration 8454, loss = 0.01960893\n",
      "Iteration 8455, loss = 0.01959942\n",
      "Iteration 8456, loss = 0.01958991\n",
      "Iteration 8457, loss = 0.01958041\n",
      "Iteration 8458, loss = 0.01957091\n",
      "Iteration 8459, loss = 0.01956141\n",
      "Iteration 8460, loss = 0.01955192\n",
      "Iteration 8461, loss = 0.01954243\n",
      "Iteration 8462, loss = 0.01953294\n",
      "Iteration 8463, loss = 0.01952345\n",
      "Iteration 8464, loss = 0.01951397\n",
      "Iteration 8465, loss = 0.01950449\n",
      "Iteration 8466, loss = 0.01949502\n",
      "Iteration 8467, loss = 0.01948555\n",
      "Iteration 8468, loss = 0.01947608\n",
      "Iteration 8469, loss = 0.01946661\n",
      "Iteration 8470, loss = 0.01945715\n",
      "Iteration 8471, loss = 0.01944769\n",
      "Iteration 8472, loss = 0.01943823\n",
      "Iteration 8473, loss = 0.01942877\n",
      "Iteration 8474, loss = 0.01941932\n",
      "Iteration 8475, loss = 0.01940987\n",
      "Iteration 8476, loss = 0.01940043\n",
      "Iteration 8477, loss = 0.01939099\n",
      "Iteration 8478, loss = 0.01938155\n",
      "Iteration 8479, loss = 0.01937211\n",
      "Iteration 8480, loss = 0.01936268\n",
      "Iteration 8481, loss = 0.01935325\n",
      "Iteration 8482, loss = 0.01934383\n",
      "Iteration 8483, loss = 0.01933440\n",
      "Iteration 8484, loss = 0.01932498\n",
      "Iteration 8485, loss = 0.01931557\n",
      "Iteration 8486, loss = 0.01930615\n",
      "Iteration 8487, loss = 0.01929674\n",
      "Iteration 8488, loss = 0.01928734\n",
      "Iteration 8489, loss = 0.01927793\n",
      "Iteration 8490, loss = 0.01926853\n",
      "Iteration 8491, loss = 0.01925913\n",
      "Iteration 8492, loss = 0.01924974\n",
      "Iteration 8493, loss = 0.01924035\n",
      "Iteration 8494, loss = 0.01923096\n",
      "Iteration 8495, loss = 0.01922157\n",
      "Iteration 8496, loss = 0.01921219\n",
      "Iteration 8497, loss = 0.01920281\n",
      "Iteration 8498, loss = 0.01919344\n",
      "Iteration 8499, loss = 0.01918407\n",
      "Iteration 8500, loss = 0.01917470\n",
      "Iteration 8501, loss = 0.01916533\n",
      "Iteration 8502, loss = 0.01915597\n",
      "Iteration 8503, loss = 0.01914661\n",
      "Iteration 8504, loss = 0.01913726\n",
      "Iteration 8505, loss = 0.01912790\n",
      "Iteration 8506, loss = 0.01911856\n",
      "Iteration 8507, loss = 0.01910921\n",
      "Iteration 8508, loss = 0.01909987\n",
      "Iteration 8509, loss = 0.01909053\n",
      "Iteration 8510, loss = 0.01908119\n",
      "Iteration 8511, loss = 0.01907186\n",
      "Iteration 8512, loss = 0.01906253\n",
      "Iteration 8513, loss = 0.01905320\n",
      "Iteration 8514, loss = 0.01904388\n",
      "Iteration 8515, loss = 0.01903456\n",
      "Iteration 8516, loss = 0.01902525\n",
      "Iteration 8517, loss = 0.01901593\n",
      "Iteration 8518, loss = 0.01900662\n",
      "Iteration 8519, loss = 0.01899732\n",
      "Iteration 8520, loss = 0.01898801\n",
      "Iteration 8521, loss = 0.01897871\n",
      "Iteration 8522, loss = 0.01896942\n",
      "Iteration 8523, loss = 0.01896013\n",
      "Iteration 8524, loss = 0.01895084\n",
      "Iteration 8525, loss = 0.01894155\n",
      "Iteration 8526, loss = 0.01893227\n",
      "Iteration 8527, loss = 0.01892299\n",
      "Iteration 8528, loss = 0.01891371\n",
      "Iteration 8529, loss = 0.01890444\n",
      "Iteration 8530, loss = 0.01889517\n",
      "Iteration 8531, loss = 0.01888590\n",
      "Iteration 8532, loss = 0.01887664\n",
      "Iteration 8533, loss = 0.01886738\n",
      "Iteration 8534, loss = 0.01885813\n",
      "Iteration 8535, loss = 0.01884887\n",
      "Iteration 8536, loss = 0.01883962\n",
      "Iteration 8537, loss = 0.01883038\n",
      "Iteration 8538, loss = 0.01882114\n",
      "Iteration 8539, loss = 0.01881190\n",
      "Iteration 8540, loss = 0.01880266\n",
      "Iteration 8541, loss = 0.01879343\n",
      "Iteration 8542, loss = 0.01878420\n",
      "Iteration 8543, loss = 0.01877498\n",
      "Iteration 8544, loss = 0.01876576\n",
      "Iteration 8545, loss = 0.01875654\n",
      "Iteration 8546, loss = 0.01874732\n",
      "Iteration 8547, loss = 0.01873811\n",
      "Iteration 8548, loss = 0.01872890\n",
      "Iteration 8549, loss = 0.01871970\n",
      "Iteration 8550, loss = 0.01871050\n",
      "Iteration 8551, loss = 0.01870130\n",
      "Iteration 8552, loss = 0.01869211\n",
      "Iteration 8553, loss = 0.01868291\n",
      "Iteration 8554, loss = 0.01867373\n",
      "Iteration 8555, loss = 0.01866454\n",
      "Iteration 8556, loss = 0.01865536\n",
      "Iteration 8557, loss = 0.01864619\n",
      "Iteration 8558, loss = 0.01863701\n",
      "Iteration 8559, loss = 0.01862784\n",
      "Iteration 8560, loss = 0.01861868\n",
      "Iteration 8561, loss = 0.01860951\n",
      "Iteration 8562, loss = 0.01860035\n",
      "Iteration 8563, loss = 0.01859120\n",
      "Iteration 8564, loss = 0.01858205\n",
      "Iteration 8565, loss = 0.01857290\n",
      "Iteration 8566, loss = 0.01856375\n",
      "Iteration 8567, loss = 0.01855461\n",
      "Iteration 8568, loss = 0.01854547\n",
      "Iteration 8569, loss = 0.01853633\n",
      "Iteration 8570, loss = 0.01852720\n",
      "Iteration 8571, loss = 0.01851808\n",
      "Iteration 8572, loss = 0.01850895\n",
      "Iteration 8573, loss = 0.01849983\n",
      "Iteration 8574, loss = 0.01849071\n",
      "Iteration 8575, loss = 0.01848160\n",
      "Iteration 8576, loss = 0.01847249\n",
      "Iteration 8577, loss = 0.01846338\n",
      "Iteration 8578, loss = 0.01845428\n",
      "Iteration 8579, loss = 0.01844518\n",
      "Iteration 8580, loss = 0.01843608\n",
      "Iteration 8581, loss = 0.01842699\n",
      "Iteration 8582, loss = 0.01841790\n",
      "Iteration 8583, loss = 0.01840881\n",
      "Iteration 8584, loss = 0.01839973\n",
      "Iteration 8585, loss = 0.01839065\n",
      "Iteration 8586, loss = 0.01838158\n",
      "Iteration 8587, loss = 0.01837251\n",
      "Iteration 8588, loss = 0.01836344\n",
      "Iteration 8589, loss = 0.01835437\n",
      "Iteration 8590, loss = 0.01834531\n",
      "Iteration 8591, loss = 0.01833626\n",
      "Iteration 8592, loss = 0.01832720\n",
      "Iteration 8593, loss = 0.01831815\n",
      "Iteration 8594, loss = 0.01830911\n",
      "Iteration 8595, loss = 0.01830006\n",
      "Iteration 8596, loss = 0.01829102\n",
      "Iteration 8597, loss = 0.01828199\n",
      "Iteration 8598, loss = 0.01827296\n",
      "Iteration 8599, loss = 0.01826393\n",
      "Iteration 8600, loss = 0.01825490\n",
      "Iteration 8601, loss = 0.01824588\n",
      "Iteration 8602, loss = 0.01823686\n",
      "Iteration 8603, loss = 0.01822785\n",
      "Iteration 8604, loss = 0.01821884\n",
      "Iteration 8605, loss = 0.01820983\n",
      "Iteration 8606, loss = 0.01820083\n",
      "Iteration 8607, loss = 0.01819183\n",
      "Iteration 8608, loss = 0.01818283\n",
      "Iteration 8609, loss = 0.01817384\n",
      "Iteration 8610, loss = 0.01816485\n",
      "Iteration 8611, loss = 0.01815586\n",
      "Iteration 8612, loss = 0.01814688\n",
      "Iteration 8613, loss = 0.01813790\n",
      "Iteration 8614, loss = 0.01812893\n",
      "Iteration 8615, loss = 0.01811996\n",
      "Iteration 8616, loss = 0.01811099\n",
      "Iteration 8617, loss = 0.01810203\n",
      "Iteration 8618, loss = 0.01809307\n",
      "Iteration 8619, loss = 0.01808411\n",
      "Iteration 8620, loss = 0.01807516\n",
      "Iteration 8621, loss = 0.01806621\n",
      "Iteration 8622, loss = 0.01805726\n",
      "Iteration 8623, loss = 0.01804832\n",
      "Iteration 8624, loss = 0.01803938\n",
      "Iteration 8625, loss = 0.01803044\n",
      "Iteration 8626, loss = 0.01802151\n",
      "Iteration 8627, loss = 0.01801259\n",
      "Iteration 8628, loss = 0.01800366\n",
      "Iteration 8629, loss = 0.01799474\n",
      "Iteration 8630, loss = 0.01798582\n",
      "Iteration 8631, loss = 0.01797691\n",
      "Iteration 8632, loss = 0.01796800\n",
      "Iteration 8633, loss = 0.01795910\n",
      "Iteration 8634, loss = 0.01795019\n",
      "Iteration 8635, loss = 0.01794130\n",
      "Iteration 8636, loss = 0.01793240\n",
      "Iteration 8637, loss = 0.01792351\n",
      "Iteration 8638, loss = 0.01791462\n",
      "Iteration 8639, loss = 0.01790574\n",
      "Iteration 8640, loss = 0.01789686\n",
      "Iteration 8641, loss = 0.01788798\n",
      "Iteration 8642, loss = 0.01787911\n",
      "Iteration 8643, loss = 0.01787024\n",
      "Iteration 8644, loss = 0.01786137\n",
      "Iteration 8645, loss = 0.01785251\n",
      "Iteration 8646, loss = 0.01784365\n",
      "Iteration 8647, loss = 0.01783480\n",
      "Iteration 8648, loss = 0.01782595\n",
      "Iteration 8649, loss = 0.01781710\n",
      "Iteration 8650, loss = 0.01780826\n",
      "Iteration 8651, loss = 0.01779942\n",
      "Iteration 8652, loss = 0.01779058\n",
      "Iteration 8653, loss = 0.01778175\n",
      "Iteration 8654, loss = 0.01777292\n",
      "Iteration 8655, loss = 0.01776409\n",
      "Iteration 8656, loss = 0.01775527\n",
      "Iteration 8657, loss = 0.01774645\n",
      "Iteration 8658, loss = 0.01773764\n",
      "Iteration 8659, loss = 0.01772883\n",
      "Iteration 8660, loss = 0.01772002\n",
      "Iteration 8661, loss = 0.01771122\n",
      "Iteration 8662, loss = 0.01770242\n",
      "Iteration 8663, loss = 0.01769362\n",
      "Iteration 8664, loss = 0.01768483\n",
      "Iteration 8665, loss = 0.01767604\n",
      "Iteration 8666, loss = 0.01766726\n",
      "Iteration 8667, loss = 0.01765848\n",
      "Iteration 8668, loss = 0.01764970\n",
      "Iteration 8669, loss = 0.01764092\n",
      "Iteration 8670, loss = 0.01763215\n",
      "Iteration 8671, loss = 0.01762339\n",
      "Iteration 8672, loss = 0.01761463\n",
      "Iteration 8673, loss = 0.01760587\n",
      "Iteration 8674, loss = 0.01759711\n",
      "Iteration 8675, loss = 0.01758836\n",
      "Iteration 8676, loss = 0.01757961\n",
      "Iteration 8677, loss = 0.01757087\n",
      "Iteration 8678, loss = 0.01756213\n",
      "Iteration 8679, loss = 0.01755339\n",
      "Iteration 8680, loss = 0.01754466\n",
      "Iteration 8681, loss = 0.01753593\n",
      "Iteration 8682, loss = 0.01752720\n",
      "Iteration 8683, loss = 0.01751848\n",
      "Iteration 8684, loss = 0.01750976\n",
      "Iteration 8685, loss = 0.01750105\n",
      "Iteration 8686, loss = 0.01749233\n",
      "Iteration 8687, loss = 0.01748363\n",
      "Iteration 8688, loss = 0.01747492\n",
      "Iteration 8689, loss = 0.01746622\n",
      "Iteration 8690, loss = 0.01745753\n",
      "Iteration 8691, loss = 0.01744883\n",
      "Iteration 8692, loss = 0.01744015\n",
      "Iteration 8693, loss = 0.01743146\n",
      "Iteration 8694, loss = 0.01742278\n",
      "Iteration 8695, loss = 0.01741410\n",
      "Iteration 8696, loss = 0.01740543\n",
      "Iteration 8697, loss = 0.01739676\n",
      "Iteration 8698, loss = 0.01738809\n",
      "Iteration 8699, loss = 0.01737943\n",
      "Iteration 8700, loss = 0.01737077\n",
      "Iteration 8701, loss = 0.01736211\n",
      "Iteration 8702, loss = 0.01735346\n",
      "Iteration 8703, loss = 0.01734481\n",
      "Iteration 8704, loss = 0.01733617\n",
      "Iteration 8705, loss = 0.01732753\n",
      "Iteration 8706, loss = 0.01731889\n",
      "Iteration 8707, loss = 0.01731026\n",
      "Iteration 8708, loss = 0.01730163\n",
      "Iteration 8709, loss = 0.01729300\n",
      "Iteration 8710, loss = 0.01728438\n",
      "Iteration 8711, loss = 0.01727576\n",
      "Iteration 8712, loss = 0.01726715\n",
      "Iteration 8713, loss = 0.01725854\n",
      "Iteration 8714, loss = 0.01724993\n",
      "Iteration 8715, loss = 0.01724133\n",
      "Iteration 8716, loss = 0.01723273\n",
      "Iteration 8717, loss = 0.01722413\n",
      "Iteration 8718, loss = 0.01721554\n",
      "Iteration 8719, loss = 0.01720695\n",
      "Iteration 8720, loss = 0.01719836\n",
      "Iteration 8721, loss = 0.01718978\n",
      "Iteration 8722, loss = 0.01718121\n",
      "Iteration 8723, loss = 0.01717263\n",
      "Iteration 8724, loss = 0.01716406\n",
      "Iteration 8725, loss = 0.01715550\n",
      "Iteration 8726, loss = 0.01714693\n",
      "Iteration 8727, loss = 0.01713837\n",
      "Iteration 8728, loss = 0.01712982\n",
      "Iteration 8729, loss = 0.01712127\n",
      "Iteration 8730, loss = 0.01711272\n",
      "Iteration 8731, loss = 0.01710418\n",
      "Iteration 8732, loss = 0.01709563\n",
      "Iteration 8733, loss = 0.01708710\n",
      "Iteration 8734, loss = 0.01707857\n",
      "Iteration 8735, loss = 0.01707004\n",
      "Iteration 8736, loss = 0.01706151\n",
      "Iteration 8737, loss = 0.01705299\n",
      "Iteration 8738, loss = 0.01704447\n",
      "Iteration 8739, loss = 0.01703596\n",
      "Iteration 8740, loss = 0.01702745\n",
      "Iteration 8741, loss = 0.01701894\n",
      "Iteration 8742, loss = 0.01701044\n",
      "Iteration 8743, loss = 0.01700194\n",
      "Iteration 8744, loss = 0.01699344\n",
      "Iteration 8745, loss = 0.01698495\n",
      "Iteration 8746, loss = 0.01697646\n",
      "Iteration 8747, loss = 0.01696798\n",
      "Iteration 8748, loss = 0.01695950\n",
      "Iteration 8749, loss = 0.01695102\n",
      "Iteration 8750, loss = 0.01694254\n",
      "Iteration 8751, loss = 0.01693407\n",
      "Iteration 8752, loss = 0.01692561\n",
      "Iteration 8753, loss = 0.01691715\n",
      "Iteration 8754, loss = 0.01690869\n",
      "Iteration 8755, loss = 0.01690023\n",
      "Iteration 8756, loss = 0.01689178\n",
      "Iteration 8757, loss = 0.01688333\n",
      "Iteration 8758, loss = 0.01687489\n",
      "Iteration 8759, loss = 0.01686645\n",
      "Iteration 8760, loss = 0.01685801\n",
      "Iteration 8761, loss = 0.01684958\n",
      "Iteration 8762, loss = 0.01684115\n",
      "Iteration 8763, loss = 0.01683273\n",
      "Iteration 8764, loss = 0.01682431\n",
      "Iteration 8765, loss = 0.01681589\n",
      "Iteration 8766, loss = 0.01680748\n",
      "Iteration 8767, loss = 0.01679907\n",
      "Iteration 8768, loss = 0.01679066\n",
      "Iteration 8769, loss = 0.01678226\n",
      "Iteration 8770, loss = 0.01677386\n",
      "Iteration 8771, loss = 0.01676546\n",
      "Iteration 8772, loss = 0.01675707\n",
      "Iteration 8773, loss = 0.01674868\n",
      "Iteration 8774, loss = 0.01674030\n",
      "Iteration 8775, loss = 0.01673192\n",
      "Iteration 8776, loss = 0.01672354\n",
      "Iteration 8777, loss = 0.01671517\n",
      "Iteration 8778, loss = 0.01670680\n",
      "Iteration 8779, loss = 0.01669843\n",
      "Iteration 8780, loss = 0.01669007\n",
      "Iteration 8781, loss = 0.01668171\n",
      "Iteration 8782, loss = 0.01667336\n",
      "Iteration 8783, loss = 0.01666501\n",
      "Iteration 8784, loss = 0.01665666\n",
      "Iteration 8785, loss = 0.01664832\n",
      "Iteration 8786, loss = 0.01663998\n",
      "Iteration 8787, loss = 0.01663164\n",
      "Iteration 8788, loss = 0.01662331\n",
      "Iteration 8789, loss = 0.01661498\n",
      "Iteration 8790, loss = 0.01660666\n",
      "Iteration 8791, loss = 0.01659833\n",
      "Iteration 8792, loss = 0.01659002\n",
      "Iteration 8793, loss = 0.01658170\n",
      "Iteration 8794, loss = 0.01657339\n",
      "Iteration 8795, loss = 0.01656509\n",
      "Iteration 8796, loss = 0.01655678\n",
      "Iteration 8797, loss = 0.01654849\n",
      "Iteration 8798, loss = 0.01654019\n",
      "Iteration 8799, loss = 0.01653190\n",
      "Iteration 8800, loss = 0.01652361\n",
      "Iteration 8801, loss = 0.01651533\n",
      "Iteration 8802, loss = 0.01650705\n",
      "Iteration 8803, loss = 0.01649877\n",
      "Iteration 8804, loss = 0.01649050\n",
      "Iteration 8805, loss = 0.01648223\n",
      "Iteration 8806, loss = 0.01647396\n",
      "Iteration 8807, loss = 0.01646570\n",
      "Iteration 8808, loss = 0.01645744\n",
      "Iteration 8809, loss = 0.01644919\n",
      "Iteration 8810, loss = 0.01644094\n",
      "Iteration 8811, loss = 0.01643269\n",
      "Iteration 8812, loss = 0.01642445\n",
      "Iteration 8813, loss = 0.01641621\n",
      "Iteration 8814, loss = 0.01640797\n",
      "Iteration 8815, loss = 0.01639974\n",
      "Iteration 8816, loss = 0.01639151\n",
      "Iteration 8817, loss = 0.01638329\n",
      "Iteration 8818, loss = 0.01637506\n",
      "Iteration 8819, loss = 0.01636685\n",
      "Iteration 8820, loss = 0.01635863\n",
      "Iteration 8821, loss = 0.01635042\n",
      "Iteration 8822, loss = 0.01634222\n",
      "Iteration 8823, loss = 0.01633401\n",
      "Iteration 8824, loss = 0.01632582\n",
      "Iteration 8825, loss = 0.01631762\n",
      "Iteration 8826, loss = 0.01630943\n",
      "Iteration 8827, loss = 0.01630124\n",
      "Iteration 8828, loss = 0.01629306\n",
      "Iteration 8829, loss = 0.01628488\n",
      "Iteration 8830, loss = 0.01627670\n",
      "Iteration 8831, loss = 0.01626853\n",
      "Iteration 8832, loss = 0.01626036\n",
      "Iteration 8833, loss = 0.01625219\n",
      "Iteration 8834, loss = 0.01624403\n",
      "Iteration 8835, loss = 0.01623587\n",
      "Iteration 8836, loss = 0.01622772\n",
      "Iteration 8837, loss = 0.01621956\n",
      "Iteration 8838, loss = 0.01621142\n",
      "Iteration 8839, loss = 0.01620327\n",
      "Iteration 8840, loss = 0.01619513\n",
      "Iteration 8841, loss = 0.01618700\n",
      "Iteration 8842, loss = 0.01617886\n",
      "Iteration 8843, loss = 0.01617074\n",
      "Iteration 8844, loss = 0.01616261\n",
      "Iteration 8845, loss = 0.01615449\n",
      "Iteration 8846, loss = 0.01614637\n",
      "Iteration 8847, loss = 0.01613826\n",
      "Iteration 8848, loss = 0.01613015\n",
      "Iteration 8849, loss = 0.01612204\n",
      "Iteration 8850, loss = 0.01611394\n",
      "Iteration 8851, loss = 0.01610584\n",
      "Iteration 8852, loss = 0.01609774\n",
      "Iteration 8853, loss = 0.01608965\n",
      "Iteration 8854, loss = 0.01608156\n",
      "Iteration 8855, loss = 0.01607348\n",
      "Iteration 8856, loss = 0.01606539\n",
      "Iteration 8857, loss = 0.01605732\n",
      "Iteration 8858, loss = 0.01604924\n",
      "Iteration 8859, loss = 0.01604117\n",
      "Iteration 8860, loss = 0.01603311\n",
      "Iteration 8861, loss = 0.01602504\n",
      "Iteration 8862, loss = 0.01601698\n",
      "Iteration 8863, loss = 0.01600893\n",
      "Iteration 8864, loss = 0.01600088\n",
      "Iteration 8865, loss = 0.01599283\n",
      "Iteration 8866, loss = 0.01598478\n",
      "Iteration 8867, loss = 0.01597674\n",
      "Iteration 8868, loss = 0.01596871\n",
      "Iteration 8869, loss = 0.01596067\n",
      "Iteration 8870, loss = 0.01595264\n",
      "Iteration 8871, loss = 0.01594462\n",
      "Iteration 8872, loss = 0.01593659\n",
      "Iteration 8873, loss = 0.01592858\n",
      "Iteration 8874, loss = 0.01592056\n",
      "Iteration 8875, loss = 0.01591255\n",
      "Iteration 8876, loss = 0.01590454\n",
      "Iteration 8877, loss = 0.01589654\n",
      "Iteration 8878, loss = 0.01588854\n",
      "Iteration 8879, loss = 0.01588054\n",
      "Iteration 8880, loss = 0.01587255\n",
      "Iteration 8881, loss = 0.01586456\n",
      "Iteration 8882, loss = 0.01585657\n",
      "Iteration 8883, loss = 0.01584859\n",
      "Iteration 8884, loss = 0.01584061\n",
      "Iteration 8885, loss = 0.01583263\n",
      "Iteration 8886, loss = 0.01582466\n",
      "Iteration 8887, loss = 0.01581670\n",
      "Iteration 8888, loss = 0.01580873\n",
      "Iteration 8889, loss = 0.01580077\n",
      "Iteration 8890, loss = 0.01579281\n",
      "Iteration 8891, loss = 0.01578486\n",
      "Iteration 8892, loss = 0.01577691\n",
      "Iteration 8893, loss = 0.01576897\n",
      "Iteration 8894, loss = 0.01576102\n",
      "Iteration 8895, loss = 0.01575308\n",
      "Iteration 8896, loss = 0.01574515\n",
      "Iteration 8897, loss = 0.01573722\n",
      "Iteration 8898, loss = 0.01572929\n",
      "Iteration 8899, loss = 0.01572137\n",
      "Iteration 8900, loss = 0.01571345\n",
      "Iteration 8901, loss = 0.01570553\n",
      "Iteration 8902, loss = 0.01569762\n",
      "Iteration 8903, loss = 0.01568971\n",
      "Iteration 8904, loss = 0.01568180\n",
      "Iteration 8905, loss = 0.01567390\n",
      "Iteration 8906, loss = 0.01566600\n",
      "Iteration 8907, loss = 0.01565810\n",
      "Iteration 8908, loss = 0.01565021\n",
      "Iteration 8909, loss = 0.01564233\n",
      "Iteration 8910, loss = 0.01563444\n",
      "Iteration 8911, loss = 0.01562656\n",
      "Iteration 8912, loss = 0.01561868\n",
      "Iteration 8913, loss = 0.01561081\n",
      "Iteration 8914, loss = 0.01560294\n",
      "Iteration 8915, loss = 0.01559507\n",
      "Iteration 8916, loss = 0.01558721\n",
      "Iteration 8917, loss = 0.01557935\n",
      "Iteration 8918, loss = 0.01557150\n",
      "Iteration 8919, loss = 0.01556365\n",
      "Iteration 8920, loss = 0.01555580\n",
      "Iteration 8921, loss = 0.01554795\n",
      "Iteration 8922, loss = 0.01554011\n",
      "Iteration 8923, loss = 0.01553228\n",
      "Iteration 8924, loss = 0.01552444\n",
      "Iteration 8925, loss = 0.01551661\n",
      "Iteration 8926, loss = 0.01550879\n",
      "Iteration 8927, loss = 0.01550096\n",
      "Iteration 8928, loss = 0.01549314\n",
      "Iteration 8929, loss = 0.01548533\n",
      "Iteration 8930, loss = 0.01547752\n",
      "Iteration 8931, loss = 0.01546971\n",
      "Iteration 8932, loss = 0.01546190\n",
      "Iteration 8933, loss = 0.01545410\n",
      "Iteration 8934, loss = 0.01544630\n",
      "Iteration 8935, loss = 0.01543851\n",
      "Iteration 8936, loss = 0.01543072\n",
      "Iteration 8937, loss = 0.01542293\n",
      "Iteration 8938, loss = 0.01541515\n",
      "Iteration 8939, loss = 0.01540737\n",
      "Iteration 8940, loss = 0.01539959\n",
      "Iteration 8941, loss = 0.01539182\n",
      "Iteration 8942, loss = 0.01538405\n",
      "Iteration 8943, loss = 0.01537629\n",
      "Iteration 8944, loss = 0.01536853\n",
      "Iteration 8945, loss = 0.01536077\n",
      "Iteration 8946, loss = 0.01535301\n",
      "Iteration 8947, loss = 0.01534526\n",
      "Iteration 8948, loss = 0.01533752\n",
      "Iteration 8949, loss = 0.01532977\n",
      "Iteration 8950, loss = 0.01532203\n",
      "Iteration 8951, loss = 0.01531429\n",
      "Iteration 8952, loss = 0.01530656\n",
      "Iteration 8953, loss = 0.01529883\n",
      "Iteration 8954, loss = 0.01529111\n",
      "Iteration 8955, loss = 0.01528338\n",
      "Iteration 8956, loss = 0.01527567\n",
      "Iteration 8957, loss = 0.01526795\n",
      "Iteration 8958, loss = 0.01526024\n",
      "Iteration 8959, loss = 0.01525253\n",
      "Iteration 8960, loss = 0.01524483\n",
      "Iteration 8961, loss = 0.01523713\n",
      "Iteration 8962, loss = 0.01522943\n",
      "Iteration 8963, loss = 0.01522173\n",
      "Iteration 8964, loss = 0.01521404\n",
      "Iteration 8965, loss = 0.01520636\n",
      "Iteration 8966, loss = 0.01519867\n",
      "Iteration 8967, loss = 0.01519100\n",
      "Iteration 8968, loss = 0.01518332\n",
      "Iteration 8969, loss = 0.01517565\n",
      "Iteration 8970, loss = 0.01516798\n",
      "Iteration 8971, loss = 0.01516031\n",
      "Iteration 8972, loss = 0.01515265\n",
      "Iteration 8973, loss = 0.01514499\n",
      "Iteration 8974, loss = 0.01513734\n",
      "Iteration 8975, loss = 0.01512969\n",
      "Iteration 8976, loss = 0.01512204\n",
      "Iteration 8977, loss = 0.01511440\n",
      "Iteration 8978, loss = 0.01510675\n",
      "Iteration 8979, loss = 0.01509912\n",
      "Iteration 8980, loss = 0.01509148\n",
      "Iteration 8981, loss = 0.01508385\n",
      "Iteration 8982, loss = 0.01507623\n",
      "Iteration 8983, loss = 0.01506861\n",
      "Iteration 8984, loss = 0.01506099\n",
      "Iteration 8985, loss = 0.01505337\n",
      "Iteration 8986, loss = 0.01504576\n",
      "Iteration 8987, loss = 0.01503815\n",
      "Iteration 8988, loss = 0.01503055\n",
      "Iteration 8989, loss = 0.01502294\n",
      "Iteration 8990, loss = 0.01501535\n",
      "Iteration 8991, loss = 0.01500775\n",
      "Iteration 8992, loss = 0.01500016\n",
      "Iteration 8993, loss = 0.01499257\n",
      "Iteration 8994, loss = 0.01498499\n",
      "Iteration 8995, loss = 0.01497741\n",
      "Iteration 8996, loss = 0.01496983\n",
      "Iteration 8997, loss = 0.01496226\n",
      "Iteration 8998, loss = 0.01495469\n",
      "Iteration 8999, loss = 0.01494712\n",
      "Iteration 9000, loss = 0.01493956\n",
      "Iteration 9001, loss = 0.01493200\n",
      "Iteration 9002, loss = 0.01492444\n",
      "Iteration 9003, loss = 0.01491689\n",
      "Iteration 9004, loss = 0.01490934\n",
      "Iteration 9005, loss = 0.01490180\n",
      "Iteration 9006, loss = 0.01489426\n",
      "Iteration 9007, loss = 0.01488672\n",
      "Iteration 9008, loss = 0.01487918\n",
      "Iteration 9009, loss = 0.01487165\n",
      "Iteration 9010, loss = 0.01486413\n",
      "Iteration 9011, loss = 0.01485660\n",
      "Iteration 9012, loss = 0.01484908\n",
      "Iteration 9013, loss = 0.01484156\n",
      "Iteration 9014, loss = 0.01483405\n",
      "Iteration 9015, loss = 0.01482654\n",
      "Iteration 9016, loss = 0.01481903\n",
      "Iteration 9017, loss = 0.01481153\n",
      "Iteration 9018, loss = 0.01480403\n",
      "Iteration 9019, loss = 0.01479653\n",
      "Iteration 9020, loss = 0.01478904\n",
      "Iteration 9021, loss = 0.01478155\n",
      "Iteration 9022, loss = 0.01477407\n",
      "Iteration 9023, loss = 0.01476658\n",
      "Iteration 9024, loss = 0.01475910\n",
      "Iteration 9025, loss = 0.01475163\n",
      "Iteration 9026, loss = 0.01474416\n",
      "Iteration 9027, loss = 0.01473669\n",
      "Iteration 9028, loss = 0.01472922\n",
      "Iteration 9029, loss = 0.01472176\n",
      "Iteration 9030, loss = 0.01471431\n",
      "Iteration 9031, loss = 0.01470685\n",
      "Iteration 9032, loss = 0.01469940\n",
      "Iteration 9033, loss = 0.01469195\n",
      "Iteration 9034, loss = 0.01468451\n",
      "Iteration 9035, loss = 0.01467707\n",
      "Iteration 9036, loss = 0.01466963\n",
      "Iteration 9037, loss = 0.01466220\n",
      "Iteration 9038, loss = 0.01465477\n",
      "Iteration 9039, loss = 0.01464734\n",
      "Iteration 9040, loss = 0.01463992\n",
      "Iteration 9041, loss = 0.01463250\n",
      "Iteration 9042, loss = 0.01462508\n",
      "Iteration 9043, loss = 0.01461767\n",
      "Iteration 9044, loss = 0.01461026\n",
      "Iteration 9045, loss = 0.01460285\n",
      "Iteration 9046, loss = 0.01459545\n",
      "Iteration 9047, loss = 0.01458805\n",
      "Iteration 9048, loss = 0.01458066\n",
      "Iteration 9049, loss = 0.01457327\n",
      "Iteration 9050, loss = 0.01456588\n",
      "Iteration 9051, loss = 0.01455849\n",
      "Iteration 9052, loss = 0.01455111\n",
      "Iteration 9053, loss = 0.01454373\n",
      "Iteration 9054, loss = 0.01453636\n",
      "Iteration 9055, loss = 0.01452899\n",
      "Iteration 9056, loss = 0.01452162\n",
      "Iteration 9057, loss = 0.01451425\n",
      "Iteration 9058, loss = 0.01450689\n",
      "Iteration 9059, loss = 0.01449953\n",
      "Iteration 9060, loss = 0.01449218\n",
      "Iteration 9061, loss = 0.01448483\n",
      "Iteration 9062, loss = 0.01447748\n",
      "Iteration 9063, loss = 0.01447014\n",
      "Iteration 9064, loss = 0.01446280\n",
      "Iteration 9065, loss = 0.01445546\n",
      "Iteration 9066, loss = 0.01444813\n",
      "Iteration 9067, loss = 0.01444080\n",
      "Iteration 9068, loss = 0.01443347\n",
      "Iteration 9069, loss = 0.01442615\n",
      "Iteration 9070, loss = 0.01441883\n",
      "Iteration 9071, loss = 0.01441151\n",
      "Iteration 9072, loss = 0.01440420\n",
      "Iteration 9073, loss = 0.01439689\n",
      "Iteration 9074, loss = 0.01438958\n",
      "Iteration 9075, loss = 0.01438228\n",
      "Iteration 9076, loss = 0.01437498\n",
      "Iteration 9077, loss = 0.01436768\n",
      "Iteration 9078, loss = 0.01436039\n",
      "Iteration 9079, loss = 0.01435310\n",
      "Iteration 9080, loss = 0.01434582\n",
      "Iteration 9081, loss = 0.01433853\n",
      "Iteration 9082, loss = 0.01433125\n",
      "Iteration 9083, loss = 0.01432398\n",
      "Iteration 9084, loss = 0.01431671\n",
      "Iteration 9085, loss = 0.01430944\n",
      "Iteration 9086, loss = 0.01430217\n",
      "Iteration 9087, loss = 0.01429491\n",
      "Iteration 9088, loss = 0.01428765\n",
      "Iteration 9089, loss = 0.01428040\n",
      "Iteration 9090, loss = 0.01427314\n",
      "Iteration 9091, loss = 0.01426590\n",
      "Iteration 9092, loss = 0.01425865\n",
      "Iteration 9093, loss = 0.01425141\n",
      "Iteration 9094, loss = 0.01424417\n",
      "Iteration 9095, loss = 0.01423694\n",
      "Iteration 9096, loss = 0.01422971\n",
      "Iteration 9097, loss = 0.01422248\n",
      "Iteration 9098, loss = 0.01421525\n",
      "Iteration 9099, loss = 0.01420803\n",
      "Iteration 9100, loss = 0.01420081\n",
      "Iteration 9101, loss = 0.01419360\n",
      "Iteration 9102, loss = 0.01418639\n",
      "Iteration 9103, loss = 0.01417918\n",
      "Iteration 9104, loss = 0.01417197\n",
      "Iteration 9105, loss = 0.01416477\n",
      "Iteration 9106, loss = 0.01415758\n",
      "Iteration 9107, loss = 0.01415038\n",
      "Iteration 9108, loss = 0.01414319\n",
      "Iteration 9109, loss = 0.01413600\n",
      "Iteration 9110, loss = 0.01412882\n",
      "Iteration 9111, loss = 0.01412164\n",
      "Iteration 9112, loss = 0.01411446\n",
      "Iteration 9113, loss = 0.01410729\n",
      "Iteration 9114, loss = 0.01410012\n",
      "Iteration 9115, loss = 0.01409295\n",
      "Iteration 9116, loss = 0.01408578\n",
      "Iteration 9117, loss = 0.01407862\n",
      "Iteration 9118, loss = 0.01407147\n",
      "Iteration 9119, loss = 0.01406431\n",
      "Iteration 9120, loss = 0.01405716\n",
      "Iteration 9121, loss = 0.01405001\n",
      "Iteration 9122, loss = 0.01404287\n",
      "Iteration 9123, loss = 0.01403573\n",
      "Iteration 9124, loss = 0.01402859\n",
      "Iteration 9125, loss = 0.01402146\n",
      "Iteration 9126, loss = 0.01401433\n",
      "Iteration 9127, loss = 0.01400720\n",
      "Iteration 9128, loss = 0.01400007\n",
      "Iteration 9129, loss = 0.01399295\n",
      "Iteration 9130, loss = 0.01398584\n",
      "Iteration 9131, loss = 0.01397872\n",
      "Iteration 9132, loss = 0.01397161\n",
      "Iteration 9133, loss = 0.01396450\n",
      "Iteration 9134, loss = 0.01395740\n",
      "Iteration 9135, loss = 0.01395030\n",
      "Iteration 9136, loss = 0.01394320\n",
      "Iteration 9137, loss = 0.01393611\n",
      "Iteration 9138, loss = 0.01392902\n",
      "Iteration 9139, loss = 0.01392193\n",
      "Iteration 9140, loss = 0.01391485\n",
      "Iteration 9141, loss = 0.01390776\n",
      "Iteration 9142, loss = 0.01390069\n",
      "Iteration 9143, loss = 0.01389361\n",
      "Iteration 9144, loss = 0.01388654\n",
      "Iteration 9145, loss = 0.01387947\n",
      "Iteration 9146, loss = 0.01387241\n",
      "Iteration 9147, loss = 0.01386535\n",
      "Iteration 9148, loss = 0.01385829\n",
      "Iteration 9149, loss = 0.01385124\n",
      "Iteration 9150, loss = 0.01384419\n",
      "Iteration 9151, loss = 0.01383714\n",
      "Iteration 9152, loss = 0.01383009\n",
      "Iteration 9153, loss = 0.01382305\n",
      "Iteration 9154, loss = 0.01381601\n",
      "Iteration 9155, loss = 0.01380898\n",
      "Iteration 9156, loss = 0.01380195\n",
      "Iteration 9157, loss = 0.01379492\n",
      "Iteration 9158, loss = 0.01378790\n",
      "Iteration 9159, loss = 0.01378087\n",
      "Iteration 9160, loss = 0.01377386\n",
      "Iteration 9161, loss = 0.01376684\n",
      "Iteration 9162, loss = 0.01375983\n",
      "Iteration 9163, loss = 0.01375282\n",
      "Iteration 9164, loss = 0.01374582\n",
      "Iteration 9165, loss = 0.01373881\n",
      "Iteration 9166, loss = 0.01373182\n",
      "Iteration 9167, loss = 0.01372482\n",
      "Iteration 9168, loss = 0.01371783\n",
      "Iteration 9169, loss = 0.01371084\n",
      "Iteration 9170, loss = 0.01370385\n",
      "Iteration 9171, loss = 0.01369687\n",
      "Iteration 9172, loss = 0.01368989\n",
      "Iteration 9173, loss = 0.01368292\n",
      "Iteration 9174, loss = 0.01367595\n",
      "Iteration 9175, loss = 0.01366898\n",
      "Iteration 9176, loss = 0.01366201\n",
      "Iteration 9177, loss = 0.01365505\n",
      "Iteration 9178, loss = 0.01364809\n",
      "Iteration 9179, loss = 0.01364113\n",
      "Iteration 9180, loss = 0.01363418\n",
      "Iteration 9181, loss = 0.01362723\n",
      "Iteration 9182, loss = 0.01362028\n",
      "Iteration 9183, loss = 0.01361334\n",
      "Iteration 9184, loss = 0.01360640\n",
      "Iteration 9185, loss = 0.01359946\n",
      "Iteration 9186, loss = 0.01359253\n",
      "Iteration 9187, loss = 0.01358560\n",
      "Iteration 9188, loss = 0.01357867\n",
      "Iteration 9189, loss = 0.01357175\n",
      "Iteration 9190, loss = 0.01356483\n",
      "Iteration 9191, loss = 0.01355791\n",
      "Iteration 9192, loss = 0.01355100\n",
      "Iteration 9193, loss = 0.01354409\n",
      "Iteration 9194, loss = 0.01353718\n",
      "Iteration 9195, loss = 0.01353028\n",
      "Iteration 9196, loss = 0.01352337\n",
      "Iteration 9197, loss = 0.01351648\n",
      "Iteration 9198, loss = 0.01350958\n",
      "Iteration 9199, loss = 0.01350269\n",
      "Iteration 9200, loss = 0.01349580\n",
      "Iteration 9201, loss = 0.01348892\n",
      "Iteration 9202, loss = 0.01348204\n",
      "Iteration 9203, loss = 0.01347516\n",
      "Iteration 9204, loss = 0.01346828\n",
      "Iteration 9205, loss = 0.01346141\n",
      "Iteration 9206, loss = 0.01345454\n",
      "Iteration 9207, loss = 0.01344767\n",
      "Iteration 9208, loss = 0.01344081\n",
      "Iteration 9209, loss = 0.01343395\n",
      "Iteration 9210, loss = 0.01342710\n",
      "Iteration 9211, loss = 0.01342024\n",
      "Iteration 9212, loss = 0.01341339\n",
      "Iteration 9213, loss = 0.01340655\n",
      "Iteration 9214, loss = 0.01339970\n",
      "Iteration 9215, loss = 0.01339286\n",
      "Iteration 9216, loss = 0.01338603\n",
      "Iteration 9217, loss = 0.01337919\n",
      "Iteration 9218, loss = 0.01337236\n",
      "Iteration 9219, loss = 0.01336554\n",
      "Iteration 9220, loss = 0.01335871\n",
      "Iteration 9221, loss = 0.01335189\n",
      "Iteration 9222, loss = 0.01334507\n",
      "Iteration 9223, loss = 0.01333826\n",
      "Iteration 9224, loss = 0.01333145\n",
      "Iteration 9225, loss = 0.01332464\n",
      "Iteration 9226, loss = 0.01331783\n",
      "Iteration 9227, loss = 0.01331103\n",
      "Iteration 9228, loss = 0.01330423\n",
      "Iteration 9229, loss = 0.01329744\n",
      "Iteration 9230, loss = 0.01329064\n",
      "Iteration 9231, loss = 0.01328386\n",
      "Iteration 9232, loss = 0.01327707\n",
      "Iteration 9233, loss = 0.01327029\n",
      "Iteration 9234, loss = 0.01326351\n",
      "Iteration 9235, loss = 0.01325673\n",
      "Iteration 9236, loss = 0.01324996\n",
      "Iteration 9237, loss = 0.01324319\n",
      "Iteration 9238, loss = 0.01323642\n",
      "Iteration 9239, loss = 0.01322965\n",
      "Iteration 9240, loss = 0.01322289\n",
      "Iteration 9241, loss = 0.01321614\n",
      "Iteration 9242, loss = 0.01320938\n",
      "Iteration 9243, loss = 0.01320263\n",
      "Iteration 9244, loss = 0.01319588\n",
      "Iteration 9245, loss = 0.01318914\n",
      "Iteration 9246, loss = 0.01318240\n",
      "Iteration 9247, loss = 0.01317566\n",
      "Iteration 9248, loss = 0.01316892\n",
      "Iteration 9249, loss = 0.01316219\n",
      "Iteration 9250, loss = 0.01315546\n",
      "Iteration 9251, loss = 0.01314873\n",
      "Iteration 9252, loss = 0.01314201\n",
      "Iteration 9253, loss = 0.01313529\n",
      "Iteration 9254, loss = 0.01312857\n",
      "Iteration 9255, loss = 0.01312186\n",
      "Iteration 9256, loss = 0.01311515\n",
      "Iteration 9257, loss = 0.01310844\n",
      "Iteration 9258, loss = 0.01310174\n",
      "Iteration 9259, loss = 0.01309503\n",
      "Iteration 9260, loss = 0.01308834\n",
      "Iteration 9261, loss = 0.01308164\n",
      "Iteration 9262, loss = 0.01307495\n",
      "Iteration 9263, loss = 0.01306826\n",
      "Iteration 9264, loss = 0.01306157\n",
      "Iteration 9265, loss = 0.01305489\n",
      "Iteration 9266, loss = 0.01304821\n",
      "Iteration 9267, loss = 0.01304154\n",
      "Iteration 9268, loss = 0.01303486\n",
      "Iteration 9269, loss = 0.01302819\n",
      "Iteration 9270, loss = 0.01302153\n",
      "Iteration 9271, loss = 0.01301486\n",
      "Iteration 9272, loss = 0.01300820\n",
      "Iteration 9273, loss = 0.01300154\n",
      "Iteration 9274, loss = 0.01299489\n",
      "Iteration 9275, loss = 0.01298824\n",
      "Iteration 9276, loss = 0.01298159\n",
      "Iteration 9277, loss = 0.01297494\n",
      "Iteration 9278, loss = 0.01296830\n",
      "Iteration 9279, loss = 0.01296166\n",
      "Iteration 9280, loss = 0.01295502\n",
      "Iteration 9281, loss = 0.01294839\n",
      "Iteration 9282, loss = 0.01294176\n",
      "Iteration 9283, loss = 0.01293513\n",
      "Iteration 9284, loss = 0.01292851\n",
      "Iteration 9285, loss = 0.01292189\n",
      "Iteration 9286, loss = 0.01291527\n",
      "Iteration 9287, loss = 0.01290866\n",
      "Iteration 9288, loss = 0.01290205\n",
      "Iteration 9289, loss = 0.01289544\n",
      "Iteration 9290, loss = 0.01288883\n",
      "Iteration 9291, loss = 0.01288223\n",
      "Iteration 9292, loss = 0.01287563\n",
      "Iteration 9293, loss = 0.01286903\n",
      "Iteration 9294, loss = 0.01286244\n",
      "Iteration 9295, loss = 0.01285585\n",
      "Iteration 9296, loss = 0.01284926\n",
      "Iteration 9297, loss = 0.01284268\n",
      "Iteration 9298, loss = 0.01283610\n",
      "Iteration 9299, loss = 0.01282952\n",
      "Iteration 9300, loss = 0.01282294\n",
      "Iteration 9301, loss = 0.01281637\n",
      "Iteration 9302, loss = 0.01280980\n",
      "Iteration 9303, loss = 0.01280324\n",
      "Iteration 9304, loss = 0.01279667\n",
      "Iteration 9305, loss = 0.01279012\n",
      "Iteration 9306, loss = 0.01278356\n",
      "Iteration 9307, loss = 0.01277700\n",
      "Iteration 9308, loss = 0.01277045\n",
      "Iteration 9309, loss = 0.01276391\n",
      "Iteration 9310, loss = 0.01275736\n",
      "Iteration 9311, loss = 0.01275082\n",
      "Iteration 9312, loss = 0.01274428\n",
      "Iteration 9313, loss = 0.01273775\n",
      "Iteration 9314, loss = 0.01273121\n",
      "Iteration 9315, loss = 0.01272468\n",
      "Iteration 9316, loss = 0.01271816\n",
      "Iteration 9317, loss = 0.01271163\n",
      "Iteration 9318, loss = 0.01270511\n",
      "Iteration 9319, loss = 0.01269860\n",
      "Iteration 9320, loss = 0.01269208\n",
      "Iteration 9321, loss = 0.01268557\n",
      "Iteration 9322, loss = 0.01267906\n",
      "Iteration 9323, loss = 0.01267256\n",
      "Iteration 9324, loss = 0.01266605\n",
      "Iteration 9325, loss = 0.01265956\n",
      "Iteration 9326, loss = 0.01265306\n",
      "Iteration 9327, loss = 0.01264657\n",
      "Iteration 9328, loss = 0.01264008\n",
      "Iteration 9329, loss = 0.01263359\n",
      "Iteration 9330, loss = 0.01262710\n",
      "Iteration 9331, loss = 0.01262062\n",
      "Iteration 9332, loss = 0.01261414\n",
      "Iteration 9333, loss = 0.01260767\n",
      "Iteration 9334, loss = 0.01260120\n",
      "Iteration 9335, loss = 0.01259473\n",
      "Iteration 9336, loss = 0.01258826\n",
      "Iteration 9337, loss = 0.01258180\n",
      "Iteration 9338, loss = 0.01257534\n",
      "Iteration 9339, loss = 0.01256888\n",
      "Iteration 9340, loss = 0.01256242\n",
      "Iteration 9341, loss = 0.01255597\n",
      "Iteration 9342, loss = 0.01254952\n",
      "Iteration 9343, loss = 0.01254308\n",
      "Iteration 9344, loss = 0.01253664\n",
      "Iteration 9345, loss = 0.01253020\n",
      "Iteration 9346, loss = 0.01252376\n",
      "Iteration 9347, loss = 0.01251733\n",
      "Iteration 9348, loss = 0.01251090\n",
      "Iteration 9349, loss = 0.01250447\n",
      "Iteration 9350, loss = 0.01249804\n",
      "Iteration 9351, loss = 0.01249162\n",
      "Iteration 9352, loss = 0.01248520\n",
      "Iteration 9353, loss = 0.01247879\n",
      "Iteration 9354, loss = 0.01247237\n",
      "Iteration 9355, loss = 0.01246596\n",
      "Iteration 9356, loss = 0.01245956\n",
      "Iteration 9357, loss = 0.01245315\n",
      "Iteration 9358, loss = 0.01244675\n",
      "Iteration 9359, loss = 0.01244035\n",
      "Iteration 9360, loss = 0.01243396\n",
      "Iteration 9361, loss = 0.01242756\n",
      "Iteration 9362, loss = 0.01242117\n",
      "Iteration 9363, loss = 0.01241479\n",
      "Iteration 9364, loss = 0.01240840\n",
      "Iteration 9365, loss = 0.01240202\n",
      "Iteration 9366, loss = 0.01239565\n",
      "Iteration 9367, loss = 0.01238927\n",
      "Iteration 9368, loss = 0.01238290\n",
      "Iteration 9369, loss = 0.01237653\n",
      "Iteration 9370, loss = 0.01237016\n",
      "Iteration 9371, loss = 0.01236380\n",
      "Iteration 9372, loss = 0.01235744\n",
      "Iteration 9373, loss = 0.01235108\n",
      "Iteration 9374, loss = 0.01234473\n",
      "Iteration 9375, loss = 0.01233838\n",
      "Iteration 9376, loss = 0.01233203\n",
      "Iteration 9377, loss = 0.01232568\n",
      "Iteration 9378, loss = 0.01231934\n",
      "Iteration 9379, loss = 0.01231300\n",
      "Iteration 9380, loss = 0.01230666\n",
      "Iteration 9381, loss = 0.01230033\n",
      "Iteration 9382, loss = 0.01229400\n",
      "Iteration 9383, loss = 0.01228767\n",
      "Iteration 9384, loss = 0.01228134\n",
      "Iteration 9385, loss = 0.01227502\n",
      "Iteration 9386, loss = 0.01226870\n",
      "Iteration 9387, loss = 0.01226238\n",
      "Iteration 9388, loss = 0.01225607\n",
      "Iteration 9389, loss = 0.01224976\n",
      "Iteration 9390, loss = 0.01224345\n",
      "Iteration 9391, loss = 0.01223715\n",
      "Iteration 9392, loss = 0.01223084\n",
      "Iteration 9393, loss = 0.01222455\n",
      "Iteration 9394, loss = 0.01221825\n",
      "Iteration 9395, loss = 0.01221196\n",
      "Iteration 9396, loss = 0.01220566\n",
      "Iteration 9397, loss = 0.01219938\n",
      "Iteration 9398, loss = 0.01219309\n",
      "Iteration 9399, loss = 0.01218681\n",
      "Iteration 9400, loss = 0.01218053\n",
      "Iteration 9401, loss = 0.01217425\n",
      "Iteration 9402, loss = 0.01216798\n",
      "Iteration 9403, loss = 0.01216171\n",
      "Iteration 9404, loss = 0.01215544\n",
      "Iteration 9405, loss = 0.01214918\n",
      "Iteration 9406, loss = 0.01214291\n",
      "Iteration 9407, loss = 0.01213666\n",
      "Iteration 9408, loss = 0.01213040\n",
      "Iteration 9409, loss = 0.01212415\n",
      "Iteration 9410, loss = 0.01211789\n",
      "Iteration 9411, loss = 0.01211165\n",
      "Iteration 9412, loss = 0.01210540\n",
      "Iteration 9413, loss = 0.01209916\n",
      "Iteration 9414, loss = 0.01209292\n",
      "Iteration 9415, loss = 0.01208668\n",
      "Iteration 9416, loss = 0.01208045\n",
      "Iteration 9417, loss = 0.01207422\n",
      "Iteration 9418, loss = 0.01206799\n",
      "Iteration 9419, loss = 0.01206177\n",
      "Iteration 9420, loss = 0.01205554\n",
      "Iteration 9421, loss = 0.01204932\n",
      "Iteration 9422, loss = 0.01204311\n",
      "Iteration 9423, loss = 0.01203689\n",
      "Iteration 9424, loss = 0.01203068\n",
      "Iteration 9425, loss = 0.01202447\n",
      "Iteration 9426, loss = 0.01201827\n",
      "Iteration 9427, loss = 0.01201207\n",
      "Iteration 9428, loss = 0.01200587\n",
      "Iteration 9429, loss = 0.01199967\n",
      "Iteration 9430, loss = 0.01199347\n",
      "Iteration 9431, loss = 0.01198728\n",
      "Iteration 9432, loss = 0.01198110\n",
      "Iteration 9433, loss = 0.01197491\n",
      "Iteration 9434, loss = 0.01196873\n",
      "Iteration 9435, loss = 0.01196255\n",
      "Iteration 9436, loss = 0.01195637\n",
      "Iteration 9437, loss = 0.01195019\n",
      "Iteration 9438, loss = 0.01194402\n",
      "Iteration 9439, loss = 0.01193785\n",
      "Iteration 9440, loss = 0.01193169\n",
      "Iteration 9441, loss = 0.01192552\n",
      "Iteration 9442, loss = 0.01191936\n",
      "Iteration 9443, loss = 0.01191321\n",
      "Iteration 9444, loss = 0.01190705\n",
      "Iteration 9445, loss = 0.01190090\n",
      "Iteration 9446, loss = 0.01189475\n",
      "Iteration 9447, loss = 0.01188860\n",
      "Iteration 9448, loss = 0.01188246\n",
      "Iteration 9449, loss = 0.01187632\n",
      "Iteration 9450, loss = 0.01187018\n",
      "Iteration 9451, loss = 0.01186404\n",
      "Iteration 9452, loss = 0.01185791\n",
      "Iteration 9453, loss = 0.01185178\n",
      "Iteration 9454, loss = 0.01184565\n",
      "Iteration 9455, loss = 0.01183953\n",
      "Iteration 9456, loss = 0.01183341\n",
      "Iteration 9457, loss = 0.01182729\n",
      "Iteration 9458, loss = 0.01182117\n",
      "Iteration 9459, loss = 0.01181506\n",
      "Iteration 9460, loss = 0.01180895\n",
      "Iteration 9461, loss = 0.01180284\n",
      "Iteration 9462, loss = 0.01179674\n",
      "Iteration 9463, loss = 0.01179063\n",
      "Iteration 9464, loss = 0.01178453\n",
      "Iteration 9465, loss = 0.01177844\n",
      "Iteration 9466, loss = 0.01177234\n",
      "Iteration 9467, loss = 0.01176625\n",
      "Iteration 9468, loss = 0.01176016\n",
      "Iteration 9469, loss = 0.01175408\n",
      "Iteration 9470, loss = 0.01174799\n",
      "Iteration 9471, loss = 0.01174191\n",
      "Iteration 9472, loss = 0.01173583\n",
      "Iteration 9473, loss = 0.01172976\n",
      "Iteration 9474, loss = 0.01172369\n",
      "Iteration 9475, loss = 0.01171762\n",
      "Iteration 9476, loss = 0.01171155\n",
      "Iteration 9477, loss = 0.01170549\n",
      "Iteration 9478, loss = 0.01169943\n",
      "Iteration 9479, loss = 0.01169337\n",
      "Iteration 9480, loss = 0.01168731\n",
      "Iteration 9481, loss = 0.01168126\n",
      "Iteration 9482, loss = 0.01167521\n",
      "Iteration 9483, loss = 0.01166916\n",
      "Iteration 9484, loss = 0.01166311\n",
      "Iteration 9485, loss = 0.01165707\n",
      "Iteration 9486, loss = 0.01165103\n",
      "Iteration 9487, loss = 0.01164500\n",
      "Iteration 9488, loss = 0.01163896\n",
      "Iteration 9489, loss = 0.01163293\n",
      "Iteration 9490, loss = 0.01162690\n",
      "Iteration 9491, loss = 0.01162087\n",
      "Iteration 9492, loss = 0.01161485\n",
      "Iteration 9493, loss = 0.01160883\n",
      "Iteration 9494, loss = 0.01160281\n",
      "Iteration 9495, loss = 0.01159680\n",
      "Iteration 9496, loss = 0.01159078\n",
      "Iteration 9497, loss = 0.01158477\n",
      "Iteration 9498, loss = 0.01157877\n",
      "Iteration 9499, loss = 0.01157276\n",
      "Iteration 9500, loss = 0.01156676\n",
      "Iteration 9501, loss = 0.01156076\n",
      "Iteration 9502, loss = 0.01155476\n",
      "Iteration 9503, loss = 0.01154877\n",
      "Iteration 9504, loss = 0.01154278\n",
      "Iteration 9505, loss = 0.01153679\n",
      "Iteration 9506, loss = 0.01153080\n",
      "Iteration 9507, loss = 0.01152482\n",
      "Iteration 9508, loss = 0.01151884\n",
      "Iteration 9509, loss = 0.01151286\n",
      "Iteration 9510, loss = 0.01150689\n",
      "Iteration 9511, loss = 0.01150091\n",
      "Iteration 9512, loss = 0.01149495\n",
      "Iteration 9513, loss = 0.01148898\n",
      "Iteration 9514, loss = 0.01148301\n",
      "Iteration 9515, loss = 0.01147705\n",
      "Iteration 9516, loss = 0.01147109\n",
      "Iteration 9517, loss = 0.01146514\n",
      "Iteration 9518, loss = 0.01145918\n",
      "Iteration 9519, loss = 0.01145323\n",
      "Iteration 9520, loss = 0.01144728\n",
      "Iteration 9521, loss = 0.01144133\n",
      "Iteration 9522, loss = 0.01143539\n",
      "Iteration 9523, loss = 0.01142945\n",
      "Iteration 9524, loss = 0.01142351\n",
      "Iteration 9525, loss = 0.01141758\n",
      "Iteration 9526, loss = 0.01141164\n",
      "Iteration 9527, loss = 0.01140571\n",
      "Iteration 9528, loss = 0.01139979\n",
      "Iteration 9529, loss = 0.01139386\n",
      "Iteration 9530, loss = 0.01138794\n",
      "Iteration 9531, loss = 0.01138202\n",
      "Iteration 9532, loss = 0.01137610\n",
      "Iteration 9533, loss = 0.01137019\n",
      "Iteration 9534, loss = 0.01136428\n",
      "Iteration 9535, loss = 0.01135837\n",
      "Iteration 9536, loss = 0.01135246\n",
      "Iteration 9537, loss = 0.01134656\n",
      "Iteration 9538, loss = 0.01134065\n",
      "Iteration 9539, loss = 0.01133476\n",
      "Iteration 9540, loss = 0.01132886\n",
      "Iteration 9541, loss = 0.01132297\n",
      "Iteration 9542, loss = 0.01131707\n",
      "Iteration 9543, loss = 0.01131119\n",
      "Iteration 9544, loss = 0.01130530\n",
      "Iteration 9545, loss = 0.01129942\n",
      "Iteration 9546, loss = 0.01129354\n",
      "Iteration 9547, loss = 0.01128766\n",
      "Iteration 9548, loss = 0.01128178\n",
      "Iteration 9549, loss = 0.01127591\n",
      "Iteration 9550, loss = 0.01127004\n",
      "Iteration 9551, loss = 0.01126417\n",
      "Iteration 9552, loss = 0.01125831\n",
      "Iteration 9553, loss = 0.01125244\n",
      "Iteration 9554, loss = 0.01124658\n",
      "Iteration 9555, loss = 0.01124073\n",
      "Iteration 9556, loss = 0.01123487\n",
      "Iteration 9557, loss = 0.01122902\n",
      "Iteration 9558, loss = 0.01122317\n",
      "Iteration 9559, loss = 0.01121732\n",
      "Iteration 9560, loss = 0.01121148\n",
      "Iteration 9561, loss = 0.01120564\n",
      "Iteration 9562, loss = 0.01119980\n",
      "Iteration 9563, loss = 0.01119396\n",
      "Iteration 9564, loss = 0.01118813\n",
      "Iteration 9565, loss = 0.01118230\n",
      "Iteration 9566, loss = 0.01117647\n",
      "Iteration 9567, loss = 0.01117064\n",
      "Iteration 9568, loss = 0.01116482\n",
      "Iteration 9569, loss = 0.01115900\n",
      "Iteration 9570, loss = 0.01115318\n",
      "Iteration 9571, loss = 0.01114736\n",
      "Iteration 9572, loss = 0.01114155\n",
      "Iteration 9573, loss = 0.01113574\n",
      "Iteration 9574, loss = 0.01112993\n",
      "Iteration 9575, loss = 0.01112412\n",
      "Iteration 9576, loss = 0.01111832\n",
      "Iteration 9577, loss = 0.01111252\n",
      "Iteration 9578, loss = 0.01110672\n",
      "Iteration 9579, loss = 0.01110092\n",
      "Iteration 9580, loss = 0.01109513\n",
      "Iteration 9581, loss = 0.01108934\n",
      "Iteration 9582, loss = 0.01108355\n",
      "Iteration 9583, loss = 0.01107776\n",
      "Iteration 9584, loss = 0.01107198\n",
      "Iteration 9585, loss = 0.01106620\n",
      "Iteration 9586, loss = 0.01106042\n",
      "Iteration 9587, loss = 0.01105465\n",
      "Iteration 9588, loss = 0.01104887\n",
      "Iteration 9589, loss = 0.01104310\n",
      "Iteration 9590, loss = 0.01103733\n",
      "Iteration 9591, loss = 0.01103157\n",
      "Iteration 9592, loss = 0.01102581\n",
      "Iteration 9593, loss = 0.01102005\n",
      "Iteration 9594, loss = 0.01101429\n",
      "Iteration 9595, loss = 0.01100853\n",
      "Iteration 9596, loss = 0.01100278\n",
      "Iteration 9597, loss = 0.01099703\n",
      "Iteration 9598, loss = 0.01099128\n",
      "Iteration 9599, loss = 0.01098554\n",
      "Iteration 9600, loss = 0.01097979\n",
      "Iteration 9601, loss = 0.01097405\n",
      "Iteration 9602, loss = 0.01096831\n",
      "Iteration 9603, loss = 0.01096258\n",
      "Iteration 9604, loss = 0.01095685\n",
      "Iteration 9605, loss = 0.01095112\n",
      "Iteration 9606, loss = 0.01094539\n",
      "Iteration 9607, loss = 0.01093966\n",
      "Iteration 9608, loss = 0.01093394\n",
      "Iteration 9609, loss = 0.01092822\n",
      "Iteration 9610, loss = 0.01092250\n",
      "Iteration 9611, loss = 0.01091678\n",
      "Iteration 9612, loss = 0.01091107\n",
      "Iteration 9613, loss = 0.01090536\n",
      "Iteration 9614, loss = 0.01089965\n",
      "Iteration 9615, loss = 0.01089395\n",
      "Iteration 9616, loss = 0.01088824\n",
      "Iteration 9617, loss = 0.01088254\n",
      "Iteration 9618, loss = 0.01087685\n",
      "Iteration 9619, loss = 0.01087115\n",
      "Iteration 9620, loss = 0.01086546\n",
      "Iteration 9621, loss = 0.01085977\n",
      "Iteration 9622, loss = 0.01085408\n",
      "Iteration 9623, loss = 0.01084839\n",
      "Iteration 9624, loss = 0.01084271\n",
      "Iteration 9625, loss = 0.01083703\n",
      "Iteration 9626, loss = 0.01083135\n",
      "Iteration 9627, loss = 0.01082567\n",
      "Iteration 9628, loss = 0.01082000\n",
      "Iteration 9629, loss = 0.01081433\n",
      "Iteration 9630, loss = 0.01080866\n",
      "Iteration 9631, loss = 0.01080299\n",
      "Iteration 9632, loss = 0.01079733\n",
      "Iteration 9633, loss = 0.01079167\n",
      "Iteration 9634, loss = 0.01078601\n",
      "Iteration 9635, loss = 0.01078035\n",
      "Iteration 9636, loss = 0.01077470\n",
      "Iteration 9637, loss = 0.01076905\n",
      "Iteration 9638, loss = 0.01076340\n",
      "Iteration 9639, loss = 0.01075775\n",
      "Iteration 9640, loss = 0.01075211\n",
      "Iteration 9641, loss = 0.01074647\n",
      "Iteration 9642, loss = 0.01074083\n",
      "Iteration 9643, loss = 0.01073519\n",
      "Iteration 9644, loss = 0.01072956\n",
      "Iteration 9645, loss = 0.01072393\n",
      "Iteration 9646, loss = 0.01071830\n",
      "Iteration 9647, loss = 0.01071267\n",
      "Iteration 9648, loss = 0.01070705\n",
      "Iteration 9649, loss = 0.01070142\n",
      "Iteration 9650, loss = 0.01069580\n",
      "Iteration 9651, loss = 0.01069019\n",
      "Iteration 9652, loss = 0.01068457\n",
      "Iteration 9653, loss = 0.01067896\n",
      "Iteration 9654, loss = 0.01067335\n",
      "Iteration 9655, loss = 0.01066774\n",
      "Iteration 9656, loss = 0.01066214\n",
      "Iteration 9657, loss = 0.01065653\n",
      "Iteration 9658, loss = 0.01065093\n",
      "Iteration 9659, loss = 0.01064534\n",
      "Iteration 9660, loss = 0.01063974\n",
      "Iteration 9661, loss = 0.01063415\n",
      "Iteration 9662, loss = 0.01062856\n",
      "Iteration 9663, loss = 0.01062297\n",
      "Iteration 9664, loss = 0.01061738\n",
      "Iteration 9665, loss = 0.01061180\n",
      "Iteration 9666, loss = 0.01060622\n",
      "Iteration 9667, loss = 0.01060064\n",
      "Iteration 9668, loss = 0.01059506\n",
      "Iteration 9669, loss = 0.01058949\n",
      "Iteration 9670, loss = 0.01058392\n",
      "Iteration 9671, loss = 0.01057835\n",
      "Iteration 9672, loss = 0.01057278\n",
      "Iteration 9673, loss = 0.01056722\n",
      "Iteration 9674, loss = 0.01056166\n",
      "Iteration 9675, loss = 0.01055610\n",
      "Iteration 9676, loss = 0.01055054\n",
      "Iteration 9677, loss = 0.01054499\n",
      "Iteration 9678, loss = 0.01053943\n",
      "Iteration 9679, loss = 0.01053389\n",
      "Iteration 9680, loss = 0.01052834\n",
      "Iteration 9681, loss = 0.01052279\n",
      "Iteration 9682, loss = 0.01051725\n",
      "Iteration 9683, loss = 0.01051171\n",
      "Iteration 9684, loss = 0.01050617\n",
      "Iteration 9685, loss = 0.01050064\n",
      "Iteration 9686, loss = 0.01049510\n",
      "Iteration 9687, loss = 0.01048957\n",
      "Iteration 9688, loss = 0.01048404\n",
      "Iteration 9689, loss = 0.01047852\n",
      "Iteration 9690, loss = 0.01047300\n",
      "Iteration 9691, loss = 0.01046747\n",
      "Iteration 9692, loss = 0.01046196\n",
      "Iteration 9693, loss = 0.01045644\n",
      "Iteration 9694, loss = 0.01045093\n",
      "Iteration 9695, loss = 0.01044541\n",
      "Iteration 9696, loss = 0.01043990\n",
      "Iteration 9697, loss = 0.01043440\n",
      "Iteration 9698, loss = 0.01042889\n",
      "Iteration 9699, loss = 0.01042339\n",
      "Iteration 9700, loss = 0.01041789\n",
      "Iteration 9701, loss = 0.01041239\n",
      "Iteration 9702, loss = 0.01040690\n",
      "Iteration 9703, loss = 0.01040141\n",
      "Iteration 9704, loss = 0.01039592\n",
      "Iteration 9705, loss = 0.01039043\n",
      "Iteration 9706, loss = 0.01038494\n",
      "Iteration 9707, loss = 0.01037946\n",
      "Iteration 9708, loss = 0.01037398\n",
      "Iteration 9709, loss = 0.01036850\n",
      "Iteration 9710, loss = 0.01036302\n",
      "Iteration 9711, loss = 0.01035755\n",
      "Iteration 9712, loss = 0.01035208\n",
      "Iteration 9713, loss = 0.01034661\n",
      "Iteration 9714, loss = 0.01034114\n",
      "Iteration 9715, loss = 0.01033568\n",
      "Iteration 9716, loss = 0.01033022\n",
      "Iteration 9717, loss = 0.01032476\n",
      "Iteration 9718, loss = 0.01031930\n",
      "Iteration 9719, loss = 0.01031385\n",
      "Iteration 9720, loss = 0.01030839\n",
      "Iteration 9721, loss = 0.01030294\n",
      "Iteration 9722, loss = 0.01029750\n",
      "Iteration 9723, loss = 0.01029205\n",
      "Iteration 9724, loss = 0.01028661\n",
      "Iteration 9725, loss = 0.01028117\n",
      "Iteration 9726, loss = 0.01027573\n",
      "Iteration 9727, loss = 0.01027029\n",
      "Iteration 9728, loss = 0.01026486\n",
      "Iteration 9729, loss = 0.01025943\n",
      "Iteration 9730, loss = 0.01025400\n",
      "Iteration 9731, loss = 0.01024857\n",
      "Iteration 9732, loss = 0.01024315\n",
      "Iteration 9733, loss = 0.01023773\n",
      "Iteration 9734, loss = 0.01023231\n",
      "Iteration 9735, loss = 0.01022689\n",
      "Iteration 9736, loss = 0.01022148\n",
      "Iteration 9737, loss = 0.01021606\n",
      "Iteration 9738, loss = 0.01021065\n",
      "Iteration 9739, loss = 0.01020525\n",
      "Iteration 9740, loss = 0.01019984\n",
      "Iteration 9741, loss = 0.01019444\n",
      "Iteration 9742, loss = 0.01018904\n",
      "Iteration 9743, loss = 0.01018364\n",
      "Iteration 9744, loss = 0.01017824\n",
      "Iteration 9745, loss = 0.01017285\n",
      "Iteration 9746, loss = 0.01016746\n",
      "Iteration 9747, loss = 0.01016207\n",
      "Iteration 9748, loss = 0.01015668\n",
      "Iteration 9749, loss = 0.01015130\n",
      "Iteration 9750, loss = 0.01014591\n",
      "Iteration 9751, loss = 0.01014053\n",
      "Iteration 9752, loss = 0.01013516\n",
      "Iteration 9753, loss = 0.01012978\n",
      "Iteration 9754, loss = 0.01012441\n",
      "Iteration 9755, loss = 0.01011904\n",
      "Iteration 9756, loss = 0.01011367\n",
      "Iteration 9757, loss = 0.01010830\n",
      "Iteration 9758, loss = 0.01010294\n",
      "Iteration 9759, loss = 0.01009758\n",
      "Iteration 9760, loss = 0.01009222\n",
      "Iteration 9761, loss = 0.01008687\n",
      "Iteration 9762, loss = 0.01008151\n",
      "Iteration 9763, loss = 0.01007616\n",
      "Iteration 9764, loss = 0.01007081\n",
      "Iteration 9765, loss = 0.01006546\n",
      "Iteration 9766, loss = 0.01006012\n",
      "Iteration 9767, loss = 0.01005478\n",
      "Iteration 9768, loss = 0.01004944\n",
      "Iteration 9769, loss = 0.01004410\n",
      "Iteration 9770, loss = 0.01003876\n",
      "Iteration 9771, loss = 0.01003343\n",
      "Iteration 9772, loss = 0.01002810\n",
      "Iteration 9773, loss = 0.01002277\n",
      "Iteration 9774, loss = 0.01001744\n",
      "Iteration 9775, loss = 0.01001212\n",
      "Iteration 9776, loss = 0.01000680\n",
      "Iteration 9777, loss = 0.01000148\n",
      "Iteration 9778, loss = 0.00999616\n",
      "Iteration 9779, loss = 0.00999085\n",
      "Iteration 9780, loss = 0.00998554\n",
      "Iteration 9781, loss = 0.00998023\n",
      "Iteration 9782, loss = 0.00997492\n",
      "Iteration 9783, loss = 0.00996961\n",
      "Iteration 9784, loss = 0.00996431\n",
      "Iteration 9785, loss = 0.00995901\n",
      "Iteration 9786, loss = 0.00995371\n",
      "Iteration 9787, loss = 0.00994842\n",
      "Iteration 9788, loss = 0.00994312\n",
      "Iteration 9789, loss = 0.00993783\n",
      "Iteration 9790, loss = 0.00993254\n",
      "Iteration 9791, loss = 0.00992726\n",
      "Iteration 9792, loss = 0.00992197\n",
      "Iteration 9793, loss = 0.00991669\n",
      "Iteration 9794, loss = 0.00991141\n",
      "Iteration 9795, loss = 0.00990613\n",
      "Iteration 9796, loss = 0.00990086\n",
      "Iteration 9797, loss = 0.00989559\n",
      "Iteration 9798, loss = 0.00989031\n",
      "Iteration 9799, loss = 0.00988505\n",
      "Iteration 9800, loss = 0.00987978\n",
      "Iteration 9801, loss = 0.00987452\n",
      "Iteration 9802, loss = 0.00986926\n",
      "Iteration 9803, loss = 0.00986400\n",
      "Iteration 9804, loss = 0.00985874\n",
      "Iteration 9805, loss = 0.00985349\n",
      "Iteration 9806, loss = 0.00984824\n",
      "Iteration 9807, loss = 0.00984299\n",
      "Iteration 9808, loss = 0.00983774\n",
      "Iteration 9809, loss = 0.00983249\n",
      "Iteration 9810, loss = 0.00982725\n",
      "Iteration 9811, loss = 0.00982201\n",
      "Iteration 9812, loss = 0.00981677\n",
      "Iteration 9813, loss = 0.00981154\n",
      "Iteration 9814, loss = 0.00980630\n",
      "Iteration 9815, loss = 0.00980107\n",
      "Iteration 9816, loss = 0.00979584\n",
      "Iteration 9817, loss = 0.00979062\n",
      "Iteration 9818, loss = 0.00978539\n",
      "Iteration 9819, loss = 0.00978017\n",
      "Iteration 9820, loss = 0.00977495\n",
      "Iteration 9821, loss = 0.00976974\n",
      "Iteration 9822, loss = 0.00976452\n",
      "Iteration 9823, loss = 0.00975931\n",
      "Iteration 9824, loss = 0.00975410\n",
      "Iteration 9825, loss = 0.00974889\n",
      "Iteration 9826, loss = 0.00974369\n",
      "Iteration 9827, loss = 0.00973848\n",
      "Iteration 9828, loss = 0.00973328\n",
      "Iteration 9829, loss = 0.00972808\n",
      "Iteration 9830, loss = 0.00972289\n",
      "Iteration 9831, loss = 0.00971769\n",
      "Iteration 9832, loss = 0.00971250\n",
      "Iteration 9833, loss = 0.00970731\n",
      "Iteration 9834, loss = 0.00970212\n",
      "Iteration 9835, loss = 0.00969694\n",
      "Iteration 9836, loss = 0.00969176\n",
      "Iteration 9837, loss = 0.00968658\n",
      "Iteration 9838, loss = 0.00968140\n",
      "Iteration 9839, loss = 0.00967622\n",
      "Iteration 9840, loss = 0.00967105\n",
      "Iteration 9841, loss = 0.00966588\n",
      "Iteration 9842, loss = 0.00966071\n",
      "Iteration 9843, loss = 0.00965555\n",
      "Iteration 9844, loss = 0.00965038\n",
      "Iteration 9845, loss = 0.00964522\n",
      "Iteration 9846, loss = 0.00964006\n",
      "Iteration 9847, loss = 0.00963490\n",
      "Iteration 9848, loss = 0.00962975\n",
      "Iteration 9849, loss = 0.00962460\n",
      "Iteration 9850, loss = 0.00961945\n",
      "Iteration 9851, loss = 0.00961430\n",
      "Iteration 9852, loss = 0.00960915\n",
      "Iteration 9853, loss = 0.00960401\n",
      "Iteration 9854, loss = 0.00959887\n",
      "Iteration 9855, loss = 0.00959373\n",
      "Iteration 9856, loss = 0.00958860\n",
      "Iteration 9857, loss = 0.00958346\n",
      "Iteration 9858, loss = 0.00957833\n",
      "Iteration 9859, loss = 0.00957320\n",
      "Iteration 9860, loss = 0.00956808\n",
      "Iteration 9861, loss = 0.00956295\n",
      "Iteration 9862, loss = 0.00955783\n",
      "Iteration 9863, loss = 0.00955271\n",
      "Iteration 9864, loss = 0.00954759\n",
      "Iteration 9865, loss = 0.00954248\n",
      "Iteration 9866, loss = 0.00953736\n",
      "Iteration 9867, loss = 0.00953225\n",
      "Iteration 9868, loss = 0.00952714\n",
      "Iteration 9869, loss = 0.00952204\n",
      "Iteration 9870, loss = 0.00951693\n",
      "Iteration 9871, loss = 0.00951183\n",
      "Iteration 9872, loss = 0.00950673\n",
      "Iteration 9873, loss = 0.00950164\n",
      "Iteration 9874, loss = 0.00949654\n",
      "Iteration 9875, loss = 0.00949145\n",
      "Iteration 9876, loss = 0.00948636\n",
      "Iteration 9877, loss = 0.00948127\n",
      "Iteration 9878, loss = 0.00947619\n",
      "Iteration 9879, loss = 0.00947110\n",
      "Iteration 9880, loss = 0.00946602\n",
      "Iteration 9881, loss = 0.00946095\n",
      "Iteration 9882, loss = 0.00945587\n",
      "Iteration 9883, loss = 0.00945080\n",
      "Iteration 9884, loss = 0.00944572\n",
      "Iteration 9885, loss = 0.00944066\n",
      "Iteration 9886, loss = 0.00943559\n",
      "Iteration 9887, loss = 0.00943052\n",
      "Iteration 9888, loss = 0.00942546\n",
      "Iteration 9889, loss = 0.00942040\n",
      "Iteration 9890, loss = 0.00941535\n",
      "Iteration 9891, loss = 0.00941029\n",
      "Iteration 9892, loss = 0.00940524\n",
      "Iteration 9893, loss = 0.00940019\n",
      "Iteration 9894, loss = 0.00939514\n",
      "Iteration 9895, loss = 0.00939009\n",
      "Iteration 9896, loss = 0.00938505\n",
      "Iteration 9897, loss = 0.00938001\n",
      "Iteration 9898, loss = 0.00937497\n",
      "Iteration 9899, loss = 0.00936993\n",
      "Iteration 9900, loss = 0.00936490\n",
      "Iteration 9901, loss = 0.00935987\n",
      "Iteration 9902, loss = 0.00935484\n",
      "Iteration 9903, loss = 0.00934981\n",
      "Iteration 9904, loss = 0.00934478\n",
      "Iteration 9905, loss = 0.00933976\n",
      "Iteration 9906, loss = 0.00933474\n",
      "Iteration 9907, loss = 0.00932972\n",
      "Iteration 9908, loss = 0.00932471\n",
      "Iteration 9909, loss = 0.00931969\n",
      "Iteration 9910, loss = 0.00931468\n",
      "Iteration 9911, loss = 0.00930967\n",
      "Iteration 9912, loss = 0.00930467\n",
      "Iteration 9913, loss = 0.00929966\n",
      "Iteration 9914, loss = 0.00929466\n",
      "Iteration 9915, loss = 0.00928966\n",
      "Iteration 9916, loss = 0.00928466\n",
      "Iteration 9917, loss = 0.00927967\n",
      "Iteration 9918, loss = 0.00927467\n",
      "Iteration 9919, loss = 0.00926968\n",
      "Iteration 9920, loss = 0.00926469\n",
      "Iteration 9921, loss = 0.00925971\n",
      "Iteration 9922, loss = 0.00925473\n",
      "Iteration 9923, loss = 0.00924974\n",
      "Iteration 9924, loss = 0.00924477\n",
      "Iteration 9925, loss = 0.00923979\n",
      "Iteration 9926, loss = 0.00923481\n",
      "Iteration 9927, loss = 0.00922984\n",
      "Iteration 9928, loss = 0.00922487\n",
      "Iteration 9929, loss = 0.00921990\n",
      "Iteration 9930, loss = 0.00921494\n",
      "Iteration 9931, loss = 0.00920998\n",
      "Iteration 9932, loss = 0.00920502\n",
      "Iteration 9933, loss = 0.00920006\n",
      "Iteration 9934, loss = 0.00919510\n",
      "Iteration 9935, loss = 0.00919015\n",
      "Iteration 9936, loss = 0.00918520\n",
      "Iteration 9937, loss = 0.00918025\n",
      "Iteration 9938, loss = 0.00917530\n",
      "Iteration 9939, loss = 0.00917036\n",
      "Iteration 9940, loss = 0.00916542\n",
      "Iteration 9941, loss = 0.00916048\n",
      "Iteration 9942, loss = 0.00915554\n",
      "Iteration 9943, loss = 0.00915060\n",
      "Iteration 9944, loss = 0.00914567\n",
      "Iteration 9945, loss = 0.00914074\n",
      "Iteration 9946, loss = 0.00913581\n",
      "Iteration 9947, loss = 0.00913089\n",
      "Iteration 9948, loss = 0.00912596\n",
      "Iteration 9949, loss = 0.00912104\n",
      "Iteration 9950, loss = 0.00911612\n",
      "Iteration 9951, loss = 0.00911120\n",
      "Iteration 9952, loss = 0.00910629\n",
      "Iteration 9953, loss = 0.00910138\n",
      "Iteration 9954, loss = 0.00909647\n",
      "Iteration 9955, loss = 0.00909156\n",
      "Iteration 9956, loss = 0.00908666\n",
      "Iteration 9957, loss = 0.00908175\n",
      "Iteration 9958, loss = 0.00907685\n",
      "Iteration 9959, loss = 0.00907195\n",
      "Iteration 9960, loss = 0.00906706\n",
      "Iteration 9961, loss = 0.00906216\n",
      "Iteration 9962, loss = 0.00905727\n",
      "Iteration 9963, loss = 0.00905238\n",
      "Iteration 9964, loss = 0.00904750\n",
      "Iteration 9965, loss = 0.00904261\n",
      "Iteration 9966, loss = 0.00903773\n",
      "Iteration 9967, loss = 0.00903285\n",
      "Iteration 9968, loss = 0.00902797\n",
      "Iteration 9969, loss = 0.00902310\n",
      "Iteration 9970, loss = 0.00901822\n",
      "Iteration 9971, loss = 0.00901335\n",
      "Iteration 9972, loss = 0.00900848\n",
      "Iteration 9973, loss = 0.00900362\n",
      "Iteration 9974, loss = 0.00899875\n",
      "Iteration 9975, loss = 0.00899389\n",
      "Iteration 9976, loss = 0.00898903\n",
      "Iteration 9977, loss = 0.00898418\n",
      "Iteration 9978, loss = 0.00897932\n",
      "Iteration 9979, loss = 0.00897447\n",
      "Iteration 9980, loss = 0.00896962\n",
      "Iteration 9981, loss = 0.00896477\n",
      "Iteration 9982, loss = 0.00895993\n",
      "Iteration 9983, loss = 0.00895508\n",
      "Iteration 9984, loss = 0.00895024\n",
      "Iteration 9985, loss = 0.00894540\n",
      "Iteration 9986, loss = 0.00894057\n",
      "Iteration 9987, loss = 0.00893573\n",
      "Iteration 9988, loss = 0.00893090\n",
      "Iteration 9989, loss = 0.00892607\n",
      "Iteration 9990, loss = 0.00892125\n",
      "Iteration 9991, loss = 0.00891642\n",
      "Iteration 9992, loss = 0.00891160\n",
      "Iteration 9993, loss = 0.00890678\n",
      "Iteration 9994, loss = 0.00890196\n",
      "Iteration 9995, loss = 0.00889714\n",
      "Iteration 9996, loss = 0.00889233\n",
      "Iteration 9997, loss = 0.00888752\n",
      "Iteration 9998, loss = 0.00888271\n",
      "Iteration 9999, loss = 0.00887790\n",
      "Iteration 10000, loss = 0.00887310\n",
      "Iteration 10001, loss = 0.00886830\n",
      "Iteration 10002, loss = 0.00886350\n",
      "Iteration 10003, loss = 0.00885870\n",
      "Iteration 10004, loss = 0.00885391\n",
      "Iteration 10005, loss = 0.00884911\n",
      "Iteration 10006, loss = 0.00884432\n",
      "Iteration 10007, loss = 0.00883953\n",
      "Iteration 10008, loss = 0.00883475\n",
      "Iteration 10009, loss = 0.00882996\n",
      "Iteration 10010, loss = 0.00882518\n",
      "Iteration 10011, loss = 0.00882040\n",
      "Iteration 10012, loss = 0.00881563\n",
      "Iteration 10013, loss = 0.00881085\n",
      "Iteration 10014, loss = 0.00880608\n",
      "Iteration 10015, loss = 0.00880131\n",
      "Iteration 10016, loss = 0.00879654\n",
      "Iteration 10017, loss = 0.00879178\n",
      "Iteration 10018, loss = 0.00878701\n",
      "Iteration 10019, loss = 0.00878225\n",
      "Iteration 10020, loss = 0.00877750\n",
      "Iteration 10021, loss = 0.00877274\n",
      "Iteration 10022, loss = 0.00876799\n",
      "Iteration 10023, loss = 0.00876323\n",
      "Iteration 10024, loss = 0.00875848\n",
      "Iteration 10025, loss = 0.00875374\n",
      "Iteration 10026, loss = 0.00874899\n",
      "Iteration 10027, loss = 0.00874425\n",
      "Iteration 10028, loss = 0.00873951\n",
      "Iteration 10029, loss = 0.00873477\n",
      "Iteration 10030, loss = 0.00873004\n",
      "Iteration 10031, loss = 0.00872530\n",
      "Iteration 10032, loss = 0.00872057\n",
      "Iteration 10033, loss = 0.00871584\n",
      "Iteration 10034, loss = 0.00871112\n",
      "Iteration 10035, loss = 0.00870639\n",
      "Iteration 10036, loss = 0.00870167\n",
      "Iteration 10037, loss = 0.00869695\n",
      "Iteration 10038, loss = 0.00869223\n",
      "Iteration 10039, loss = 0.00868752\n",
      "Iteration 10040, loss = 0.00868280\n",
      "Iteration 10041, loss = 0.00867809\n",
      "Iteration 10042, loss = 0.00867338\n",
      "Iteration 10043, loss = 0.00866868\n",
      "Iteration 10044, loss = 0.00866397\n",
      "Iteration 10045, loss = 0.00865927\n",
      "Iteration 10046, loss = 0.00865457\n",
      "Iteration 10047, loss = 0.00864988\n",
      "Iteration 10048, loss = 0.00864518\n",
      "Iteration 10049, loss = 0.00864049\n",
      "Iteration 10050, loss = 0.00863580\n",
      "Iteration 10051, loss = 0.00863111\n",
      "Iteration 10052, loss = 0.00862642\n",
      "Iteration 10053, loss = 0.00862174\n",
      "Iteration 10054, loss = 0.00861706\n",
      "Iteration 10055, loss = 0.00861238\n",
      "Iteration 10056, loss = 0.00860770\n",
      "Iteration 10057, loss = 0.00860303\n",
      "Iteration 10058, loss = 0.00859836\n",
      "Iteration 10059, loss = 0.00859369\n",
      "Iteration 10060, loss = 0.00858902\n",
      "Iteration 10061, loss = 0.00858436\n",
      "Iteration 10062, loss = 0.00857969\n",
      "Iteration 10063, loss = 0.00857503\n",
      "Iteration 10064, loss = 0.00857037\n",
      "Iteration 10065, loss = 0.00856572\n",
      "Iteration 10066, loss = 0.00856106\n",
      "Iteration 10067, loss = 0.00855641\n",
      "Iteration 10068, loss = 0.00855176\n",
      "Iteration 10069, loss = 0.00854711\n",
      "Iteration 10070, loss = 0.00854247\n",
      "Iteration 10071, loss = 0.00853783\n",
      "Iteration 10072, loss = 0.00853319\n",
      "Iteration 10073, loss = 0.00852855\n",
      "Iteration 10074, loss = 0.00852391\n",
      "Iteration 10075, loss = 0.00851928\n",
      "Iteration 10076, loss = 0.00851465\n",
      "Iteration 10077, loss = 0.00851002\n",
      "Iteration 10078, loss = 0.00850539\n",
      "Iteration 10079, loss = 0.00850077\n",
      "Iteration 10080, loss = 0.00849614\n",
      "Iteration 10081, loss = 0.00849152\n",
      "Iteration 10082, loss = 0.00848690\n",
      "Iteration 10083, loss = 0.00848229\n",
      "Iteration 10084, loss = 0.00847768\n",
      "Iteration 10085, loss = 0.00847306\n",
      "Iteration 10086, loss = 0.00846846\n",
      "Iteration 10087, loss = 0.00846385\n",
      "Iteration 10088, loss = 0.00845924\n",
      "Iteration 10089, loss = 0.00845464\n",
      "Iteration 10090, loss = 0.00845004\n",
      "Iteration 10091, loss = 0.00844544\n",
      "Iteration 10092, loss = 0.00844085\n",
      "Iteration 10093, loss = 0.00843626\n",
      "Iteration 10094, loss = 0.00843166\n",
      "Iteration 10095, loss = 0.00842708\n",
      "Iteration 10096, loss = 0.00842249\n",
      "Iteration 10097, loss = 0.00841791\n",
      "Iteration 10098, loss = 0.00841332\n",
      "Iteration 10099, loss = 0.00840874\n",
      "Iteration 10100, loss = 0.00840417\n",
      "Iteration 10101, loss = 0.00839959\n",
      "Iteration 10102, loss = 0.00839502\n",
      "Iteration 10103, loss = 0.00839045\n",
      "Iteration 10104, loss = 0.00838588\n",
      "Iteration 10105, loss = 0.00838131\n",
      "Iteration 10106, loss = 0.00837675\n",
      "Iteration 10107, loss = 0.00837219\n",
      "Iteration 10108, loss = 0.00836763\n",
      "Iteration 10109, loss = 0.00836307\n",
      "Iteration 10110, loss = 0.00835851\n",
      "Iteration 10111, loss = 0.00835396\n",
      "Iteration 10112, loss = 0.00834941\n",
      "Iteration 10113, loss = 0.00834486\n",
      "Iteration 10114, loss = 0.00834032\n",
      "Iteration 10115, loss = 0.00833577\n",
      "Iteration 10116, loss = 0.00833123\n",
      "Iteration 10117, loss = 0.00832669\n",
      "Iteration 10118, loss = 0.00832215\n",
      "Iteration 10119, loss = 0.00831762\n",
      "Iteration 10120, loss = 0.00831309\n",
      "Iteration 10121, loss = 0.00830855\n",
      "Iteration 10122, loss = 0.00830403\n",
      "Iteration 10123, loss = 0.00829950\n",
      "Iteration 10124, loss = 0.00829498\n",
      "Iteration 10125, loss = 0.00829045\n",
      "Iteration 10126, loss = 0.00828593\n",
      "Iteration 10127, loss = 0.00828142\n",
      "Iteration 10128, loss = 0.00827690\n",
      "Iteration 10129, loss = 0.00827239\n",
      "Iteration 10130, loss = 0.00826788\n",
      "Iteration 10131, loss = 0.00826337\n",
      "Iteration 10132, loss = 0.00825886\n",
      "Iteration 10133, loss = 0.00825436\n",
      "Iteration 10134, loss = 0.00824986\n",
      "Iteration 10135, loss = 0.00824536\n",
      "Iteration 10136, loss = 0.00824086\n",
      "Iteration 10137, loss = 0.00823637\n",
      "Iteration 10138, loss = 0.00823187\n",
      "Iteration 10139, loss = 0.00822738\n",
      "Iteration 10140, loss = 0.00822290\n",
      "Iteration 10141, loss = 0.00821841\n",
      "Iteration 10142, loss = 0.00821393\n",
      "Iteration 10143, loss = 0.00820944\n",
      "Iteration 10144, loss = 0.00820496\n",
      "Iteration 10145, loss = 0.00820049\n",
      "Iteration 10146, loss = 0.00819601\n",
      "Iteration 10147, loss = 0.00819154\n",
      "Iteration 10148, loss = 0.00818707\n",
      "Iteration 10149, loss = 0.00818260\n",
      "Iteration 10150, loss = 0.00817813\n",
      "Iteration 10151, loss = 0.00817367\n",
      "Iteration 10152, loss = 0.00816921\n",
      "Iteration 10153, loss = 0.00816475\n",
      "Iteration 10154, loss = 0.00816029\n",
      "Iteration 10155, loss = 0.00815584\n",
      "Iteration 10156, loss = 0.00815138\n",
      "Iteration 10157, loss = 0.00814693\n",
      "Iteration 10158, loss = 0.00814248\n",
      "Iteration 10159, loss = 0.00813804\n",
      "Iteration 10160, loss = 0.00813359\n",
      "Iteration 10161, loss = 0.00812915\n",
      "Iteration 10162, loss = 0.00812471\n",
      "Iteration 10163, loss = 0.00812027\n",
      "Iteration 10164, loss = 0.00811584\n",
      "Iteration 10165, loss = 0.00811141\n",
      "Iteration 10166, loss = 0.00810697\n",
      "Iteration 10167, loss = 0.00810255\n",
      "Iteration 10168, loss = 0.00809812\n",
      "Iteration 10169, loss = 0.00809370\n",
      "Iteration 10170, loss = 0.00808927\n",
      "Iteration 10171, loss = 0.00808485\n",
      "Iteration 10172, loss = 0.00808044\n",
      "Iteration 10173, loss = 0.00807602\n",
      "Iteration 10174, loss = 0.00807161\n",
      "Iteration 10175, loss = 0.00806720\n",
      "Iteration 10176, loss = 0.00806279\n",
      "Iteration 10177, loss = 0.00805838\n",
      "Iteration 10178, loss = 0.00805397\n",
      "Iteration 10179, loss = 0.00804957\n",
      "Iteration 10180, loss = 0.00804517\n",
      "Iteration 10181, loss = 0.00804077\n",
      "Iteration 10182, loss = 0.00803638\n",
      "Iteration 10183, loss = 0.00803198\n",
      "Iteration 10184, loss = 0.00802759\n",
      "Iteration 10185, loss = 0.00802320\n",
      "Iteration 10186, loss = 0.00801882\n",
      "Iteration 10187, loss = 0.00801443\n",
      "Iteration 10188, loss = 0.00801005\n",
      "Iteration 10189, loss = 0.00800567\n",
      "Iteration 10190, loss = 0.00800129\n",
      "Iteration 10191, loss = 0.00799691\n",
      "Iteration 10192, loss = 0.00799254\n",
      "Iteration 10193, loss = 0.00798817\n",
      "Iteration 10194, loss = 0.00798380\n",
      "Iteration 10195, loss = 0.00797943\n",
      "Iteration 10196, loss = 0.00797507\n",
      "Iteration 10197, loss = 0.00797070\n",
      "Iteration 10198, loss = 0.00796634\n",
      "Iteration 10199, loss = 0.00796198\n",
      "Iteration 10200, loss = 0.00795763\n",
      "Iteration 10201, loss = 0.00795327\n",
      "Iteration 10202, loss = 0.00794892\n",
      "Iteration 10203, loss = 0.00794457\n",
      "Iteration 10204, loss = 0.00794022\n",
      "Iteration 10205, loss = 0.00793588\n",
      "Iteration 10206, loss = 0.00793153\n",
      "Iteration 10207, loss = 0.00792719\n",
      "Iteration 10208, loss = 0.00792285\n",
      "Iteration 10209, loss = 0.00791852\n",
      "Iteration 10210, loss = 0.00791418\n",
      "Iteration 10211, loss = 0.00790985\n",
      "Iteration 10212, loss = 0.00790552\n",
      "Iteration 10213, loss = 0.00790119\n",
      "Iteration 10214, loss = 0.00789686\n",
      "Iteration 10215, loss = 0.00789254\n",
      "Iteration 10216, loss = 0.00788822\n",
      "Iteration 10217, loss = 0.00788390\n",
      "Iteration 10218, loss = 0.00787958\n",
      "Iteration 10219, loss = 0.00787527\n",
      "Iteration 10220, loss = 0.00787095\n",
      "Iteration 10221, loss = 0.00786664\n",
      "Iteration 10222, loss = 0.00786233\n",
      "Iteration 10223, loss = 0.00785803\n",
      "Iteration 10224, loss = 0.00785372\n",
      "Iteration 10225, loss = 0.00784942\n",
      "Iteration 10226, loss = 0.00784512\n",
      "Iteration 10227, loss = 0.00784082\n",
      "Iteration 10228, loss = 0.00783652\n",
      "Iteration 10229, loss = 0.00783223\n",
      "Iteration 10230, loss = 0.00782794\n",
      "Iteration 10231, loss = 0.00782365\n",
      "Iteration 10232, loss = 0.00781936\n",
      "Iteration 10233, loss = 0.00781508\n",
      "Iteration 10234, loss = 0.00781079\n",
      "Iteration 10235, loss = 0.00780651\n",
      "Iteration 10236, loss = 0.00780224\n",
      "Iteration 10237, loss = 0.00779796\n",
      "Iteration 10238, loss = 0.00779368\n",
      "Iteration 10239, loss = 0.00778941\n",
      "Iteration 10240, loss = 0.00778514\n",
      "Iteration 10241, loss = 0.00778087\n",
      "Iteration 10242, loss = 0.00777661\n",
      "Iteration 10243, loss = 0.00777234\n",
      "Iteration 10244, loss = 0.00776808\n",
      "Iteration 10245, loss = 0.00776382\n",
      "Iteration 10246, loss = 0.00775957\n",
      "Iteration 10247, loss = 0.00775531\n",
      "Iteration 10248, loss = 0.00775106\n",
      "Iteration 10249, loss = 0.00774681\n",
      "Iteration 10250, loss = 0.00774256\n",
      "Iteration 10251, loss = 0.00773831\n",
      "Iteration 10252, loss = 0.00773407\n",
      "Iteration 10253, loss = 0.00772983\n",
      "Iteration 10254, loss = 0.00772559\n",
      "Iteration 10255, loss = 0.00772135\n",
      "Iteration 10256, loss = 0.00771711\n",
      "Iteration 10257, loss = 0.00771288\n",
      "Iteration 10258, loss = 0.00770865\n",
      "Iteration 10259, loss = 0.00770442\n",
      "Iteration 10260, loss = 0.00770019\n",
      "Iteration 10261, loss = 0.00769597\n",
      "Iteration 10262, loss = 0.00769174\n",
      "Iteration 10263, loss = 0.00768752\n",
      "Iteration 10264, loss = 0.00768330\n",
      "Iteration 10265, loss = 0.00767909\n",
      "Iteration 10266, loss = 0.00767487\n",
      "Iteration 10267, loss = 0.00767066\n",
      "Iteration 10268, loss = 0.00766645\n",
      "Iteration 10269, loss = 0.00766224\n",
      "Iteration 10270, loss = 0.00765804\n",
      "Iteration 10271, loss = 0.00765383\n",
      "Iteration 10272, loss = 0.00764963\n",
      "Iteration 10273, loss = 0.00764543\n",
      "Iteration 10274, loss = 0.00764123\n",
      "Iteration 10275, loss = 0.00763704\n",
      "Iteration 10276, loss = 0.00763285\n",
      "Iteration 10277, loss = 0.00762865\n",
      "Iteration 10278, loss = 0.00762447\n",
      "Iteration 10279, loss = 0.00762028\n",
      "Iteration 10280, loss = 0.00761609\n",
      "Iteration 10281, loss = 0.00761191\n",
      "Iteration 10282, loss = 0.00760773\n",
      "Iteration 10283, loss = 0.00760355\n",
      "Iteration 10284, loss = 0.00759938\n",
      "Iteration 10285, loss = 0.00759520\n",
      "Iteration 10286, loss = 0.00759103\n",
      "Iteration 10287, loss = 0.00758686\n",
      "Iteration 10288, loss = 0.00758269\n",
      "Iteration 10289, loss = 0.00757853\n",
      "Iteration 10290, loss = 0.00757436\n",
      "Iteration 10291, loss = 0.00757020\n",
      "Iteration 10292, loss = 0.00756604\n",
      "Iteration 10293, loss = 0.00756189\n",
      "Iteration 10294, loss = 0.00755773\n",
      "Iteration 10295, loss = 0.00755358\n",
      "Iteration 10296, loss = 0.00754943\n",
      "Iteration 10297, loss = 0.00754528\n",
      "Iteration 10298, loss = 0.00754113\n",
      "Iteration 10299, loss = 0.00753699\n",
      "Iteration 10300, loss = 0.00753284\n",
      "Iteration 10301, loss = 0.00752870\n",
      "Iteration 10302, loss = 0.00752457\n",
      "Iteration 10303, loss = 0.00752043\n",
      "Iteration 10304, loss = 0.00751630\n",
      "Iteration 10305, loss = 0.00751216\n",
      "Iteration 10306, loss = 0.00750803\n",
      "Iteration 10307, loss = 0.00750391\n",
      "Iteration 10308, loss = 0.00749978\n",
      "Iteration 10309, loss = 0.00749566\n",
      "Iteration 10310, loss = 0.00749154\n",
      "Iteration 10311, loss = 0.00748742\n",
      "Iteration 10312, loss = 0.00748330\n",
      "Iteration 10313, loss = 0.00747918\n",
      "Iteration 10314, loss = 0.00747507\n",
      "Iteration 10315, loss = 0.00747096\n",
      "Iteration 10316, loss = 0.00746685\n",
      "Iteration 10317, loss = 0.00746275\n",
      "Iteration 10318, loss = 0.00745864\n",
      "Iteration 10319, loss = 0.00745454\n",
      "Iteration 10320, loss = 0.00745044\n",
      "Iteration 10321, loss = 0.00744634\n",
      "Iteration 10322, loss = 0.00744224\n",
      "Iteration 10323, loss = 0.00743815\n",
      "Iteration 10324, loss = 0.00743406\n",
      "Iteration 10325, loss = 0.00742997\n",
      "Iteration 10326, loss = 0.00742588\n",
      "Iteration 10327, loss = 0.00742179\n",
      "Iteration 10328, loss = 0.00741771\n",
      "Iteration 10329, loss = 0.00741363\n",
      "Iteration 10330, loss = 0.00740955\n",
      "Iteration 10331, loss = 0.00740547\n",
      "Iteration 10332, loss = 0.00740139\n",
      "Iteration 10333, loss = 0.00739732\n",
      "Iteration 10334, loss = 0.00739325\n",
      "Iteration 10335, loss = 0.00738918\n",
      "Iteration 10336, loss = 0.00738511\n",
      "Iteration 10337, loss = 0.00738105\n",
      "Iteration 10338, loss = 0.00737699\n",
      "Iteration 10339, loss = 0.00737293\n",
      "Iteration 10340, loss = 0.00736887\n",
      "Iteration 10341, loss = 0.00736481\n",
      "Iteration 10342, loss = 0.00736076\n",
      "Iteration 10343, loss = 0.00735670\n",
      "Iteration 10344, loss = 0.00735265\n",
      "Iteration 10345, loss = 0.00734860\n",
      "Iteration 10346, loss = 0.00734456\n",
      "Iteration 10347, loss = 0.00734051\n",
      "Iteration 10348, loss = 0.00733647\n",
      "Iteration 10349, loss = 0.00733243\n",
      "Iteration 10350, loss = 0.00732839\n",
      "Iteration 10351, loss = 0.00732436\n",
      "Iteration 10352, loss = 0.00732032\n",
      "Iteration 10353, loss = 0.00731629\n",
      "Iteration 10354, loss = 0.00731226\n",
      "Iteration 10355, loss = 0.00730823\n",
      "Iteration 10356, loss = 0.00730421\n",
      "Iteration 10357, loss = 0.00730018\n",
      "Iteration 10358, loss = 0.00729616\n",
      "Iteration 10359, loss = 0.00729214\n",
      "Iteration 10360, loss = 0.00728813\n",
      "Iteration 10361, loss = 0.00728411\n",
      "Iteration 10362, loss = 0.00728010\n",
      "Iteration 10363, loss = 0.00727609\n",
      "Iteration 10364, loss = 0.00727208\n",
      "Iteration 10365, loss = 0.00726807\n",
      "Iteration 10366, loss = 0.00726406\n",
      "Iteration 10367, loss = 0.00726006\n",
      "Iteration 10368, loss = 0.00725606\n",
      "Iteration 10369, loss = 0.00725206\n",
      "Iteration 10370, loss = 0.00724807\n",
      "Iteration 10371, loss = 0.00724407\n",
      "Iteration 10372, loss = 0.00724008\n",
      "Iteration 10373, loss = 0.00723609\n",
      "Iteration 10374, loss = 0.00723210\n",
      "Iteration 10375, loss = 0.00722811\n",
      "Iteration 10376, loss = 0.00722413\n",
      "Iteration 10377, loss = 0.00722014\n",
      "Iteration 10378, loss = 0.00721616\n",
      "Iteration 10379, loss = 0.00721219\n",
      "Iteration 10380, loss = 0.00720821\n",
      "Iteration 10381, loss = 0.00720424\n",
      "Iteration 10382, loss = 0.00720026\n",
      "Iteration 10383, loss = 0.00719629\n",
      "Iteration 10384, loss = 0.00719233\n",
      "Iteration 10385, loss = 0.00718836\n",
      "Iteration 10386, loss = 0.00718440\n",
      "Iteration 10387, loss = 0.00718043\n",
      "Iteration 10388, loss = 0.00717647\n",
      "Iteration 10389, loss = 0.00717252\n",
      "Iteration 10390, loss = 0.00716856\n",
      "Iteration 10391, loss = 0.00716461\n",
      "Iteration 10392, loss = 0.00716065\n",
      "Iteration 10393, loss = 0.00715670\n",
      "Iteration 10394, loss = 0.00715276\n",
      "Iteration 10395, loss = 0.00714881\n",
      "Iteration 10396, loss = 0.00714487\n",
      "Iteration 10397, loss = 0.00714093\n",
      "Iteration 10398, loss = 0.00713699\n",
      "Iteration 10399, loss = 0.00713305\n",
      "Iteration 10400, loss = 0.00712911\n",
      "Iteration 10401, loss = 0.00712518\n",
      "Iteration 10402, loss = 0.00712125\n",
      "Iteration 10403, loss = 0.00711732\n",
      "Iteration 10404, loss = 0.00711339\n",
      "Iteration 10405, loss = 0.00710947\n",
      "Iteration 10406, loss = 0.00710554\n",
      "Iteration 10407, loss = 0.00710162\n",
      "Iteration 10408, loss = 0.00709770\n",
      "Iteration 10409, loss = 0.00709379\n",
      "Iteration 10410, loss = 0.00708987\n",
      "Iteration 10411, loss = 0.00708596\n",
      "Iteration 10412, loss = 0.00708205\n",
      "Iteration 10413, loss = 0.00707814\n",
      "Iteration 10414, loss = 0.00707423\n",
      "Iteration 10415, loss = 0.00707032\n",
      "Iteration 10416, loss = 0.00706642\n",
      "Iteration 10417, loss = 0.00706252\n",
      "Iteration 10418, loss = 0.00705862\n",
      "Iteration 10419, loss = 0.00705472\n",
      "Iteration 10420, loss = 0.00705083\n",
      "Iteration 10421, loss = 0.00704694\n",
      "Iteration 10422, loss = 0.00704305\n",
      "Iteration 10423, loss = 0.00703916\n",
      "Iteration 10424, loss = 0.00703527\n",
      "Iteration 10425, loss = 0.00703139\n",
      "Iteration 10426, loss = 0.00702750\n",
      "Iteration 10427, loss = 0.00702362\n",
      "Iteration 10428, loss = 0.00701974\n",
      "Iteration 10429, loss = 0.00701587\n",
      "Iteration 10430, loss = 0.00701199\n",
      "Iteration 10431, loss = 0.00700812\n",
      "Iteration 10432, loss = 0.00700425\n",
      "Iteration 10433, loss = 0.00700038\n",
      "Iteration 10434, loss = 0.00699651\n",
      "Iteration 10435, loss = 0.00699265\n",
      "Iteration 10436, loss = 0.00698878\n",
      "Iteration 10437, loss = 0.00698492\n",
      "Iteration 10438, loss = 0.00698107\n",
      "Iteration 10439, loss = 0.00697721\n",
      "Iteration 10440, loss = 0.00697335\n",
      "Iteration 10441, loss = 0.00696950\n",
      "Iteration 10442, loss = 0.00696565\n",
      "Iteration 10443, loss = 0.00696180\n",
      "Iteration 10444, loss = 0.00695796\n",
      "Iteration 10445, loss = 0.00695411\n",
      "Iteration 10446, loss = 0.00695027\n",
      "Iteration 10447, loss = 0.00694643\n",
      "Iteration 10448, loss = 0.00694259\n",
      "Iteration 10449, loss = 0.00693875\n",
      "Iteration 10450, loss = 0.00693492\n",
      "Iteration 10451, loss = 0.00693108\n",
      "Iteration 10452, loss = 0.00692725\n",
      "Iteration 10453, loss = 0.00692343\n",
      "Iteration 10454, loss = 0.00691960\n",
      "Iteration 10455, loss = 0.00691577\n",
      "Iteration 10456, loss = 0.00691195\n",
      "Iteration 10457, loss = 0.00690813\n",
      "Iteration 10458, loss = 0.00690431\n",
      "Iteration 10459, loss = 0.00690049\n",
      "Iteration 10460, loss = 0.00689668\n",
      "Iteration 10461, loss = 0.00689287\n",
      "Iteration 10462, loss = 0.00688906\n",
      "Iteration 10463, loss = 0.00688525\n",
      "Iteration 10464, loss = 0.00688144\n",
      "Iteration 10465, loss = 0.00687764\n",
      "Iteration 10466, loss = 0.00687383\n",
      "Iteration 10467, loss = 0.00687003\n",
      "Iteration 10468, loss = 0.00686623\n",
      "Iteration 10469, loss = 0.00686244\n",
      "Iteration 10470, loss = 0.00685864\n",
      "Iteration 10471, loss = 0.00685485\n",
      "Iteration 10472, loss = 0.00685106\n",
      "Iteration 10473, loss = 0.00684727\n",
      "Iteration 10474, loss = 0.00684348\n",
      "Iteration 10475, loss = 0.00683970\n",
      "Iteration 10476, loss = 0.00683592\n",
      "Iteration 10477, loss = 0.00683213\n",
      "Iteration 10478, loss = 0.00682836\n",
      "Iteration 10479, loss = 0.00682458\n",
      "Iteration 10480, loss = 0.00682080\n",
      "Iteration 10481, loss = 0.00681703\n",
      "Iteration 10482, loss = 0.00681326\n",
      "Iteration 10483, loss = 0.00680949\n",
      "Iteration 10484, loss = 0.00680572\n",
      "Iteration 10485, loss = 0.00680196\n",
      "Iteration 10486, loss = 0.00679819\n",
      "Iteration 10487, loss = 0.00679443\n",
      "Iteration 10488, loss = 0.00679067\n",
      "Iteration 10489, loss = 0.00678692\n",
      "Iteration 10490, loss = 0.00678316\n",
      "Iteration 10491, loss = 0.00677941\n",
      "Iteration 10492, loss = 0.00677566\n",
      "Iteration 10493, loss = 0.00677191\n",
      "Iteration 10494, loss = 0.00676816\n",
      "Iteration 10495, loss = 0.00676442\n",
      "Iteration 10496, loss = 0.00676067\n",
      "Iteration 10497, loss = 0.00675693\n",
      "Iteration 10498, loss = 0.00675319\n",
      "Iteration 10499, loss = 0.00674945\n",
      "Iteration 10500, loss = 0.00674572\n",
      "Iteration 10501, loss = 0.00674198\n",
      "Iteration 10502, loss = 0.00673825\n",
      "Iteration 10503, loss = 0.00673452\n",
      "Iteration 10504, loss = 0.00673079\n",
      "Iteration 10505, loss = 0.00672707\n",
      "Iteration 10506, loss = 0.00672335\n",
      "Iteration 10507, loss = 0.00671962\n",
      "Iteration 10508, loss = 0.00671590\n",
      "Iteration 10509, loss = 0.00671219\n",
      "Iteration 10510, loss = 0.00670847\n",
      "Iteration 10511, loss = 0.00670476\n",
      "Iteration 10512, loss = 0.00670104\n",
      "Iteration 10513, loss = 0.00669733\n",
      "Iteration 10514, loss = 0.00669363\n",
      "Iteration 10515, loss = 0.00668992\n",
      "Iteration 10516, loss = 0.00668621\n",
      "Iteration 10517, loss = 0.00668251\n",
      "Iteration 10518, loss = 0.00667881\n",
      "Iteration 10519, loss = 0.00667511\n",
      "Iteration 10520, loss = 0.00667142\n",
      "Iteration 10521, loss = 0.00666772\n",
      "Iteration 10522, loss = 0.00666403\n",
      "Iteration 10523, loss = 0.00666034\n",
      "Iteration 10524, loss = 0.00665665\n",
      "Iteration 10525, loss = 0.00665296\n",
      "Iteration 10526, loss = 0.00664928\n",
      "Iteration 10527, loss = 0.00664560\n",
      "Iteration 10528, loss = 0.00664192\n",
      "Iteration 10529, loss = 0.00663824\n",
      "Iteration 10530, loss = 0.00663456\n",
      "Iteration 10531, loss = 0.00663089\n",
      "Iteration 10532, loss = 0.00662721\n",
      "Iteration 10533, loss = 0.00662354\n",
      "Iteration 10534, loss = 0.00661987\n",
      "Iteration 10535, loss = 0.00661620\n",
      "Iteration 10536, loss = 0.00661254\n",
      "Iteration 10537, loss = 0.00660888\n",
      "Iteration 10538, loss = 0.00660521\n",
      "Iteration 10539, loss = 0.00660155\n",
      "Iteration 10540, loss = 0.00659790\n",
      "Iteration 10541, loss = 0.00659424\n",
      "Iteration 10542, loss = 0.00659059\n",
      "Iteration 10543, loss = 0.00658694\n",
      "Iteration 10544, loss = 0.00658329\n",
      "Iteration 10545, loss = 0.00657964\n",
      "Iteration 10546, loss = 0.00657599\n",
      "Iteration 10547, loss = 0.00657235\n",
      "Iteration 10548, loss = 0.00656871\n",
      "Iteration 10549, loss = 0.00656507\n",
      "Iteration 10550, loss = 0.00656143\n",
      "Iteration 10551, loss = 0.00655779\n",
      "Iteration 10552, loss = 0.00655416\n",
      "Iteration 10553, loss = 0.00655053\n",
      "Iteration 10554, loss = 0.00654689\n",
      "Iteration 10555, loss = 0.00654327\n",
      "Iteration 10556, loss = 0.00653964\n",
      "Iteration 10557, loss = 0.00653602\n",
      "Iteration 10558, loss = 0.00653239\n",
      "Iteration 10559, loss = 0.00652877\n",
      "Iteration 10560, loss = 0.00652515\n",
      "Iteration 10561, loss = 0.00652154\n",
      "Iteration 10562, loss = 0.00651792\n",
      "Iteration 10563, loss = 0.00651431\n",
      "Iteration 10564, loss = 0.00651070\n",
      "Iteration 10565, loss = 0.00650709\n",
      "Iteration 10566, loss = 0.00650348\n",
      "Iteration 10567, loss = 0.00649987\n",
      "Iteration 10568, loss = 0.00649627\n",
      "Iteration 10569, loss = 0.00649267\n",
      "Iteration 10570, loss = 0.00648907\n",
      "Iteration 10571, loss = 0.00648547\n",
      "Iteration 10572, loss = 0.00648188\n",
      "Iteration 10573, loss = 0.00647828\n",
      "Iteration 10574, loss = 0.00647469\n",
      "Iteration 10575, loss = 0.00647110\n",
      "Iteration 10576, loss = 0.00646751\n",
      "Iteration 10577, loss = 0.00646392\n",
      "Iteration 10578, loss = 0.00646034\n",
      "Iteration 10579, loss = 0.00645676\n",
      "Iteration 10580, loss = 0.00645318\n",
      "Iteration 10581, loss = 0.00644960\n",
      "Iteration 10582, loss = 0.00644602\n",
      "Iteration 10583, loss = 0.00644245\n",
      "Iteration 10584, loss = 0.00643887\n",
      "Iteration 10585, loss = 0.00643530\n",
      "Iteration 10586, loss = 0.00643173\n",
      "Iteration 10587, loss = 0.00642817\n",
      "Iteration 10588, loss = 0.00642460\n",
      "Iteration 10589, loss = 0.00642104\n",
      "Iteration 10590, loss = 0.00641748\n",
      "Iteration 10591, loss = 0.00641392\n",
      "Iteration 10592, loss = 0.00641036\n",
      "Iteration 10593, loss = 0.00640680\n",
      "Iteration 10594, loss = 0.00640325\n",
      "Iteration 10595, loss = 0.00639970\n",
      "Iteration 10596, loss = 0.00639615\n",
      "Iteration 10597, loss = 0.00639260\n",
      "Iteration 10598, loss = 0.00638905\n",
      "Iteration 10599, loss = 0.00638551\n",
      "Iteration 10600, loss = 0.00638197\n",
      "Iteration 10601, loss = 0.00637843\n",
      "Iteration 10602, loss = 0.00637489\n",
      "Iteration 10603, loss = 0.00637135\n",
      "Iteration 10604, loss = 0.00636782\n",
      "Iteration 10605, loss = 0.00636428\n",
      "Iteration 10606, loss = 0.00636075\n",
      "Iteration 10607, loss = 0.00635722\n",
      "Iteration 10608, loss = 0.00635369\n",
      "Iteration 10609, loss = 0.00635017\n",
      "Iteration 10610, loss = 0.00634665\n",
      "Iteration 10611, loss = 0.00634312\n",
      "Iteration 10612, loss = 0.00633960\n",
      "Iteration 10613, loss = 0.00633609\n",
      "Iteration 10614, loss = 0.00633257\n",
      "Iteration 10615, loss = 0.00632906\n",
      "Iteration 10616, loss = 0.00632554\n",
      "Iteration 10617, loss = 0.00632203\n",
      "Iteration 10618, loss = 0.00631853\n",
      "Iteration 10619, loss = 0.00631502\n",
      "Iteration 10620, loss = 0.00631151\n",
      "Iteration 10621, loss = 0.00630801\n",
      "Iteration 10622, loss = 0.00630451\n",
      "Iteration 10623, loss = 0.00630101\n",
      "Iteration 10624, loss = 0.00629751\n",
      "Iteration 10625, loss = 0.00629402\n",
      "Iteration 10626, loss = 0.00629053\n",
      "Iteration 10627, loss = 0.00628703\n",
      "Iteration 10628, loss = 0.00628354\n",
      "Iteration 10629, loss = 0.00628006\n",
      "Iteration 10630, loss = 0.00627657\n",
      "Iteration 10631, loss = 0.00627309\n",
      "Iteration 10632, loss = 0.00626961\n",
      "Iteration 10633, loss = 0.00626613\n",
      "Iteration 10634, loss = 0.00626265\n",
      "Iteration 10635, loss = 0.00625917\n",
      "Iteration 10636, loss = 0.00625570\n",
      "Iteration 10637, loss = 0.00625222\n",
      "Iteration 10638, loss = 0.00624875\n",
      "Iteration 10639, loss = 0.00624528\n",
      "Iteration 10640, loss = 0.00624182\n",
      "Iteration 10641, loss = 0.00623835\n",
      "Iteration 10642, loss = 0.00623489\n",
      "Iteration 10643, loss = 0.00623143\n",
      "Iteration 10644, loss = 0.00622797\n",
      "Iteration 10645, loss = 0.00622451\n",
      "Iteration 10646, loss = 0.00622105\n",
      "Iteration 10647, loss = 0.00621760\n",
      "Iteration 10648, loss = 0.00621415\n",
      "Iteration 10649, loss = 0.00621070\n",
      "Iteration 10650, loss = 0.00620725\n",
      "Iteration 10651, loss = 0.00620380\n",
      "Iteration 10652, loss = 0.00620036\n",
      "Iteration 10653, loss = 0.00619691\n",
      "Iteration 10654, loss = 0.00619347\n",
      "Iteration 10655, loss = 0.00619003\n",
      "Iteration 10656, loss = 0.00618660\n",
      "Iteration 10657, loss = 0.00618316\n",
      "Iteration 10658, loss = 0.00617973\n",
      "Iteration 10659, loss = 0.00617630\n",
      "Iteration 10660, loss = 0.00617287\n",
      "Iteration 10661, loss = 0.00616944\n",
      "Iteration 10662, loss = 0.00616601\n",
      "Iteration 10663, loss = 0.00616259\n",
      "Iteration 10664, loss = 0.00615917\n",
      "Iteration 10665, loss = 0.00615575\n",
      "Iteration 10666, loss = 0.00615233\n",
      "Iteration 10667, loss = 0.00614891\n",
      "Iteration 10668, loss = 0.00614550\n",
      "Iteration 10669, loss = 0.00614208\n",
      "Iteration 10670, loss = 0.00613867\n",
      "Iteration 10671, loss = 0.00613526\n",
      "Iteration 10672, loss = 0.00613186\n",
      "Iteration 10673, loss = 0.00612845\n",
      "Iteration 10674, loss = 0.00612505\n",
      "Iteration 10675, loss = 0.00612164\n",
      "Iteration 10676, loss = 0.00611824\n",
      "Iteration 10677, loss = 0.00611485\n",
      "Iteration 10678, loss = 0.00611145\n",
      "Iteration 10679, loss = 0.00610805\n",
      "Iteration 10680, loss = 0.00610466\n",
      "Iteration 10681, loss = 0.00610127\n",
      "Iteration 10682, loss = 0.00609788\n",
      "Iteration 10683, loss = 0.00609449\n",
      "Iteration 10684, loss = 0.00609111\n",
      "Iteration 10685, loss = 0.00608773\n",
      "Iteration 10686, loss = 0.00608434\n",
      "Iteration 10687, loss = 0.00608096\n",
      "Iteration 10688, loss = 0.00607759\n",
      "Iteration 10689, loss = 0.00607421\n",
      "Iteration 10690, loss = 0.00607084\n",
      "Iteration 10691, loss = 0.00606746\n",
      "Iteration 10692, loss = 0.00606409\n",
      "Iteration 10693, loss = 0.00606072\n",
      "Iteration 10694, loss = 0.00605736\n",
      "Iteration 10695, loss = 0.00605399\n",
      "Iteration 10696, loss = 0.00605063\n",
      "Iteration 10697, loss = 0.00604727\n",
      "Iteration 10698, loss = 0.00604391\n",
      "Iteration 10699, loss = 0.00604055\n",
      "Iteration 10700, loss = 0.00603720\n",
      "Iteration 10701, loss = 0.00603384\n",
      "Iteration 10702, loss = 0.00603049\n",
      "Iteration 10703, loss = 0.00602714\n",
      "Iteration 10704, loss = 0.00602379\n",
      "Iteration 10705, loss = 0.00602044\n",
      "Iteration 10706, loss = 0.00601710\n",
      "Iteration 10707, loss = 0.00601376\n",
      "Iteration 10708, loss = 0.00601041\n",
      "Iteration 10709, loss = 0.00600707\n",
      "Iteration 10710, loss = 0.00600374\n",
      "Iteration 10711, loss = 0.00600040\n",
      "Iteration 10712, loss = 0.00599707\n",
      "Iteration 10713, loss = 0.00599374\n",
      "Iteration 10714, loss = 0.00599041\n",
      "Iteration 10715, loss = 0.00598708\n",
      "Iteration 10716, loss = 0.00598375\n",
      "Iteration 10717, loss = 0.00598043\n",
      "Iteration 10718, loss = 0.00597710\n",
      "Iteration 10719, loss = 0.00597378\n",
      "Iteration 10720, loss = 0.00597046\n",
      "Iteration 10721, loss = 0.00596714\n",
      "Iteration 10722, loss = 0.00596383\n",
      "Iteration 10723, loss = 0.00596052\n",
      "Iteration 10724, loss = 0.00595720\n",
      "Iteration 10725, loss = 0.00595389\n",
      "Iteration 10726, loss = 0.00595058\n",
      "Iteration 10727, loss = 0.00594728\n",
      "Iteration 10728, loss = 0.00594397\n",
      "Iteration 10729, loss = 0.00594067\n",
      "Iteration 10730, loss = 0.00593737\n",
      "Iteration 10731, loss = 0.00593407\n",
      "Iteration 10732, loss = 0.00593077\n",
      "Iteration 10733, loss = 0.00592748\n",
      "Iteration 10734, loss = 0.00592418\n",
      "Iteration 10735, loss = 0.00592089\n",
      "Iteration 10736, loss = 0.00591760\n",
      "Iteration 10737, loss = 0.00591431\n",
      "Iteration 10738, loss = 0.00591103\n",
      "Iteration 10739, loss = 0.00590774\n",
      "Iteration 10740, loss = 0.00590446\n",
      "Iteration 10741, loss = 0.00590118\n",
      "Iteration 10742, loss = 0.00589790\n",
      "Iteration 10743, loss = 0.00589462\n",
      "Iteration 10744, loss = 0.00589134\n",
      "Iteration 10745, loss = 0.00588807\n",
      "Iteration 10746, loss = 0.00588480\n",
      "Iteration 10747, loss = 0.00588153\n",
      "Iteration 10748, loss = 0.00587826\n",
      "Iteration 10749, loss = 0.00587499\n",
      "Iteration 10750, loss = 0.00587173\n",
      "Iteration 10751, loss = 0.00586846\n",
      "Iteration 10752, loss = 0.00586520\n",
      "Iteration 10753, loss = 0.00586194\n",
      "Iteration 10754, loss = 0.00585869\n",
      "Iteration 10755, loss = 0.00585543\n",
      "Iteration 10756, loss = 0.00585218\n",
      "Iteration 10757, loss = 0.00584892\n",
      "Iteration 10758, loss = 0.00584567\n",
      "Iteration 10759, loss = 0.00584242\n",
      "Iteration 10760, loss = 0.00583918\n",
      "Iteration 10761, loss = 0.00583593\n",
      "Iteration 10762, loss = 0.00583269\n",
      "Iteration 10763, loss = 0.00582945\n",
      "Iteration 10764, loss = 0.00582621\n",
      "Iteration 10765, loss = 0.00582297\n",
      "Iteration 10766, loss = 0.00581973\n",
      "Iteration 10767, loss = 0.00581650\n",
      "Iteration 10768, loss = 0.00581327\n",
      "Iteration 10769, loss = 0.00581004\n",
      "Iteration 10770, loss = 0.00580681\n",
      "Iteration 10771, loss = 0.00580358\n",
      "Iteration 10772, loss = 0.00580035\n",
      "Iteration 10773, loss = 0.00579713\n",
      "Iteration 10774, loss = 0.00579391\n",
      "Iteration 10775, loss = 0.00579069\n",
      "Iteration 10776, loss = 0.00578747\n",
      "Iteration 10777, loss = 0.00578425\n",
      "Iteration 10778, loss = 0.00578104\n",
      "Iteration 10779, loss = 0.00577783\n",
      "Iteration 10780, loss = 0.00577461\n",
      "Iteration 10781, loss = 0.00577141\n",
      "Iteration 10782, loss = 0.00576820\n",
      "Iteration 10783, loss = 0.00576499\n",
      "Iteration 10784, loss = 0.00576179\n",
      "Iteration 10785, loss = 0.00575859\n",
      "Iteration 10786, loss = 0.00575539\n",
      "Iteration 10787, loss = 0.00575219\n",
      "Iteration 10788, loss = 0.00574899\n",
      "Iteration 10789, loss = 0.00574579\n",
      "Iteration 10790, loss = 0.00574260\n",
      "Iteration 10791, loss = 0.00573941\n",
      "Iteration 10792, loss = 0.00573622\n",
      "Iteration 10793, loss = 0.00573303\n",
      "Iteration 10794, loss = 0.00572985\n",
      "Iteration 10795, loss = 0.00572666\n",
      "Iteration 10796, loss = 0.00572348\n",
      "Iteration 10797, loss = 0.00572030\n",
      "Iteration 10798, loss = 0.00571712\n",
      "Iteration 10799, loss = 0.00571394\n",
      "Iteration 10800, loss = 0.00571077\n",
      "Iteration 10801, loss = 0.00570759\n",
      "Iteration 10802, loss = 0.00570442\n",
      "Iteration 10803, loss = 0.00570125\n",
      "Iteration 10804, loss = 0.00569808\n",
      "Iteration 10805, loss = 0.00569491\n",
      "Iteration 10806, loss = 0.00569175\n",
      "Iteration 10807, loss = 0.00568859\n",
      "Iteration 10808, loss = 0.00568543\n",
      "Iteration 10809, loss = 0.00568227\n",
      "Iteration 10810, loss = 0.00567911\n",
      "Iteration 10811, loss = 0.00567595\n",
      "Iteration 10812, loss = 0.00567280\n",
      "Iteration 10813, loss = 0.00566964\n",
      "Iteration 10814, loss = 0.00566649\n",
      "Iteration 10815, loss = 0.00566334\n",
      "Iteration 10816, loss = 0.00566020\n",
      "Iteration 10817, loss = 0.00565705\n",
      "Iteration 10818, loss = 0.00565391\n",
      "Iteration 10819, loss = 0.00565077\n",
      "Iteration 10820, loss = 0.00564763\n",
      "Iteration 10821, loss = 0.00564449\n",
      "Iteration 10822, loss = 0.00564135\n",
      "Iteration 10823, loss = 0.00563822\n",
      "Iteration 10824, loss = 0.00563508\n",
      "Iteration 10825, loss = 0.00563195\n",
      "Iteration 10826, loss = 0.00562882\n",
      "Iteration 10827, loss = 0.00562569\n",
      "Iteration 10828, loss = 0.00562257\n",
      "Iteration 10829, loss = 0.00561944\n",
      "Iteration 10830, loss = 0.00561632\n",
      "Iteration 10831, loss = 0.00561320\n",
      "Iteration 10832, loss = 0.00561008\n",
      "Iteration 10833, loss = 0.00560696\n",
      "Iteration 10834, loss = 0.00560385\n",
      "Iteration 10835, loss = 0.00560073\n",
      "Iteration 10836, loss = 0.00559762\n",
      "Iteration 10837, loss = 0.00559451\n",
      "Iteration 10838, loss = 0.00559140\n",
      "Iteration 10839, loss = 0.00558829\n",
      "Iteration 10840, loss = 0.00558519\n",
      "Iteration 10841, loss = 0.00558209\n",
      "Iteration 10842, loss = 0.00557898\n",
      "Iteration 10843, loss = 0.00557588\n",
      "Iteration 10844, loss = 0.00557279\n",
      "Iteration 10845, loss = 0.00556969\n",
      "Iteration 10846, loss = 0.00556660\n",
      "Iteration 10847, loss = 0.00556350\n",
      "Iteration 10848, loss = 0.00556041\n",
      "Iteration 10849, loss = 0.00555732\n",
      "Iteration 10850, loss = 0.00555423\n",
      "Iteration 10851, loss = 0.00555115\n",
      "Iteration 10852, loss = 0.00554806\n",
      "Iteration 10853, loss = 0.00554498\n",
      "Iteration 10854, loss = 0.00554190\n",
      "Iteration 10855, loss = 0.00553882\n",
      "Iteration 10856, loss = 0.00553574\n",
      "Iteration 10857, loss = 0.00553267\n",
      "Iteration 10858, loss = 0.00552959\n",
      "Iteration 10859, loss = 0.00552652\n",
      "Iteration 10860, loss = 0.00552345\n",
      "Iteration 10861, loss = 0.00552038\n",
      "Iteration 10862, loss = 0.00551732\n",
      "Iteration 10863, loss = 0.00551425\n",
      "Iteration 10864, loss = 0.00551119\n",
      "Iteration 10865, loss = 0.00550813\n",
      "Iteration 10866, loss = 0.00550507\n",
      "Iteration 10867, loss = 0.00550201\n",
      "Iteration 10868, loss = 0.00549895\n",
      "Iteration 10869, loss = 0.00549590\n",
      "Iteration 10870, loss = 0.00549284\n",
      "Iteration 10871, loss = 0.00548979\n",
      "Iteration 10872, loss = 0.00548674\n",
      "Iteration 10873, loss = 0.00548369\n",
      "Iteration 10874, loss = 0.00548065\n",
      "Iteration 10875, loss = 0.00547760\n",
      "Iteration 10876, loss = 0.00547456\n",
      "Iteration 10877, loss = 0.00547152\n",
      "Iteration 10878, loss = 0.00546848\n",
      "Iteration 10879, loss = 0.00546544\n",
      "Iteration 10880, loss = 0.00546241\n",
      "Iteration 10881, loss = 0.00545937\n",
      "Iteration 10882, loss = 0.00545634\n",
      "Iteration 10883, loss = 0.00545331\n",
      "Iteration 10884, loss = 0.00545028\n",
      "Iteration 10885, loss = 0.00544725\n",
      "Iteration 10886, loss = 0.00544423\n",
      "Iteration 10887, loss = 0.00544121\n",
      "Iteration 10888, loss = 0.00543818\n",
      "Iteration 10889, loss = 0.00543516\n",
      "Iteration 10890, loss = 0.00543214\n",
      "Iteration 10891, loss = 0.00542913\n",
      "Iteration 10892, loss = 0.00542611\n",
      "Iteration 10893, loss = 0.00542310\n",
      "Iteration 10894, loss = 0.00542009\n",
      "Iteration 10895, loss = 0.00541708\n",
      "Iteration 10896, loss = 0.00541407\n",
      "Iteration 10897, loss = 0.00541106\n",
      "Iteration 10898, loss = 0.00540806\n",
      "Iteration 10899, loss = 0.00540505\n",
      "Iteration 10900, loss = 0.00540205\n",
      "Iteration 10901, loss = 0.00539905\n",
      "Iteration 10902, loss = 0.00539606\n",
      "Iteration 10903, loss = 0.00539306\n",
      "Iteration 10904, loss = 0.00539006\n",
      "Iteration 10905, loss = 0.00538707\n",
      "Iteration 10906, loss = 0.00538408\n",
      "Iteration 10907, loss = 0.00538109\n",
      "Iteration 10908, loss = 0.00537810\n",
      "Iteration 10909, loss = 0.00537512\n",
      "Iteration 10910, loss = 0.00537213\n",
      "Iteration 10911, loss = 0.00536915\n",
      "Iteration 10912, loss = 0.00536617\n",
      "Iteration 10913, loss = 0.00536319\n",
      "Iteration 10914, loss = 0.00536021\n",
      "Iteration 10915, loss = 0.00535724\n",
      "Iteration 10916, loss = 0.00535426\n",
      "Iteration 10917, loss = 0.00535129\n",
      "Iteration 10918, loss = 0.00534832\n",
      "Iteration 10919, loss = 0.00534535\n",
      "Iteration 10920, loss = 0.00534238\n",
      "Iteration 10921, loss = 0.00533942\n",
      "Iteration 10922, loss = 0.00533645\n",
      "Iteration 10923, loss = 0.00533349\n",
      "Iteration 10924, loss = 0.00533053\n",
      "Iteration 10925, loss = 0.00532757\n",
      "Iteration 10926, loss = 0.00532461\n",
      "Iteration 10927, loss = 0.00532166\n",
      "Iteration 10928, loss = 0.00531870\n",
      "Iteration 10929, loss = 0.00531575\n",
      "Iteration 10930, loss = 0.00531280\n",
      "Iteration 10931, loss = 0.00530985\n",
      "Iteration 10932, loss = 0.00530691\n",
      "Iteration 10933, loss = 0.00530396\n",
      "Iteration 10934, loss = 0.00530102\n",
      "Iteration 10935, loss = 0.00529807\n",
      "Iteration 10936, loss = 0.00529513\n",
      "Iteration 10937, loss = 0.00529220\n",
      "Iteration 10938, loss = 0.00528926\n",
      "Iteration 10939, loss = 0.00528632\n",
      "Iteration 10940, loss = 0.00528339\n",
      "Iteration 10941, loss = 0.00528046\n",
      "Iteration 10942, loss = 0.00527753\n",
      "Iteration 10943, loss = 0.00527460\n",
      "Iteration 10944, loss = 0.00527167\n",
      "Iteration 10945, loss = 0.00526875\n",
      "Iteration 10946, loss = 0.00526582\n",
      "Iteration 10947, loss = 0.00526290\n",
      "Iteration 10948, loss = 0.00525998\n",
      "Iteration 10949, loss = 0.00525706\n",
      "Iteration 10950, loss = 0.00525415\n",
      "Iteration 10951, loss = 0.00525123\n",
      "Iteration 10952, loss = 0.00524832\n",
      "Iteration 10953, loss = 0.00524541\n",
      "Iteration 10954, loss = 0.00524250\n",
      "Iteration 10955, loss = 0.00523959\n",
      "Iteration 10956, loss = 0.00523668\n",
      "Iteration 10957, loss = 0.00523378\n",
      "Iteration 10958, loss = 0.00523087\n",
      "Iteration 10959, loss = 0.00522797\n",
      "Iteration 10960, loss = 0.00522507\n",
      "Iteration 10961, loss = 0.00522217\n",
      "Iteration 10962, loss = 0.00521928\n",
      "Iteration 10963, loss = 0.00521638\n",
      "Iteration 10964, loss = 0.00521349\n",
      "Iteration 10965, loss = 0.00521060\n",
      "Iteration 10966, loss = 0.00520771\n",
      "Iteration 10967, loss = 0.00520482\n",
      "Iteration 10968, loss = 0.00520193\n",
      "Iteration 10969, loss = 0.00519905\n",
      "Iteration 10970, loss = 0.00519616\n",
      "Iteration 10971, loss = 0.00519328\n",
      "Iteration 10972, loss = 0.00519040\n",
      "Iteration 10973, loss = 0.00518752\n",
      "Iteration 10974, loss = 0.00518465\n",
      "Iteration 10975, loss = 0.00518177\n",
      "Iteration 10976, loss = 0.00517890\n",
      "Iteration 10977, loss = 0.00517603\n",
      "Iteration 10978, loss = 0.00517316\n",
      "Iteration 10979, loss = 0.00517029\n",
      "Iteration 10980, loss = 0.00516742\n",
      "Iteration 10981, loss = 0.00516456\n",
      "Iteration 10982, loss = 0.00516169\n",
      "Iteration 10983, loss = 0.00515883\n",
      "Iteration 10984, loss = 0.00515597\n",
      "Iteration 10985, loss = 0.00515311\n",
      "Iteration 10986, loss = 0.00515026\n",
      "Iteration 10987, loss = 0.00514740\n",
      "Iteration 10988, loss = 0.00514455\n",
      "Iteration 10989, loss = 0.00514170\n",
      "Iteration 10990, loss = 0.00513885\n",
      "Iteration 10991, loss = 0.00513600\n",
      "Iteration 10992, loss = 0.00513315\n",
      "Iteration 10993, loss = 0.00513031\n",
      "Iteration 10994, loss = 0.00512746\n",
      "Iteration 10995, loss = 0.00512462\n",
      "Iteration 10996, loss = 0.00512178\n",
      "Iteration 10997, loss = 0.00511894\n",
      "Iteration 10998, loss = 0.00511610\n",
      "Iteration 10999, loss = 0.00511327\n",
      "Iteration 11000, loss = 0.00511044\n",
      "Iteration 11001, loss = 0.00510760\n",
      "Iteration 11002, loss = 0.00510477\n",
      "Iteration 11003, loss = 0.00510195\n",
      "Iteration 11004, loss = 0.00509912\n",
      "Iteration 11005, loss = 0.00509629\n",
      "Iteration 11006, loss = 0.00509347\n",
      "Iteration 11007, loss = 0.00509065\n",
      "Iteration 11008, loss = 0.00508783\n",
      "Iteration 11009, loss = 0.00508501\n",
      "Iteration 11010, loss = 0.00508219\n",
      "Iteration 11011, loss = 0.00507938\n",
      "Iteration 11012, loss = 0.00507656\n",
      "Iteration 11013, loss = 0.00507375\n",
      "Iteration 11014, loss = 0.00507094\n",
      "Iteration 11015, loss = 0.00506813\n",
      "Iteration 11016, loss = 0.00506532\n",
      "Iteration 11017, loss = 0.00506252\n",
      "Iteration 11018, loss = 0.00505971\n",
      "Iteration 11019, loss = 0.00505691\n",
      "Iteration 11020, loss = 0.00505411\n",
      "Iteration 11021, loss = 0.00505131\n",
      "Iteration 11022, loss = 0.00504851\n",
      "Iteration 11023, loss = 0.00504572\n",
      "Iteration 11024, loss = 0.00504292\n",
      "Iteration 11025, loss = 0.00504013\n",
      "Iteration 11026, loss = 0.00503734\n",
      "Iteration 11027, loss = 0.00503455\n",
      "Iteration 11028, loss = 0.00503176\n",
      "Iteration 11029, loss = 0.00502898\n",
      "Iteration 11030, loss = 0.00502619\n",
      "Iteration 11031, loss = 0.00502341\n",
      "Iteration 11032, loss = 0.00502063\n",
      "Iteration 11033, loss = 0.00501785\n",
      "Iteration 11034, loss = 0.00501507\n",
      "Iteration 11035, loss = 0.00501229\n",
      "Iteration 11036, loss = 0.00500952\n",
      "Iteration 11037, loss = 0.00500675\n",
      "Iteration 11038, loss = 0.00500397\n",
      "Iteration 11039, loss = 0.00500120\n",
      "Iteration 11040, loss = 0.00499844\n",
      "Iteration 11041, loss = 0.00499567\n",
      "Iteration 11042, loss = 0.00499291\n",
      "Iteration 11043, loss = 0.00499014\n",
      "Iteration 11044, loss = 0.00498738\n",
      "Iteration 11045, loss = 0.00498462\n",
      "Iteration 11046, loss = 0.00498186\n",
      "Iteration 11047, loss = 0.00497910\n",
      "Iteration 11048, loss = 0.00497635\n",
      "Iteration 11049, loss = 0.00497360\n",
      "Iteration 11050, loss = 0.00497084\n",
      "Iteration 11051, loss = 0.00496809\n",
      "Iteration 11052, loss = 0.00496534\n",
      "Iteration 11053, loss = 0.00496260\n",
      "Iteration 11054, loss = 0.00495985\n",
      "Iteration 11055, loss = 0.00495711\n",
      "Iteration 11056, loss = 0.00495437\n",
      "Iteration 11057, loss = 0.00495163\n",
      "Iteration 11058, loss = 0.00494889\n",
      "Iteration 11059, loss = 0.00494615\n",
      "Iteration 11060, loss = 0.00494341\n",
      "Iteration 11061, loss = 0.00494068\n",
      "Iteration 11062, loss = 0.00493795\n",
      "Iteration 11063, loss = 0.00493522\n",
      "Iteration 11064, loss = 0.00493249\n",
      "Iteration 11065, loss = 0.00492976\n",
      "Iteration 11066, loss = 0.00492703\n",
      "Iteration 11067, loss = 0.00492431\n",
      "Iteration 11068, loss = 0.00492158\n",
      "Iteration 11069, loss = 0.00491886\n",
      "Iteration 11070, loss = 0.00491614\n",
      "Iteration 11071, loss = 0.00491342\n",
      "Iteration 11072, loss = 0.00491071\n",
      "Iteration 11073, loss = 0.00490799\n",
      "Iteration 11074, loss = 0.00490528\n",
      "Iteration 11075, loss = 0.00490257\n",
      "Iteration 11076, loss = 0.00489986\n",
      "Iteration 11077, loss = 0.00489715\n",
      "Iteration 11078, loss = 0.00489444\n",
      "Iteration 11079, loss = 0.00489174\n",
      "Iteration 11080, loss = 0.00488903\n",
      "Iteration 11081, loss = 0.00488633\n",
      "Iteration 11082, loss = 0.00488363\n",
      "Iteration 11083, loss = 0.00488093\n",
      "Iteration 11084, loss = 0.00487823\n",
      "Iteration 11085, loss = 0.00487554\n",
      "Iteration 11086, loss = 0.00487284\n",
      "Iteration 11087, loss = 0.00487015\n",
      "Iteration 11088, loss = 0.00486746\n",
      "Iteration 11089, loss = 0.00486477\n",
      "Iteration 11090, loss = 0.00486208\n",
      "Iteration 11091, loss = 0.00485940\n",
      "Iteration 11092, loss = 0.00485671\n",
      "Iteration 11093, loss = 0.00485403\n",
      "Iteration 11094, loss = 0.00485135\n",
      "Iteration 11095, loss = 0.00484867\n",
      "Iteration 11096, loss = 0.00484599\n",
      "Iteration 11097, loss = 0.00484331\n",
      "Iteration 11098, loss = 0.00484064\n",
      "Iteration 11099, loss = 0.00483796\n",
      "Iteration 11100, loss = 0.00483529\n",
      "Iteration 11101, loss = 0.00483262\n",
      "Iteration 11102, loss = 0.00482995\n",
      "Iteration 11103, loss = 0.00482729\n",
      "Iteration 11104, loss = 0.00482462\n",
      "Iteration 11105, loss = 0.00482196\n",
      "Iteration 11106, loss = 0.00481929\n",
      "Iteration 11107, loss = 0.00481663\n",
      "Iteration 11108, loss = 0.00481397\n",
      "Iteration 11109, loss = 0.00481132\n",
      "Iteration 11110, loss = 0.00480866\n",
      "Iteration 11111, loss = 0.00480601\n",
      "Iteration 11112, loss = 0.00480335\n",
      "Iteration 11113, loss = 0.00480070\n",
      "Iteration 11114, loss = 0.00479805\n",
      "Iteration 11115, loss = 0.00479540\n",
      "Iteration 11116, loss = 0.00479276\n",
      "Iteration 11117, loss = 0.00479011\n",
      "Iteration 11118, loss = 0.00478747\n",
      "Iteration 11119, loss = 0.00478483\n",
      "Iteration 11120, loss = 0.00478219\n",
      "Iteration 11121, loss = 0.00477955\n",
      "Iteration 11122, loss = 0.00477691\n",
      "Iteration 11123, loss = 0.00477427\n",
      "Iteration 11124, loss = 0.00477164\n",
      "Iteration 11125, loss = 0.00476901\n",
      "Iteration 11126, loss = 0.00476638\n",
      "Iteration 11127, loss = 0.00476375\n",
      "Iteration 11128, loss = 0.00476112\n",
      "Iteration 11129, loss = 0.00475849\n",
      "Iteration 11130, loss = 0.00475587\n",
      "Iteration 11131, loss = 0.00475324\n",
      "Iteration 11132, loss = 0.00475062\n",
      "Iteration 11133, loss = 0.00474800\n",
      "Iteration 11134, loss = 0.00474538\n",
      "Iteration 11135, loss = 0.00474277\n",
      "Iteration 11136, loss = 0.00474015\n",
      "Iteration 11137, loss = 0.00473754\n",
      "Iteration 11138, loss = 0.00473493\n",
      "Iteration 11139, loss = 0.00473232\n",
      "Iteration 11140, loss = 0.00472971\n",
      "Iteration 11141, loss = 0.00472710\n",
      "Iteration 11142, loss = 0.00472449\n",
      "Iteration 11143, loss = 0.00472189\n",
      "Iteration 11144, loss = 0.00471929\n",
      "Iteration 11145, loss = 0.00471669\n",
      "Iteration 11146, loss = 0.00471409\n",
      "Iteration 11147, loss = 0.00471149\n",
      "Iteration 11148, loss = 0.00470889\n",
      "Iteration 11149, loss = 0.00470630\n",
      "Iteration 11150, loss = 0.00470370\n",
      "Iteration 11151, loss = 0.00470111\n",
      "Iteration 11152, loss = 0.00469852\n",
      "Iteration 11153, loss = 0.00469593\n",
      "Iteration 11154, loss = 0.00469334\n",
      "Iteration 11155, loss = 0.00469076\n",
      "Iteration 11156, loss = 0.00468817\n",
      "Iteration 11157, loss = 0.00468559\n",
      "Iteration 11158, loss = 0.00468301\n",
      "Iteration 11159, loss = 0.00468043\n",
      "Iteration 11160, loss = 0.00467785\n",
      "Iteration 11161, loss = 0.00467528\n",
      "Iteration 11162, loss = 0.00467270\n",
      "Iteration 11163, loss = 0.00467013\n",
      "Iteration 11164, loss = 0.00466756\n",
      "Iteration 11165, loss = 0.00466499\n",
      "Iteration 11166, loss = 0.00466242\n",
      "Iteration 11167, loss = 0.00465985\n",
      "Iteration 11168, loss = 0.00465729\n",
      "Iteration 11169, loss = 0.00465472\n",
      "Iteration 11170, loss = 0.00465216\n",
      "Iteration 11171, loss = 0.00464960\n",
      "Iteration 11172, loss = 0.00464704\n",
      "Iteration 11173, loss = 0.00464448\n",
      "Iteration 11174, loss = 0.00464192\n",
      "Iteration 11175, loss = 0.00463937\n",
      "Iteration 11176, loss = 0.00463682\n",
      "Iteration 11177, loss = 0.00463426\n",
      "Iteration 11178, loss = 0.00463171\n",
      "Iteration 11179, loss = 0.00462917\n",
      "Iteration 11180, loss = 0.00462662\n",
      "Iteration 11181, loss = 0.00462407\n",
      "Iteration 11182, loss = 0.00462153\n",
      "Iteration 11183, loss = 0.00461899\n",
      "Iteration 11184, loss = 0.00461645\n",
      "Iteration 11185, loss = 0.00461391\n",
      "Iteration 11186, loss = 0.00461137\n",
      "Iteration 11187, loss = 0.00460883\n",
      "Iteration 11188, loss = 0.00460630\n",
      "Iteration 11189, loss = 0.00460376\n",
      "Iteration 11190, loss = 0.00460123\n",
      "Iteration 11191, loss = 0.00459870\n",
      "Iteration 11192, loss = 0.00459617\n",
      "Iteration 11193, loss = 0.00459365\n",
      "Iteration 11194, loss = 0.00459112\n",
      "Iteration 11195, loss = 0.00458860\n",
      "Iteration 11196, loss = 0.00458607\n",
      "Iteration 11197, loss = 0.00458355\n",
      "Iteration 11198, loss = 0.00458103\n",
      "Iteration 11199, loss = 0.00457851\n",
      "Iteration 11200, loss = 0.00457600\n",
      "Iteration 11201, loss = 0.00457348\n",
      "Iteration 11202, loss = 0.00457097\n",
      "Iteration 11203, loss = 0.00456846\n",
      "Iteration 11204, loss = 0.00456595\n",
      "Iteration 11205, loss = 0.00456344\n",
      "Iteration 11206, loss = 0.00456093\n",
      "Iteration 11207, loss = 0.00455843\n",
      "Iteration 11208, loss = 0.00455592\n",
      "Iteration 11209, loss = 0.00455342\n",
      "Iteration 11210, loss = 0.00455092\n",
      "Iteration 11211, loss = 0.00454842\n",
      "Iteration 11212, loss = 0.00454592\n",
      "Iteration 11213, loss = 0.00454342\n",
      "Iteration 11214, loss = 0.00454093\n",
      "Iteration 11215, loss = 0.00453843\n",
      "Iteration 11216, loss = 0.00453594\n",
      "Iteration 11217, loss = 0.00453345\n",
      "Iteration 11218, loss = 0.00453096\n",
      "Iteration 11219, loss = 0.00452847\n",
      "Iteration 11220, loss = 0.00452599\n",
      "Iteration 11221, loss = 0.00452350\n",
      "Iteration 11222, loss = 0.00452102\n",
      "Iteration 11223, loss = 0.00451854\n",
      "Iteration 11224, loss = 0.00451606\n",
      "Iteration 11225, loss = 0.00451358\n",
      "Iteration 11226, loss = 0.00451110\n",
      "Iteration 11227, loss = 0.00450863\n",
      "Iteration 11228, loss = 0.00450615\n",
      "Iteration 11229, loss = 0.00450368\n",
      "Iteration 11230, loss = 0.00450121\n",
      "Iteration 11231, loss = 0.00449874\n",
      "Iteration 11232, loss = 0.00449627\n",
      "Iteration 11233, loss = 0.00449380\n",
      "Iteration 11234, loss = 0.00449134\n",
      "Iteration 11235, loss = 0.00448888\n",
      "Iteration 11236, loss = 0.00448641\n",
      "Iteration 11237, loss = 0.00448395\n",
      "Iteration 11238, loss = 0.00448149\n",
      "Iteration 11239, loss = 0.00447904\n",
      "Iteration 11240, loss = 0.00447658\n",
      "Iteration 11241, loss = 0.00447413\n",
      "Iteration 11242, loss = 0.00447167\n",
      "Iteration 11243, loss = 0.00446922\n",
      "Iteration 11244, loss = 0.00446677\n",
      "Iteration 11245, loss = 0.00446432\n",
      "Iteration 11246, loss = 0.00446187\n",
      "Iteration 11247, loss = 0.00445943\n",
      "Iteration 11248, loss = 0.00445699\n",
      "Iteration 11249, loss = 0.00445454\n",
      "Iteration 11250, loss = 0.00445210\n",
      "Iteration 11251, loss = 0.00444966\n",
      "Iteration 11252, loss = 0.00444722\n",
      "Iteration 11253, loss = 0.00444479\n",
      "Iteration 11254, loss = 0.00444235\n",
      "Iteration 11255, loss = 0.00443992\n",
      "Iteration 11256, loss = 0.00443749\n",
      "Iteration 11257, loss = 0.00443506\n",
      "Iteration 11258, loss = 0.00443263\n",
      "Iteration 11259, loss = 0.00443020\n",
      "Iteration 11260, loss = 0.00442777\n",
      "Iteration 11261, loss = 0.00442535\n",
      "Iteration 11262, loss = 0.00442292\n",
      "Iteration 11263, loss = 0.00442050\n",
      "Iteration 11264, loss = 0.00441808\n",
      "Iteration 11265, loss = 0.00441566\n",
      "Iteration 11266, loss = 0.00441325\n",
      "Iteration 11267, loss = 0.00441083\n",
      "Iteration 11268, loss = 0.00440842\n",
      "Iteration 11269, loss = 0.00440600\n",
      "Iteration 11270, loss = 0.00440359\n",
      "Iteration 11271, loss = 0.00440118\n",
      "Iteration 11272, loss = 0.00439877\n",
      "Iteration 11273, loss = 0.00439637\n",
      "Iteration 11274, loss = 0.00439396\n",
      "Iteration 11275, loss = 0.00439156\n",
      "Iteration 11276, loss = 0.00438916\n",
      "Iteration 11277, loss = 0.00438675\n",
      "Iteration 11278, loss = 0.00438435\n",
      "Iteration 11279, loss = 0.00438196\n",
      "Iteration 11280, loss = 0.00437956\n",
      "Iteration 11281, loss = 0.00437716\n",
      "Iteration 11282, loss = 0.00437477\n",
      "Iteration 11283, loss = 0.00437238\n",
      "Iteration 11284, loss = 0.00436999\n",
      "Iteration 11285, loss = 0.00436760\n",
      "Iteration 11286, loss = 0.00436521\n",
      "Iteration 11287, loss = 0.00436283\n",
      "Iteration 11288, loss = 0.00436044\n",
      "Iteration 11289, loss = 0.00435806\n",
      "Iteration 11290, loss = 0.00435568\n",
      "Iteration 11291, loss = 0.00435329\n",
      "Iteration 11292, loss = 0.00435092\n",
      "Iteration 11293, loss = 0.00434854\n",
      "Iteration 11294, loss = 0.00434616\n",
      "Iteration 11295, loss = 0.00434379\n",
      "Iteration 11296, loss = 0.00434141\n",
      "Iteration 11297, loss = 0.00433904\n",
      "Iteration 11298, loss = 0.00433667\n",
      "Iteration 11299, loss = 0.00433430\n",
      "Iteration 11300, loss = 0.00433194\n",
      "Iteration 11301, loss = 0.00432957\n",
      "Iteration 11302, loss = 0.00432721\n",
      "Iteration 11303, loss = 0.00432484\n",
      "Iteration 11304, loss = 0.00432248\n",
      "Iteration 11305, loss = 0.00432012\n",
      "Iteration 11306, loss = 0.00431776\n",
      "Iteration 11307, loss = 0.00431541\n",
      "Iteration 11308, loss = 0.00431305\n",
      "Iteration 11309, loss = 0.00431070\n",
      "Iteration 11310, loss = 0.00430835\n",
      "Iteration 11311, loss = 0.00430599\n",
      "Iteration 11312, loss = 0.00430364\n",
      "Iteration 11313, loss = 0.00430130\n",
      "Iteration 11314, loss = 0.00429895\n",
      "Iteration 11315, loss = 0.00429660\n",
      "Iteration 11316, loss = 0.00429426\n",
      "Iteration 11317, loss = 0.00429192\n",
      "Iteration 11318, loss = 0.00428958\n",
      "Iteration 11319, loss = 0.00428724\n",
      "Iteration 11320, loss = 0.00428490\n",
      "Iteration 11321, loss = 0.00428256\n",
      "Iteration 11322, loss = 0.00428023\n",
      "Iteration 11323, loss = 0.00427789\n",
      "Iteration 11324, loss = 0.00427556\n",
      "Iteration 11325, loss = 0.00427323\n",
      "Iteration 11326, loss = 0.00427090\n",
      "Iteration 11327, loss = 0.00426857\n",
      "Iteration 11328, loss = 0.00426625\n",
      "Iteration 11329, loss = 0.00426392\n",
      "Iteration 11330, loss = 0.00426160\n",
      "Iteration 11331, loss = 0.00425928\n",
      "Iteration 11332, loss = 0.00425696\n",
      "Iteration 11333, loss = 0.00425464\n",
      "Iteration 11334, loss = 0.00425232\n",
      "Iteration 11335, loss = 0.00425000\n",
      "Iteration 11336, loss = 0.00424769\n",
      "Iteration 11337, loss = 0.00424537\n",
      "Iteration 11338, loss = 0.00424306\n",
      "Iteration 11339, loss = 0.00424075\n",
      "Iteration 11340, loss = 0.00423844\n",
      "Iteration 11341, loss = 0.00423613\n",
      "Iteration 11342, loss = 0.00423383\n",
      "Iteration 11343, loss = 0.00423152\n",
      "Iteration 11344, loss = 0.00422922\n",
      "Iteration 11345, loss = 0.00422692\n",
      "Iteration 11346, loss = 0.00422462\n",
      "Iteration 11347, loss = 0.00422232\n",
      "Iteration 11348, loss = 0.00422002\n",
      "Iteration 11349, loss = 0.00421772\n",
      "Iteration 11350, loss = 0.00421543\n",
      "Iteration 11351, loss = 0.00421314\n",
      "Iteration 11352, loss = 0.00421084\n",
      "Iteration 11353, loss = 0.00420855\n",
      "Iteration 11354, loss = 0.00420626\n",
      "Iteration 11355, loss = 0.00420398\n",
      "Iteration 11356, loss = 0.00420169\n",
      "Iteration 11357, loss = 0.00419941\n",
      "Iteration 11358, loss = 0.00419712\n",
      "Iteration 11359, loss = 0.00419484\n",
      "Iteration 11360, loss = 0.00419256\n",
      "Iteration 11361, loss = 0.00419028\n",
      "Iteration 11362, loss = 0.00418800\n",
      "Iteration 11363, loss = 0.00418573\n",
      "Iteration 11364, loss = 0.00418345\n",
      "Iteration 11365, loss = 0.00418118\n",
      "Iteration 11366, loss = 0.00417891\n",
      "Iteration 11367, loss = 0.00417664\n",
      "Iteration 11368, loss = 0.00417437\n",
      "Iteration 11369, loss = 0.00417210\n",
      "Iteration 11370, loss = 0.00416983\n",
      "Iteration 11371, loss = 0.00416757\n",
      "Iteration 11372, loss = 0.00416531\n",
      "Iteration 11373, loss = 0.00416304\n",
      "Iteration 11374, loss = 0.00416078\n",
      "Iteration 11375, loss = 0.00415852\n",
      "Iteration 11376, loss = 0.00415627\n",
      "Iteration 11377, loss = 0.00415401\n",
      "Iteration 11378, loss = 0.00415176\n",
      "Iteration 11379, loss = 0.00414950\n",
      "Iteration 11380, loss = 0.00414725\n",
      "Iteration 11381, loss = 0.00414500\n",
      "Iteration 11382, loss = 0.00414275\n",
      "Iteration 11383, loss = 0.00414050\n",
      "Iteration 11384, loss = 0.00413826\n",
      "Iteration 11385, loss = 0.00413601\n",
      "Iteration 11386, loss = 0.00413377\n",
      "Iteration 11387, loss = 0.00413152\n",
      "Iteration 11388, loss = 0.00412928\n",
      "Iteration 11389, loss = 0.00412704\n",
      "Iteration 11390, loss = 0.00412481\n",
      "Iteration 11391, loss = 0.00412257\n",
      "Iteration 11392, loss = 0.00412033\n",
      "Iteration 11393, loss = 0.00411810\n",
      "Iteration 11394, loss = 0.00411587\n",
      "Iteration 11395, loss = 0.00411364\n",
      "Iteration 11396, loss = 0.00411141\n",
      "Iteration 11397, loss = 0.00410918\n",
      "Iteration 11398, loss = 0.00410695\n",
      "Iteration 11399, loss = 0.00410473\n",
      "Iteration 11400, loss = 0.00410250\n",
      "Iteration 11401, loss = 0.00410028\n",
      "Iteration 11402, loss = 0.00409806\n",
      "Iteration 11403, loss = 0.00409584\n",
      "Iteration 11404, loss = 0.00409362\n",
      "Iteration 11405, loss = 0.00409141\n",
      "Iteration 11406, loss = 0.00408919\n",
      "Iteration 11407, loss = 0.00408698\n",
      "Iteration 11408, loss = 0.00408476\n",
      "Iteration 11409, loss = 0.00408255\n",
      "Iteration 11410, loss = 0.00408034\n",
      "Iteration 11411, loss = 0.00407813\n",
      "Iteration 11412, loss = 0.00407593\n",
      "Iteration 11413, loss = 0.00407372\n",
      "Iteration 11414, loss = 0.00407152\n",
      "Iteration 11415, loss = 0.00406931\n",
      "Iteration 11416, loss = 0.00406711\n",
      "Iteration 11417, loss = 0.00406491\n",
      "Iteration 11418, loss = 0.00406271\n",
      "Iteration 11419, loss = 0.00406052\n",
      "Iteration 11420, loss = 0.00405832\n",
      "Iteration 11421, loss = 0.00405613\n",
      "Iteration 11422, loss = 0.00405393\n",
      "Iteration 11423, loss = 0.00405174\n",
      "Iteration 11424, loss = 0.00404955\n",
      "Iteration 11425, loss = 0.00404736\n",
      "Iteration 11426, loss = 0.00404517\n",
      "Iteration 11427, loss = 0.00404299\n",
      "Iteration 11428, loss = 0.00404080\n",
      "Iteration 11429, loss = 0.00403862\n",
      "Iteration 11430, loss = 0.00403644\n",
      "Iteration 11431, loss = 0.00403426\n",
      "Iteration 11432, loss = 0.00403208\n",
      "Iteration 11433, loss = 0.00402990\n",
      "Iteration 11434, loss = 0.00402772\n",
      "Iteration 11435, loss = 0.00402555\n",
      "Iteration 11436, loss = 0.00402337\n",
      "Iteration 11437, loss = 0.00402120\n",
      "Iteration 11438, loss = 0.00401903\n",
      "Iteration 11439, loss = 0.00401686\n",
      "Iteration 11440, loss = 0.00401469\n",
      "Iteration 11441, loss = 0.00401253\n",
      "Iteration 11442, loss = 0.00401036\n",
      "Iteration 11443, loss = 0.00400820\n",
      "Iteration 11444, loss = 0.00400603\n",
      "Iteration 11445, loss = 0.00400387\n",
      "Iteration 11446, loss = 0.00400171\n",
      "Iteration 11447, loss = 0.00399955\n",
      "Iteration 11448, loss = 0.00399740\n",
      "Iteration 11449, loss = 0.00399524\n",
      "Iteration 11450, loss = 0.00399309\n",
      "Iteration 11451, loss = 0.00399093\n",
      "Iteration 11452, loss = 0.00398878\n",
      "Iteration 11453, loss = 0.00398663\n",
      "Iteration 11454, loss = 0.00398448\n",
      "Iteration 11455, loss = 0.00398233\n",
      "Iteration 11456, loss = 0.00398019\n",
      "Iteration 11457, loss = 0.00397804\n",
      "Iteration 11458, loss = 0.00397590\n",
      "Iteration 11459, loss = 0.00397376\n",
      "Iteration 11460, loss = 0.00397162\n",
      "Iteration 11461, loss = 0.00396948\n",
      "Iteration 11462, loss = 0.00396734\n",
      "Iteration 11463, loss = 0.00396520\n",
      "Iteration 11464, loss = 0.00396307\n",
      "Iteration 11465, loss = 0.00396093\n",
      "Iteration 11466, loss = 0.00395880\n",
      "Iteration 11467, loss = 0.00395667\n",
      "Iteration 11468, loss = 0.00395454\n",
      "Iteration 11469, loss = 0.00395241\n",
      "Iteration 11470, loss = 0.00395028\n",
      "Iteration 11471, loss = 0.00394816\n",
      "Iteration 11472, loss = 0.00394603\n",
      "Iteration 11473, loss = 0.00394391\n",
      "Iteration 11474, loss = 0.00394179\n",
      "Iteration 11475, loss = 0.00393967\n",
      "Iteration 11476, loss = 0.00393755\n",
      "Iteration 11477, loss = 0.00393543\n",
      "Iteration 11478, loss = 0.00393332\n",
      "Iteration 11479, loss = 0.00393120\n",
      "Iteration 11480, loss = 0.00392909\n",
      "Iteration 11481, loss = 0.00392697\n",
      "Iteration 11482, loss = 0.00392486\n",
      "Iteration 11483, loss = 0.00392275\n",
      "Iteration 11484, loss = 0.00392065\n",
      "Iteration 11485, loss = 0.00391854\n",
      "Iteration 11486, loss = 0.00391643\n",
      "Iteration 11487, loss = 0.00391433\n",
      "Iteration 11488, loss = 0.00391223\n",
      "Iteration 11489, loss = 0.00391013\n",
      "Iteration 11490, loss = 0.00390803\n",
      "Iteration 11491, loss = 0.00390593\n",
      "Iteration 11492, loss = 0.00390383\n",
      "Iteration 11493, loss = 0.00390173\n",
      "Iteration 11494, loss = 0.00389964\n",
      "Iteration 11495, loss = 0.00389755\n",
      "Iteration 11496, loss = 0.00389545\n",
      "Iteration 11497, loss = 0.00389336\n",
      "Iteration 11498, loss = 0.00389127\n",
      "Iteration 11499, loss = 0.00388919\n",
      "Iteration 11500, loss = 0.00388710\n",
      "Iteration 11501, loss = 0.00388501\n",
      "Iteration 11502, loss = 0.00388293\n",
      "Iteration 11503, loss = 0.00388085\n",
      "Iteration 11504, loss = 0.00387877\n",
      "Iteration 11505, loss = 0.00387669\n",
      "Iteration 11506, loss = 0.00387461\n",
      "Iteration 11507, loss = 0.00387253\n",
      "Iteration 11508, loss = 0.00387045\n",
      "Iteration 11509, loss = 0.00386838\n",
      "Iteration 11510, loss = 0.00386631\n",
      "Iteration 11511, loss = 0.00386423\n",
      "Iteration 11512, loss = 0.00386216\n",
      "Iteration 11513, loss = 0.00386009\n",
      "Iteration 11514, loss = 0.00385803\n",
      "Iteration 11515, loss = 0.00385596\n",
      "Iteration 11516, loss = 0.00385389\n",
      "Iteration 11517, loss = 0.00385183\n",
      "Iteration 11518, loss = 0.00384977\n",
      "Iteration 11519, loss = 0.00384771\n",
      "Iteration 11520, loss = 0.00384565\n",
      "Iteration 11521, loss = 0.00384359\n",
      "Iteration 11522, loss = 0.00384153\n",
      "Iteration 11523, loss = 0.00383948\n",
      "Iteration 11524, loss = 0.00383742\n",
      "Iteration 11525, loss = 0.00383537\n",
      "Iteration 11526, loss = 0.00383332\n",
      "Iteration 11527, loss = 0.00383126\n",
      "Iteration 11528, loss = 0.00382922\n",
      "Iteration 11529, loss = 0.00382717\n",
      "Iteration 11530, loss = 0.00382512\n",
      "Iteration 11531, loss = 0.00382308\n",
      "Iteration 11532, loss = 0.00382103\n",
      "Iteration 11533, loss = 0.00381899\n",
      "Iteration 11534, loss = 0.00381695\n",
      "Iteration 11535, loss = 0.00381491\n",
      "Iteration 11536, loss = 0.00381287\n",
      "Iteration 11537, loss = 0.00381083\n",
      "Iteration 11538, loss = 0.00380879\n",
      "Iteration 11539, loss = 0.00380676\n",
      "Iteration 11540, loss = 0.00380473\n",
      "Iteration 11541, loss = 0.00380269\n",
      "Iteration 11542, loss = 0.00380066\n",
      "Iteration 11543, loss = 0.00379863\n",
      "Iteration 11544, loss = 0.00379661\n",
      "Iteration 11545, loss = 0.00379458\n",
      "Iteration 11546, loss = 0.00379255\n",
      "Iteration 11547, loss = 0.00379053\n",
      "Iteration 11548, loss = 0.00378851\n",
      "Iteration 11549, loss = 0.00378648\n",
      "Iteration 11550, loss = 0.00378446\n",
      "Iteration 11551, loss = 0.00378245\n",
      "Iteration 11552, loss = 0.00378043\n",
      "Iteration 11553, loss = 0.00377841\n",
      "Iteration 11554, loss = 0.00377640\n",
      "Iteration 11555, loss = 0.00377438\n",
      "Iteration 11556, loss = 0.00377237\n",
      "Iteration 11557, loss = 0.00377036\n",
      "Iteration 11558, loss = 0.00376835\n",
      "Iteration 11559, loss = 0.00376634\n",
      "Iteration 11560, loss = 0.00376433\n",
      "Iteration 11561, loss = 0.00376233\n",
      "Iteration 11562, loss = 0.00376032\n",
      "Iteration 11563, loss = 0.00375832\n",
      "Iteration 11564, loss = 0.00375632\n",
      "Iteration 11565, loss = 0.00375432\n",
      "Iteration 11566, loss = 0.00375232\n",
      "Iteration 11567, loss = 0.00375032\n",
      "Iteration 11568, loss = 0.00374832\n",
      "Iteration 11569, loss = 0.00374633\n",
      "Iteration 11570, loss = 0.00374433\n",
      "Iteration 11571, loss = 0.00374234\n",
      "Iteration 11572, loss = 0.00374035\n",
      "Iteration 11573, loss = 0.00373836\n",
      "Iteration 11574, loss = 0.00373637\n",
      "Iteration 11575, loss = 0.00373438\n",
      "Iteration 11576, loss = 0.00373239\n",
      "Iteration 11577, loss = 0.00373041\n",
      "Iteration 11578, loss = 0.00372842\n",
      "Iteration 11579, loss = 0.00372644\n",
      "Iteration 11580, loss = 0.00372446\n",
      "Iteration 11581, loss = 0.00372248\n",
      "Iteration 11582, loss = 0.00372050\n",
      "Iteration 11583, loss = 0.00371852\n",
      "Iteration 11584, loss = 0.00371655\n",
      "Iteration 11585, loss = 0.00371457\n",
      "Iteration 11586, loss = 0.00371260\n",
      "Iteration 11587, loss = 0.00371063\n",
      "Iteration 11588, loss = 0.00370866\n",
      "Iteration 11589, loss = 0.00370669\n",
      "Iteration 11590, loss = 0.00370472\n",
      "Iteration 11591, loss = 0.00370275\n",
      "Iteration 11592, loss = 0.00370079\n",
      "Iteration 11593, loss = 0.00369882\n",
      "Iteration 11594, loss = 0.00369686\n",
      "Iteration 11595, loss = 0.00369490\n",
      "Iteration 11596, loss = 0.00369293\n",
      "Iteration 11597, loss = 0.00369098\n",
      "Iteration 11598, loss = 0.00368902\n",
      "Iteration 11599, loss = 0.00368706\n",
      "Iteration 11600, loss = 0.00368510\n",
      "Iteration 11601, loss = 0.00368315\n",
      "Iteration 11602, loss = 0.00368120\n",
      "Iteration 11603, loss = 0.00367925\n",
      "Iteration 11604, loss = 0.00367729\n",
      "Iteration 11605, loss = 0.00367535\n",
      "Iteration 11606, loss = 0.00367340\n",
      "Iteration 11607, loss = 0.00367145\n",
      "Iteration 11608, loss = 0.00366951\n",
      "Iteration 11609, loss = 0.00366756\n",
      "Iteration 11610, loss = 0.00366562\n",
      "Iteration 11611, loss = 0.00366368\n",
      "Iteration 11612, loss = 0.00366174\n",
      "Iteration 11613, loss = 0.00365980\n",
      "Iteration 11614, loss = 0.00365786\n",
      "Iteration 11615, loss = 0.00365592\n",
      "Iteration 11616, loss = 0.00365399\n",
      "Iteration 11617, loss = 0.00365205\n",
      "Iteration 11618, loss = 0.00365012\n",
      "Iteration 11619, loss = 0.00364819\n",
      "Iteration 11620, loss = 0.00364626\n",
      "Iteration 11621, loss = 0.00364433\n",
      "Iteration 11622, loss = 0.00364240\n",
      "Iteration 11623, loss = 0.00364048\n",
      "Iteration 11624, loss = 0.00363855\n",
      "Iteration 11625, loss = 0.00363663\n",
      "Iteration 11626, loss = 0.00363470\n",
      "Iteration 11627, loss = 0.00363278\n",
      "Iteration 11628, loss = 0.00363086\n",
      "Iteration 11629, loss = 0.00362894\n",
      "Iteration 11630, loss = 0.00362703\n",
      "Iteration 11631, loss = 0.00362511\n",
      "Iteration 11632, loss = 0.00362319\n",
      "Iteration 11633, loss = 0.00362128\n",
      "Iteration 11634, loss = 0.00361937\n",
      "Iteration 11635, loss = 0.00361746\n",
      "Iteration 11636, loss = 0.00361555\n",
      "Iteration 11637, loss = 0.00361364\n",
      "Iteration 11638, loss = 0.00361173\n",
      "Iteration 11639, loss = 0.00360982\n",
      "Iteration 11640, loss = 0.00360792\n",
      "Iteration 11641, loss = 0.00360602\n",
      "Iteration 11642, loss = 0.00360411\n",
      "Iteration 11643, loss = 0.00360221\n",
      "Iteration 11644, loss = 0.00360031\n",
      "Iteration 11645, loss = 0.00359841\n",
      "Iteration 11646, loss = 0.00359652\n",
      "Iteration 11647, loss = 0.00359462\n",
      "Iteration 11648, loss = 0.00359272\n",
      "Iteration 11649, loss = 0.00359083\n",
      "Iteration 11650, loss = 0.00358894\n",
      "Iteration 11651, loss = 0.00358705\n",
      "Iteration 11652, loss = 0.00358516\n",
      "Iteration 11653, loss = 0.00358327\n",
      "Iteration 11654, loss = 0.00358138\n",
      "Iteration 11655, loss = 0.00357949\n",
      "Iteration 11656, loss = 0.00357761\n",
      "Iteration 11657, loss = 0.00357572\n",
      "Iteration 11658, loss = 0.00357384\n",
      "Iteration 11659, loss = 0.00357196\n",
      "Iteration 11660, loss = 0.00357008\n",
      "Iteration 11661, loss = 0.00356820\n",
      "Iteration 11662, loss = 0.00356632\n",
      "Iteration 11663, loss = 0.00356445\n",
      "Iteration 11664, loss = 0.00356257\n",
      "Iteration 11665, loss = 0.00356070\n",
      "Iteration 11666, loss = 0.00355882\n",
      "Iteration 11667, loss = 0.00355695\n",
      "Iteration 11668, loss = 0.00355508\n",
      "Iteration 11669, loss = 0.00355321\n",
      "Iteration 11670, loss = 0.00355135\n",
      "Iteration 11671, loss = 0.00354948\n",
      "Iteration 11672, loss = 0.00354761\n",
      "Iteration 11673, loss = 0.00354575\n",
      "Iteration 11674, loss = 0.00354389\n",
      "Iteration 11675, loss = 0.00354203\n",
      "Iteration 11676, loss = 0.00354016\n",
      "Iteration 11677, loss = 0.00353831\n",
      "Iteration 11678, loss = 0.00353645\n",
      "Iteration 11679, loss = 0.00353459\n",
      "Iteration 11680, loss = 0.00353274\n",
      "Iteration 11681, loss = 0.00353088\n",
      "Iteration 11682, loss = 0.00352903\n",
      "Iteration 11683, loss = 0.00352718\n",
      "Iteration 11684, loss = 0.00352533\n",
      "Iteration 11685, loss = 0.00352348\n",
      "Iteration 11686, loss = 0.00352163\n",
      "Iteration 11687, loss = 0.00351978\n",
      "Iteration 11688, loss = 0.00351794\n",
      "Iteration 11689, loss = 0.00351609\n",
      "Iteration 11690, loss = 0.00351425\n",
      "Iteration 11691, loss = 0.00351241\n",
      "Iteration 11692, loss = 0.00351056\n",
      "Iteration 11693, loss = 0.00350872\n",
      "Iteration 11694, loss = 0.00350689\n",
      "Iteration 11695, loss = 0.00350505\n",
      "Iteration 11696, loss = 0.00350321\n",
      "Iteration 11697, loss = 0.00350138\n",
      "Iteration 11698, loss = 0.00349954\n",
      "Iteration 11699, loss = 0.00349771\n",
      "Iteration 11700, loss = 0.00349588\n",
      "Iteration 11701, loss = 0.00349405\n",
      "Iteration 11702, loss = 0.00349222\n",
      "Iteration 11703, loss = 0.00349039\n",
      "Iteration 11704, loss = 0.00348857\n",
      "Iteration 11705, loss = 0.00348674\n",
      "Iteration 11706, loss = 0.00348492\n",
      "Iteration 11707, loss = 0.00348310\n",
      "Iteration 11708, loss = 0.00348127\n",
      "Iteration 11709, loss = 0.00347945\n",
      "Iteration 11710, loss = 0.00347764\n",
      "Iteration 11711, loss = 0.00347582\n",
      "Iteration 11712, loss = 0.00347400\n",
      "Iteration 11713, loss = 0.00347219\n",
      "Iteration 11714, loss = 0.00347037\n",
      "Iteration 11715, loss = 0.00346856\n",
      "Iteration 11716, loss = 0.00346675\n",
      "Iteration 11717, loss = 0.00346494\n",
      "Iteration 11718, loss = 0.00346313\n",
      "Iteration 11719, loss = 0.00346132\n",
      "Iteration 11720, loss = 0.00345951\n",
      "Iteration 11721, loss = 0.00345770\n",
      "Iteration 11722, loss = 0.00345590\n",
      "Iteration 11723, loss = 0.00345410\n",
      "Iteration 11724, loss = 0.00345229\n",
      "Iteration 11725, loss = 0.00345049\n",
      "Iteration 11726, loss = 0.00344869\n",
      "Iteration 11727, loss = 0.00344689\n",
      "Iteration 11728, loss = 0.00344510\n",
      "Iteration 11729, loss = 0.00344330\n",
      "Iteration 11730, loss = 0.00344151\n",
      "Iteration 11731, loss = 0.00343971\n",
      "Iteration 11732, loss = 0.00343792\n",
      "Iteration 11733, loss = 0.00343613\n",
      "Iteration 11734, loss = 0.00343434\n",
      "Iteration 11735, loss = 0.00343255\n",
      "Iteration 11736, loss = 0.00343076\n",
      "Iteration 11737, loss = 0.00342897\n",
      "Iteration 11738, loss = 0.00342719\n",
      "Iteration 11739, loss = 0.00342540\n",
      "Iteration 11740, loss = 0.00342362\n",
      "Iteration 11741, loss = 0.00342184\n",
      "Iteration 11742, loss = 0.00342006\n",
      "Iteration 11743, loss = 0.00341828\n",
      "Iteration 11744, loss = 0.00341650\n",
      "Iteration 11745, loss = 0.00341472\n",
      "Iteration 11746, loss = 0.00341294\n",
      "Iteration 11747, loss = 0.00341117\n",
      "Iteration 11748, loss = 0.00340940\n",
      "Iteration 11749, loss = 0.00340762\n",
      "Iteration 11750, loss = 0.00340585\n",
      "Iteration 11751, loss = 0.00340408\n",
      "Iteration 11752, loss = 0.00340231\n",
      "Iteration 11753, loss = 0.00340054\n",
      "Iteration 11754, loss = 0.00339878\n",
      "Iteration 11755, loss = 0.00339701\n",
      "Iteration 11756, loss = 0.00339525\n",
      "Iteration 11757, loss = 0.00339348\n",
      "Iteration 11758, loss = 0.00339172\n",
      "Iteration 11759, loss = 0.00338996\n",
      "Iteration 11760, loss = 0.00338820\n",
      "Iteration 11761, loss = 0.00338644\n",
      "Iteration 11762, loss = 0.00338469\n",
      "Iteration 11763, loss = 0.00338293\n",
      "Iteration 11764, loss = 0.00338117\n",
      "Iteration 11765, loss = 0.00337942\n",
      "Iteration 11766, loss = 0.00337767\n",
      "Iteration 11767, loss = 0.00337592\n",
      "Iteration 11768, loss = 0.00337417\n",
      "Iteration 11769, loss = 0.00337242\n",
      "Iteration 11770, loss = 0.00337067\n",
      "Iteration 11771, loss = 0.00336892\n",
      "Iteration 11772, loss = 0.00336718\n",
      "Iteration 11773, loss = 0.00336543\n",
      "Iteration 11774, loss = 0.00336369\n",
      "Iteration 11775, loss = 0.00336195\n",
      "Iteration 11776, loss = 0.00336020\n",
      "Iteration 11777, loss = 0.00335846\n",
      "Iteration 11778, loss = 0.00335673\n",
      "Iteration 11779, loss = 0.00335499\n",
      "Iteration 11780, loss = 0.00335325\n",
      "Iteration 11781, loss = 0.00335152\n",
      "Iteration 11782, loss = 0.00334978\n",
      "Iteration 11783, loss = 0.00334805\n",
      "Iteration 11784, loss = 0.00334632\n",
      "Iteration 11785, loss = 0.00334459\n",
      "Iteration 11786, loss = 0.00334286\n",
      "Iteration 11787, loss = 0.00334113\n",
      "Iteration 11788, loss = 0.00333940\n",
      "Iteration 11789, loss = 0.00333768\n",
      "Iteration 11790, loss = 0.00333595\n",
      "Iteration 11791, loss = 0.00333423\n",
      "Iteration 11792, loss = 0.00333251\n",
      "Iteration 11793, loss = 0.00333078\n",
      "Iteration 11794, loss = 0.00332906\n",
      "Iteration 11795, loss = 0.00332734\n",
      "Iteration 11796, loss = 0.00332563\n",
      "Iteration 11797, loss = 0.00332391\n",
      "Iteration 11798, loss = 0.00332219\n",
      "Iteration 11799, loss = 0.00332048\n",
      "Iteration 11800, loss = 0.00331877\n",
      "Iteration 11801, loss = 0.00331705\n",
      "Iteration 11802, loss = 0.00331534\n",
      "Iteration 11803, loss = 0.00331363\n",
      "Iteration 11804, loss = 0.00331192\n",
      "Iteration 11805, loss = 0.00331022\n",
      "Iteration 11806, loss = 0.00330851\n",
      "Iteration 11807, loss = 0.00330680\n",
      "Iteration 11808, loss = 0.00330510\n",
      "Iteration 11809, loss = 0.00330340\n",
      "Iteration 11810, loss = 0.00330169\n",
      "Iteration 11811, loss = 0.00329999\n",
      "Iteration 11812, loss = 0.00329829\n",
      "Iteration 11813, loss = 0.00329660\n",
      "Iteration 11814, loss = 0.00329490\n",
      "Iteration 11815, loss = 0.00329320\n",
      "Iteration 11816, loss = 0.00329151\n",
      "Iteration 11817, loss = 0.00328981\n",
      "Iteration 11818, loss = 0.00328812\n",
      "Iteration 11819, loss = 0.00328643\n",
      "Iteration 11820, loss = 0.00328474\n",
      "Iteration 11821, loss = 0.00328305\n",
      "Iteration 11822, loss = 0.00328136\n",
      "Iteration 11823, loss = 0.00327967\n",
      "Iteration 11824, loss = 0.00327799\n",
      "Iteration 11825, loss = 0.00327630\n",
      "Iteration 11826, loss = 0.00327462\n",
      "Iteration 11827, loss = 0.00327293\n",
      "Iteration 11828, loss = 0.00327125\n",
      "Iteration 11829, loss = 0.00326957\n",
      "Iteration 11830, loss = 0.00326789\n",
      "Iteration 11831, loss = 0.00326621\n",
      "Iteration 11832, loss = 0.00326454\n",
      "Iteration 11833, loss = 0.00326286\n",
      "Iteration 11834, loss = 0.00326119\n",
      "Iteration 11835, loss = 0.00325951\n",
      "Iteration 11836, loss = 0.00325784\n",
      "Iteration 11837, loss = 0.00325617\n",
      "Iteration 11838, loss = 0.00325450\n",
      "Iteration 11839, loss = 0.00325283\n",
      "Iteration 11840, loss = 0.00325116\n",
      "Iteration 11841, loss = 0.00324949\n",
      "Iteration 11842, loss = 0.00324783\n",
      "Iteration 11843, loss = 0.00324616\n",
      "Iteration 11844, loss = 0.00324450\n",
      "Iteration 11845, loss = 0.00324284\n",
      "Iteration 11846, loss = 0.00324118\n",
      "Iteration 11847, loss = 0.00323952\n",
      "Iteration 11848, loss = 0.00323786\n",
      "Iteration 11849, loss = 0.00323620\n",
      "Iteration 11850, loss = 0.00323454\n",
      "Iteration 11851, loss = 0.00323289\n",
      "Iteration 11852, loss = 0.00323123\n",
      "Iteration 11853, loss = 0.00322958\n",
      "Iteration 11854, loss = 0.00322792\n",
      "Iteration 11855, loss = 0.00322627\n",
      "Iteration 11856, loss = 0.00322462\n",
      "Iteration 11857, loss = 0.00322297\n",
      "Iteration 11858, loss = 0.00322133\n",
      "Iteration 11859, loss = 0.00321968\n",
      "Iteration 11860, loss = 0.00321803\n",
      "Iteration 11861, loss = 0.00321639\n",
      "Iteration 11862, loss = 0.00321474\n",
      "Iteration 11863, loss = 0.00321310\n",
      "Iteration 11864, loss = 0.00321146\n",
      "Iteration 11865, loss = 0.00320982\n",
      "Iteration 11866, loss = 0.00320818\n",
      "Iteration 11867, loss = 0.00320654\n",
      "Iteration 11868, loss = 0.00320491\n",
      "Iteration 11869, loss = 0.00320327\n",
      "Iteration 11870, loss = 0.00320163\n",
      "Iteration 11871, loss = 0.00320000\n",
      "Iteration 11872, loss = 0.00319837\n",
      "Iteration 11873, loss = 0.00319674\n",
      "Iteration 11874, loss = 0.00319511\n",
      "Iteration 11875, loss = 0.00319348\n",
      "Iteration 11876, loss = 0.00319185\n",
      "Iteration 11877, loss = 0.00319022\n",
      "Iteration 11878, loss = 0.00318859\n",
      "Iteration 11879, loss = 0.00318697\n",
      "Iteration 11880, loss = 0.00318535\n",
      "Iteration 11881, loss = 0.00318372\n",
      "Iteration 11882, loss = 0.00318210\n",
      "Iteration 11883, loss = 0.00318048\n",
      "Iteration 11884, loss = 0.00317886\n",
      "Iteration 11885, loss = 0.00317724\n",
      "Iteration 11886, loss = 0.00317563\n",
      "Iteration 11887, loss = 0.00317401\n",
      "Iteration 11888, loss = 0.00317239\n",
      "Iteration 11889, loss = 0.00317078\n",
      "Iteration 11890, loss = 0.00316917\n",
      "Iteration 11891, loss = 0.00316755\n",
      "Iteration 11892, loss = 0.00316594\n",
      "Iteration 11893, loss = 0.00316433\n",
      "Iteration 11894, loss = 0.00316273\n",
      "Iteration 11895, loss = 0.00316112\n",
      "Iteration 11896, loss = 0.00315951\n",
      "Iteration 11897, loss = 0.00315791\n",
      "Iteration 11898, loss = 0.00315630\n",
      "Iteration 11899, loss = 0.00315470\n",
      "Iteration 11900, loss = 0.00315310\n",
      "Iteration 11901, loss = 0.00315149\n",
      "Iteration 11902, loss = 0.00314989\n",
      "Iteration 11903, loss = 0.00314830\n",
      "Iteration 11904, loss = 0.00314670\n",
      "Iteration 11905, loss = 0.00314510\n",
      "Iteration 11906, loss = 0.00314351\n",
      "Iteration 11907, loss = 0.00314191\n",
      "Iteration 11908, loss = 0.00314032\n",
      "Iteration 11909, loss = 0.00313872\n",
      "Iteration 11910, loss = 0.00313713\n",
      "Iteration 11911, loss = 0.00313554\n",
      "Iteration 11912, loss = 0.00313395\n",
      "Iteration 11913, loss = 0.00313237\n",
      "Iteration 11914, loss = 0.00313078\n",
      "Iteration 11915, loss = 0.00312919\n",
      "Iteration 11916, loss = 0.00312761\n",
      "Iteration 11917, loss = 0.00312602\n",
      "Iteration 11918, loss = 0.00312444\n",
      "Iteration 11919, loss = 0.00312286\n",
      "Iteration 11920, loss = 0.00312128\n",
      "Iteration 11921, loss = 0.00311970\n",
      "Iteration 11922, loss = 0.00311812\n",
      "Iteration 11923, loss = 0.00311654\n",
      "Iteration 11924, loss = 0.00311497\n",
      "Iteration 11925, loss = 0.00311339\n",
      "Iteration 11926, loss = 0.00311182\n",
      "Iteration 11927, loss = 0.00311024\n",
      "Iteration 11928, loss = 0.00310867\n",
      "Iteration 11929, loss = 0.00310710\n",
      "Iteration 11930, loss = 0.00310553\n",
      "Iteration 11931, loss = 0.00310396\n",
      "Iteration 11932, loss = 0.00310239\n",
      "Iteration 11933, loss = 0.00310083\n",
      "Iteration 11934, loss = 0.00309926\n",
      "Iteration 11935, loss = 0.00309769\n",
      "Iteration 11936, loss = 0.00309613\n",
      "Iteration 11937, loss = 0.00309457\n",
      "Iteration 11938, loss = 0.00309301\n",
      "Iteration 11939, loss = 0.00309145\n",
      "Iteration 11940, loss = 0.00308989\n",
      "Iteration 11941, loss = 0.00308833\n",
      "Iteration 11942, loss = 0.00308677\n",
      "Iteration 11943, loss = 0.00308521\n",
      "Iteration 11944, loss = 0.00308366\n",
      "Iteration 11945, loss = 0.00308211\n",
      "Iteration 11946, loss = 0.00308055\n",
      "Iteration 11947, loss = 0.00307900\n",
      "Iteration 11948, loss = 0.00307745\n",
      "Iteration 11949, loss = 0.00307590\n",
      "Iteration 11950, loss = 0.00307435\n",
      "Iteration 11951, loss = 0.00307280\n",
      "Iteration 11952, loss = 0.00307125\n",
      "Iteration 11953, loss = 0.00306971\n",
      "Iteration 11954, loss = 0.00306816\n",
      "Iteration 11955, loss = 0.00306662\n",
      "Iteration 11956, loss = 0.00306508\n",
      "Iteration 11957, loss = 0.00306354\n",
      "Iteration 11958, loss = 0.00306200\n",
      "Iteration 11959, loss = 0.00306046\n",
      "Iteration 11960, loss = 0.00305892\n",
      "Iteration 11961, loss = 0.00305738\n",
      "Iteration 11962, loss = 0.00305584\n",
      "Iteration 11963, loss = 0.00305431\n",
      "Iteration 11964, loss = 0.00305277\n",
      "Iteration 11965, loss = 0.00305124\n",
      "Iteration 11966, loss = 0.00304971\n",
      "Iteration 11967, loss = 0.00304818\n",
      "Iteration 11968, loss = 0.00304665\n",
      "Iteration 11969, loss = 0.00304512\n",
      "Iteration 11970, loss = 0.00304359\n",
      "Iteration 11971, loss = 0.00304206\n",
      "Iteration 11972, loss = 0.00304054\n",
      "Iteration 11973, loss = 0.00303901\n",
      "Iteration 11974, loss = 0.00303749\n",
      "Iteration 11975, loss = 0.00303596\n",
      "Iteration 11976, loss = 0.00303444\n",
      "Iteration 11977, loss = 0.00303292\n",
      "Iteration 11978, loss = 0.00303140\n",
      "Iteration 11979, loss = 0.00302988\n",
      "Iteration 11980, loss = 0.00302836\n",
      "Iteration 11981, loss = 0.00302685\n",
      "Iteration 11982, loss = 0.00302533\n",
      "Iteration 11983, loss = 0.00302382\n",
      "Iteration 11984, loss = 0.00302230\n",
      "Iteration 11985, loss = 0.00302079\n",
      "Iteration 11986, loss = 0.00301928\n",
      "Iteration 11987, loss = 0.00301777\n",
      "Iteration 11988, loss = 0.00301626\n",
      "Iteration 11989, loss = 0.00301475\n",
      "Iteration 11990, loss = 0.00301324\n",
      "Iteration 11991, loss = 0.00301174\n",
      "Iteration 11992, loss = 0.00301023\n",
      "Iteration 11993, loss = 0.00300873\n",
      "Iteration 11994, loss = 0.00300722\n",
      "Iteration 11995, loss = 0.00300572\n",
      "Iteration 11996, loss = 0.00300422\n",
      "Iteration 11997, loss = 0.00300272\n",
      "Iteration 11998, loss = 0.00300122\n",
      "Iteration 11999, loss = 0.00299972\n",
      "Iteration 12000, loss = 0.00299822\n",
      "Iteration 12001, loss = 0.00299673\n",
      "Iteration 12002, loss = 0.00299523\n",
      "Iteration 12003, loss = 0.00299374\n",
      "Iteration 12004, loss = 0.00299224\n",
      "Iteration 12005, loss = 0.00299075\n",
      "Iteration 12006, loss = 0.00298926\n",
      "Iteration 12007, loss = 0.00298777\n",
      "Iteration 12008, loss = 0.00298628\n",
      "Iteration 12009, loss = 0.00298479\n",
      "Iteration 12010, loss = 0.00298331\n",
      "Iteration 12011, loss = 0.00298182\n",
      "Iteration 12012, loss = 0.00298033\n",
      "Iteration 12013, loss = 0.00297885\n",
      "Iteration 12014, loss = 0.00297737\n",
      "Iteration 12015, loss = 0.00297588\n",
      "Iteration 12016, loss = 0.00297440\n",
      "Iteration 12017, loss = 0.00297292\n",
      "Iteration 12018, loss = 0.00297144\n",
      "Iteration 12019, loss = 0.00296997\n",
      "Iteration 12020, loss = 0.00296849\n",
      "Iteration 12021, loss = 0.00296701\n",
      "Iteration 12022, loss = 0.00296554\n",
      "Iteration 12023, loss = 0.00296406\n",
      "Iteration 12024, loss = 0.00296259\n",
      "Iteration 12025, loss = 0.00296112\n",
      "Iteration 12026, loss = 0.00295965\n",
      "Iteration 12027, loss = 0.00295818\n",
      "Iteration 12028, loss = 0.00295671\n",
      "Iteration 12029, loss = 0.00295524\n",
      "Iteration 12030, loss = 0.00295377\n",
      "Iteration 12031, loss = 0.00295231\n",
      "Iteration 12032, loss = 0.00295084\n",
      "Iteration 12033, loss = 0.00294938\n",
      "Iteration 12034, loss = 0.00294791\n",
      "Iteration 12035, loss = 0.00294645\n",
      "Iteration 12036, loss = 0.00294499\n",
      "Iteration 12037, loss = 0.00294353\n",
      "Iteration 12038, loss = 0.00294207\n",
      "Iteration 12039, loss = 0.00294061\n",
      "Iteration 12040, loss = 0.00293916\n",
      "Iteration 12041, loss = 0.00293770\n",
      "Iteration 12042, loss = 0.00293625\n",
      "Iteration 12043, loss = 0.00293479\n",
      "Iteration 12044, loss = 0.00293334\n",
      "Iteration 12045, loss = 0.00293189\n",
      "Iteration 12046, loss = 0.00293044\n",
      "Iteration 12047, loss = 0.00292899\n",
      "Iteration 12048, loss = 0.00292754\n",
      "Iteration 12049, loss = 0.00292609\n",
      "Iteration 12050, loss = 0.00292464\n",
      "Iteration 12051, loss = 0.00292319\n",
      "Iteration 12052, loss = 0.00292175\n",
      "Iteration 12053, loss = 0.00292030\n",
      "Iteration 12054, loss = 0.00291886\n",
      "Iteration 12055, loss = 0.00291742\n",
      "Iteration 12056, loss = 0.00291598\n",
      "Iteration 12057, loss = 0.00291454\n",
      "Iteration 12058, loss = 0.00291310\n",
      "Iteration 12059, loss = 0.00291166\n",
      "Iteration 12060, loss = 0.00291022\n",
      "Iteration 12061, loss = 0.00290879\n",
      "Iteration 12062, loss = 0.00290735\n",
      "Iteration 12063, loss = 0.00290592\n",
      "Iteration 12064, loss = 0.00290448\n",
      "Iteration 12065, loss = 0.00290305\n",
      "Iteration 12066, loss = 0.00290162\n",
      "Iteration 12067, loss = 0.00290019\n",
      "Iteration 12068, loss = 0.00289876\n",
      "Iteration 12069, loss = 0.00289733\n",
      "Iteration 12070, loss = 0.00289590\n",
      "Iteration 12071, loss = 0.00289448\n",
      "Iteration 12072, loss = 0.00289305\n",
      "Iteration 12073, loss = 0.00289163\n",
      "Iteration 12074, loss = 0.00289020\n",
      "Iteration 12075, loss = 0.00288878\n",
      "Iteration 12076, loss = 0.00288736\n",
      "Iteration 12077, loss = 0.00288594\n",
      "Iteration 12078, loss = 0.00288452\n",
      "Iteration 12079, loss = 0.00288310\n",
      "Iteration 12080, loss = 0.00288168\n",
      "Iteration 12081, loss = 0.00288026\n",
      "Iteration 12082, loss = 0.00287885\n",
      "Iteration 12083, loss = 0.00287743\n",
      "Iteration 12084, loss = 0.00287602\n",
      "Iteration 12085, loss = 0.00287461\n",
      "Iteration 12086, loss = 0.00287319\n",
      "Iteration 12087, loss = 0.00287178\n",
      "Iteration 12088, loss = 0.00287037\n",
      "Iteration 12089, loss = 0.00286896\n",
      "Iteration 12090, loss = 0.00286755\n",
      "Iteration 12091, loss = 0.00286615\n",
      "Iteration 12092, loss = 0.00286474\n",
      "Iteration 12093, loss = 0.00286334\n",
      "Iteration 12094, loss = 0.00286193\n",
      "Iteration 12095, loss = 0.00286053\n",
      "Iteration 12096, loss = 0.00285913\n",
      "Iteration 12097, loss = 0.00285772\n",
      "Iteration 12098, loss = 0.00285632\n",
      "Iteration 12099, loss = 0.00285492\n",
      "Iteration 12100, loss = 0.00285353\n",
      "Iteration 12101, loss = 0.00285213\n",
      "Iteration 12102, loss = 0.00285073\n",
      "Iteration 12103, loss = 0.00284934\n",
      "Iteration 12104, loss = 0.00284794\n",
      "Iteration 12105, loss = 0.00284655\n",
      "Iteration 12106, loss = 0.00284515\n",
      "Iteration 12107, loss = 0.00284376\n",
      "Iteration 12108, loss = 0.00284237\n",
      "Iteration 12109, loss = 0.00284098\n",
      "Iteration 12110, loss = 0.00283959\n",
      "Iteration 12111, loss = 0.00283820\n",
      "Iteration 12112, loss = 0.00283682\n",
      "Iteration 12113, loss = 0.00283543\n",
      "Iteration 12114, loss = 0.00283405\n",
      "Iteration 12115, loss = 0.00283266\n",
      "Iteration 12116, loss = 0.00283128\n",
      "Iteration 12117, loss = 0.00282990\n",
      "Iteration 12118, loss = 0.00282851\n",
      "Iteration 12119, loss = 0.00282713\n",
      "Iteration 12120, loss = 0.00282575\n",
      "Iteration 12121, loss = 0.00282438\n",
      "Iteration 12122, loss = 0.00282300\n",
      "Iteration 12123, loss = 0.00282162\n",
      "Iteration 12124, loss = 0.00282025\n",
      "Iteration 12125, loss = 0.00281887\n",
      "Iteration 12126, loss = 0.00281750\n",
      "Iteration 12127, loss = 0.00281612\n",
      "Iteration 12128, loss = 0.00281475\n",
      "Iteration 12129, loss = 0.00281338\n",
      "Iteration 12130, loss = 0.00281201\n",
      "Iteration 12131, loss = 0.00281064\n",
      "Iteration 12132, loss = 0.00280927\n",
      "Iteration 12133, loss = 0.00280791\n",
      "Iteration 12134, loss = 0.00280654\n",
      "Iteration 12135, loss = 0.00280517\n",
      "Iteration 12136, loss = 0.00280381\n",
      "Iteration 12137, loss = 0.00280245\n",
      "Iteration 12138, loss = 0.00280108\n",
      "Iteration 12139, loss = 0.00279972\n",
      "Iteration 12140, loss = 0.00279836\n",
      "Iteration 12141, loss = 0.00279700\n",
      "Iteration 12142, loss = 0.00279564\n",
      "Iteration 12143, loss = 0.00279429\n",
      "Iteration 12144, loss = 0.00279293\n",
      "Iteration 12145, loss = 0.00279157\n",
      "Iteration 12146, loss = 0.00279022\n",
      "Iteration 12147, loss = 0.00278886\n",
      "Iteration 12148, loss = 0.00278751\n",
      "Iteration 12149, loss = 0.00278616\n",
      "Iteration 12150, loss = 0.00278481\n",
      "Iteration 12151, loss = 0.00278346\n",
      "Iteration 12152, loss = 0.00278211\n",
      "Iteration 12153, loss = 0.00278076\n",
      "Iteration 12154, loss = 0.00277941\n",
      "Iteration 12155, loss = 0.00277806\n",
      "Iteration 12156, loss = 0.00277672\n",
      "Iteration 12157, loss = 0.00277537\n",
      "Iteration 12158, loss = 0.00277403\n",
      "Iteration 12159, loss = 0.00277269\n",
      "Iteration 12160, loss = 0.00277134\n",
      "Iteration 12161, loss = 0.00277000\n",
      "Iteration 12162, loss = 0.00276866\n",
      "Iteration 12163, loss = 0.00276732\n",
      "Iteration 12164, loss = 0.00276598\n",
      "Iteration 12165, loss = 0.00276465\n",
      "Iteration 12166, loss = 0.00276331\n",
      "Iteration 12167, loss = 0.00276197\n",
      "Iteration 12168, loss = 0.00276064\n",
      "Iteration 12169, loss = 0.00275931\n",
      "Iteration 12170, loss = 0.00275797\n",
      "Iteration 12171, loss = 0.00275664\n",
      "Iteration 12172, loss = 0.00275531\n",
      "Iteration 12173, loss = 0.00275398\n",
      "Iteration 12174, loss = 0.00275265\n",
      "Iteration 12175, loss = 0.00275132\n",
      "Iteration 12176, loss = 0.00274999\n",
      "Iteration 12177, loss = 0.00274867\n",
      "Iteration 12178, loss = 0.00274734\n",
      "Iteration 12179, loss = 0.00274602\n",
      "Iteration 12180, loss = 0.00274469\n",
      "Iteration 12181, loss = 0.00274337\n",
      "Iteration 12182, loss = 0.00274205\n",
      "Iteration 12183, loss = 0.00274073\n",
      "Iteration 12184, loss = 0.00273941\n",
      "Iteration 12185, loss = 0.00273809\n",
      "Iteration 12186, loss = 0.00273677\n",
      "Iteration 12187, loss = 0.00273545\n",
      "Iteration 12188, loss = 0.00273414\n",
      "Iteration 12189, loss = 0.00273282\n",
      "Iteration 12190, loss = 0.00273151\n",
      "Iteration 12191, loss = 0.00273019\n",
      "Iteration 12192, loss = 0.00272888\n",
      "Iteration 12193, loss = 0.00272757\n",
      "Iteration 12194, loss = 0.00272626\n",
      "Iteration 12195, loss = 0.00272495\n",
      "Iteration 12196, loss = 0.00272364\n",
      "Iteration 12197, loss = 0.00272233\n",
      "Iteration 12198, loss = 0.00272102\n",
      "Iteration 12199, loss = 0.00271972\n",
      "Iteration 12200, loss = 0.00271841\n",
      "Iteration 12201, loss = 0.00271711\n",
      "Iteration 12202, loss = 0.00271580\n",
      "Iteration 12203, loss = 0.00271450\n",
      "Iteration 12204, loss = 0.00271320\n",
      "Iteration 12205, loss = 0.00271190\n",
      "Iteration 12206, loss = 0.00271060\n",
      "Iteration 12207, loss = 0.00270930\n",
      "Iteration 12208, loss = 0.00270800\n",
      "Iteration 12209, loss = 0.00270670\n",
      "Iteration 12210, loss = 0.00270540\n",
      "Iteration 12211, loss = 0.00270411\n",
      "Iteration 12212, loss = 0.00270281\n",
      "Iteration 12213, loss = 0.00270152\n",
      "Iteration 12214, loss = 0.00270023\n",
      "Iteration 12215, loss = 0.00269893\n",
      "Iteration 12216, loss = 0.00269764\n",
      "Iteration 12217, loss = 0.00269635\n",
      "Iteration 12218, loss = 0.00269506\n",
      "Iteration 12219, loss = 0.00269377\n",
      "Iteration 12220, loss = 0.00269249\n",
      "Iteration 12221, loss = 0.00269120\n",
      "Iteration 12222, loss = 0.00268991\n",
      "Iteration 12223, loss = 0.00268863\n",
      "Iteration 12224, loss = 0.00268734\n",
      "Iteration 12225, loss = 0.00268606\n",
      "Iteration 12226, loss = 0.00268478\n",
      "Iteration 12227, loss = 0.00268350\n",
      "Iteration 12228, loss = 0.00268222\n",
      "Iteration 12229, loss = 0.00268094\n",
      "Iteration 12230, loss = 0.00267966\n",
      "Iteration 12231, loss = 0.00267838\n",
      "Iteration 12232, loss = 0.00267710\n",
      "Iteration 12233, loss = 0.00267583\n",
      "Iteration 12234, loss = 0.00267455\n",
      "Iteration 12235, loss = 0.00267328\n",
      "Iteration 12236, loss = 0.00267200\n",
      "Iteration 12237, loss = 0.00267073\n",
      "Iteration 12238, loss = 0.00266946\n",
      "Iteration 12239, loss = 0.00266819\n",
      "Iteration 12240, loss = 0.00266692\n",
      "Iteration 12241, loss = 0.00266565\n",
      "Iteration 12242, loss = 0.00266438\n",
      "Iteration 12243, loss = 0.00266311\n",
      "Iteration 12244, loss = 0.00266185\n",
      "Iteration 12245, loss = 0.00266058\n",
      "Iteration 12246, loss = 0.00265932\n",
      "Iteration 12247, loss = 0.00265805\n",
      "Iteration 12248, loss = 0.00265679\n",
      "Iteration 12249, loss = 0.00265553\n",
      "Iteration 12250, loss = 0.00265427\n",
      "Iteration 12251, loss = 0.00265301\n",
      "Iteration 12252, loss = 0.00265175\n",
      "Iteration 12253, loss = 0.00265049\n",
      "Iteration 12254, loss = 0.00264923\n",
      "Iteration 12255, loss = 0.00264797\n",
      "Iteration 12256, loss = 0.00264672\n",
      "Iteration 12257, loss = 0.00264546\n",
      "Iteration 12258, loss = 0.00264421\n",
      "Iteration 12259, loss = 0.00264296\n",
      "Iteration 12260, loss = 0.00264170\n",
      "Iteration 12261, loss = 0.00264045\n",
      "Iteration 12262, loss = 0.00263920\n",
      "Iteration 12263, loss = 0.00263795\n",
      "Iteration 12264, loss = 0.00263670\n",
      "Iteration 12265, loss = 0.00263545\n",
      "Iteration 12266, loss = 0.00263421\n",
      "Iteration 12267, loss = 0.00263296\n",
      "Iteration 12268, loss = 0.00263171\n",
      "Iteration 12269, loss = 0.00263047\n",
      "Iteration 12270, loss = 0.00262923\n",
      "Iteration 12271, loss = 0.00262798\n",
      "Iteration 12272, loss = 0.00262674\n",
      "Iteration 12273, loss = 0.00262550\n",
      "Iteration 12274, loss = 0.00262426\n",
      "Iteration 12275, loss = 0.00262302\n",
      "Iteration 12276, loss = 0.00262178\n",
      "Iteration 12277, loss = 0.00262054\n",
      "Iteration 12278, loss = 0.00261931\n",
      "Iteration 12279, loss = 0.00261807\n",
      "Iteration 12280, loss = 0.00261684\n",
      "Iteration 12281, loss = 0.00261560\n",
      "Iteration 12282, loss = 0.00261437\n",
      "Iteration 12283, loss = 0.00261314\n",
      "Iteration 12284, loss = 0.00261190\n",
      "Iteration 12285, loss = 0.00261067\n",
      "Iteration 12286, loss = 0.00260944\n",
      "Iteration 12287, loss = 0.00260821\n",
      "Iteration 12288, loss = 0.00260698\n",
      "Iteration 12289, loss = 0.00260576\n",
      "Iteration 12290, loss = 0.00260453\n",
      "Iteration 12291, loss = 0.00260331\n",
      "Iteration 12292, loss = 0.00260208\n",
      "Iteration 12293, loss = 0.00260086\n",
      "Iteration 12294, loss = 0.00259963\n",
      "Iteration 12295, loss = 0.00259841\n",
      "Iteration 12296, loss = 0.00259719\n",
      "Iteration 12297, loss = 0.00259597\n",
      "Iteration 12298, loss = 0.00259475\n",
      "Iteration 12299, loss = 0.00259353\n",
      "Iteration 12300, loss = 0.00259231\n",
      "Iteration 12301, loss = 0.00259109\n",
      "Iteration 12302, loss = 0.00258988\n",
      "Iteration 12303, loss = 0.00258866\n",
      "Iteration 12304, loss = 0.00258745\n",
      "Iteration 12305, loss = 0.00258623\n",
      "Iteration 12306, loss = 0.00258502\n",
      "Iteration 12307, loss = 0.00258381\n",
      "Iteration 12308, loss = 0.00258260\n",
      "Iteration 12309, loss = 0.00258139\n",
      "Iteration 12310, loss = 0.00258018\n",
      "Iteration 12311, loss = 0.00257897\n",
      "Iteration 12312, loss = 0.00257776\n",
      "Iteration 12313, loss = 0.00257655\n",
      "Iteration 12314, loss = 0.00257535\n",
      "Iteration 12315, loss = 0.00257414\n",
      "Iteration 12316, loss = 0.00257294\n",
      "Iteration 12317, loss = 0.00257173\n",
      "Iteration 12318, loss = 0.00257053\n",
      "Iteration 12319, loss = 0.00256933\n",
      "Iteration 12320, loss = 0.00256813\n",
      "Iteration 12321, loss = 0.00256693\n",
      "Iteration 12322, loss = 0.00256573\n",
      "Iteration 12323, loss = 0.00256453\n",
      "Iteration 12324, loss = 0.00256333\n",
      "Iteration 12325, loss = 0.00256213\n",
      "Iteration 12326, loss = 0.00256094\n",
      "Iteration 12327, loss = 0.00255974\n",
      "Iteration 12328, loss = 0.00255855\n",
      "Iteration 12329, loss = 0.00255735\n",
      "Iteration 12330, loss = 0.00255616\n",
      "Iteration 12331, loss = 0.00255497\n",
      "Iteration 12332, loss = 0.00255378\n",
      "Iteration 12333, loss = 0.00255259\n",
      "Iteration 12334, loss = 0.00255140\n",
      "Iteration 12335, loss = 0.00255021\n",
      "Iteration 12336, loss = 0.00254902\n",
      "Iteration 12337, loss = 0.00254783\n",
      "Iteration 12338, loss = 0.00254665\n",
      "Iteration 12339, loss = 0.00254546\n",
      "Iteration 12340, loss = 0.00254428\n",
      "Iteration 12341, loss = 0.00254309\n",
      "Iteration 12342, loss = 0.00254191\n",
      "Iteration 12343, loss = 0.00254073\n",
      "Iteration 12344, loss = 0.00253955\n",
      "Iteration 12345, loss = 0.00253836\n",
      "Iteration 12346, loss = 0.00253719\n",
      "Iteration 12347, loss = 0.00253601\n",
      "Iteration 12348, loss = 0.00253483\n",
      "Iteration 12349, loss = 0.00253365\n",
      "Iteration 12350, loss = 0.00253247\n",
      "Iteration 12351, loss = 0.00253130\n",
      "Iteration 12352, loss = 0.00253012\n",
      "Iteration 12353, loss = 0.00252895\n",
      "Iteration 12354, loss = 0.00252778\n",
      "Iteration 12355, loss = 0.00252661\n",
      "Iteration 12356, loss = 0.00252543\n",
      "Iteration 12357, loss = 0.00252426\n",
      "Iteration 12358, loss = 0.00252309\n",
      "Iteration 12359, loss = 0.00252192\n",
      "Iteration 12360, loss = 0.00252076\n",
      "Iteration 12361, loss = 0.00251959\n",
      "Iteration 12362, loss = 0.00251842\n",
      "Iteration 12363, loss = 0.00251726\n",
      "Iteration 12364, loss = 0.00251609\n",
      "Iteration 12365, loss = 0.00251493\n",
      "Iteration 12366, loss = 0.00251376\n",
      "Iteration 12367, loss = 0.00251260\n",
      "Iteration 12368, loss = 0.00251144\n",
      "Iteration 12369, loss = 0.00251028\n",
      "Iteration 12370, loss = 0.00250912\n",
      "Iteration 12371, loss = 0.00250796\n",
      "Iteration 12372, loss = 0.00250680\n",
      "Iteration 12373, loss = 0.00250564\n",
      "Iteration 12374, loss = 0.00250449\n",
      "Iteration 12375, loss = 0.00250333\n",
      "Iteration 12376, loss = 0.00250218\n",
      "Iteration 12377, loss = 0.00250102\n",
      "Iteration 12378, loss = 0.00249987\n",
      "Iteration 12379, loss = 0.00249872\n",
      "Iteration 12380, loss = 0.00249756\n",
      "Iteration 12381, loss = 0.00249641\n",
      "Iteration 12382, loss = 0.00249526\n",
      "Iteration 12383, loss = 0.00249411\n",
      "Iteration 12384, loss = 0.00249296\n",
      "Iteration 12385, loss = 0.00249182\n",
      "Iteration 12386, loss = 0.00249067\n",
      "Iteration 12387, loss = 0.00248952\n",
      "Iteration 12388, loss = 0.00248838\n",
      "Iteration 12389, loss = 0.00248723\n",
      "Iteration 12390, loss = 0.00248609\n",
      "Iteration 12391, loss = 0.00248495\n",
      "Iteration 12392, loss = 0.00248380\n",
      "Iteration 12393, loss = 0.00248266\n",
      "Iteration 12394, loss = 0.00248152\n",
      "Iteration 12395, loss = 0.00248038\n",
      "Iteration 12396, loss = 0.00247924\n",
      "Iteration 12397, loss = 0.00247810\n",
      "Iteration 12398, loss = 0.00247697\n",
      "Iteration 12399, loss = 0.00247583\n",
      "Iteration 12400, loss = 0.00247469\n",
      "Iteration 12401, loss = 0.00247356\n",
      "Iteration 12402, loss = 0.00247242\n",
      "Iteration 12403, loss = 0.00247129\n",
      "Iteration 12404, loss = 0.00247016\n",
      "Iteration 12405, loss = 0.00246903\n",
      "Iteration 12406, loss = 0.00246789\n",
      "Iteration 12407, loss = 0.00246676\n",
      "Iteration 12408, loss = 0.00246563\n",
      "Iteration 12409, loss = 0.00246451\n",
      "Iteration 12410, loss = 0.00246338\n",
      "Iteration 12411, loss = 0.00246225\n",
      "Iteration 12412, loss = 0.00246112\n",
      "Iteration 12413, loss = 0.00246000\n",
      "Iteration 12414, loss = 0.00245887\n",
      "Iteration 12415, loss = 0.00245775\n",
      "Iteration 12416, loss = 0.00245663\n",
      "Iteration 12417, loss = 0.00245550\n",
      "Iteration 12418, loss = 0.00245438\n",
      "Iteration 12419, loss = 0.00245326\n",
      "Iteration 12420, loss = 0.00245214\n",
      "Iteration 12421, loss = 0.00245102\n",
      "Iteration 12422, loss = 0.00244990\n",
      "Iteration 12423, loss = 0.00244879\n",
      "Iteration 12424, loss = 0.00244767\n",
      "Iteration 12425, loss = 0.00244655\n",
      "Iteration 12426, loss = 0.00244544\n",
      "Iteration 12427, loss = 0.00244432\n",
      "Iteration 12428, loss = 0.00244321\n",
      "Iteration 12429, loss = 0.00244210\n",
      "Iteration 12430, loss = 0.00244098\n",
      "Iteration 12431, loss = 0.00243987\n",
      "Iteration 12432, loss = 0.00243876\n",
      "Iteration 12433, loss = 0.00243765\n",
      "Iteration 12434, loss = 0.00243654\n",
      "Iteration 12435, loss = 0.00243544\n",
      "Iteration 12436, loss = 0.00243433\n",
      "Iteration 12437, loss = 0.00243322\n",
      "Iteration 12438, loss = 0.00243211\n",
      "Iteration 12439, loss = 0.00243101\n",
      "Iteration 12440, loss = 0.00242991\n",
      "Iteration 12441, loss = 0.00242880\n",
      "Iteration 12442, loss = 0.00242770\n",
      "Iteration 12443, loss = 0.00242660\n",
      "Iteration 12444, loss = 0.00242550\n",
      "Iteration 12445, loss = 0.00242439\n",
      "Iteration 12446, loss = 0.00242330\n",
      "Iteration 12447, loss = 0.00242220\n",
      "Iteration 12448, loss = 0.00242110\n",
      "Iteration 12449, loss = 0.00242000\n",
      "Iteration 12450, loss = 0.00241890\n",
      "Iteration 12451, loss = 0.00241781\n",
      "Iteration 12452, loss = 0.00241671\n",
      "Iteration 12453, loss = 0.00241562\n",
      "Iteration 12454, loss = 0.00241452\n",
      "Iteration 12455, loss = 0.00241343\n",
      "Iteration 12456, loss = 0.00241234\n",
      "Iteration 12457, loss = 0.00241125\n",
      "Iteration 12458, loss = 0.00241016\n",
      "Iteration 12459, loss = 0.00240907\n",
      "Iteration 12460, loss = 0.00240798\n",
      "Iteration 12461, loss = 0.00240689\n",
      "Iteration 12462, loss = 0.00240580\n",
      "Iteration 12463, loss = 0.00240472\n",
      "Iteration 12464, loss = 0.00240363\n",
      "Iteration 12465, loss = 0.00240255\n",
      "Iteration 12466, loss = 0.00240146\n",
      "Iteration 12467, loss = 0.00240038\n",
      "Iteration 12468, loss = 0.00239930\n",
      "Iteration 12469, loss = 0.00239821\n",
      "Iteration 12470, loss = 0.00239713\n",
      "Iteration 12471, loss = 0.00239605\n",
      "Iteration 12472, loss = 0.00239497\n",
      "Iteration 12473, loss = 0.00239389\n",
      "Iteration 12474, loss = 0.00239281\n",
      "Iteration 12475, loss = 0.00239174\n",
      "Iteration 12476, loss = 0.00239066\n",
      "Iteration 12477, loss = 0.00238958\n",
      "Iteration 12478, loss = 0.00238851\n",
      "Iteration 12479, loss = 0.00238743\n",
      "Iteration 12480, loss = 0.00238636\n",
      "Iteration 12481, loss = 0.00238529\n",
      "Iteration 12482, loss = 0.00238422\n",
      "Iteration 12483, loss = 0.00238314\n",
      "Iteration 12484, loss = 0.00238207\n",
      "Iteration 12485, loss = 0.00238100\n",
      "Iteration 12486, loss = 0.00237993\n",
      "Iteration 12487, loss = 0.00237887\n",
      "Iteration 12488, loss = 0.00237780\n",
      "Iteration 12489, loss = 0.00237673\n",
      "Iteration 12490, loss = 0.00237567\n",
      "Iteration 12491, loss = 0.00237460\n",
      "Iteration 12492, loss = 0.00237354\n",
      "Iteration 12493, loss = 0.00237247\n",
      "Iteration 12494, loss = 0.00237141\n",
      "Iteration 12495, loss = 0.00237035\n",
      "Iteration 12496, loss = 0.00236928\n",
      "Iteration 12497, loss = 0.00236822\n",
      "Iteration 12498, loss = 0.00236716\n",
      "Iteration 12499, loss = 0.00236610\n",
      "Iteration 12500, loss = 0.00236505\n",
      "Iteration 12501, loss = 0.00236399\n",
      "Iteration 12502, loss = 0.00236293\n",
      "Iteration 12503, loss = 0.00236187\n",
      "Iteration 12504, loss = 0.00236082\n",
      "Iteration 12505, loss = 0.00235976\n",
      "Iteration 12506, loss = 0.00235871\n",
      "Iteration 12507, loss = 0.00235766\n",
      "Iteration 12508, loss = 0.00235660\n",
      "Iteration 12509, loss = 0.00235555\n",
      "Iteration 12510, loss = 0.00235450\n",
      "Iteration 12511, loss = 0.00235345\n",
      "Iteration 12512, loss = 0.00235240\n",
      "Iteration 12513, loss = 0.00235135\n",
      "Iteration 12514, loss = 0.00235030\n",
      "Iteration 12515, loss = 0.00234926\n",
      "Iteration 12516, loss = 0.00234821\n",
      "Iteration 12517, loss = 0.00234716\n",
      "Iteration 12518, loss = 0.00234612\n",
      "Iteration 12519, loss = 0.00234507\n",
      "Iteration 12520, loss = 0.00234403\n",
      "Iteration 12521, loss = 0.00234299\n",
      "Iteration 12522, loss = 0.00234194\n",
      "Iteration 12523, loss = 0.00234090\n",
      "Iteration 12524, loss = 0.00233986\n",
      "Iteration 12525, loss = 0.00233882\n",
      "Iteration 12526, loss = 0.00233778\n",
      "Iteration 12527, loss = 0.00233674\n",
      "Iteration 12528, loss = 0.00233571\n",
      "Iteration 12529, loss = 0.00233467\n",
      "Iteration 12530, loss = 0.00233363\n",
      "Iteration 12531, loss = 0.00233260\n",
      "Iteration 12532, loss = 0.00233156\n",
      "Iteration 12533, loss = 0.00233053\n",
      "Iteration 12534, loss = 0.00232949\n",
      "Iteration 12535, loss = 0.00232846\n",
      "Iteration 12536, loss = 0.00232743\n",
      "Iteration 12537, loss = 0.00232640\n",
      "Iteration 12538, loss = 0.00232537\n",
      "Iteration 12539, loss = 0.00232434\n",
      "Iteration 12540, loss = 0.00232331\n",
      "Iteration 12541, loss = 0.00232228\n",
      "Iteration 12542, loss = 0.00232125\n",
      "Iteration 12543, loss = 0.00232022\n",
      "Iteration 12544, loss = 0.00231920\n",
      "Iteration 12545, loss = 0.00231817\n",
      "Iteration 12546, loss = 0.00231715\n",
      "Iteration 12547, loss = 0.00231612\n",
      "Iteration 12548, loss = 0.00231510\n",
      "Iteration 12549, loss = 0.00231408\n",
      "Iteration 12550, loss = 0.00231306\n",
      "Iteration 12551, loss = 0.00231204\n",
      "Iteration 12552, loss = 0.00231102\n",
      "Iteration 12553, loss = 0.00231000\n",
      "Iteration 12554, loss = 0.00230898\n",
      "Iteration 12555, loss = 0.00230796\n",
      "Iteration 12556, loss = 0.00230694\n",
      "Iteration 12557, loss = 0.00230592\n",
      "Iteration 12558, loss = 0.00230491\n",
      "Iteration 12559, loss = 0.00230389\n",
      "Iteration 12560, loss = 0.00230288\n",
      "Iteration 12561, loss = 0.00230186\n",
      "Iteration 12562, loss = 0.00230085\n",
      "Iteration 12563, loss = 0.00229984\n",
      "Iteration 12564, loss = 0.00229883\n",
      "Iteration 12565, loss = 0.00229782\n",
      "Iteration 12566, loss = 0.00229681\n",
      "Iteration 12567, loss = 0.00229580\n",
      "Iteration 12568, loss = 0.00229479\n",
      "Iteration 12569, loss = 0.00229378\n",
      "Iteration 12570, loss = 0.00229277\n",
      "Iteration 12571, loss = 0.00229177\n",
      "Iteration 12572, loss = 0.00229076\n",
      "Iteration 12573, loss = 0.00228975\n",
      "Iteration 12574, loss = 0.00228875\n",
      "Iteration 12575, loss = 0.00228775\n",
      "Iteration 12576, loss = 0.00228674\n",
      "Iteration 12577, loss = 0.00228574\n",
      "Iteration 12578, loss = 0.00228474\n",
      "Iteration 12579, loss = 0.00228374\n",
      "Iteration 12580, loss = 0.00228274\n",
      "Iteration 12581, loss = 0.00228174\n",
      "Iteration 12582, loss = 0.00228074\n",
      "Iteration 12583, loss = 0.00227974\n",
      "Iteration 12584, loss = 0.00227874\n",
      "Iteration 12585, loss = 0.00227775\n",
      "Iteration 12586, loss = 0.00227675\n",
      "Iteration 12587, loss = 0.00227576\n",
      "Iteration 12588, loss = 0.00227476\n",
      "Iteration 12589, loss = 0.00227377\n",
      "Iteration 12590, loss = 0.00227277\n",
      "Iteration 12591, loss = 0.00227178\n",
      "Training loss did not improve more than tol=0.000001 for 10 consecutive epochs. Stopping.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.82      1.00      0.90        14\n",
      "           2       0.94      0.85      0.89        20\n",
      "           3       0.95      0.90      0.92        20\n",
      "\n",
      "    accuracy                           0.91        54\n",
      "   macro avg       0.91      0.92      0.91        54\n",
      "weighted avg       0.91      0.91      0.91        54\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "hidden_layers = (30,)\n",
    "activation = 'logistic'\n",
    "opts = dict(hidden_layer_sizes = hidden_layers,verbose = True,activation = activation,tol = 1e-6,max_iter = int(1e6))\n",
    "solver = 'adam'\n",
    "clf_MLP = MLPClassifier(solver = solver, **opts)\n",
    "clf_MLP.fit(Z_train,y_train)\n",
    "predictions = clf_MLP.predict(Z_test)\n",
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00        45\n",
      "           2       1.00      1.00      1.00        51\n",
      "           3       1.00      1.00      1.00        28\n",
      "\n",
      "    accuracy                           1.00       124\n",
      "   macro avg       1.00      1.00      1.00       124\n",
      "weighted avg       1.00      1.00      1.00       124\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred = clf_MLP.predict(Z_train)\n",
    "print(classification_report(y_train,pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- hidden_layers = (512,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.18123021\n",
      "Iteration 2, loss = 1.10115075\n",
      "Iteration 3, loss = 1.07781314\n",
      "Iteration 4, loss = 1.08883961\n",
      "Iteration 5, loss = 1.09155232\n",
      "Iteration 6, loss = 1.07651814\n",
      "Iteration 7, loss = 1.05448096\n",
      "Iteration 8, loss = 1.03605757\n",
      "Iteration 9, loss = 1.02547738\n",
      "Iteration 10, loss = 1.02043284\n",
      "Iteration 11, loss = 1.01604903\n",
      "Iteration 12, loss = 1.00892246\n",
      "Iteration 13, loss = 0.99830569\n",
      "Iteration 14, loss = 0.98553558\n",
      "Iteration 15, loss = 0.97282845\n",
      "Iteration 16, loss = 0.96193745\n",
      "Iteration 17, loss = 0.95324753\n",
      "Iteration 18, loss = 0.94576825\n",
      "Iteration 19, loss = 0.93800071\n",
      "Iteration 20, loss = 0.92900820\n",
      "Iteration 21, loss = 0.91886992\n",
      "Iteration 22, loss = 0.90832363\n",
      "Iteration 23, loss = 0.89808829\n",
      "Iteration 24, loss = 0.88845313\n",
      "Iteration 25, loss = 0.87930972\n",
      "Iteration 26, loss = 0.87037870\n",
      "Iteration 27, loss = 0.86136669\n",
      "Iteration 28, loss = 0.85204852\n",
      "Iteration 29, loss = 0.84235475\n",
      "Iteration 30, loss = 0.83241469\n",
      "Iteration 31, loss = 0.82247258\n",
      "Iteration 32, loss = 0.81272308\n",
      "Iteration 33, loss = 0.80319756\n",
      "Iteration 34, loss = 0.79377654\n",
      "Iteration 35, loss = 0.78429097\n",
      "Iteration 36, loss = 0.77462266\n",
      "Iteration 37, loss = 0.76475165\n",
      "Iteration 38, loss = 0.75475298\n",
      "Iteration 39, loss = 0.74475513\n",
      "Iteration 40, loss = 0.73486779\n",
      "Iteration 41, loss = 0.72511038\n",
      "Iteration 42, loss = 0.71539741\n",
      "Iteration 43, loss = 0.70560462\n",
      "Iteration 44, loss = 0.69566584\n",
      "Iteration 45, loss = 0.68561876\n",
      "Iteration 46, loss = 0.67556733\n",
      "Iteration 47, loss = 0.66560154\n",
      "Iteration 48, loss = 0.65574066\n",
      "Iteration 49, loss = 0.64593538\n",
      "Iteration 50, loss = 0.63611642\n",
      "Iteration 51, loss = 0.62624891\n",
      "Iteration 52, loss = 0.61635528\n",
      "Iteration 53, loss = 0.60649567\n",
      "Iteration 54, loss = 0.59672463\n",
      "Iteration 55, loss = 0.58705744\n",
      "Iteration 56, loss = 0.57746909\n",
      "Iteration 57, loss = 0.56792286\n",
      "Iteration 58, loss = 0.55840303\n",
      "Iteration 59, loss = 0.54892675\n",
      "Iteration 60, loss = 0.53953003\n",
      "Iteration 61, loss = 0.53024300\n",
      "Iteration 62, loss = 0.52107435\n",
      "Iteration 63, loss = 0.51201371\n",
      "Iteration 64, loss = 0.50304663\n",
      "Iteration 65, loss = 0.49416897\n",
      "Iteration 66, loss = 0.48539103\n",
      "Iteration 67, loss = 0.47673080\n",
      "Iteration 68, loss = 0.46820317\n",
      "Iteration 69, loss = 0.45981312\n",
      "Iteration 70, loss = 0.45155685\n",
      "Iteration 71, loss = 0.44342825\n",
      "Iteration 72, loss = 0.43542548\n",
      "Iteration 73, loss = 0.42755303\n",
      "Iteration 74, loss = 0.41981900\n",
      "Iteration 75, loss = 0.41223039\n",
      "Iteration 76, loss = 0.40479009\n",
      "Iteration 77, loss = 0.39749685\n",
      "Iteration 78, loss = 0.39034781\n",
      "Iteration 79, loss = 0.38334114\n",
      "Iteration 80, loss = 0.37647727\n",
      "Iteration 81, loss = 0.36975822\n",
      "Iteration 82, loss = 0.36318594\n",
      "Iteration 83, loss = 0.35676083\n",
      "Iteration 84, loss = 0.35048148\n",
      "Iteration 85, loss = 0.34434530\n",
      "Iteration 86, loss = 0.33834968\n",
      "Iteration 87, loss = 0.33249270\n",
      "Iteration 88, loss = 0.32677328\n",
      "Iteration 89, loss = 0.32119060\n",
      "Iteration 90, loss = 0.31574354\n",
      "Iteration 91, loss = 0.31043026\n",
      "Iteration 92, loss = 0.30524826\n",
      "Iteration 93, loss = 0.30019474\n",
      "Iteration 94, loss = 0.29526691\n",
      "Iteration 95, loss = 0.29046227\n",
      "Iteration 96, loss = 0.28577855\n",
      "Iteration 97, loss = 0.28121355\n",
      "Iteration 98, loss = 0.27676489\n",
      "Iteration 99, loss = 0.27242999\n",
      "Iteration 100, loss = 0.26820604\n",
      "Iteration 101, loss = 0.26409018\n",
      "Iteration 102, loss = 0.26007957\n",
      "Iteration 103, loss = 0.25617154\n",
      "Iteration 104, loss = 0.25236354\n",
      "Iteration 105, loss = 0.24865307\n",
      "Iteration 106, loss = 0.24503768\n",
      "Iteration 107, loss = 0.24151483\n",
      "Iteration 108, loss = 0.23808199\n",
      "Iteration 109, loss = 0.23473661\n",
      "Iteration 110, loss = 0.23147617\n",
      "Iteration 111, loss = 0.22829824\n",
      "Iteration 112, loss = 0.22520047\n",
      "Iteration 113, loss = 0.22218058\n",
      "Iteration 114, loss = 0.21923635\n",
      "Iteration 115, loss = 0.21636558\n",
      "Iteration 116, loss = 0.21356613\n",
      "Iteration 117, loss = 0.21083585\n",
      "Iteration 118, loss = 0.20817268\n",
      "Iteration 119, loss = 0.20557460\n",
      "Iteration 120, loss = 0.20303965\n",
      "Iteration 121, loss = 0.20056595\n",
      "Iteration 122, loss = 0.19815167\n",
      "Iteration 123, loss = 0.19579504\n",
      "Iteration 124, loss = 0.19349435\n",
      "Iteration 125, loss = 0.19124792\n",
      "Iteration 126, loss = 0.18905411\n",
      "Iteration 127, loss = 0.18691134\n",
      "Iteration 128, loss = 0.18481807\n",
      "Iteration 129, loss = 0.18277282\n",
      "Iteration 130, loss = 0.18077415\n",
      "Iteration 131, loss = 0.17882069\n",
      "Iteration 132, loss = 0.17691107\n",
      "Iteration 133, loss = 0.17504402\n",
      "Iteration 134, loss = 0.17321827\n",
      "Iteration 135, loss = 0.17143260\n",
      "Iteration 136, loss = 0.16968583\n",
      "Iteration 137, loss = 0.16797682\n",
      "Iteration 138, loss = 0.16630447\n",
      "Iteration 139, loss = 0.16466771\n",
      "Iteration 140, loss = 0.16306552\n",
      "Iteration 141, loss = 0.16149691\n",
      "Iteration 142, loss = 0.15996091\n",
      "Iteration 143, loss = 0.15845660\n",
      "Iteration 144, loss = 0.15698309\n",
      "Iteration 145, loss = 0.15553950\n",
      "Iteration 146, loss = 0.15412499\n",
      "Iteration 147, loss = 0.15273877\n",
      "Iteration 148, loss = 0.15138004\n",
      "Iteration 149, loss = 0.15004804\n",
      "Iteration 150, loss = 0.14874205\n",
      "Iteration 151, loss = 0.14746136\n",
      "Iteration 152, loss = 0.14620529\n",
      "Iteration 153, loss = 0.14497317\n",
      "Iteration 154, loss = 0.14376436\n",
      "Iteration 155, loss = 0.14257825\n",
      "Iteration 156, loss = 0.14141424\n",
      "Iteration 157, loss = 0.14027176\n",
      "Iteration 158, loss = 0.13915023\n",
      "Iteration 159, loss = 0.13804912\n",
      "Iteration 160, loss = 0.13696791\n",
      "Iteration 161, loss = 0.13590609\n",
      "Iteration 162, loss = 0.13486317\n",
      "Iteration 163, loss = 0.13383867\n",
      "Iteration 164, loss = 0.13283213\n",
      "Iteration 165, loss = 0.13184312\n",
      "Iteration 166, loss = 0.13087119\n",
      "Iteration 167, loss = 0.12991594\n",
      "Iteration 168, loss = 0.12897695\n",
      "Iteration 169, loss = 0.12805383\n",
      "Iteration 170, loss = 0.12714621\n",
      "Iteration 171, loss = 0.12625371\n",
      "Iteration 172, loss = 0.12537598\n",
      "Iteration 173, loss = 0.12451268\n",
      "Iteration 174, loss = 0.12366346\n",
      "Iteration 175, loss = 0.12282800\n",
      "Iteration 176, loss = 0.12200598\n",
      "Iteration 177, loss = 0.12119711\n",
      "Iteration 178, loss = 0.12040107\n",
      "Iteration 179, loss = 0.11961759\n",
      "Iteration 180, loss = 0.11884638\n",
      "Iteration 181, loss = 0.11808717\n",
      "Iteration 182, loss = 0.11733970\n",
      "Iteration 183, loss = 0.11660370\n",
      "Iteration 184, loss = 0.11587894\n",
      "Iteration 185, loss = 0.11516516\n",
      "Iteration 186, loss = 0.11446213\n",
      "Iteration 187, loss = 0.11376963\n",
      "Iteration 188, loss = 0.11308742\n",
      "Iteration 189, loss = 0.11241530\n",
      "Iteration 190, loss = 0.11175306\n",
      "Iteration 191, loss = 0.11110048\n",
      "Iteration 192, loss = 0.11045737\n",
      "Iteration 193, loss = 0.10982354\n",
      "Iteration 194, loss = 0.10919880\n",
      "Iteration 195, loss = 0.10858296\n",
      "Iteration 196, loss = 0.10797585\n",
      "Iteration 197, loss = 0.10737729\n",
      "Iteration 198, loss = 0.10678711\n",
      "Iteration 199, loss = 0.10620515\n",
      "Iteration 200, loss = 0.10563125\n",
      "Iteration 201, loss = 0.10506525\n",
      "Iteration 202, loss = 0.10450699\n",
      "Iteration 203, loss = 0.10395633\n",
      "Iteration 204, loss = 0.10341313\n",
      "Iteration 205, loss = 0.10287724\n",
      "Iteration 206, loss = 0.10234853\n",
      "Iteration 207, loss = 0.10182686\n",
      "Iteration 208, loss = 0.10131209\n",
      "Iteration 209, loss = 0.10080411\n",
      "Iteration 210, loss = 0.10030279\n",
      "Iteration 211, loss = 0.09980800\n",
      "Iteration 212, loss = 0.09931963\n",
      "Iteration 213, loss = 0.09883756\n",
      "Iteration 214, loss = 0.09836168\n",
      "Iteration 215, loss = 0.09789188\n",
      "Iteration 216, loss = 0.09742804\n",
      "Iteration 217, loss = 0.09697007\n",
      "Iteration 218, loss = 0.09651786\n",
      "Iteration 219, loss = 0.09607131\n",
      "Iteration 220, loss = 0.09563033\n",
      "Iteration 221, loss = 0.09519481\n",
      "Iteration 222, loss = 0.09476466\n",
      "Iteration 223, loss = 0.09433979\n",
      "Iteration 224, loss = 0.09392012\n",
      "Iteration 225, loss = 0.09350554\n",
      "Iteration 226, loss = 0.09309599\n",
      "Iteration 227, loss = 0.09269137\n",
      "Iteration 228, loss = 0.09229160\n",
      "Iteration 229, loss = 0.09189660\n",
      "Iteration 230, loss = 0.09150630\n",
      "Iteration 231, loss = 0.09112061\n",
      "Iteration 232, loss = 0.09073947\n",
      "Iteration 233, loss = 0.09036279\n",
      "Iteration 234, loss = 0.08999051\n",
      "Iteration 235, loss = 0.08962255\n",
      "Iteration 236, loss = 0.08925885\n",
      "Iteration 237, loss = 0.08889934\n",
      "Iteration 238, loss = 0.08854395\n",
      "Iteration 239, loss = 0.08819262\n",
      "Iteration 240, loss = 0.08784528\n",
      "Iteration 241, loss = 0.08750188\n",
      "Iteration 242, loss = 0.08716235\n",
      "Iteration 243, loss = 0.08682663\n",
      "Iteration 244, loss = 0.08649466\n",
      "Iteration 245, loss = 0.08616639\n",
      "Iteration 246, loss = 0.08584176\n",
      "Iteration 247, loss = 0.08552071\n",
      "Iteration 248, loss = 0.08520320\n",
      "Iteration 249, loss = 0.08488916\n",
      "Iteration 250, loss = 0.08457855\n",
      "Iteration 251, loss = 0.08427132\n",
      "Iteration 252, loss = 0.08396741\n",
      "Iteration 253, loss = 0.08366677\n",
      "Iteration 254, loss = 0.08336937\n",
      "Iteration 255, loss = 0.08307515\n",
      "Iteration 256, loss = 0.08278406\n",
      "Iteration 257, loss = 0.08249606\n",
      "Iteration 258, loss = 0.08221111\n",
      "Iteration 259, loss = 0.08192916\n",
      "Iteration 260, loss = 0.08165017\n",
      "Iteration 261, loss = 0.08137409\n",
      "Iteration 262, loss = 0.08110089\n",
      "Iteration 263, loss = 0.08083053\n",
      "Iteration 264, loss = 0.08056296\n",
      "Iteration 265, loss = 0.08029815\n",
      "Iteration 266, loss = 0.08003606\n",
      "Iteration 267, loss = 0.07977664\n",
      "Iteration 268, loss = 0.07951987\n",
      "Iteration 269, loss = 0.07926571\n",
      "Iteration 270, loss = 0.07901412\n",
      "Iteration 271, loss = 0.07876506\n",
      "Iteration 272, loss = 0.07851851\n",
      "Iteration 273, loss = 0.07827442\n",
      "Iteration 274, loss = 0.07803277\n",
      "Iteration 275, loss = 0.07779352\n",
      "Iteration 276, loss = 0.07755664\n",
      "Iteration 277, loss = 0.07732210\n",
      "Iteration 278, loss = 0.07708987\n",
      "Iteration 279, loss = 0.07685991\n",
      "Iteration 280, loss = 0.07663220\n",
      "Iteration 281, loss = 0.07640671\n",
      "Iteration 282, loss = 0.07618341\n",
      "Iteration 283, loss = 0.07596226\n",
      "Iteration 284, loss = 0.07574325\n",
      "Iteration 285, loss = 0.07552634\n",
      "Iteration 286, loss = 0.07531151\n",
      "Iteration 287, loss = 0.07509873\n",
      "Iteration 288, loss = 0.07488798\n",
      "Iteration 289, loss = 0.07467922\n",
      "Iteration 290, loss = 0.07447244\n",
      "Iteration 291, loss = 0.07426760\n",
      "Iteration 292, loss = 0.07406469\n",
      "Iteration 293, loss = 0.07386368\n",
      "Iteration 294, loss = 0.07366454\n",
      "Iteration 295, loss = 0.07346726\n",
      "Iteration 296, loss = 0.07327180\n",
      "Iteration 297, loss = 0.07307815\n",
      "Iteration 298, loss = 0.07288629\n",
      "Iteration 299, loss = 0.07269619\n",
      "Iteration 300, loss = 0.07250783\n",
      "Iteration 301, loss = 0.07232119\n",
      "Iteration 302, loss = 0.07213624\n",
      "Iteration 303, loss = 0.07195298\n",
      "Iteration 304, loss = 0.07177137\n",
      "Iteration 305, loss = 0.07159140\n",
      "Iteration 306, loss = 0.07141305\n",
      "Iteration 307, loss = 0.07123630\n",
      "Iteration 308, loss = 0.07106112\n",
      "Iteration 309, loss = 0.07088751\n",
      "Iteration 310, loss = 0.07071544\n",
      "Iteration 311, loss = 0.07054489\n",
      "Iteration 312, loss = 0.07037585\n",
      "Iteration 313, loss = 0.07020830\n",
      "Iteration 314, loss = 0.07004222\n",
      "Iteration 315, loss = 0.06987759\n",
      "Iteration 316, loss = 0.06971439\n",
      "Iteration 317, loss = 0.06955262\n",
      "Iteration 318, loss = 0.06939225\n",
      "Iteration 319, loss = 0.06923327\n",
      "Iteration 320, loss = 0.06907566\n",
      "Iteration 321, loss = 0.06891941\n",
      "Iteration 322, loss = 0.06876449\n",
      "Iteration 323, loss = 0.06861090\n",
      "Iteration 324, loss = 0.06845862\n",
      "Iteration 325, loss = 0.06830764\n",
      "Iteration 326, loss = 0.06815793\n",
      "Iteration 327, loss = 0.06800949\n",
      "Iteration 328, loss = 0.06786230\n",
      "Iteration 329, loss = 0.06771635\n",
      "Iteration 330, loss = 0.06757162\n",
      "Iteration 331, loss = 0.06742811\n",
      "Iteration 332, loss = 0.06728579\n",
      "Iteration 333, loss = 0.06714465\n",
      "Iteration 334, loss = 0.06700468\n",
      "Iteration 335, loss = 0.06686587\n",
      "Iteration 336, loss = 0.06672821\n",
      "Iteration 337, loss = 0.06659167\n",
      "Iteration 338, loss = 0.06645626\n",
      "Iteration 339, loss = 0.06632195\n",
      "Iteration 340, loss = 0.06618874\n",
      "Iteration 341, loss = 0.06605661\n",
      "Iteration 342, loss = 0.06592556\n",
      "Iteration 343, loss = 0.06579556\n",
      "Iteration 344, loss = 0.06566662\n",
      "Iteration 345, loss = 0.06553871\n",
      "Iteration 346, loss = 0.06541183\n",
      "Iteration 347, loss = 0.06528596\n",
      "Iteration 348, loss = 0.06516110\n",
      "Iteration 349, loss = 0.06503723\n",
      "Iteration 350, loss = 0.06491434\n",
      "Iteration 351, loss = 0.06479243\n",
      "Iteration 352, loss = 0.06467148\n",
      "Iteration 353, loss = 0.06455149\n",
      "Iteration 354, loss = 0.06443243\n",
      "Iteration 355, loss = 0.06431431\n",
      "Iteration 356, loss = 0.06419712\n",
      "Iteration 357, loss = 0.06408083\n",
      "Iteration 358, loss = 0.06396545\n",
      "Iteration 359, loss = 0.06385097\n",
      "Iteration 360, loss = 0.06373737\n",
      "Iteration 361, loss = 0.06362464\n",
      "Iteration 362, loss = 0.06351279\n",
      "Iteration 363, loss = 0.06340179\n",
      "Iteration 364, loss = 0.06329164\n",
      "Iteration 365, loss = 0.06318234\n",
      "Iteration 366, loss = 0.06307386\n",
      "Iteration 367, loss = 0.06296621\n",
      "Iteration 368, loss = 0.06285938\n",
      "Iteration 369, loss = 0.06275335\n",
      "Iteration 370, loss = 0.06264812\n",
      "Iteration 371, loss = 0.06254369\n",
      "Iteration 372, loss = 0.06244003\n",
      "Iteration 373, loss = 0.06233716\n",
      "Iteration 374, loss = 0.06223505\n",
      "Iteration 375, loss = 0.06213370\n",
      "Iteration 376, loss = 0.06203310\n",
      "Iteration 377, loss = 0.06193325\n",
      "Iteration 378, loss = 0.06183413\n",
      "Iteration 379, loss = 0.06173575\n",
      "Iteration 380, loss = 0.06163809\n",
      "Iteration 381, loss = 0.06154114\n",
      "Iteration 382, loss = 0.06144491\n",
      "Iteration 383, loss = 0.06134937\n",
      "Iteration 384, loss = 0.06125454\n",
      "Iteration 385, loss = 0.06116039\n",
      "Iteration 386, loss = 0.06106692\n",
      "Iteration 387, loss = 0.06097413\n",
      "Iteration 388, loss = 0.06088200\n",
      "Iteration 389, loss = 0.06079054\n",
      "Iteration 390, loss = 0.06069973\n",
      "Iteration 391, loss = 0.06060958\n",
      "Iteration 392, loss = 0.06052006\n",
      "Iteration 393, loss = 0.06043119\n",
      "Iteration 394, loss = 0.06034294\n",
      "Iteration 395, loss = 0.06025532\n",
      "Iteration 396, loss = 0.06016832\n",
      "Iteration 397, loss = 0.06008194\n",
      "Iteration 398, loss = 0.05999616\n",
      "Iteration 399, loss = 0.05991098\n",
      "Iteration 400, loss = 0.05982640\n",
      "Iteration 401, loss = 0.05974240\n",
      "Iteration 402, loss = 0.05965900\n",
      "Iteration 403, loss = 0.05957617\n",
      "Iteration 404, loss = 0.05949392\n",
      "Iteration 405, loss = 0.05941223\n",
      "Iteration 406, loss = 0.05933111\n",
      "Iteration 407, loss = 0.05925055\n",
      "Iteration 408, loss = 0.05917054\n",
      "Iteration 409, loss = 0.05909107\n",
      "Iteration 410, loss = 0.05901215\n",
      "Iteration 411, loss = 0.05893377\n",
      "Iteration 412, loss = 0.05885592\n",
      "Iteration 413, loss = 0.05877860\n",
      "Iteration 414, loss = 0.05870180\n",
      "Iteration 415, loss = 0.05862552\n",
      "Iteration 416, loss = 0.05854976\n",
      "Iteration 417, loss = 0.05847450\n",
      "Iteration 418, loss = 0.05839974\n",
      "Iteration 419, loss = 0.05832549\n",
      "Iteration 420, loss = 0.05825173\n",
      "Iteration 421, loss = 0.05817846\n",
      "Iteration 422, loss = 0.05810568\n",
      "Iteration 423, loss = 0.05803338\n",
      "Iteration 424, loss = 0.05796156\n",
      "Iteration 425, loss = 0.05789021\n",
      "Iteration 426, loss = 0.05781933\n",
      "Iteration 427, loss = 0.05774892\n",
      "Iteration 428, loss = 0.05767896\n",
      "Iteration 429, loss = 0.05760946\n",
      "Iteration 430, loss = 0.05754042\n",
      "Iteration 431, loss = 0.05747182\n",
      "Iteration 432, loss = 0.05740367\n",
      "Iteration 433, loss = 0.05733596\n",
      "Iteration 434, loss = 0.05726869\n",
      "Iteration 435, loss = 0.05720184\n",
      "Iteration 436, loss = 0.05713543\n",
      "Iteration 437, loss = 0.05706944\n",
      "Iteration 438, loss = 0.05700388\n",
      "Iteration 439, loss = 0.05693873\n",
      "Iteration 440, loss = 0.05687400\n",
      "Iteration 441, loss = 0.05680968\n",
      "Iteration 442, loss = 0.05674576\n",
      "Iteration 443, loss = 0.05668225\n",
      "Iteration 444, loss = 0.05661914\n",
      "Iteration 445, loss = 0.05655642\n",
      "Iteration 446, loss = 0.05649410\n",
      "Iteration 447, loss = 0.05643217\n",
      "Iteration 448, loss = 0.05637062\n",
      "Iteration 449, loss = 0.05630946\n",
      "Iteration 450, loss = 0.05624868\n",
      "Iteration 451, loss = 0.05618827\n",
      "Iteration 452, loss = 0.05612824\n",
      "Iteration 453, loss = 0.05606858\n",
      "Iteration 454, loss = 0.05600929\n",
      "Iteration 455, loss = 0.05595036\n",
      "Iteration 456, loss = 0.05589179\n",
      "Iteration 457, loss = 0.05583358\n",
      "Iteration 458, loss = 0.05577572\n",
      "Iteration 459, loss = 0.05571821\n",
      "Iteration 460, loss = 0.05566106\n",
      "Iteration 461, loss = 0.05560425\n",
      "Iteration 462, loss = 0.05554778\n",
      "Iteration 463, loss = 0.05549166\n",
      "Iteration 464, loss = 0.05543587\n",
      "Iteration 465, loss = 0.05538041\n",
      "Iteration 466, loss = 0.05532529\n",
      "Iteration 467, loss = 0.05527050\n",
      "Iteration 468, loss = 0.05521603\n",
      "Iteration 469, loss = 0.05516188\n",
      "Iteration 470, loss = 0.05510806\n",
      "Iteration 471, loss = 0.05505456\n",
      "Iteration 472, loss = 0.05500137\n",
      "Iteration 473, loss = 0.05494849\n",
      "Iteration 474, loss = 0.05489593\n",
      "Iteration 475, loss = 0.05484367\n",
      "Iteration 476, loss = 0.05479172\n",
      "Iteration 477, loss = 0.05474007\n",
      "Iteration 478, loss = 0.05468872\n",
      "Iteration 479, loss = 0.05463767\n",
      "Iteration 480, loss = 0.05458692\n",
      "Iteration 481, loss = 0.05453646\n",
      "Iteration 482, loss = 0.05448629\n",
      "Iteration 483, loss = 0.05443641\n",
      "Iteration 484, loss = 0.05438681\n",
      "Iteration 485, loss = 0.05433750\n",
      "Iteration 486, loss = 0.05428847\n",
      "Iteration 487, loss = 0.05423972\n",
      "Iteration 488, loss = 0.05419124\n",
      "Iteration 489, loss = 0.05414304\n",
      "Iteration 490, loss = 0.05409512\n",
      "Iteration 491, loss = 0.05404746\n",
      "Iteration 492, loss = 0.05400008\n",
      "Iteration 493, loss = 0.05395295\n",
      "Iteration 494, loss = 0.05390610\n",
      "Iteration 495, loss = 0.05385950\n",
      "Iteration 496, loss = 0.05381317\n",
      "Iteration 497, loss = 0.05376709\n",
      "Iteration 498, loss = 0.05372127\n",
      "Iteration 499, loss = 0.05367570\n",
      "Iteration 500, loss = 0.05363039\n",
      "Iteration 501, loss = 0.05358532\n",
      "Iteration 502, loss = 0.05354050\n",
      "Iteration 503, loss = 0.05349593\n",
      "Iteration 504, loss = 0.05345160\n",
      "Iteration 505, loss = 0.05340752\n",
      "Iteration 506, loss = 0.05336367\n",
      "Iteration 507, loss = 0.05332006\n",
      "Iteration 508, loss = 0.05327669\n",
      "Iteration 509, loss = 0.05323356\n",
      "Iteration 510, loss = 0.05319066\n",
      "Iteration 511, loss = 0.05314798\n",
      "Iteration 512, loss = 0.05310554\n",
      "Iteration 513, loss = 0.05306333\n",
      "Iteration 514, loss = 0.05302134\n",
      "Iteration 515, loss = 0.05297957\n",
      "Iteration 516, loss = 0.05293803\n",
      "Iteration 517, loss = 0.05289671\n",
      "Iteration 518, loss = 0.05285560\n",
      "Iteration 519, loss = 0.05281472\n",
      "Iteration 520, loss = 0.05277405\n",
      "Iteration 521, loss = 0.05273359\n",
      "Iteration 522, loss = 0.05269335\n",
      "Iteration 523, loss = 0.05265332\n",
      "Iteration 524, loss = 0.05261349\n",
      "Iteration 525, loss = 0.05257388\n",
      "Iteration 526, loss = 0.05253447\n",
      "Iteration 527, loss = 0.05249526\n",
      "Iteration 528, loss = 0.05245626\n",
      "Iteration 529, loss = 0.05241746\n",
      "Iteration 530, loss = 0.05237885\n",
      "Iteration 531, loss = 0.05234045\n",
      "Iteration 532, loss = 0.05230224\n",
      "Iteration 533, loss = 0.05226423\n",
      "Iteration 534, loss = 0.05222642\n",
      "Iteration 535, loss = 0.05218879\n",
      "Iteration 536, loss = 0.05215136\n",
      "Iteration 537, loss = 0.05211412\n",
      "Iteration 538, loss = 0.05207706\n",
      "Iteration 539, loss = 0.05204019\n",
      "Iteration 540, loss = 0.05200351\n",
      "Iteration 541, loss = 0.05196701\n",
      "Iteration 542, loss = 0.05193070\n",
      "Iteration 543, loss = 0.05189456\n",
      "Iteration 544, loss = 0.05185861\n",
      "Iteration 545, loss = 0.05182283\n",
      "Iteration 546, loss = 0.05178724\n",
      "Iteration 547, loss = 0.05175182\n",
      "Iteration 548, loss = 0.05171657\n",
      "Iteration 549, loss = 0.05168150\n",
      "Iteration 550, loss = 0.05164660\n",
      "Iteration 551, loss = 0.05161187\n",
      "Iteration 552, loss = 0.05157731\n",
      "Iteration 553, loss = 0.05154292\n",
      "Iteration 554, loss = 0.05150869\n",
      "Iteration 555, loss = 0.05147464\n",
      "Iteration 556, loss = 0.05144074\n",
      "Iteration 557, loss = 0.05140702\n",
      "Iteration 558, loss = 0.05137345\n",
      "Iteration 559, loss = 0.05134005\n",
      "Iteration 560, loss = 0.05130680\n",
      "Iteration 561, loss = 0.05127372\n",
      "Iteration 562, loss = 0.05124079\n",
      "Iteration 563, loss = 0.05120802\n",
      "Iteration 564, loss = 0.05117541\n",
      "Iteration 565, loss = 0.05114295\n",
      "Iteration 566, loss = 0.05111065\n",
      "Iteration 567, loss = 0.05107849\n",
      "Iteration 568, loss = 0.05104649\n",
      "Iteration 569, loss = 0.05101464\n",
      "Iteration 570, loss = 0.05098294\n",
      "Iteration 571, loss = 0.05095139\n",
      "Iteration 572, loss = 0.05091998\n",
      "Iteration 573, loss = 0.05088872\n",
      "Iteration 574, loss = 0.05085761\n",
      "Iteration 575, loss = 0.05082664\n",
      "Iteration 576, loss = 0.05079581\n",
      "Iteration 577, loss = 0.05076512\n",
      "Iteration 578, loss = 0.05073458\n",
      "Iteration 579, loss = 0.05070418\n",
      "Iteration 580, loss = 0.05067391\n",
      "Iteration 581, loss = 0.05064379\n",
      "Iteration 582, loss = 0.05061380\n",
      "Iteration 583, loss = 0.05058394\n",
      "Iteration 584, loss = 0.05055423\n",
      "Iteration 585, loss = 0.05052464\n",
      "Iteration 586, loss = 0.05049520\n",
      "Iteration 587, loss = 0.05046588\n",
      "Iteration 588, loss = 0.05043669\n",
      "Iteration 589, loss = 0.05040764\n",
      "Iteration 590, loss = 0.05037872\n",
      "Iteration 591, loss = 0.05034992\n",
      "Iteration 592, loss = 0.05032125\n",
      "Iteration 593, loss = 0.05029271\n",
      "Iteration 594, loss = 0.05026430\n",
      "Iteration 595, loss = 0.05023601\n",
      "Iteration 596, loss = 0.05020785\n",
      "Iteration 597, loss = 0.05017981\n",
      "Iteration 598, loss = 0.05015189\n",
      "Iteration 599, loss = 0.05012410\n",
      "Iteration 600, loss = 0.05009643\n",
      "Iteration 601, loss = 0.05006887\n",
      "Iteration 602, loss = 0.05004144\n",
      "Iteration 603, loss = 0.05001413\n",
      "Iteration 604, loss = 0.04998693\n",
      "Iteration 605, loss = 0.04995985\n",
      "Iteration 606, loss = 0.04993289\n",
      "Iteration 607, loss = 0.04990605\n",
      "Iteration 608, loss = 0.04987931\n",
      "Iteration 609, loss = 0.04985270\n",
      "Iteration 610, loss = 0.04982619\n",
      "Iteration 611, loss = 0.04979980\n",
      "Iteration 612, loss = 0.04977353\n",
      "Iteration 613, loss = 0.04974736\n",
      "Iteration 614, loss = 0.04972130\n",
      "Iteration 615, loss = 0.04969535\n",
      "Iteration 616, loss = 0.04966952\n",
      "Iteration 617, loss = 0.04964379\n",
      "Iteration 618, loss = 0.04961816\n",
      "Iteration 619, loss = 0.04959265\n",
      "Iteration 620, loss = 0.04956724\n",
      "Iteration 621, loss = 0.04954194\n",
      "Iteration 622, loss = 0.04951674\n",
      "Iteration 623, loss = 0.04949164\n",
      "Iteration 624, loss = 0.04946665\n",
      "Iteration 625, loss = 0.04944176\n",
      "Iteration 626, loss = 0.04941698\n",
      "Iteration 627, loss = 0.04939229\n",
      "Iteration 628, loss = 0.04936771\n",
      "Iteration 629, loss = 0.04934323\n",
      "Iteration 630, loss = 0.04931884\n",
      "Iteration 631, loss = 0.04929456\n",
      "Iteration 632, loss = 0.04927037\n",
      "Iteration 633, loss = 0.04924628\n",
      "Iteration 634, loss = 0.04922229\n",
      "Iteration 635, loss = 0.04919839\n",
      "Iteration 636, loss = 0.04917459\n",
      "Iteration 637, loss = 0.04915089\n",
      "Iteration 638, loss = 0.04912728\n",
      "Iteration 639, loss = 0.04910376\n",
      "Iteration 640, loss = 0.04908034\n",
      "Iteration 641, loss = 0.04905701\n",
      "Iteration 642, loss = 0.04903377\n",
      "Iteration 643, loss = 0.04901062\n",
      "Iteration 644, loss = 0.04898756\n",
      "Iteration 645, loss = 0.04896460\n",
      "Iteration 646, loss = 0.04894172\n",
      "Iteration 647, loss = 0.04891894\n",
      "Iteration 648, loss = 0.04889624\n",
      "Iteration 649, loss = 0.04887363\n",
      "Iteration 650, loss = 0.04885111\n",
      "Iteration 651, loss = 0.04882867\n",
      "Iteration 652, loss = 0.04880633\n",
      "Iteration 653, loss = 0.04878406\n",
      "Iteration 654, loss = 0.04876189\n",
      "Iteration 655, loss = 0.04873979\n",
      "Iteration 656, loss = 0.04871779\n",
      "Iteration 657, loss = 0.04869586\n",
      "Iteration 658, loss = 0.04867402\n",
      "Iteration 659, loss = 0.04865227\n",
      "Iteration 660, loss = 0.04863059\n",
      "Iteration 661, loss = 0.04860900\n",
      "Iteration 662, loss = 0.04858749\n",
      "Iteration 663, loss = 0.04856605\n",
      "Iteration 664, loss = 0.04854470\n",
      "Iteration 665, loss = 0.04852343\n",
      "Iteration 666, loss = 0.04850224\n",
      "Iteration 667, loss = 0.04848113\n",
      "Iteration 668, loss = 0.04846010\n",
      "Iteration 669, loss = 0.04843914\n",
      "Iteration 670, loss = 0.04841826\n",
      "Iteration 671, loss = 0.04839746\n",
      "Iteration 672, loss = 0.04837673\n",
      "Iteration 673, loss = 0.04835608\n",
      "Iteration 674, loss = 0.04833551\n",
      "Iteration 675, loss = 0.04831501\n",
      "Iteration 676, loss = 0.04829459\n",
      "Iteration 677, loss = 0.04827424\n",
      "Iteration 678, loss = 0.04825396\n",
      "Iteration 679, loss = 0.04823376\n",
      "Iteration 680, loss = 0.04821363\n",
      "Iteration 681, loss = 0.04819357\n",
      "Iteration 682, loss = 0.04817359\n",
      "Iteration 683, loss = 0.04815367\n",
      "Iteration 684, loss = 0.04813383\n",
      "Iteration 685, loss = 0.04811406\n",
      "Iteration 686, loss = 0.04809436\n",
      "Iteration 687, loss = 0.04807473\n",
      "Iteration 688, loss = 0.04805517\n",
      "Iteration 689, loss = 0.04803567\n",
      "Iteration 690, loss = 0.04801625\n",
      "Iteration 691, loss = 0.04799689\n",
      "Iteration 692, loss = 0.04797761\n",
      "Iteration 693, loss = 0.04795838\n",
      "Iteration 694, loss = 0.04793923\n",
      "Iteration 695, loss = 0.04792015\n",
      "Iteration 696, loss = 0.04790113\n",
      "Iteration 697, loss = 0.04788217\n",
      "Iteration 698, loss = 0.04786328\n",
      "Iteration 699, loss = 0.04784446\n",
      "Iteration 700, loss = 0.04782570\n",
      "Iteration 701, loss = 0.04780701\n",
      "Iteration 702, loss = 0.04778838\n",
      "Iteration 703, loss = 0.04776981\n",
      "Iteration 704, loss = 0.04775131\n",
      "Iteration 705, loss = 0.04773287\n",
      "Iteration 706, loss = 0.04771449\n",
      "Iteration 707, loss = 0.04769617\n",
      "Iteration 708, loss = 0.04767792\n",
      "Iteration 709, loss = 0.04765973\n",
      "Iteration 710, loss = 0.04764160\n",
      "Iteration 711, loss = 0.04762353\n",
      "Iteration 712, loss = 0.04760552\n",
      "Iteration 713, loss = 0.04758757\n",
      "Iteration 714, loss = 0.04756968\n",
      "Iteration 715, loss = 0.04755185\n",
      "Iteration 716, loss = 0.04753408\n",
      "Iteration 717, loss = 0.04751637\n",
      "Iteration 718, loss = 0.04749872\n",
      "Iteration 719, loss = 0.04748112\n",
      "Iteration 720, loss = 0.04746358\n",
      "Iteration 721, loss = 0.04744610\n",
      "Iteration 722, loss = 0.04742868\n",
      "Iteration 723, loss = 0.04741132\n",
      "Iteration 724, loss = 0.04739401\n",
      "Iteration 725, loss = 0.04737675\n",
      "Iteration 726, loss = 0.04735956\n",
      "Iteration 727, loss = 0.04734241\n",
      "Iteration 728, loss = 0.04732533\n",
      "Iteration 729, loss = 0.04730830\n",
      "Iteration 730, loss = 0.04729132\n",
      "Iteration 731, loss = 0.04727440\n",
      "Iteration 732, loss = 0.04725753\n",
      "Iteration 733, loss = 0.04724072\n",
      "Iteration 734, loss = 0.04722396\n",
      "Iteration 735, loss = 0.04720725\n",
      "Iteration 736, loss = 0.04719059\n",
      "Iteration 737, loss = 0.04717399\n",
      "Iteration 738, loss = 0.04715744\n",
      "Iteration 739, loss = 0.04714094\n",
      "Iteration 740, loss = 0.04712450\n",
      "Iteration 741, loss = 0.04710810\n",
      "Iteration 742, loss = 0.04709176\n",
      "Iteration 743, loss = 0.04707547\n",
      "Iteration 744, loss = 0.04705923\n",
      "Iteration 745, loss = 0.04704304\n",
      "Iteration 746, loss = 0.04702690\n",
      "Iteration 747, loss = 0.04701081\n",
      "Iteration 748, loss = 0.04699476\n",
      "Iteration 749, loss = 0.04697877\n",
      "Iteration 750, loss = 0.04696283\n",
      "Iteration 751, loss = 0.04694694\n",
      "Iteration 752, loss = 0.04693109\n",
      "Iteration 753, loss = 0.04691529\n",
      "Iteration 754, loss = 0.04689954\n",
      "Iteration 755, loss = 0.04688384\n",
      "Iteration 756, loss = 0.04686819\n",
      "Iteration 757, loss = 0.04685258\n",
      "Iteration 758, loss = 0.04683702\n",
      "Iteration 759, loss = 0.04682151\n",
      "Iteration 760, loss = 0.04680604\n",
      "Iteration 761, loss = 0.04679062\n",
      "Iteration 762, loss = 0.04677525\n",
      "Iteration 763, loss = 0.04675992\n",
      "Iteration 764, loss = 0.04674464\n",
      "Iteration 765, loss = 0.04672940\n",
      "Iteration 766, loss = 0.04671421\n",
      "Iteration 767, loss = 0.04669906\n",
      "Iteration 768, loss = 0.04668396\n",
      "Iteration 769, loss = 0.04666890\n",
      "Iteration 770, loss = 0.04665389\n",
      "Iteration 771, loss = 0.04663892\n",
      "Iteration 772, loss = 0.04662399\n",
      "Iteration 773, loss = 0.04660911\n",
      "Iteration 774, loss = 0.04659427\n",
      "Iteration 775, loss = 0.04657947\n",
      "Iteration 776, loss = 0.04656472\n",
      "Iteration 777, loss = 0.04655001\n",
      "Iteration 778, loss = 0.04653534\n",
      "Iteration 779, loss = 0.04652071\n",
      "Iteration 780, loss = 0.04650613\n",
      "Iteration 781, loss = 0.04649159\n",
      "Iteration 782, loss = 0.04647709\n",
      "Iteration 783, loss = 0.04646263\n",
      "Iteration 784, loss = 0.04644821\n",
      "Iteration 785, loss = 0.04643383\n",
      "Iteration 786, loss = 0.04641950\n",
      "Iteration 787, loss = 0.04640520\n",
      "Iteration 788, loss = 0.04639094\n",
      "Iteration 789, loss = 0.04637673\n",
      "Iteration 790, loss = 0.04636255\n",
      "Iteration 791, loss = 0.04634842\n",
      "Iteration 792, loss = 0.04633432\n",
      "Iteration 793, loss = 0.04632026\n",
      "Iteration 794, loss = 0.04630625\n",
      "Iteration 795, loss = 0.04629227\n",
      "Iteration 796, loss = 0.04627833\n",
      "Iteration 797, loss = 0.04626442\n",
      "Iteration 798, loss = 0.04625056\n",
      "Iteration 799, loss = 0.04623674\n",
      "Iteration 800, loss = 0.04622295\n",
      "Iteration 801, loss = 0.04620920\n",
      "Iteration 802, loss = 0.04619549\n",
      "Iteration 803, loss = 0.04618182\n",
      "Iteration 804, loss = 0.04616818\n",
      "Iteration 805, loss = 0.04615458\n",
      "Iteration 806, loss = 0.04614102\n",
      "Iteration 807, loss = 0.04612749\n",
      "Iteration 808, loss = 0.04611400\n",
      "Iteration 809, loss = 0.04610055\n",
      "Iteration 810, loss = 0.04608713\n",
      "Iteration 811, loss = 0.04607375\n",
      "Iteration 812, loss = 0.04606041\n",
      "Iteration 813, loss = 0.04604710\n",
      "Iteration 814, loss = 0.04603382\n",
      "Iteration 815, loss = 0.04602059\n",
      "Iteration 816, loss = 0.04600738\n",
      "Iteration 817, loss = 0.04599421\n",
      "Iteration 818, loss = 0.04598108\n",
      "Iteration 819, loss = 0.04596798\n",
      "Iteration 820, loss = 0.04595492\n",
      "Iteration 821, loss = 0.04594189\n",
      "Iteration 822, loss = 0.04592889\n",
      "Iteration 823, loss = 0.04591593\n",
      "Iteration 824, loss = 0.04590301\n",
      "Iteration 825, loss = 0.04589011\n",
      "Iteration 826, loss = 0.04587725\n",
      "Iteration 827, loss = 0.04586443\n",
      "Iteration 828, loss = 0.04585163\n",
      "Iteration 829, loss = 0.04583887\n",
      "Iteration 830, loss = 0.04582615\n",
      "Iteration 831, loss = 0.04581345\n",
      "Iteration 832, loss = 0.04580079\n",
      "Iteration 833, loss = 0.04578816\n",
      "Iteration 834, loss = 0.04577557\n",
      "Iteration 835, loss = 0.04576300\n",
      "Iteration 836, loss = 0.04575047\n",
      "Iteration 837, loss = 0.04573797\n",
      "Iteration 838, loss = 0.04572550\n",
      "Iteration 839, loss = 0.04571306\n",
      "Iteration 840, loss = 0.04570066\n",
      "Iteration 841, loss = 0.04568828\n",
      "Iteration 842, loss = 0.04567594\n",
      "Iteration 843, loss = 0.04566363\n",
      "Iteration 844, loss = 0.04565135\n",
      "Iteration 845, loss = 0.04563910\n",
      "Iteration 846, loss = 0.04562688\n",
      "Iteration 847, loss = 0.04561469\n",
      "Iteration 848, loss = 0.04560253\n",
      "Iteration 849, loss = 0.04559040\n",
      "Iteration 850, loss = 0.04557830\n",
      "Iteration 851, loss = 0.04556624\n",
      "Iteration 852, loss = 0.04555420\n",
      "Iteration 853, loss = 0.04554219\n",
      "Iteration 854, loss = 0.04553021\n",
      "Iteration 855, loss = 0.04551826\n",
      "Iteration 856, loss = 0.04550634\n",
      "Iteration 857, loss = 0.04549445\n",
      "Iteration 858, loss = 0.04548259\n",
      "Iteration 859, loss = 0.04547076\n",
      "Iteration 860, loss = 0.04545895\n",
      "Iteration 861, loss = 0.04544718\n",
      "Iteration 862, loss = 0.04543543\n",
      "Iteration 863, loss = 0.04542371\n",
      "Iteration 864, loss = 0.04541202\n",
      "Iteration 865, loss = 0.04540036\n",
      "Iteration 866, loss = 0.04538872\n",
      "Iteration 867, loss = 0.04537712\n",
      "Iteration 868, loss = 0.04536554\n",
      "Iteration 869, loss = 0.04535399\n",
      "Iteration 870, loss = 0.04534247\n",
      "Iteration 871, loss = 0.04533097\n",
      "Iteration 872, loss = 0.04531950\n",
      "Iteration 873, loss = 0.04530806\n",
      "Iteration 874, loss = 0.04529665\n",
      "Iteration 875, loss = 0.04528526\n",
      "Iteration 876, loss = 0.04527390\n",
      "Iteration 877, loss = 0.04526257\n",
      "Iteration 878, loss = 0.04525127\n",
      "Iteration 879, loss = 0.04523999\n",
      "Iteration 880, loss = 0.04522873\n",
      "Iteration 881, loss = 0.04521751\n",
      "Iteration 882, loss = 0.04520631\n",
      "Iteration 883, loss = 0.04519513\n",
      "Iteration 884, loss = 0.04518398\n",
      "Iteration 885, loss = 0.04517286\n",
      "Iteration 886, loss = 0.04516177\n",
      "Iteration 887, loss = 0.04515069\n",
      "Iteration 888, loss = 0.04513965\n",
      "Iteration 889, loss = 0.04512863\n",
      "Iteration 890, loss = 0.04511764\n",
      "Iteration 891, loss = 0.04510667\n",
      "Iteration 892, loss = 0.04509572\n",
      "Iteration 893, loss = 0.04508480\n",
      "Iteration 894, loss = 0.04507391\n",
      "Iteration 895, loss = 0.04506304\n",
      "Iteration 896, loss = 0.04505220\n",
      "Iteration 897, loss = 0.04504138\n",
      "Iteration 898, loss = 0.04503058\n",
      "Iteration 899, loss = 0.04501981\n",
      "Iteration 900, loss = 0.04500907\n",
      "Iteration 901, loss = 0.04499834\n",
      "Iteration 902, loss = 0.04498765\n",
      "Iteration 903, loss = 0.04497697\n",
      "Iteration 904, loss = 0.04496632\n",
      "Iteration 905, loss = 0.04495570\n",
      "Iteration 906, loss = 0.04494509\n",
      "Iteration 907, loss = 0.04493452\n",
      "Iteration 908, loss = 0.04492396\n",
      "Iteration 909, loss = 0.04491343\n",
      "Iteration 910, loss = 0.04490292\n",
      "Iteration 911, loss = 0.04489244\n",
      "Iteration 912, loss = 0.04488198\n",
      "Iteration 913, loss = 0.04487154\n",
      "Iteration 914, loss = 0.04486112\n",
      "Iteration 915, loss = 0.04485073\n",
      "Iteration 916, loss = 0.04484036\n",
      "Iteration 917, loss = 0.04483002\n",
      "Iteration 918, loss = 0.04481969\n",
      "Iteration 919, loss = 0.04480939\n",
      "Iteration 920, loss = 0.04479911\n",
      "Iteration 921, loss = 0.04478886\n",
      "Iteration 922, loss = 0.04477862\n",
      "Iteration 923, loss = 0.04476841\n",
      "Iteration 924, loss = 0.04475822\n",
      "Iteration 925, loss = 0.04474805\n",
      "Iteration 926, loss = 0.04473791\n",
      "Iteration 927, loss = 0.04472779\n",
      "Iteration 928, loss = 0.04471768\n",
      "Iteration 929, loss = 0.04470760\n",
      "Iteration 930, loss = 0.04469755\n",
      "Iteration 931, loss = 0.04468751\n",
      "Iteration 932, loss = 0.04467749\n",
      "Iteration 933, loss = 0.04466750\n",
      "Iteration 934, loss = 0.04465753\n",
      "Iteration 935, loss = 0.04464758\n",
      "Iteration 936, loss = 0.04463765\n",
      "Iteration 937, loss = 0.04462774\n",
      "Iteration 938, loss = 0.04461785\n",
      "Iteration 939, loss = 0.04460798\n",
      "Iteration 940, loss = 0.04459814\n",
      "Iteration 941, loss = 0.04458831\n",
      "Iteration 942, loss = 0.04457851\n",
      "Iteration 943, loss = 0.04456872\n",
      "Iteration 944, loss = 0.04455896\n",
      "Iteration 945, loss = 0.04454922\n",
      "Iteration 946, loss = 0.04453949\n",
      "Iteration 947, loss = 0.04452979\n",
      "Iteration 948, loss = 0.04452011\n",
      "Iteration 949, loss = 0.04451045\n",
      "Iteration 950, loss = 0.04450081\n",
      "Iteration 951, loss = 0.04449119\n",
      "Iteration 952, loss = 0.04448158\n",
      "Iteration 953, loss = 0.04447200\n",
      "Iteration 954, loss = 0.04446244\n",
      "Iteration 955, loss = 0.04445290\n",
      "Iteration 956, loss = 0.04444338\n",
      "Iteration 957, loss = 0.04443387\n",
      "Iteration 958, loss = 0.04442439\n",
      "Iteration 959, loss = 0.04441493\n",
      "Iteration 960, loss = 0.04440548\n",
      "Iteration 961, loss = 0.04439606\n",
      "Iteration 962, loss = 0.04438665\n",
      "Iteration 963, loss = 0.04437726\n",
      "Iteration 964, loss = 0.04436790\n",
      "Iteration 965, loss = 0.04435855\n",
      "Iteration 966, loss = 0.04434922\n",
      "Iteration 967, loss = 0.04433991\n",
      "Iteration 968, loss = 0.04433062\n",
      "Iteration 969, loss = 0.04432134\n",
      "Iteration 970, loss = 0.04431209\n",
      "Iteration 971, loss = 0.04430285\n",
      "Iteration 972, loss = 0.04429364\n",
      "Iteration 973, loss = 0.04428444\n",
      "Iteration 974, loss = 0.04427526\n",
      "Iteration 975, loss = 0.04426609\n",
      "Iteration 976, loss = 0.04425695\n",
      "Iteration 977, loss = 0.04424783\n",
      "Iteration 978, loss = 0.04423872\n",
      "Iteration 979, loss = 0.04422963\n",
      "Iteration 980, loss = 0.04422056\n",
      "Iteration 981, loss = 0.04421151\n",
      "Iteration 982, loss = 0.04420247\n",
      "Iteration 983, loss = 0.04419345\n",
      "Iteration 984, loss = 0.04418446\n",
      "Iteration 985, loss = 0.04417547\n",
      "Iteration 986, loss = 0.04416651\n",
      "Iteration 987, loss = 0.04415756\n",
      "Iteration 988, loss = 0.04414864\n",
      "Iteration 989, loss = 0.04413972\n",
      "Iteration 990, loss = 0.04413083\n",
      "Iteration 991, loss = 0.04412196\n",
      "Iteration 992, loss = 0.04411310\n",
      "Iteration 993, loss = 0.04410425\n",
      "Iteration 994, loss = 0.04409543\n",
      "Iteration 995, loss = 0.04408662\n",
      "Iteration 996, loss = 0.04407783\n",
      "Iteration 997, loss = 0.04406906\n",
      "Iteration 998, loss = 0.04406030\n",
      "Iteration 999, loss = 0.04405157\n",
      "Iteration 1000, loss = 0.04404284\n",
      "Iteration 1001, loss = 0.04403414\n",
      "Iteration 1002, loss = 0.04402545\n",
      "Iteration 1003, loss = 0.04401678\n",
      "Iteration 1004, loss = 0.04400812\n",
      "Iteration 1005, loss = 0.04399948\n",
      "Iteration 1006, loss = 0.04399086\n",
      "Iteration 1007, loss = 0.04398226\n",
      "Iteration 1008, loss = 0.04397367\n",
      "Iteration 1009, loss = 0.04396509\n",
      "Iteration 1010, loss = 0.04395654\n",
      "Iteration 1011, loss = 0.04394800\n",
      "Iteration 1012, loss = 0.04393947\n",
      "Iteration 1013, loss = 0.04393097\n",
      "Iteration 1014, loss = 0.04392247\n",
      "Iteration 1015, loss = 0.04391400\n",
      "Iteration 1016, loss = 0.04390554\n",
      "Iteration 1017, loss = 0.04389710\n",
      "Iteration 1018, loss = 0.04388867\n",
      "Iteration 1019, loss = 0.04388026\n",
      "Iteration 1020, loss = 0.04387186\n",
      "Iteration 1021, loss = 0.04386348\n",
      "Iteration 1022, loss = 0.04385512\n",
      "Iteration 1023, loss = 0.04384677\n",
      "Iteration 1024, loss = 0.04383843\n",
      "Iteration 1025, loss = 0.04383012\n",
      "Iteration 1026, loss = 0.04382181\n",
      "Iteration 1027, loss = 0.04381353\n",
      "Iteration 1028, loss = 0.04380526\n",
      "Iteration 1029, loss = 0.04379700\n",
      "Iteration 1030, loss = 0.04378876\n",
      "Iteration 1031, loss = 0.04378053\n",
      "Iteration 1032, loss = 0.04377232\n",
      "Iteration 1033, loss = 0.04376413\n",
      "Iteration 1034, loss = 0.04375595\n",
      "Iteration 1035, loss = 0.04374778\n",
      "Iteration 1036, loss = 0.04373963\n",
      "Iteration 1037, loss = 0.04373150\n",
      "Iteration 1038, loss = 0.04372338\n",
      "Iteration 1039, loss = 0.04371528\n",
      "Iteration 1040, loss = 0.04370719\n",
      "Iteration 1041, loss = 0.04369911\n",
      "Iteration 1042, loss = 0.04369105\n",
      "Iteration 1043, loss = 0.04368300\n",
      "Iteration 1044, loss = 0.04367497\n",
      "Iteration 1045, loss = 0.04366696\n",
      "Iteration 1046, loss = 0.04365895\n",
      "Iteration 1047, loss = 0.04365097\n",
      "Iteration 1048, loss = 0.04364299\n",
      "Iteration 1049, loss = 0.04363503\n",
      "Iteration 1050, loss = 0.04362709\n",
      "Iteration 1051, loss = 0.04361916\n",
      "Iteration 1052, loss = 0.04361125\n",
      "Iteration 1053, loss = 0.04360334\n",
      "Iteration 1054, loss = 0.04359546\n",
      "Iteration 1055, loss = 0.04358758\n",
      "Iteration 1056, loss = 0.04357973\n",
      "Iteration 1057, loss = 0.04357188\n",
      "Iteration 1058, loss = 0.04356405\n",
      "Iteration 1059, loss = 0.04355623\n",
      "Iteration 1060, loss = 0.04354843\n",
      "Iteration 1061, loss = 0.04354064\n",
      "Iteration 1062, loss = 0.04353287\n",
      "Iteration 1063, loss = 0.04352511\n",
      "Iteration 1064, loss = 0.04351736\n",
      "Iteration 1065, loss = 0.04350963\n",
      "Iteration 1066, loss = 0.04350191\n",
      "Iteration 1067, loss = 0.04349420\n",
      "Iteration 1068, loss = 0.04348651\n",
      "Iteration 1069, loss = 0.04347883\n",
      "Iteration 1070, loss = 0.04347116\n",
      "Iteration 1071, loss = 0.04346351\n",
      "Iteration 1072, loss = 0.04345587\n",
      "Iteration 1073, loss = 0.04344825\n",
      "Iteration 1074, loss = 0.04344064\n",
      "Iteration 1075, loss = 0.04343304\n",
      "Iteration 1076, loss = 0.04342545\n",
      "Iteration 1077, loss = 0.04341788\n",
      "Iteration 1078, loss = 0.04341032\n",
      "Iteration 1079, loss = 0.04340278\n",
      "Iteration 1080, loss = 0.04339525\n",
      "Iteration 1081, loss = 0.04338773\n",
      "Iteration 1082, loss = 0.04338022\n",
      "Iteration 1083, loss = 0.04337273\n",
      "Iteration 1084, loss = 0.04336525\n",
      "Iteration 1085, loss = 0.04335778\n",
      "Iteration 1086, loss = 0.04335033\n",
      "Iteration 1087, loss = 0.04334289\n",
      "Iteration 1088, loss = 0.04333546\n",
      "Iteration 1089, loss = 0.04332804\n",
      "Iteration 1090, loss = 0.04332064\n",
      "Iteration 1091, loss = 0.04331325\n",
      "Iteration 1092, loss = 0.04330588\n",
      "Iteration 1093, loss = 0.04329851\n",
      "Iteration 1094, loss = 0.04329116\n",
      "Iteration 1095, loss = 0.04328382\n",
      "Iteration 1096, loss = 0.04327649\n",
      "Iteration 1097, loss = 0.04326918\n",
      "Iteration 1098, loss = 0.04326188\n",
      "Iteration 1099, loss = 0.04325459\n",
      "Iteration 1100, loss = 0.04324731\n",
      "Iteration 1101, loss = 0.04324005\n",
      "Iteration 1102, loss = 0.04323280\n",
      "Iteration 1103, loss = 0.04322556\n",
      "Iteration 1104, loss = 0.04321833\n",
      "Iteration 1105, loss = 0.04321112\n",
      "Iteration 1106, loss = 0.04320391\n",
      "Iteration 1107, loss = 0.04319672\n",
      "Iteration 1108, loss = 0.04318954\n",
      "Iteration 1109, loss = 0.04318238\n",
      "Iteration 1110, loss = 0.04317522\n",
      "Iteration 1111, loss = 0.04316808\n",
      "Iteration 1112, loss = 0.04316095\n",
      "Iteration 1113, loss = 0.04315383\n",
      "Iteration 1114, loss = 0.04314673\n",
      "Iteration 1115, loss = 0.04313963\n",
      "Iteration 1116, loss = 0.04313255\n",
      "Iteration 1117, loss = 0.04312548\n",
      "Iteration 1118, loss = 0.04311842\n",
      "Iteration 1119, loss = 0.04311137\n",
      "Iteration 1120, loss = 0.04310434\n",
      "Iteration 1121, loss = 0.04309732\n",
      "Iteration 1122, loss = 0.04309030\n",
      "Iteration 1123, loss = 0.04308330\n",
      "Iteration 1124, loss = 0.04307631\n",
      "Iteration 1125, loss = 0.04306934\n",
      "Iteration 1126, loss = 0.04306237\n",
      "Iteration 1127, loss = 0.04305542\n",
      "Iteration 1128, loss = 0.04304848\n",
      "Iteration 1129, loss = 0.04304155\n",
      "Iteration 1130, loss = 0.04303463\n",
      "Iteration 1131, loss = 0.04302772\n",
      "Iteration 1132, loss = 0.04302082\n",
      "Iteration 1133, loss = 0.04301394\n",
      "Iteration 1134, loss = 0.04300706\n",
      "Iteration 1135, loss = 0.04300020\n",
      "Iteration 1136, loss = 0.04299335\n",
      "Iteration 1137, loss = 0.04298651\n",
      "Iteration 1138, loss = 0.04297968\n",
      "Iteration 1139, loss = 0.04297286\n",
      "Iteration 1140, loss = 0.04296605\n",
      "Iteration 1141, loss = 0.04295926\n",
      "Iteration 1142, loss = 0.04295247\n",
      "Iteration 1143, loss = 0.04294570\n",
      "Iteration 1144, loss = 0.04293894\n",
      "Iteration 1145, loss = 0.04293219\n",
      "Iteration 1146, loss = 0.04292545\n",
      "Iteration 1147, loss = 0.04291872\n",
      "Iteration 1148, loss = 0.04291200\n",
      "Iteration 1149, loss = 0.04290529\n",
      "Iteration 1150, loss = 0.04289859\n",
      "Iteration 1151, loss = 0.04289191\n",
      "Iteration 1152, loss = 0.04288523\n",
      "Iteration 1153, loss = 0.04287857\n",
      "Iteration 1154, loss = 0.04287191\n",
      "Iteration 1155, loss = 0.04286527\n",
      "Iteration 1156, loss = 0.04285864\n",
      "Iteration 1157, loss = 0.04285202\n",
      "Iteration 1158, loss = 0.04284540\n",
      "Iteration 1159, loss = 0.04283880\n",
      "Iteration 1160, loss = 0.04283221\n",
      "Iteration 1161, loss = 0.04282563\n",
      "Iteration 1162, loss = 0.04281906\n",
      "Iteration 1163, loss = 0.04281251\n",
      "Iteration 1164, loss = 0.04280596\n",
      "Iteration 1165, loss = 0.04279942\n",
      "Iteration 1166, loss = 0.04279289\n",
      "Iteration 1167, loss = 0.04278638\n",
      "Iteration 1168, loss = 0.04277987\n",
      "Iteration 1169, loss = 0.04277337\n",
      "Iteration 1170, loss = 0.04276689\n",
      "Iteration 1171, loss = 0.04276041\n",
      "Iteration 1172, loss = 0.04275395\n",
      "Iteration 1173, loss = 0.04274749\n",
      "Iteration 1174, loss = 0.04274105\n",
      "Iteration 1175, loss = 0.04273461\n",
      "Iteration 1176, loss = 0.04272819\n",
      "Iteration 1177, loss = 0.04272177\n",
      "Iteration 1178, loss = 0.04271537\n",
      "Iteration 1179, loss = 0.04270897\n",
      "Iteration 1180, loss = 0.04270259\n",
      "Iteration 1181, loss = 0.04269621\n",
      "Iteration 1182, loss = 0.04268985\n",
      "Iteration 1183, loss = 0.04268349\n",
      "Iteration 1184, loss = 0.04267715\n",
      "Iteration 1185, loss = 0.04267081\n",
      "Iteration 1186, loss = 0.04266449\n",
      "Iteration 1187, loss = 0.04265817\n",
      "Iteration 1188, loss = 0.04265187\n",
      "Iteration 1189, loss = 0.04264557\n",
      "Iteration 1190, loss = 0.04263929\n",
      "Iteration 1191, loss = 0.04263301\n",
      "Iteration 1192, loss = 0.04262675\n",
      "Iteration 1193, loss = 0.04262049\n",
      "Iteration 1194, loss = 0.04261424\n",
      "Iteration 1195, loss = 0.04260801\n",
      "Iteration 1196, loss = 0.04260178\n",
      "Iteration 1197, loss = 0.04259556\n",
      "Iteration 1198, loss = 0.04258935\n",
      "Iteration 1199, loss = 0.04258315\n",
      "Iteration 1200, loss = 0.04257696\n",
      "Iteration 1201, loss = 0.04257079\n",
      "Iteration 1202, loss = 0.04256462\n",
      "Iteration 1203, loss = 0.04255845\n",
      "Iteration 1204, loss = 0.04255230\n",
      "Iteration 1205, loss = 0.04254616\n",
      "Iteration 1206, loss = 0.04254003\n",
      "Iteration 1207, loss = 0.04253391\n",
      "Iteration 1208, loss = 0.04252779\n",
      "Iteration 1209, loss = 0.04252169\n",
      "Iteration 1210, loss = 0.04251559\n",
      "Iteration 1211, loss = 0.04250951\n",
      "Iteration 1212, loss = 0.04250343\n",
      "Iteration 1213, loss = 0.04249736\n",
      "Iteration 1214, loss = 0.04249131\n",
      "Iteration 1215, loss = 0.04248526\n",
      "Iteration 1216, loss = 0.04247922\n",
      "Iteration 1217, loss = 0.04247319\n",
      "Iteration 1218, loss = 0.04246716\n",
      "Iteration 1219, loss = 0.04246115\n",
      "Iteration 1220, loss = 0.04245515\n",
      "Iteration 1221, loss = 0.04244915\n",
      "Iteration 1222, loss = 0.04244317\n",
      "Iteration 1223, loss = 0.04243719\n",
      "Iteration 1224, loss = 0.04243123\n",
      "Iteration 1225, loss = 0.04242527\n",
      "Iteration 1226, loss = 0.04241932\n",
      "Iteration 1227, loss = 0.04241338\n",
      "Iteration 1228, loss = 0.04240745\n",
      "Iteration 1229, loss = 0.04240152\n",
      "Iteration 1230, loss = 0.04239561\n",
      "Iteration 1231, loss = 0.04238971\n",
      "Iteration 1232, loss = 0.04238381\n",
      "Iteration 1233, loss = 0.04237792\n",
      "Iteration 1234, loss = 0.04237204\n",
      "Iteration 1235, loss = 0.04236617\n",
      "Iteration 1236, loss = 0.04236031\n",
      "Iteration 1237, loss = 0.04235446\n",
      "Iteration 1238, loss = 0.04234862\n",
      "Iteration 1239, loss = 0.04234278\n",
      "Iteration 1240, loss = 0.04233695\n",
      "Iteration 1241, loss = 0.04233114\n",
      "Iteration 1242, loss = 0.04232533\n",
      "Iteration 1243, loss = 0.04231953\n",
      "Iteration 1244, loss = 0.04231374\n",
      "Iteration 1245, loss = 0.04230795\n",
      "Iteration 1246, loss = 0.04230218\n",
      "Iteration 1247, loss = 0.04229641\n",
      "Iteration 1248, loss = 0.04229065\n",
      "Iteration 1249, loss = 0.04228490\n",
      "Iteration 1250, loss = 0.04227916\n",
      "Iteration 1251, loss = 0.04227343\n",
      "Iteration 1252, loss = 0.04226771\n",
      "Iteration 1253, loss = 0.04226199\n",
      "Iteration 1254, loss = 0.04225628\n",
      "Iteration 1255, loss = 0.04225058\n",
      "Iteration 1256, loss = 0.04224489\n",
      "Iteration 1257, loss = 0.04223921\n",
      "Iteration 1258, loss = 0.04223354\n",
      "Iteration 1259, loss = 0.04222787\n",
      "Iteration 1260, loss = 0.04222221\n",
      "Iteration 1261, loss = 0.04221656\n",
      "Iteration 1262, loss = 0.04221092\n",
      "Iteration 1263, loss = 0.04220529\n",
      "Iteration 1264, loss = 0.04219966\n",
      "Iteration 1265, loss = 0.04219405\n",
      "Iteration 1266, loss = 0.04218844\n",
      "Iteration 1267, loss = 0.04218284\n",
      "Iteration 1268, loss = 0.04217724\n",
      "Iteration 1269, loss = 0.04217166\n",
      "Iteration 1270, loss = 0.04216608\n",
      "Iteration 1271, loss = 0.04216052\n",
      "Iteration 1272, loss = 0.04215496\n",
      "Iteration 1273, loss = 0.04214940\n",
      "Iteration 1274, loss = 0.04214386\n",
      "Iteration 1275, loss = 0.04213832\n",
      "Iteration 1276, loss = 0.04213279\n",
      "Iteration 1277, loss = 0.04212727\n",
      "Iteration 1278, loss = 0.04212176\n",
      "Iteration 1279, loss = 0.04211626\n",
      "Iteration 1280, loss = 0.04211076\n",
      "Iteration 1281, loss = 0.04210527\n",
      "Iteration 1282, loss = 0.04209979\n",
      "Iteration 1283, loss = 0.04209431\n",
      "Iteration 1284, loss = 0.04208885\n",
      "Iteration 1285, loss = 0.04208339\n",
      "Iteration 1286, loss = 0.04207794\n",
      "Iteration 1287, loss = 0.04207250\n",
      "Iteration 1288, loss = 0.04206706\n",
      "Iteration 1289, loss = 0.04206164\n",
      "Iteration 1290, loss = 0.04205622\n",
      "Iteration 1291, loss = 0.04205081\n",
      "Iteration 1292, loss = 0.04204540\n",
      "Iteration 1293, loss = 0.04204001\n",
      "Iteration 1294, loss = 0.04203462\n",
      "Iteration 1295, loss = 0.04202924\n",
      "Iteration 1296, loss = 0.04202386\n",
      "Iteration 1297, loss = 0.04201850\n",
      "Iteration 1298, loss = 0.04201314\n",
      "Iteration 1299, loss = 0.04200779\n",
      "Iteration 1300, loss = 0.04200244\n",
      "Iteration 1301, loss = 0.04199711\n",
      "Iteration 1302, loss = 0.04199178\n",
      "Iteration 1303, loss = 0.04198646\n",
      "Iteration 1304, loss = 0.04198114\n",
      "Iteration 1305, loss = 0.04197584\n",
      "Iteration 1306, loss = 0.04197054\n",
      "Iteration 1307, loss = 0.04196525\n",
      "Iteration 1308, loss = 0.04195996\n",
      "Iteration 1309, loss = 0.04195469\n",
      "Iteration 1310, loss = 0.04194942\n",
      "Iteration 1311, loss = 0.04194416\n",
      "Iteration 1312, loss = 0.04193890\n",
      "Iteration 1313, loss = 0.04193365\n",
      "Iteration 1314, loss = 0.04192841\n",
      "Iteration 1315, loss = 0.04192318\n",
      "Iteration 1316, loss = 0.04191796\n",
      "Iteration 1317, loss = 0.04191274\n",
      "Iteration 1318, loss = 0.04190753\n",
      "Iteration 1319, loss = 0.04190232\n",
      "Iteration 1320, loss = 0.04189712\n",
      "Iteration 1321, loss = 0.04189194\n",
      "Iteration 1322, loss = 0.04188675\n",
      "Iteration 1323, loss = 0.04188158\n",
      "Iteration 1324, loss = 0.04187641\n",
      "Iteration 1325, loss = 0.04187125\n",
      "Iteration 1326, loss = 0.04186609\n",
      "Iteration 1327, loss = 0.04186095\n",
      "Iteration 1328, loss = 0.04185581\n",
      "Iteration 1329, loss = 0.04185067\n",
      "Iteration 1330, loss = 0.04184555\n",
      "Iteration 1331, loss = 0.04184043\n",
      "Iteration 1332, loss = 0.04183532\n",
      "Iteration 1333, loss = 0.04183021\n",
      "Iteration 1334, loss = 0.04182511\n",
      "Iteration 1335, loss = 0.04182002\n",
      "Iteration 1336, loss = 0.04181494\n",
      "Iteration 1337, loss = 0.04180986\n",
      "Iteration 1338, loss = 0.04180479\n",
      "Iteration 1339, loss = 0.04179972\n",
      "Iteration 1340, loss = 0.04179467\n",
      "Iteration 1341, loss = 0.04178962\n",
      "Iteration 1342, loss = 0.04178457\n",
      "Iteration 1343, loss = 0.04177954\n",
      "Iteration 1344, loss = 0.04177451\n",
      "Iteration 1345, loss = 0.04176949\n",
      "Iteration 1346, loss = 0.04176447\n",
      "Iteration 1347, loss = 0.04175946\n",
      "Iteration 1348, loss = 0.04175446\n",
      "Iteration 1349, loss = 0.04174946\n",
      "Iteration 1350, loss = 0.04174447\n",
      "Iteration 1351, loss = 0.04173949\n",
      "Iteration 1352, loss = 0.04173451\n",
      "Iteration 1353, loss = 0.04172955\n",
      "Iteration 1354, loss = 0.04172458\n",
      "Iteration 1355, loss = 0.04171963\n",
      "Iteration 1356, loss = 0.04171468\n",
      "Iteration 1357, loss = 0.04170973\n",
      "Iteration 1358, loss = 0.04170480\n",
      "Iteration 1359, loss = 0.04169987\n",
      "Iteration 1360, loss = 0.04169495\n",
      "Iteration 1361, loss = 0.04169003\n",
      "Iteration 1362, loss = 0.04168512\n",
      "Iteration 1363, loss = 0.04168022\n",
      "Iteration 1364, loss = 0.04167532\n",
      "Iteration 1365, loss = 0.04167043\n",
      "Iteration 1366, loss = 0.04166555\n",
      "Iteration 1367, loss = 0.04166067\n",
      "Iteration 1368, loss = 0.04165580\n",
      "Iteration 1369, loss = 0.04165093\n",
      "Iteration 1370, loss = 0.04164607\n",
      "Iteration 1371, loss = 0.04164122\n",
      "Iteration 1372, loss = 0.04163638\n",
      "Iteration 1373, loss = 0.04163154\n",
      "Iteration 1374, loss = 0.04162670\n",
      "Iteration 1375, loss = 0.04162188\n",
      "Iteration 1376, loss = 0.04161706\n",
      "Iteration 1377, loss = 0.04161224\n",
      "Iteration 1378, loss = 0.04160744\n",
      "Iteration 1379, loss = 0.04160264\n",
      "Iteration 1380, loss = 0.04159784\n",
      "Iteration 1381, loss = 0.04159305\n",
      "Iteration 1382, loss = 0.04158827\n",
      "Iteration 1383, loss = 0.04158349\n",
      "Iteration 1384, loss = 0.04157872\n",
      "Iteration 1385, loss = 0.04157396\n",
      "Iteration 1386, loss = 0.04156920\n",
      "Iteration 1387, loss = 0.04156445\n",
      "Iteration 1388, loss = 0.04155971\n",
      "Iteration 1389, loss = 0.04155497\n",
      "Iteration 1390, loss = 0.04155023\n",
      "Iteration 1391, loss = 0.04154551\n",
      "Iteration 1392, loss = 0.04154079\n",
      "Iteration 1393, loss = 0.04153607\n",
      "Iteration 1394, loss = 0.04153136\n",
      "Iteration 1395, loss = 0.04152666\n",
      "Iteration 1396, loss = 0.04152196\n",
      "Iteration 1397, loss = 0.04151727\n",
      "Iteration 1398, loss = 0.04151259\n",
      "Iteration 1399, loss = 0.04150791\n",
      "Iteration 1400, loss = 0.04150324\n",
      "Iteration 1401, loss = 0.04149857\n",
      "Iteration 1402, loss = 0.04149391\n",
      "Iteration 1403, loss = 0.04148926\n",
      "Iteration 1404, loss = 0.04148461\n",
      "Iteration 1405, loss = 0.04147997\n",
      "Iteration 1406, loss = 0.04147533\n",
      "Iteration 1407, loss = 0.04147070\n",
      "Iteration 1408, loss = 0.04146607\n",
      "Iteration 1409, loss = 0.04146146\n",
      "Iteration 1410, loss = 0.04145684\n",
      "Iteration 1411, loss = 0.04145224\n",
      "Iteration 1412, loss = 0.04144763\n",
      "Iteration 1413, loss = 0.04144304\n",
      "Iteration 1414, loss = 0.04143845\n",
      "Iteration 1415, loss = 0.04143387\n",
      "Iteration 1416, loss = 0.04142929\n",
      "Iteration 1417, loss = 0.04142471\n",
      "Iteration 1418, loss = 0.04142015\n",
      "Iteration 1419, loss = 0.04141559\n",
      "Iteration 1420, loss = 0.04141103\n",
      "Iteration 1421, loss = 0.04140648\n",
      "Iteration 1422, loss = 0.04140194\n",
      "Iteration 1423, loss = 0.04139740\n",
      "Iteration 1424, loss = 0.04139287\n",
      "Iteration 1425, loss = 0.04138834\n",
      "Iteration 1426, loss = 0.04138382\n",
      "Iteration 1427, loss = 0.04137931\n",
      "Iteration 1428, loss = 0.04137480\n",
      "Iteration 1429, loss = 0.04137030\n",
      "Iteration 1430, loss = 0.04136580\n",
      "Iteration 1431, loss = 0.04136130\n",
      "Iteration 1432, loss = 0.04135682\n",
      "Iteration 1433, loss = 0.04135234\n",
      "Iteration 1434, loss = 0.04134786\n",
      "Iteration 1435, loss = 0.04134339\n",
      "Iteration 1436, loss = 0.04133893\n",
      "Iteration 1437, loss = 0.04133447\n",
      "Iteration 1438, loss = 0.04133001\n",
      "Iteration 1439, loss = 0.04132557\n",
      "Iteration 1440, loss = 0.04132112\n",
      "Iteration 1441, loss = 0.04131669\n",
      "Iteration 1442, loss = 0.04131225\n",
      "Iteration 1443, loss = 0.04130783\n",
      "Iteration 1444, loss = 0.04130341\n",
      "Iteration 1445, loss = 0.04129899\n",
      "Iteration 1446, loss = 0.04129458\n",
      "Iteration 1447, loss = 0.04129018\n",
      "Iteration 1448, loss = 0.04128578\n",
      "Iteration 1449, loss = 0.04128139\n",
      "Iteration 1450, loss = 0.04127700\n",
      "Iteration 1451, loss = 0.04127262\n",
      "Iteration 1452, loss = 0.04126824\n",
      "Iteration 1453, loss = 0.04126387\n",
      "Iteration 1454, loss = 0.04125950\n",
      "Iteration 1455, loss = 0.04125514\n",
      "Iteration 1456, loss = 0.04125078\n",
      "Iteration 1457, loss = 0.04124643\n",
      "Iteration 1458, loss = 0.04124209\n",
      "Iteration 1459, loss = 0.04123775\n",
      "Iteration 1460, loss = 0.04123341\n",
      "Iteration 1461, loss = 0.04122908\n",
      "Iteration 1462, loss = 0.04122476\n",
      "Iteration 1463, loss = 0.04122044\n",
      "Iteration 1464, loss = 0.04121613\n",
      "Iteration 1465, loss = 0.04121182\n",
      "Iteration 1466, loss = 0.04120752\n",
      "Iteration 1467, loss = 0.04120322\n",
      "Iteration 1468, loss = 0.04119893\n",
      "Iteration 1469, loss = 0.04119464\n",
      "Iteration 1470, loss = 0.04119036\n",
      "Iteration 1471, loss = 0.04118608\n",
      "Iteration 1472, loss = 0.04118181\n",
      "Iteration 1473, loss = 0.04117754\n",
      "Iteration 1474, loss = 0.04117328\n",
      "Iteration 1475, loss = 0.04116902\n",
      "Iteration 1476, loss = 0.04116477\n",
      "Iteration 1477, loss = 0.04116052\n",
      "Iteration 1478, loss = 0.04115628\n",
      "Iteration 1479, loss = 0.04115205\n",
      "Iteration 1480, loss = 0.04114781\n",
      "Iteration 1481, loss = 0.04114359\n",
      "Iteration 1482, loss = 0.04113937\n",
      "Iteration 1483, loss = 0.04113515\n",
      "Iteration 1484, loss = 0.04113094\n",
      "Iteration 1485, loss = 0.04112673\n",
      "Iteration 1486, loss = 0.04112253\n",
      "Iteration 1487, loss = 0.04111834\n",
      "Iteration 1488, loss = 0.04111415\n",
      "Iteration 1489, loss = 0.04110996\n",
      "Iteration 1490, loss = 0.04110578\n",
      "Iteration 1491, loss = 0.04110160\n",
      "Iteration 1492, loss = 0.04109743\n",
      "Iteration 1493, loss = 0.04109327\n",
      "Iteration 1494, loss = 0.04108910\n",
      "Iteration 1495, loss = 0.04108495\n",
      "Iteration 1496, loss = 0.04108080\n",
      "Iteration 1497, loss = 0.04107665\n",
      "Iteration 1498, loss = 0.04107251\n",
      "Iteration 1499, loss = 0.04106837\n",
      "Iteration 1500, loss = 0.04106424\n",
      "Iteration 1501, loss = 0.04106011\n",
      "Iteration 1502, loss = 0.04105599\n",
      "Iteration 1503, loss = 0.04105187\n",
      "Iteration 1504, loss = 0.04104776\n",
      "Iteration 1505, loss = 0.04104365\n",
      "Iteration 1506, loss = 0.04103955\n",
      "Iteration 1507, loss = 0.04103545\n",
      "Iteration 1508, loss = 0.04103136\n",
      "Iteration 1509, loss = 0.04102727\n",
      "Iteration 1510, loss = 0.04102319\n",
      "Iteration 1511, loss = 0.04101911\n",
      "Iteration 1512, loss = 0.04101503\n",
      "Iteration 1513, loss = 0.04101096\n",
      "Iteration 1514, loss = 0.04100690\n",
      "Iteration 1515, loss = 0.04100284\n",
      "Iteration 1516, loss = 0.04099878\n",
      "Iteration 1517, loss = 0.04099473\n",
      "Iteration 1518, loss = 0.04099069\n",
      "Iteration 1519, loss = 0.04098665\n",
      "Iteration 1520, loss = 0.04098261\n",
      "Iteration 1521, loss = 0.04097858\n",
      "Iteration 1522, loss = 0.04097455\n",
      "Iteration 1523, loss = 0.04097053\n",
      "Iteration 1524, loss = 0.04096651\n",
      "Iteration 1525, loss = 0.04096250\n",
      "Iteration 1526, loss = 0.04095849\n",
      "Iteration 1527, loss = 0.04095448\n",
      "Iteration 1528, loss = 0.04095048\n",
      "Iteration 1529, loss = 0.04094649\n",
      "Iteration 1530, loss = 0.04094250\n",
      "Iteration 1531, loss = 0.04093851\n",
      "Iteration 1532, loss = 0.04093453\n",
      "Iteration 1533, loss = 0.04093056\n",
      "Iteration 1534, loss = 0.04092658\n",
      "Iteration 1535, loss = 0.04092262\n",
      "Iteration 1536, loss = 0.04091865\n",
      "Iteration 1537, loss = 0.04091470\n",
      "Iteration 1538, loss = 0.04091074\n",
      "Iteration 1539, loss = 0.04090679\n",
      "Iteration 1540, loss = 0.04090285\n",
      "Iteration 1541, loss = 0.04089891\n",
      "Iteration 1542, loss = 0.04089497\n",
      "Iteration 1543, loss = 0.04089104\n",
      "Iteration 1544, loss = 0.04088711\n",
      "Iteration 1545, loss = 0.04088319\n",
      "Iteration 1546, loss = 0.04087927\n",
      "Iteration 1547, loss = 0.04087536\n",
      "Iteration 1548, loss = 0.04087145\n",
      "Iteration 1549, loss = 0.04086754\n",
      "Iteration 1550, loss = 0.04086364\n",
      "Iteration 1551, loss = 0.04085975\n",
      "Iteration 1552, loss = 0.04085586\n",
      "Iteration 1553, loss = 0.04085197\n",
      "Iteration 1554, loss = 0.04084808\n",
      "Iteration 1555, loss = 0.04084421\n",
      "Iteration 1556, loss = 0.04084033\n",
      "Iteration 1557, loss = 0.04083646\n",
      "Iteration 1558, loss = 0.04083260\n",
      "Iteration 1559, loss = 0.04082874\n",
      "Iteration 1560, loss = 0.04082488\n",
      "Iteration 1561, loss = 0.04082103\n",
      "Iteration 1562, loss = 0.04081718\n",
      "Iteration 1563, loss = 0.04081333\n",
      "Iteration 1564, loss = 0.04080949\n",
      "Iteration 1565, loss = 0.04080566\n",
      "Iteration 1566, loss = 0.04080183\n",
      "Iteration 1567, loss = 0.04079800\n",
      "Iteration 1568, loss = 0.04079418\n",
      "Iteration 1569, loss = 0.04079036\n",
      "Iteration 1570, loss = 0.04078655\n",
      "Iteration 1571, loss = 0.04078274\n",
      "Iteration 1572, loss = 0.04077893\n",
      "Iteration 1573, loss = 0.04077513\n",
      "Iteration 1574, loss = 0.04077133\n",
      "Iteration 1575, loss = 0.04076754\n",
      "Iteration 1576, loss = 0.04076375\n",
      "Iteration 1577, loss = 0.04075997\n",
      "Iteration 1578, loss = 0.04075619\n",
      "Iteration 1579, loss = 0.04075241\n",
      "Iteration 1580, loss = 0.04074864\n",
      "Iteration 1581, loss = 0.04074487\n",
      "Iteration 1582, loss = 0.04074110\n",
      "Iteration 1583, loss = 0.04073735\n",
      "Iteration 1584, loss = 0.04073359\n",
      "Iteration 1585, loss = 0.04072984\n",
      "Iteration 1586, loss = 0.04072609\n",
      "Iteration 1587, loss = 0.04072235\n",
      "Iteration 1588, loss = 0.04071861\n",
      "Iteration 1589, loss = 0.04071487\n",
      "Iteration 1590, loss = 0.04071114\n",
      "Iteration 1591, loss = 0.04070742\n",
      "Iteration 1592, loss = 0.04070369\n",
      "Iteration 1593, loss = 0.04069997\n",
      "Iteration 1594, loss = 0.04069626\n",
      "Iteration 1595, loss = 0.04069255\n",
      "Iteration 1596, loss = 0.04068884\n",
      "Iteration 1597, loss = 0.04068514\n",
      "Iteration 1598, loss = 0.04068144\n",
      "Iteration 1599, loss = 0.04067775\n",
      "Iteration 1600, loss = 0.04067406\n",
      "Iteration 1601, loss = 0.04067037\n",
      "Iteration 1602, loss = 0.04066669\n",
      "Iteration 1603, loss = 0.04066301\n",
      "Iteration 1604, loss = 0.04065933\n",
      "Iteration 1605, loss = 0.04065566\n",
      "Iteration 1606, loss = 0.04065200\n",
      "Iteration 1607, loss = 0.04064833\n",
      "Iteration 1608, loss = 0.04064467\n",
      "Iteration 1609, loss = 0.04064102\n",
      "Iteration 1610, loss = 0.04063737\n",
      "Iteration 1611, loss = 0.04063372\n",
      "Iteration 1612, loss = 0.04063008\n",
      "Iteration 1613, loss = 0.04062644\n",
      "Iteration 1614, loss = 0.04062280\n",
      "Iteration 1615, loss = 0.04061917\n",
      "Iteration 1616, loss = 0.04061555\n",
      "Iteration 1617, loss = 0.04061192\n",
      "Iteration 1618, loss = 0.04060830\n",
      "Iteration 1619, loss = 0.04060469\n",
      "Iteration 1620, loss = 0.04060107\n",
      "Iteration 1621, loss = 0.04059747\n",
      "Iteration 1622, loss = 0.04059386\n",
      "Iteration 1623, loss = 0.04059026\n",
      "Iteration 1624, loss = 0.04058666\n",
      "Iteration 1625, loss = 0.04058307\n",
      "Iteration 1626, loss = 0.04057948\n",
      "Iteration 1627, loss = 0.04057590\n",
      "Iteration 1628, loss = 0.04057232\n",
      "Iteration 1629, loss = 0.04056874\n",
      "Iteration 1630, loss = 0.04056516\n",
      "Iteration 1631, loss = 0.04056159\n",
      "Iteration 1632, loss = 0.04055803\n",
      "Iteration 1633, loss = 0.04055447\n",
      "Iteration 1634, loss = 0.04055091\n",
      "Iteration 1635, loss = 0.04054735\n",
      "Iteration 1636, loss = 0.04054380\n",
      "Iteration 1637, loss = 0.04054025\n",
      "Iteration 1638, loss = 0.04053671\n",
      "Iteration 1639, loss = 0.04053317\n",
      "Iteration 1640, loss = 0.04052963\n",
      "Iteration 1641, loss = 0.04052610\n",
      "Iteration 1642, loss = 0.04052257\n",
      "Iteration 1643, loss = 0.04051905\n",
      "Iteration 1644, loss = 0.04051553\n",
      "Iteration 1645, loss = 0.04051201\n",
      "Iteration 1646, loss = 0.04050849\n",
      "Iteration 1647, loss = 0.04050498\n",
      "Iteration 1648, loss = 0.04050148\n",
      "Iteration 1649, loss = 0.04049797\n",
      "Iteration 1650, loss = 0.04049447\n",
      "Iteration 1651, loss = 0.04049098\n",
      "Iteration 1652, loss = 0.04048749\n",
      "Iteration 1653, loss = 0.04048400\n",
      "Iteration 1654, loss = 0.04048051\n",
      "Iteration 1655, loss = 0.04047703\n",
      "Iteration 1656, loss = 0.04047355\n",
      "Iteration 1657, loss = 0.04047008\n",
      "Iteration 1658, loss = 0.04046661\n",
      "Iteration 1659, loss = 0.04046314\n",
      "Iteration 1660, loss = 0.04045968\n",
      "Iteration 1661, loss = 0.04045622\n",
      "Iteration 1662, loss = 0.04045276\n",
      "Iteration 1663, loss = 0.04044931\n",
      "Iteration 1664, loss = 0.04044586\n",
      "Iteration 1665, loss = 0.04044242\n",
      "Iteration 1666, loss = 0.04043897\n",
      "Iteration 1667, loss = 0.04043554\n",
      "Iteration 1668, loss = 0.04043210\n",
      "Iteration 1669, loss = 0.04042867\n",
      "Iteration 1670, loss = 0.04042524\n",
      "Iteration 1671, loss = 0.04042182\n",
      "Iteration 1672, loss = 0.04041840\n",
      "Iteration 1673, loss = 0.04041498\n",
      "Iteration 1674, loss = 0.04041157\n",
      "Iteration 1675, loss = 0.04040816\n",
      "Iteration 1676, loss = 0.04040475\n",
      "Iteration 1677, loss = 0.04040135\n",
      "Iteration 1678, loss = 0.04039795\n",
      "Iteration 1679, loss = 0.04039455\n",
      "Iteration 1680, loss = 0.04039116\n",
      "Iteration 1681, loss = 0.04038777\n",
      "Iteration 1682, loss = 0.04038438\n",
      "Iteration 1683, loss = 0.04038100\n",
      "Iteration 1684, loss = 0.04037762\n",
      "Iteration 1685, loss = 0.04037424\n",
      "Iteration 1686, loss = 0.04037087\n",
      "Iteration 1687, loss = 0.04036750\n",
      "Iteration 1688, loss = 0.04036414\n",
      "Iteration 1689, loss = 0.04036078\n",
      "Iteration 1690, loss = 0.04035742\n",
      "Iteration 1691, loss = 0.04035406\n",
      "Iteration 1692, loss = 0.04035071\n",
      "Iteration 1693, loss = 0.04034736\n",
      "Iteration 1694, loss = 0.04034402\n",
      "Iteration 1695, loss = 0.04034067\n",
      "Iteration 1696, loss = 0.04033734\n",
      "Iteration 1697, loss = 0.04033400\n",
      "Iteration 1698, loss = 0.04033067\n",
      "Iteration 1699, loss = 0.04032734\n",
      "Iteration 1700, loss = 0.04032402\n",
      "Iteration 1701, loss = 0.04032070\n",
      "Iteration 1702, loss = 0.04031738\n",
      "Iteration 1703, loss = 0.04031406\n",
      "Iteration 1704, loss = 0.04031075\n",
      "Iteration 1705, loss = 0.04030744\n",
      "Iteration 1706, loss = 0.04030414\n",
      "Iteration 1707, loss = 0.04030084\n",
      "Iteration 1708, loss = 0.04029754\n",
      "Iteration 1709, loss = 0.04029424\n",
      "Iteration 1710, loss = 0.04029095\n",
      "Iteration 1711, loss = 0.04028766\n",
      "Iteration 1712, loss = 0.04028438\n",
      "Iteration 1713, loss = 0.04028110\n",
      "Iteration 1714, loss = 0.04027782\n",
      "Iteration 1715, loss = 0.04027454\n",
      "Iteration 1716, loss = 0.04027127\n",
      "Iteration 1717, loss = 0.04026800\n",
      "Iteration 1718, loss = 0.04026473\n",
      "Iteration 1719, loss = 0.04026147\n",
      "Iteration 1720, loss = 0.04025821\n",
      "Iteration 1721, loss = 0.04025496\n",
      "Iteration 1722, loss = 0.04025170\n",
      "Iteration 1723, loss = 0.04024845\n",
      "Iteration 1724, loss = 0.04024521\n",
      "Iteration 1725, loss = 0.04024197\n",
      "Iteration 1726, loss = 0.04023873\n",
      "Iteration 1727, loss = 0.04023549\n",
      "Iteration 1728, loss = 0.04023226\n",
      "Iteration 1729, loss = 0.04022902\n",
      "Iteration 1730, loss = 0.04022580\n",
      "Iteration 1731, loss = 0.04022257\n",
      "Iteration 1732, loss = 0.04021935\n",
      "Iteration 1733, loss = 0.04021614\n",
      "Iteration 1734, loss = 0.04021292\n",
      "Iteration 1735, loss = 0.04020971\n",
      "Iteration 1736, loss = 0.04020650\n",
      "Iteration 1737, loss = 0.04020330\n",
      "Iteration 1738, loss = 0.04020009\n",
      "Iteration 1739, loss = 0.04019690\n",
      "Iteration 1740, loss = 0.04019370\n",
      "Iteration 1741, loss = 0.04019051\n",
      "Iteration 1742, loss = 0.04018732\n",
      "Iteration 1743, loss = 0.04018413\n",
      "Iteration 1744, loss = 0.04018095\n",
      "Iteration 1745, loss = 0.04017777\n",
      "Iteration 1746, loss = 0.04017459\n",
      "Iteration 1747, loss = 0.04017142\n",
      "Iteration 1748, loss = 0.04016825\n",
      "Iteration 1749, loss = 0.04016508\n",
      "Iteration 1750, loss = 0.04016191\n",
      "Iteration 1751, loss = 0.04015875\n",
      "Iteration 1752, loss = 0.04015559\n",
      "Iteration 1753, loss = 0.04015244\n",
      "Iteration 1754, loss = 0.04014929\n",
      "Iteration 1755, loss = 0.04014614\n",
      "Iteration 1756, loss = 0.04014299\n",
      "Iteration 1757, loss = 0.04013985\n",
      "Iteration 1758, loss = 0.04013671\n",
      "Iteration 1759, loss = 0.04013357\n",
      "Iteration 1760, loss = 0.04013043\n",
      "Iteration 1761, loss = 0.04012730\n",
      "Iteration 1762, loss = 0.04012417\n",
      "Iteration 1763, loss = 0.04012105\n",
      "Iteration 1764, loss = 0.04011793\n",
      "Iteration 1765, loss = 0.04011481\n",
      "Iteration 1766, loss = 0.04011169\n",
      "Iteration 1767, loss = 0.04010858\n",
      "Iteration 1768, loss = 0.04010547\n",
      "Iteration 1769, loss = 0.04010236\n",
      "Iteration 1770, loss = 0.04009926\n",
      "Iteration 1771, loss = 0.04009616\n",
      "Iteration 1772, loss = 0.04009306\n",
      "Iteration 1773, loss = 0.04008996\n",
      "Iteration 1774, loss = 0.04008687\n",
      "Iteration 1775, loss = 0.04008378\n",
      "Iteration 1776, loss = 0.04008069\n",
      "Iteration 1777, loss = 0.04007761\n",
      "Iteration 1778, loss = 0.04007453\n",
      "Iteration 1779, loss = 0.04007145\n",
      "Iteration 1780, loss = 0.04006838\n",
      "Iteration 1781, loss = 0.04006530\n",
      "Iteration 1782, loss = 0.04006223\n",
      "Iteration 1783, loss = 0.04005917\n",
      "Iteration 1784, loss = 0.04005611\n",
      "Iteration 1785, loss = 0.04005305\n",
      "Iteration 1786, loss = 0.04004999\n",
      "Iteration 1787, loss = 0.04004693\n",
      "Iteration 1788, loss = 0.04004388\n",
      "Iteration 1789, loss = 0.04004083\n",
      "Iteration 1790, loss = 0.04003779\n",
      "Iteration 1791, loss = 0.04003474\n",
      "Iteration 1792, loss = 0.04003170\n",
      "Iteration 1793, loss = 0.04002867\n",
      "Iteration 1794, loss = 0.04002563\n",
      "Iteration 1795, loss = 0.04002260\n",
      "Iteration 1796, loss = 0.04001957\n",
      "Iteration 1797, loss = 0.04001655\n",
      "Iteration 1798, loss = 0.04001352\n",
      "Iteration 1799, loss = 0.04001050\n",
      "Iteration 1800, loss = 0.04000749\n",
      "Iteration 1801, loss = 0.04000447\n",
      "Iteration 1802, loss = 0.04000146\n",
      "Iteration 1803, loss = 0.03999845\n",
      "Iteration 1804, loss = 0.03999544\n",
      "Iteration 1805, loss = 0.03999244\n",
      "Iteration 1806, loss = 0.03998944\n",
      "Iteration 1807, loss = 0.03998644\n",
      "Iteration 1808, loss = 0.03998345\n",
      "Iteration 1809, loss = 0.03998046\n",
      "Iteration 1810, loss = 0.03997747\n",
      "Iteration 1811, loss = 0.03997448\n",
      "Iteration 1812, loss = 0.03997150\n",
      "Iteration 1813, loss = 0.03996852\n",
      "Iteration 1814, loss = 0.03996554\n",
      "Iteration 1815, loss = 0.03996256\n",
      "Iteration 1816, loss = 0.03995959\n",
      "Iteration 1817, loss = 0.03995662\n",
      "Iteration 1818, loss = 0.03995365\n",
      "Iteration 1819, loss = 0.03995069\n",
      "Iteration 1820, loss = 0.03994773\n",
      "Iteration 1821, loss = 0.03994477\n",
      "Iteration 1822, loss = 0.03994181\n",
      "Iteration 1823, loss = 0.03993886\n",
      "Iteration 1824, loss = 0.03993591\n",
      "Iteration 1825, loss = 0.03993296\n",
      "Iteration 1826, loss = 0.03993002\n",
      "Iteration 1827, loss = 0.03992707\n",
      "Iteration 1828, loss = 0.03992413\n",
      "Iteration 1829, loss = 0.03992120\n",
      "Iteration 1830, loss = 0.03991826\n",
      "Iteration 1831, loss = 0.03991533\n",
      "Iteration 1832, loss = 0.03991240\n",
      "Iteration 1833, loss = 0.03990947\n",
      "Iteration 1834, loss = 0.03990655\n",
      "Iteration 1835, loss = 0.03990363\n",
      "Iteration 1836, loss = 0.03990071\n",
      "Iteration 1837, loss = 0.03989780\n",
      "Iteration 1838, loss = 0.03989488\n",
      "Iteration 1839, loss = 0.03989197\n",
      "Iteration 1840, loss = 0.03988907\n",
      "Iteration 1841, loss = 0.03988616\n",
      "Iteration 1842, loss = 0.03988326\n",
      "Iteration 1843, loss = 0.03988036\n",
      "Iteration 1844, loss = 0.03987746\n",
      "Iteration 1845, loss = 0.03987457\n",
      "Iteration 1846, loss = 0.03987168\n",
      "Iteration 1847, loss = 0.03986879\n",
      "Iteration 1848, loss = 0.03986590\n",
      "Iteration 1849, loss = 0.03986302\n",
      "Iteration 1850, loss = 0.03986014\n",
      "Iteration 1851, loss = 0.03985726\n",
      "Iteration 1852, loss = 0.03985438\n",
      "Iteration 1853, loss = 0.03985151\n",
      "Iteration 1854, loss = 0.03984864\n",
      "Iteration 1855, loss = 0.03984577\n",
      "Iteration 1856, loss = 0.03984290\n",
      "Iteration 1857, loss = 0.03984004\n",
      "Iteration 1858, loss = 0.03983718\n",
      "Iteration 1859, loss = 0.03983432\n",
      "Iteration 1860, loss = 0.03983147\n",
      "Iteration 1861, loss = 0.03982861\n",
      "Iteration 1862, loss = 0.03982576\n",
      "Iteration 1863, loss = 0.03982292\n",
      "Iteration 1864, loss = 0.03982007\n",
      "Iteration 1865, loss = 0.03981723\n",
      "Iteration 1866, loss = 0.03981439\n",
      "Iteration 1867, loss = 0.03981155\n",
      "Iteration 1868, loss = 0.03980872\n",
      "Iteration 1869, loss = 0.03980589\n",
      "Iteration 1870, loss = 0.03980306\n",
      "Iteration 1871, loss = 0.03980023\n",
      "Iteration 1872, loss = 0.03979740\n",
      "Iteration 1873, loss = 0.03979458\n",
      "Iteration 1874, loss = 0.03979176\n",
      "Iteration 1875, loss = 0.03978895\n",
      "Iteration 1876, loss = 0.03978613\n",
      "Iteration 1877, loss = 0.03978332\n",
      "Iteration 1878, loss = 0.03978051\n",
      "Iteration 1879, loss = 0.03977770\n",
      "Iteration 1880, loss = 0.03977490\n",
      "Iteration 1881, loss = 0.03977210\n",
      "Iteration 1882, loss = 0.03976930\n",
      "Iteration 1883, loss = 0.03976650\n",
      "Iteration 1884, loss = 0.03976371\n",
      "Iteration 1885, loss = 0.03976091\n",
      "Iteration 1886, loss = 0.03975813\n",
      "Iteration 1887, loss = 0.03975534\n",
      "Iteration 1888, loss = 0.03975255\n",
      "Iteration 1889, loss = 0.03974977\n",
      "Iteration 1890, loss = 0.03974699\n",
      "Iteration 1891, loss = 0.03974422\n",
      "Iteration 1892, loss = 0.03974144\n",
      "Iteration 1893, loss = 0.03973867\n",
      "Iteration 1894, loss = 0.03973590\n",
      "Iteration 1895, loss = 0.03973313\n",
      "Iteration 1896, loss = 0.03973037\n",
      "Iteration 1897, loss = 0.03972760\n",
      "Iteration 1898, loss = 0.03972484\n",
      "Iteration 1899, loss = 0.03972209\n",
      "Iteration 1900, loss = 0.03971933\n",
      "Iteration 1901, loss = 0.03971658\n",
      "Iteration 1902, loss = 0.03971383\n",
      "Iteration 1903, loss = 0.03971108\n",
      "Iteration 1904, loss = 0.03970834\n",
      "Iteration 1905, loss = 0.03970559\n",
      "Iteration 1906, loss = 0.03970285\n",
      "Iteration 1907, loss = 0.03970012\n",
      "Iteration 1908, loss = 0.03969738\n",
      "Iteration 1909, loss = 0.03969465\n",
      "Iteration 1910, loss = 0.03969192\n",
      "Iteration 1911, loss = 0.03968919\n",
      "Iteration 1912, loss = 0.03968646\n",
      "Iteration 1913, loss = 0.03968374\n",
      "Iteration 1914, loss = 0.03968102\n",
      "Iteration 1915, loss = 0.03967830\n",
      "Iteration 1916, loss = 0.03967558\n",
      "Iteration 1917, loss = 0.03967287\n",
      "Iteration 1918, loss = 0.03967015\n",
      "Iteration 1919, loss = 0.03966744\n",
      "Iteration 1920, loss = 0.03966474\n",
      "Iteration 1921, loss = 0.03966203\n",
      "Iteration 1922, loss = 0.03965933\n",
      "Iteration 1923, loss = 0.03965663\n",
      "Iteration 1924, loss = 0.03965393\n",
      "Iteration 1925, loss = 0.03965124\n",
      "Iteration 1926, loss = 0.03964854\n",
      "Iteration 1927, loss = 0.03964585\n",
      "Iteration 1928, loss = 0.03964316\n",
      "Iteration 1929, loss = 0.03964048\n",
      "Iteration 1930, loss = 0.03963780\n",
      "Iteration 1931, loss = 0.03963511\n",
      "Iteration 1932, loss = 0.03963243\n",
      "Iteration 1933, loss = 0.03962976\n",
      "Iteration 1934, loss = 0.03962708\n",
      "Iteration 1935, loss = 0.03962441\n",
      "Iteration 1936, loss = 0.03962174\n",
      "Iteration 1937, loss = 0.03961907\n",
      "Iteration 1938, loss = 0.03961641\n",
      "Iteration 1939, loss = 0.03961375\n",
      "Iteration 1940, loss = 0.03961109\n",
      "Iteration 1941, loss = 0.03960843\n",
      "Iteration 1942, loss = 0.03960577\n",
      "Iteration 1943, loss = 0.03960312\n",
      "Iteration 1944, loss = 0.03960047\n",
      "Iteration 1945, loss = 0.03959782\n",
      "Iteration 1946, loss = 0.03959517\n",
      "Iteration 1947, loss = 0.03959253\n",
      "Iteration 1948, loss = 0.03958988\n",
      "Iteration 1949, loss = 0.03958724\n",
      "Iteration 1950, loss = 0.03958461\n",
      "Iteration 1951, loss = 0.03958197\n",
      "Iteration 1952, loss = 0.03957934\n",
      "Iteration 1953, loss = 0.03957671\n",
      "Iteration 1954, loss = 0.03957408\n",
      "Iteration 1955, loss = 0.03957145\n",
      "Iteration 1956, loss = 0.03956883\n",
      "Iteration 1957, loss = 0.03956620\n",
      "Iteration 1958, loss = 0.03956358\n",
      "Iteration 1959, loss = 0.03956097\n",
      "Iteration 1960, loss = 0.03955835\n",
      "Iteration 1961, loss = 0.03955574\n",
      "Iteration 1962, loss = 0.03955313\n",
      "Iteration 1963, loss = 0.03955052\n",
      "Iteration 1964, loss = 0.03954791\n",
      "Iteration 1965, loss = 0.03954531\n",
      "Iteration 1966, loss = 0.03954271\n",
      "Iteration 1967, loss = 0.03954011\n",
      "Iteration 1968, loss = 0.03953751\n",
      "Iteration 1969, loss = 0.03953491\n",
      "Iteration 1970, loss = 0.03953232\n",
      "Iteration 1971, loss = 0.03952973\n",
      "Iteration 1972, loss = 0.03952714\n",
      "Iteration 1973, loss = 0.03952455\n",
      "Iteration 1974, loss = 0.03952197\n",
      "Iteration 1975, loss = 0.03951939\n",
      "Iteration 1976, loss = 0.03951681\n",
      "Iteration 1977, loss = 0.03951423\n",
      "Iteration 1978, loss = 0.03951165\n",
      "Iteration 1979, loss = 0.03950908\n",
      "Iteration 1980, loss = 0.03950651\n",
      "Iteration 1981, loss = 0.03950394\n",
      "Iteration 1982, loss = 0.03950137\n",
      "Iteration 1983, loss = 0.03949881\n",
      "Iteration 1984, loss = 0.03949625\n",
      "Iteration 1985, loss = 0.03949368\n",
      "Iteration 1986, loss = 0.03949113\n",
      "Iteration 1987, loss = 0.03948857\n",
      "Iteration 1988, loss = 0.03948602\n",
      "Iteration 1989, loss = 0.03948346\n",
      "Iteration 1990, loss = 0.03948091\n",
      "Iteration 1991, loss = 0.03947837\n",
      "Iteration 1992, loss = 0.03947582\n",
      "Iteration 1993, loss = 0.03947328\n",
      "Iteration 1994, loss = 0.03947074\n",
      "Iteration 1995, loss = 0.03946820\n",
      "Iteration 1996, loss = 0.03946566\n",
      "Iteration 1997, loss = 0.03946313\n",
      "Iteration 1998, loss = 0.03946059\n",
      "Iteration 1999, loss = 0.03945806\n",
      "Iteration 2000, loss = 0.03945553\n",
      "Iteration 2001, loss = 0.03945301\n",
      "Iteration 2002, loss = 0.03945048\n",
      "Iteration 2003, loss = 0.03944796\n",
      "Iteration 2004, loss = 0.03944544\n",
      "Iteration 2005, loss = 0.03944292\n",
      "Iteration 2006, loss = 0.03944041\n",
      "Iteration 2007, loss = 0.03943789\n",
      "Iteration 2008, loss = 0.03943538\n",
      "Iteration 2009, loss = 0.03943287\n",
      "Iteration 2010, loss = 0.03943036\n",
      "Iteration 2011, loss = 0.03942786\n",
      "Iteration 2012, loss = 0.03942535\n",
      "Iteration 2013, loss = 0.03942285\n",
      "Iteration 2014, loss = 0.03942035\n",
      "Iteration 2015, loss = 0.03941786\n",
      "Iteration 2016, loss = 0.03941536\n",
      "Iteration 2017, loss = 0.03941287\n",
      "Iteration 2018, loss = 0.03941038\n",
      "Iteration 2019, loss = 0.03940789\n",
      "Iteration 2020, loss = 0.03940540\n",
      "Iteration 2021, loss = 0.03940292\n",
      "Iteration 2022, loss = 0.03940043\n",
      "Iteration 2023, loss = 0.03939795\n",
      "Iteration 2024, loss = 0.03939547\n",
      "Iteration 2025, loss = 0.03939300\n",
      "Iteration 2026, loss = 0.03939052\n",
      "Iteration 2027, loss = 0.03938805\n",
      "Iteration 2028, loss = 0.03938558\n",
      "Iteration 2029, loss = 0.03938311\n",
      "Iteration 2030, loss = 0.03938065\n",
      "Iteration 2031, loss = 0.03937818\n",
      "Iteration 2032, loss = 0.03937572\n",
      "Iteration 2033, loss = 0.03937326\n",
      "Iteration 2034, loss = 0.03937080\n",
      "Iteration 2035, loss = 0.03936834\n",
      "Iteration 2036, loss = 0.03936589\n",
      "Iteration 2037, loss = 0.03936344\n",
      "Iteration 2038, loss = 0.03936099\n",
      "Iteration 2039, loss = 0.03935854\n",
      "Iteration 2040, loss = 0.03935609\n",
      "Iteration 2041, loss = 0.03935365\n",
      "Iteration 2042, loss = 0.03935120\n",
      "Iteration 2043, loss = 0.03934876\n",
      "Iteration 2044, loss = 0.03934633\n",
      "Iteration 2045, loss = 0.03934389\n",
      "Iteration 2046, loss = 0.03934146\n",
      "Iteration 2047, loss = 0.03933902\n",
      "Iteration 2048, loss = 0.03933659\n",
      "Iteration 2049, loss = 0.03933416\n",
      "Iteration 2050, loss = 0.03933174\n",
      "Iteration 2051, loss = 0.03932931\n",
      "Iteration 2052, loss = 0.03932689\n",
      "Iteration 2053, loss = 0.03932447\n",
      "Iteration 2054, loss = 0.03932205\n",
      "Iteration 2055, loss = 0.03931963\n",
      "Iteration 2056, loss = 0.03931722\n",
      "Iteration 2057, loss = 0.03931481\n",
      "Iteration 2058, loss = 0.03931240\n",
      "Iteration 2059, loss = 0.03930999\n",
      "Iteration 2060, loss = 0.03930758\n",
      "Iteration 2061, loss = 0.03930518\n",
      "Iteration 2062, loss = 0.03930277\n",
      "Iteration 2063, loss = 0.03930037\n",
      "Iteration 2064, loss = 0.03929797\n",
      "Iteration 2065, loss = 0.03929558\n",
      "Iteration 2066, loss = 0.03929318\n",
      "Iteration 2067, loss = 0.03929079\n",
      "Iteration 2068, loss = 0.03928840\n",
      "Iteration 2069, loss = 0.03928601\n",
      "Iteration 2070, loss = 0.03928362\n",
      "Iteration 2071, loss = 0.03928123\n",
      "Iteration 2072, loss = 0.03927885\n",
      "Iteration 2073, loss = 0.03927647\n",
      "Iteration 2074, loss = 0.03927409\n",
      "Iteration 2075, loss = 0.03927171\n",
      "Iteration 2076, loss = 0.03926933\n",
      "Iteration 2077, loss = 0.03926696\n",
      "Iteration 2078, loss = 0.03926459\n",
      "Iteration 2079, loss = 0.03926222\n",
      "Iteration 2080, loss = 0.03925985\n",
      "Iteration 2081, loss = 0.03925748\n",
      "Iteration 2082, loss = 0.03925512\n",
      "Iteration 2083, loss = 0.03925275\n",
      "Iteration 2084, loss = 0.03925039\n",
      "Iteration 2085, loss = 0.03924803\n",
      "Iteration 2086, loss = 0.03924568\n",
      "Iteration 2087, loss = 0.03924332\n",
      "Iteration 2088, loss = 0.03924097\n",
      "Iteration 2089, loss = 0.03923862\n",
      "Iteration 2090, loss = 0.03923627\n",
      "Iteration 2091, loss = 0.03923392\n",
      "Iteration 2092, loss = 0.03923157\n",
      "Iteration 2093, loss = 0.03922923\n",
      "Iteration 2094, loss = 0.03922689\n",
      "Iteration 2095, loss = 0.03922455\n",
      "Iteration 2096, loss = 0.03922221\n",
      "Iteration 2097, loss = 0.03921987\n",
      "Iteration 2098, loss = 0.03921754\n",
      "Iteration 2099, loss = 0.03921521\n",
      "Iteration 2100, loss = 0.03921287\n",
      "Iteration 2101, loss = 0.03921054\n",
      "Iteration 2102, loss = 0.03920822\n",
      "Iteration 2103, loss = 0.03920589\n",
      "Iteration 2104, loss = 0.03920357\n",
      "Iteration 2105, loss = 0.03920125\n",
      "Iteration 2106, loss = 0.03919893\n",
      "Iteration 2107, loss = 0.03919661\n",
      "Iteration 2108, loss = 0.03919429\n",
      "Iteration 2109, loss = 0.03919198\n",
      "Iteration 2110, loss = 0.03918967\n",
      "Iteration 2111, loss = 0.03918735\n",
      "Iteration 2112, loss = 0.03918505\n",
      "Iteration 2113, loss = 0.03918274\n",
      "Iteration 2114, loss = 0.03918043\n",
      "Iteration 2115, loss = 0.03917813\n",
      "Iteration 2116, loss = 0.03917583\n",
      "Iteration 2117, loss = 0.03917353\n",
      "Iteration 2118, loss = 0.03917123\n",
      "Iteration 2119, loss = 0.03916893\n",
      "Iteration 2120, loss = 0.03916664\n",
      "Iteration 2121, loss = 0.03916435\n",
      "Iteration 2122, loss = 0.03916206\n",
      "Iteration 2123, loss = 0.03915977\n",
      "Iteration 2124, loss = 0.03915748\n",
      "Iteration 2125, loss = 0.03915519\n",
      "Iteration 2126, loss = 0.03915291\n",
      "Iteration 2127, loss = 0.03915063\n",
      "Iteration 2128, loss = 0.03914835\n",
      "Iteration 2129, loss = 0.03914607\n",
      "Iteration 2130, loss = 0.03914379\n",
      "Iteration 2131, loss = 0.03914152\n",
      "Iteration 2132, loss = 0.03913924\n",
      "Iteration 2133, loss = 0.03913697\n",
      "Iteration 2134, loss = 0.03913470\n",
      "Iteration 2135, loss = 0.03913244\n",
      "Iteration 2136, loss = 0.03913017\n",
      "Iteration 2137, loss = 0.03912791\n",
      "Iteration 2138, loss = 0.03912564\n",
      "Iteration 2139, loss = 0.03912338\n",
      "Iteration 2140, loss = 0.03912112\n",
      "Iteration 2141, loss = 0.03911887\n",
      "Iteration 2142, loss = 0.03911661\n",
      "Iteration 2143, loss = 0.03911436\n",
      "Iteration 2144, loss = 0.03911211\n",
      "Iteration 2145, loss = 0.03910986\n",
      "Iteration 2146, loss = 0.03910761\n",
      "Iteration 2147, loss = 0.03910536\n",
      "Iteration 2148, loss = 0.03910312\n",
      "Iteration 2149, loss = 0.03910087\n",
      "Iteration 2150, loss = 0.03909863\n",
      "Iteration 2151, loss = 0.03909639\n",
      "Iteration 2152, loss = 0.03909415\n",
      "Iteration 2153, loss = 0.03909192\n",
      "Iteration 2154, loss = 0.03908968\n",
      "Iteration 2155, loss = 0.03908745\n",
      "Iteration 2156, loss = 0.03908522\n",
      "Iteration 2157, loss = 0.03908299\n",
      "Iteration 2158, loss = 0.03908076\n",
      "Iteration 2159, loss = 0.03907854\n",
      "Iteration 2160, loss = 0.03907631\n",
      "Iteration 2161, loss = 0.03907409\n",
      "Iteration 2162, loss = 0.03907187\n",
      "Iteration 2163, loss = 0.03906965\n",
      "Iteration 2164, loss = 0.03906743\n",
      "Iteration 2165, loss = 0.03906522\n",
      "Iteration 2166, loss = 0.03906300\n",
      "Iteration 2167, loss = 0.03906079\n",
      "Iteration 2168, loss = 0.03905858\n",
      "Iteration 2169, loss = 0.03905637\n",
      "Iteration 2170, loss = 0.03905416\n",
      "Iteration 2171, loss = 0.03905196\n",
      "Iteration 2172, loss = 0.03904975\n",
      "Iteration 2173, loss = 0.03904755\n",
      "Iteration 2174, loss = 0.03904535\n",
      "Iteration 2175, loss = 0.03904315\n",
      "Iteration 2176, loss = 0.03904095\n",
      "Iteration 2177, loss = 0.03903876\n",
      "Iteration 2178, loss = 0.03903657\n",
      "Iteration 2179, loss = 0.03903437\n",
      "Iteration 2180, loss = 0.03903218\n",
      "Iteration 2181, loss = 0.03902999\n",
      "Iteration 2182, loss = 0.03902781\n",
      "Iteration 2183, loss = 0.03902562\n",
      "Iteration 2184, loss = 0.03902344\n",
      "Iteration 2185, loss = 0.03902126\n",
      "Iteration 2186, loss = 0.03901908\n",
      "Iteration 2187, loss = 0.03901690\n",
      "Iteration 2188, loss = 0.03901472\n",
      "Iteration 2189, loss = 0.03901254\n",
      "Iteration 2190, loss = 0.03901037\n",
      "Iteration 2191, loss = 0.03900820\n",
      "Iteration 2192, loss = 0.03900603\n",
      "Iteration 2193, loss = 0.03900386\n",
      "Iteration 2194, loss = 0.03900169\n",
      "Iteration 2195, loss = 0.03899953\n",
      "Iteration 2196, loss = 0.03899736\n",
      "Iteration 2197, loss = 0.03899520\n",
      "Iteration 2198, loss = 0.03899304\n",
      "Iteration 2199, loss = 0.03899088\n",
      "Iteration 2200, loss = 0.03898872\n",
      "Iteration 2201, loss = 0.03898657\n",
      "Iteration 2202, loss = 0.03898441\n",
      "Iteration 2203, loss = 0.03898226\n",
      "Iteration 2204, loss = 0.03898011\n",
      "Iteration 2205, loss = 0.03897796\n",
      "Iteration 2206, loss = 0.03897581\n",
      "Iteration 2207, loss = 0.03897367\n",
      "Iteration 2208, loss = 0.03897152\n",
      "Iteration 2209, loss = 0.03896938\n",
      "Iteration 2210, loss = 0.03896724\n",
      "Iteration 2211, loss = 0.03896510\n",
      "Iteration 2212, loss = 0.03896296\n",
      "Iteration 2213, loss = 0.03896082\n",
      "Iteration 2214, loss = 0.03895869\n",
      "Iteration 2215, loss = 0.03895655\n",
      "Iteration 2216, loss = 0.03895442\n",
      "Iteration 2217, loss = 0.03895229\n",
      "Iteration 2218, loss = 0.03895016\n",
      "Iteration 2219, loss = 0.03894804\n",
      "Iteration 2220, loss = 0.03894591\n",
      "Iteration 2221, loss = 0.03894379\n",
      "Iteration 2222, loss = 0.03894167\n",
      "Iteration 2223, loss = 0.03893955\n",
      "Iteration 2224, loss = 0.03893743\n",
      "Iteration 2225, loss = 0.03893531\n",
      "Iteration 2226, loss = 0.03893319\n",
      "Iteration 2227, loss = 0.03893108\n",
      "Iteration 2228, loss = 0.03892897\n",
      "Iteration 2229, loss = 0.03892685\n",
      "Iteration 2230, loss = 0.03892475\n",
      "Iteration 2231, loss = 0.03892264\n",
      "Iteration 2232, loss = 0.03892053\n",
      "Iteration 2233, loss = 0.03891843\n",
      "Iteration 2234, loss = 0.03891632\n",
      "Iteration 2235, loss = 0.03891422\n",
      "Iteration 2236, loss = 0.03891212\n",
      "Iteration 2237, loss = 0.03891002\n",
      "Iteration 2238, loss = 0.03890793\n",
      "Iteration 2239, loss = 0.03890583\n",
      "Iteration 2240, loss = 0.03890374\n",
      "Iteration 2241, loss = 0.03890164\n",
      "Iteration 2242, loss = 0.03889955\n",
      "Iteration 2243, loss = 0.03889746\n",
      "Iteration 2244, loss = 0.03889538\n",
      "Iteration 2245, loss = 0.03889329\n",
      "Iteration 2246, loss = 0.03889120\n",
      "Iteration 2247, loss = 0.03888912\n",
      "Iteration 2248, loss = 0.03888704\n",
      "Iteration 2249, loss = 0.03888496\n",
      "Iteration 2250, loss = 0.03888288\n",
      "Iteration 2251, loss = 0.03888080\n",
      "Iteration 2252, loss = 0.03887873\n",
      "Iteration 2253, loss = 0.03887665\n",
      "Iteration 2254, loss = 0.03887458\n",
      "Iteration 2255, loss = 0.03887251\n",
      "Iteration 2256, loss = 0.03887044\n",
      "Iteration 2257, loss = 0.03886837\n",
      "Iteration 2258, loss = 0.03886631\n",
      "Iteration 2259, loss = 0.03886424\n",
      "Iteration 2260, loss = 0.03886218\n",
      "Iteration 2261, loss = 0.03886012\n",
      "Iteration 2262, loss = 0.03885806\n",
      "Iteration 2263, loss = 0.03885600\n",
      "Iteration 2264, loss = 0.03885394\n",
      "Iteration 2265, loss = 0.03885188\n",
      "Iteration 2266, loss = 0.03884983\n",
      "Iteration 2267, loss = 0.03884778\n",
      "Iteration 2268, loss = 0.03884573\n",
      "Iteration 2269, loss = 0.03884368\n",
      "Iteration 2270, loss = 0.03884163\n",
      "Iteration 2271, loss = 0.03883958\n",
      "Iteration 2272, loss = 0.03883754\n",
      "Iteration 2273, loss = 0.03883549\n",
      "Iteration 2274, loss = 0.03883345\n",
      "Iteration 2275, loss = 0.03883141\n",
      "Iteration 2276, loss = 0.03882937\n",
      "Iteration 2277, loss = 0.03882733\n",
      "Iteration 2278, loss = 0.03882529\n",
      "Iteration 2279, loss = 0.03882326\n",
      "Iteration 2280, loss = 0.03882123\n",
      "Iteration 2281, loss = 0.03881919\n",
      "Iteration 2282, loss = 0.03881716\n",
      "Iteration 2283, loss = 0.03881513\n",
      "Iteration 2284, loss = 0.03881311\n",
      "Iteration 2285, loss = 0.03881108\n",
      "Iteration 2286, loss = 0.03880905\n",
      "Iteration 2287, loss = 0.03880703\n",
      "Iteration 2288, loss = 0.03880501\n",
      "Iteration 2289, loss = 0.03880299\n",
      "Iteration 2290, loss = 0.03880097\n",
      "Iteration 2291, loss = 0.03879895\n",
      "Iteration 2292, loss = 0.03879694\n",
      "Iteration 2293, loss = 0.03879492\n",
      "Iteration 2294, loss = 0.03879291\n",
      "Iteration 2295, loss = 0.03879090\n",
      "Iteration 2296, loss = 0.03878889\n",
      "Iteration 2297, loss = 0.03878688\n",
      "Iteration 2298, loss = 0.03878487\n",
      "Iteration 2299, loss = 0.03878286\n",
      "Iteration 2300, loss = 0.03878086\n",
      "Iteration 2301, loss = 0.03877886\n",
      "Iteration 2302, loss = 0.03877686\n",
      "Iteration 2303, loss = 0.03877486\n",
      "Iteration 2304, loss = 0.03877286\n",
      "Iteration 2305, loss = 0.03877086\n",
      "Iteration 2306, loss = 0.03876886\n",
      "Iteration 2307, loss = 0.03876687\n",
      "Iteration 2308, loss = 0.03876488\n",
      "Iteration 2309, loss = 0.03876288\n",
      "Iteration 2310, loss = 0.03876089\n",
      "Iteration 2311, loss = 0.03875890\n",
      "Iteration 2312, loss = 0.03875692\n",
      "Iteration 2313, loss = 0.03875493\n",
      "Iteration 2314, loss = 0.03875295\n",
      "Iteration 2315, loss = 0.03875096\n",
      "Iteration 2316, loss = 0.03874898\n",
      "Iteration 2317, loss = 0.03874700\n",
      "Iteration 2318, loss = 0.03874502\n",
      "Iteration 2319, loss = 0.03874305\n",
      "Iteration 2320, loss = 0.03874107\n",
      "Iteration 2321, loss = 0.03873909\n",
      "Iteration 2322, loss = 0.03873712\n",
      "Iteration 2323, loss = 0.03873515\n",
      "Iteration 2324, loss = 0.03873318\n",
      "Iteration 2325, loss = 0.03873121\n",
      "Iteration 2326, loss = 0.03872924\n",
      "Iteration 2327, loss = 0.03872727\n",
      "Iteration 2328, loss = 0.03872531\n",
      "Iteration 2329, loss = 0.03872335\n",
      "Iteration 2330, loss = 0.03872138\n",
      "Iteration 2331, loss = 0.03871942\n",
      "Iteration 2332, loss = 0.03871746\n",
      "Iteration 2333, loss = 0.03871551\n",
      "Iteration 2334, loss = 0.03871355\n",
      "Iteration 2335, loss = 0.03871159\n",
      "Iteration 2336, loss = 0.03870964\n",
      "Iteration 2337, loss = 0.03870769\n",
      "Iteration 2338, loss = 0.03870574\n",
      "Iteration 2339, loss = 0.03870379\n",
      "Iteration 2340, loss = 0.03870184\n",
      "Iteration 2341, loss = 0.03869989\n",
      "Iteration 2342, loss = 0.03869794\n",
      "Iteration 2343, loss = 0.03869600\n",
      "Iteration 2344, loss = 0.03869406\n",
      "Iteration 2345, loss = 0.03869211\n",
      "Iteration 2346, loss = 0.03869017\n",
      "Iteration 2347, loss = 0.03868824\n",
      "Iteration 2348, loss = 0.03868630\n",
      "Iteration 2349, loss = 0.03868436\n",
      "Iteration 2350, loss = 0.03868243\n",
      "Iteration 2351, loss = 0.03868049\n",
      "Iteration 2352, loss = 0.03867856\n",
      "Iteration 2353, loss = 0.03867663\n",
      "Iteration 2354, loss = 0.03867470\n",
      "Iteration 2355, loss = 0.03867277\n",
      "Iteration 2356, loss = 0.03867085\n",
      "Iteration 2357, loss = 0.03866892\n",
      "Iteration 2358, loss = 0.03866700\n",
      "Iteration 2359, loss = 0.03866507\n",
      "Iteration 2360, loss = 0.03866315\n",
      "Iteration 2361, loss = 0.03866123\n",
      "Iteration 2362, loss = 0.03865931\n",
      "Iteration 2363, loss = 0.03865739\n",
      "Iteration 2364, loss = 0.03865548\n",
      "Iteration 2365, loss = 0.03865356\n",
      "Iteration 2366, loss = 0.03865165\n",
      "Iteration 2367, loss = 0.03864974\n",
      "Iteration 2368, loss = 0.03864783\n",
      "Iteration 2369, loss = 0.03864592\n",
      "Iteration 2370, loss = 0.03864401\n",
      "Iteration 2371, loss = 0.03864210\n",
      "Iteration 2372, loss = 0.03864019\n",
      "Iteration 2373, loss = 0.03863829\n",
      "Iteration 2374, loss = 0.03863639\n",
      "Iteration 2375, loss = 0.03863449\n",
      "Iteration 2376, loss = 0.03863258\n",
      "Iteration 2377, loss = 0.03863069\n",
      "Iteration 2378, loss = 0.03862879\n",
      "Iteration 2379, loss = 0.03862689\n",
      "Iteration 2380, loss = 0.03862500\n",
      "Iteration 2381, loss = 0.03862310\n",
      "Iteration 2382, loss = 0.03862121\n",
      "Iteration 2383, loss = 0.03861932\n",
      "Iteration 2384, loss = 0.03861743\n",
      "Iteration 2385, loss = 0.03861554\n",
      "Iteration 2386, loss = 0.03861365\n",
      "Iteration 2387, loss = 0.03861176\n",
      "Iteration 2388, loss = 0.03860988\n",
      "Iteration 2389, loss = 0.03860800\n",
      "Iteration 2390, loss = 0.03860611\n",
      "Iteration 2391, loss = 0.03860423\n",
      "Iteration 2392, loss = 0.03860235\n",
      "Iteration 2393, loss = 0.03860047\n",
      "Iteration 2394, loss = 0.03859860\n",
      "Iteration 2395, loss = 0.03859672\n",
      "Iteration 2396, loss = 0.03859485\n",
      "Iteration 2397, loss = 0.03859297\n",
      "Iteration 2398, loss = 0.03859110\n",
      "Iteration 2399, loss = 0.03858923\n",
      "Iteration 2400, loss = 0.03858736\n",
      "Iteration 2401, loss = 0.03858549\n",
      "Iteration 2402, loss = 0.03858362\n",
      "Iteration 2403, loss = 0.03858176\n",
      "Iteration 2404, loss = 0.03857989\n",
      "Iteration 2405, loss = 0.03857803\n",
      "Iteration 2406, loss = 0.03857617\n",
      "Iteration 2407, loss = 0.03857431\n",
      "Iteration 2408, loss = 0.03857245\n",
      "Iteration 2409, loss = 0.03857059\n",
      "Iteration 2410, loss = 0.03856873\n",
      "Iteration 2411, loss = 0.03856687\n",
      "Iteration 2412, loss = 0.03856502\n",
      "Iteration 2413, loss = 0.03856317\n",
      "Iteration 2414, loss = 0.03856131\n",
      "Iteration 2415, loss = 0.03855946\n",
      "Iteration 2416, loss = 0.03855761\n",
      "Iteration 2417, loss = 0.03855576\n",
      "Iteration 2418, loss = 0.03855392\n",
      "Iteration 2419, loss = 0.03855207\n",
      "Iteration 2420, loss = 0.03855023\n",
      "Iteration 2421, loss = 0.03854838\n",
      "Iteration 2422, loss = 0.03854654\n",
      "Iteration 2423, loss = 0.03854470\n",
      "Iteration 2424, loss = 0.03854286\n",
      "Iteration 2425, loss = 0.03854102\n",
      "Iteration 2426, loss = 0.03853918\n",
      "Iteration 2427, loss = 0.03853735\n",
      "Iteration 2428, loss = 0.03853551\n",
      "Iteration 2429, loss = 0.03853368\n",
      "Iteration 2430, loss = 0.03853184\n",
      "Iteration 2431, loss = 0.03853001\n",
      "Iteration 2432, loss = 0.03852818\n",
      "Iteration 2433, loss = 0.03852635\n",
      "Iteration 2434, loss = 0.03852452\n",
      "Iteration 2435, loss = 0.03852270\n",
      "Iteration 2436, loss = 0.03852087\n",
      "Iteration 2437, loss = 0.03851905\n",
      "Iteration 2438, loss = 0.03851723\n",
      "Iteration 2439, loss = 0.03851540\n",
      "Iteration 2440, loss = 0.03851358\n",
      "Iteration 2441, loss = 0.03851176\n",
      "Iteration 2442, loss = 0.03850994\n",
      "Iteration 2443, loss = 0.03850813\n",
      "Iteration 2444, loss = 0.03850631\n",
      "Iteration 2445, loss = 0.03850450\n",
      "Iteration 2446, loss = 0.03850268\n",
      "Iteration 2447, loss = 0.03850087\n",
      "Iteration 2448, loss = 0.03849906\n",
      "Iteration 2449, loss = 0.03849725\n",
      "Iteration 2450, loss = 0.03849544\n",
      "Iteration 2451, loss = 0.03849363\n",
      "Iteration 2452, loss = 0.03849183\n",
      "Iteration 2453, loss = 0.03849002\n",
      "Iteration 2454, loss = 0.03848822\n",
      "Iteration 2455, loss = 0.03848641\n",
      "Iteration 2456, loss = 0.03848461\n",
      "Iteration 2457, loss = 0.03848281\n",
      "Iteration 2458, loss = 0.03848101\n",
      "Iteration 2459, loss = 0.03847921\n",
      "Iteration 2460, loss = 0.03847742\n",
      "Iteration 2461, loss = 0.03847562\n",
      "Iteration 2462, loss = 0.03847383\n",
      "Iteration 2463, loss = 0.03847203\n",
      "Iteration 2464, loss = 0.03847024\n",
      "Iteration 2465, loss = 0.03846845\n",
      "Iteration 2466, loss = 0.03846666\n",
      "Iteration 2467, loss = 0.03846487\n",
      "Iteration 2468, loss = 0.03846308\n",
      "Iteration 2469, loss = 0.03846130\n",
      "Iteration 2470, loss = 0.03845951\n",
      "Iteration 2471, loss = 0.03845773\n",
      "Iteration 2472, loss = 0.03845594\n",
      "Iteration 2473, loss = 0.03845416\n",
      "Iteration 2474, loss = 0.03845238\n",
      "Iteration 2475, loss = 0.03845060\n",
      "Iteration 2476, loss = 0.03844882\n",
      "Iteration 2477, loss = 0.03844704\n",
      "Iteration 2478, loss = 0.03844527\n",
      "Iteration 2479, loss = 0.03844349\n",
      "Iteration 2480, loss = 0.03844172\n",
      "Iteration 2481, loss = 0.03843994\n",
      "Iteration 2482, loss = 0.03843817\n",
      "Iteration 2483, loss = 0.03843640\n",
      "Iteration 2484, loss = 0.03843463\n",
      "Iteration 2485, loss = 0.03843286\n",
      "Iteration 2486, loss = 0.03843109\n",
      "Iteration 2487, loss = 0.03842933\n",
      "Iteration 2488, loss = 0.03842756\n",
      "Iteration 2489, loss = 0.03842580\n",
      "Iteration 2490, loss = 0.03842404\n",
      "Iteration 2491, loss = 0.03842227\n",
      "Iteration 2492, loss = 0.03842051\n",
      "Iteration 2493, loss = 0.03841875\n",
      "Iteration 2494, loss = 0.03841700\n",
      "Iteration 2495, loss = 0.03841524\n",
      "Iteration 2496, loss = 0.03841348\n",
      "Iteration 2497, loss = 0.03841173\n",
      "Iteration 2498, loss = 0.03840997\n",
      "Iteration 2499, loss = 0.03840822\n",
      "Iteration 2500, loss = 0.03840647\n",
      "Iteration 2501, loss = 0.03840472\n",
      "Iteration 2502, loss = 0.03840297\n",
      "Iteration 2503, loss = 0.03840122\n",
      "Iteration 2504, loss = 0.03839947\n",
      "Iteration 2505, loss = 0.03839772\n",
      "Iteration 2506, loss = 0.03839598\n",
      "Iteration 2507, loss = 0.03839423\n",
      "Iteration 2508, loss = 0.03839249\n",
      "Iteration 2509, loss = 0.03839075\n",
      "Iteration 2510, loss = 0.03838901\n",
      "Iteration 2511, loss = 0.03838727\n",
      "Iteration 2512, loss = 0.03838553\n",
      "Iteration 2513, loss = 0.03838379\n",
      "Iteration 2514, loss = 0.03838205\n",
      "Iteration 2515, loss = 0.03838032\n",
      "Iteration 2516, loss = 0.03837858\n",
      "Iteration 2517, loss = 0.03837685\n",
      "Iteration 2518, loss = 0.03837512\n",
      "Iteration 2519, loss = 0.03837338\n",
      "Iteration 2520, loss = 0.03837165\n",
      "Iteration 2521, loss = 0.03836993\n",
      "Iteration 2522, loss = 0.03836820\n",
      "Iteration 2523, loss = 0.03836647\n",
      "Iteration 2524, loss = 0.03836474\n",
      "Iteration 2525, loss = 0.03836302\n",
      "Iteration 2526, loss = 0.03836129\n",
      "Iteration 2527, loss = 0.03835957\n",
      "Iteration 2528, loss = 0.03835785\n",
      "Iteration 2529, loss = 0.03835613\n",
      "Iteration 2530, loss = 0.03835441\n",
      "Iteration 2531, loss = 0.03835269\n",
      "Iteration 2532, loss = 0.03835097\n",
      "Iteration 2533, loss = 0.03834925\n",
      "Iteration 2534, loss = 0.03834754\n",
      "Iteration 2535, loss = 0.03834582\n",
      "Iteration 2536, loss = 0.03834411\n",
      "Iteration 2537, loss = 0.03834240\n",
      "Iteration 2538, loss = 0.03834069\n",
      "Iteration 2539, loss = 0.03833898\n",
      "Iteration 2540, loss = 0.03833727\n",
      "Iteration 2541, loss = 0.03833556\n",
      "Iteration 2542, loss = 0.03833385\n",
      "Iteration 2543, loss = 0.03833214\n",
      "Iteration 2544, loss = 0.03833044\n",
      "Iteration 2545, loss = 0.03832873\n",
      "Iteration 2546, loss = 0.03832703\n",
      "Iteration 2547, loss = 0.03832533\n",
      "Iteration 2548, loss = 0.03832363\n",
      "Iteration 2549, loss = 0.03832193\n",
      "Iteration 2550, loss = 0.03832023\n",
      "Iteration 2551, loss = 0.03831853\n",
      "Iteration 2552, loss = 0.03831683\n",
      "Iteration 2553, loss = 0.03831514\n",
      "Iteration 2554, loss = 0.03831344\n",
      "Iteration 2555, loss = 0.03831175\n",
      "Iteration 2556, loss = 0.03831005\n",
      "Iteration 2557, loss = 0.03830836\n",
      "Iteration 2558, loss = 0.03830667\n",
      "Iteration 2559, loss = 0.03830498\n",
      "Iteration 2560, loss = 0.03830329\n",
      "Iteration 2561, loss = 0.03830160\n",
      "Iteration 2562, loss = 0.03829991\n",
      "Iteration 2563, loss = 0.03829823\n",
      "Iteration 2564, loss = 0.03829654\n",
      "Iteration 2565, loss = 0.03829486\n",
      "Iteration 2566, loss = 0.03829317\n",
      "Iteration 2567, loss = 0.03829149\n",
      "Iteration 2568, loss = 0.03828981\n",
      "Iteration 2569, loss = 0.03828813\n",
      "Iteration 2570, loss = 0.03828645\n",
      "Iteration 2571, loss = 0.03828477\n",
      "Iteration 2572, loss = 0.03828310\n",
      "Iteration 2573, loss = 0.03828142\n",
      "Iteration 2574, loss = 0.03827974\n",
      "Iteration 2575, loss = 0.03827807\n",
      "Iteration 2576, loss = 0.03827640\n",
      "Iteration 2577, loss = 0.03827472\n",
      "Iteration 2578, loss = 0.03827305\n",
      "Iteration 2579, loss = 0.03827138\n",
      "Iteration 2580, loss = 0.03826971\n",
      "Iteration 2581, loss = 0.03826804\n",
      "Iteration 2582, loss = 0.03826637\n",
      "Iteration 2583, loss = 0.03826471\n",
      "Iteration 2584, loss = 0.03826304\n",
      "Iteration 2585, loss = 0.03826138\n",
      "Iteration 2586, loss = 0.03825971\n",
      "Iteration 2587, loss = 0.03825805\n",
      "Iteration 2588, loss = 0.03825639\n",
      "Iteration 2589, loss = 0.03825473\n",
      "Iteration 2590, loss = 0.03825307\n",
      "Iteration 2591, loss = 0.03825141\n",
      "Iteration 2592, loss = 0.03824975\n",
      "Iteration 2593, loss = 0.03824809\n",
      "Iteration 2594, loss = 0.03824643\n",
      "Iteration 2595, loss = 0.03824478\n",
      "Iteration 2596, loss = 0.03824312\n",
      "Iteration 2597, loss = 0.03824147\n",
      "Iteration 2598, loss = 0.03823982\n",
      "Iteration 2599, loss = 0.03823817\n",
      "Iteration 2600, loss = 0.03823651\n",
      "Iteration 2601, loss = 0.03823486\n",
      "Iteration 2602, loss = 0.03823322\n",
      "Iteration 2603, loss = 0.03823157\n",
      "Iteration 2604, loss = 0.03822992\n",
      "Iteration 2605, loss = 0.03822827\n",
      "Iteration 2606, loss = 0.03822663\n",
      "Iteration 2607, loss = 0.03822498\n",
      "Iteration 2608, loss = 0.03822334\n",
      "Iteration 2609, loss = 0.03822170\n",
      "Iteration 2610, loss = 0.03822006\n",
      "Iteration 2611, loss = 0.03821842\n",
      "Iteration 2612, loss = 0.03821678\n",
      "Iteration 2613, loss = 0.03821514\n",
      "Iteration 2614, loss = 0.03821350\n",
      "Iteration 2615, loss = 0.03821186\n",
      "Iteration 2616, loss = 0.03821023\n",
      "Iteration 2617, loss = 0.03820859\n",
      "Iteration 2618, loss = 0.03820696\n",
      "Iteration 2619, loss = 0.03820532\n",
      "Iteration 2620, loss = 0.03820369\n",
      "Iteration 2621, loss = 0.03820206\n",
      "Iteration 2622, loss = 0.03820043\n",
      "Iteration 2623, loss = 0.03819880\n",
      "Iteration 2624, loss = 0.03819717\n",
      "Iteration 2625, loss = 0.03819554\n",
      "Iteration 2626, loss = 0.03819391\n",
      "Iteration 2627, loss = 0.03819228\n",
      "Iteration 2628, loss = 0.03819066\n",
      "Iteration 2629, loss = 0.03818903\n",
      "Iteration 2630, loss = 0.03818741\n",
      "Iteration 2631, loss = 0.03818579\n",
      "Iteration 2632, loss = 0.03818416\n",
      "Iteration 2633, loss = 0.03818254\n",
      "Iteration 2634, loss = 0.03818092\n",
      "Iteration 2635, loss = 0.03817930\n",
      "Iteration 2636, loss = 0.03817768\n",
      "Iteration 2637, loss = 0.03817607\n",
      "Iteration 2638, loss = 0.03817445\n",
      "Iteration 2639, loss = 0.03817283\n",
      "Iteration 2640, loss = 0.03817122\n",
      "Iteration 2641, loss = 0.03816960\n",
      "Iteration 2642, loss = 0.03816799\n",
      "Iteration 2643, loss = 0.03816638\n",
      "Iteration 2644, loss = 0.03816477\n",
      "Iteration 2645, loss = 0.03816315\n",
      "Iteration 2646, loss = 0.03816154\n",
      "Iteration 2647, loss = 0.03815994\n",
      "Iteration 2648, loss = 0.03815833\n",
      "Iteration 2649, loss = 0.03815672\n",
      "Iteration 2650, loss = 0.03815511\n",
      "Iteration 2651, loss = 0.03815351\n",
      "Iteration 2652, loss = 0.03815190\n",
      "Iteration 2653, loss = 0.03815030\n",
      "Iteration 2654, loss = 0.03814869\n",
      "Iteration 2655, loss = 0.03814709\n",
      "Iteration 2656, loss = 0.03814549\n",
      "Iteration 2657, loss = 0.03814389\n",
      "Iteration 2658, loss = 0.03814229\n",
      "Iteration 2659, loss = 0.03814069\n",
      "Iteration 2660, loss = 0.03813909\n",
      "Iteration 2661, loss = 0.03813749\n",
      "Iteration 2662, loss = 0.03813589\n",
      "Iteration 2663, loss = 0.03813430\n",
      "Iteration 2664, loss = 0.03813270\n",
      "Iteration 2665, loss = 0.03813111\n",
      "Iteration 2666, loss = 0.03812951\n",
      "Iteration 2667, loss = 0.03812792\n",
      "Iteration 2668, loss = 0.03812633\n",
      "Iteration 2669, loss = 0.03812474\n",
      "Iteration 2670, loss = 0.03812315\n",
      "Iteration 2671, loss = 0.03812156\n",
      "Iteration 2672, loss = 0.03811997\n",
      "Iteration 2673, loss = 0.03811838\n",
      "Iteration 2674, loss = 0.03811679\n",
      "Iteration 2675, loss = 0.03811521\n",
      "Iteration 2676, loss = 0.03811362\n",
      "Iteration 2677, loss = 0.03811204\n",
      "Iteration 2678, loss = 0.03811045\n",
      "Iteration 2679, loss = 0.03810887\n",
      "Iteration 2680, loss = 0.03810729\n",
      "Iteration 2681, loss = 0.03810570\n",
      "Iteration 2682, loss = 0.03810412\n",
      "Iteration 2683, loss = 0.03810254\n",
      "Iteration 2684, loss = 0.03810096\n",
      "Iteration 2685, loss = 0.03809938\n",
      "Iteration 2686, loss = 0.03809781\n",
      "Iteration 2687, loss = 0.03809623\n",
      "Iteration 2688, loss = 0.03809465\n",
      "Iteration 2689, loss = 0.03809308\n",
      "Iteration 2690, loss = 0.03809150\n",
      "Iteration 2691, loss = 0.03808993\n",
      "Iteration 2692, loss = 0.03808835\n",
      "Iteration 2693, loss = 0.03808678\n",
      "Iteration 2694, loss = 0.03808521\n",
      "Iteration 2695, loss = 0.03808364\n",
      "Iteration 2696, loss = 0.03808207\n",
      "Iteration 2697, loss = 0.03808050\n",
      "Iteration 2698, loss = 0.03807893\n",
      "Iteration 2699, loss = 0.03807736\n",
      "Iteration 2700, loss = 0.03807579\n",
      "Iteration 2701, loss = 0.03807423\n",
      "Iteration 2702, loss = 0.03807266\n",
      "Iteration 2703, loss = 0.03807110\n",
      "Iteration 2704, loss = 0.03806953\n",
      "Iteration 2705, loss = 0.03806797\n",
      "Iteration 2706, loss = 0.03806640\n",
      "Iteration 2707, loss = 0.03806484\n",
      "Iteration 2708, loss = 0.03806328\n",
      "Iteration 2709, loss = 0.03806172\n",
      "Iteration 2710, loss = 0.03806016\n",
      "Iteration 2711, loss = 0.03805860\n",
      "Iteration 2712, loss = 0.03805704\n",
      "Iteration 2713, loss = 0.03805548\n",
      "Iteration 2714, loss = 0.03805392\n",
      "Iteration 2715, loss = 0.03805237\n",
      "Iteration 2716, loss = 0.03805081\n",
      "Iteration 2717, loss = 0.03804926\n",
      "Iteration 2718, loss = 0.03804770\n",
      "Iteration 2719, loss = 0.03804615\n",
      "Iteration 2720, loss = 0.03804459\n",
      "Iteration 2721, loss = 0.03804304\n",
      "Iteration 2722, loss = 0.03804149\n",
      "Iteration 2723, loss = 0.03803994\n",
      "Iteration 2724, loss = 0.03803839\n",
      "Iteration 2725, loss = 0.03803684\n",
      "Iteration 2726, loss = 0.03803529\n",
      "Iteration 2727, loss = 0.03803374\n",
      "Iteration 2728, loss = 0.03803219\n",
      "Iteration 2729, loss = 0.03803065\n",
      "Iteration 2730, loss = 0.03802910\n",
      "Iteration 2731, loss = 0.03802756\n",
      "Iteration 2732, loss = 0.03802601\n",
      "Iteration 2733, loss = 0.03802447\n",
      "Iteration 2734, loss = 0.03802292\n",
      "Iteration 2735, loss = 0.03802138\n",
      "Iteration 2736, loss = 0.03801984\n",
      "Iteration 2737, loss = 0.03801830\n",
      "Iteration 2738, loss = 0.03801675\n",
      "Iteration 2739, loss = 0.03801521\n",
      "Iteration 2740, loss = 0.03801367\n",
      "Iteration 2741, loss = 0.03801214\n",
      "Iteration 2742, loss = 0.03801060\n",
      "Iteration 2743, loss = 0.03800906\n",
      "Iteration 2744, loss = 0.03800752\n",
      "Iteration 2745, loss = 0.03800599\n",
      "Iteration 2746, loss = 0.03800445\n",
      "Iteration 2747, loss = 0.03800291\n",
      "Iteration 2748, loss = 0.03800138\n",
      "Iteration 2749, loss = 0.03799985\n",
      "Iteration 2750, loss = 0.03799831\n",
      "Iteration 2751, loss = 0.03799678\n",
      "Iteration 2752, loss = 0.03799525\n",
      "Iteration 2753, loss = 0.03799372\n",
      "Iteration 2754, loss = 0.03799219\n",
      "Iteration 2755, loss = 0.03799066\n",
      "Iteration 2756, loss = 0.03798913\n",
      "Iteration 2757, loss = 0.03798760\n",
      "Iteration 2758, loss = 0.03798607\n",
      "Iteration 2759, loss = 0.03798454\n",
      "Iteration 2760, loss = 0.03798301\n",
      "Iteration 2761, loss = 0.03798149\n",
      "Iteration 2762, loss = 0.03797996\n",
      "Iteration 2763, loss = 0.03797844\n",
      "Iteration 2764, loss = 0.03797691\n",
      "Iteration 2765, loss = 0.03797539\n",
      "Iteration 2766, loss = 0.03797386\n",
      "Iteration 2767, loss = 0.03797234\n",
      "Iteration 2768, loss = 0.03797082\n",
      "Iteration 2769, loss = 0.03796930\n",
      "Iteration 2770, loss = 0.03796778\n",
      "Iteration 2771, loss = 0.03796626\n",
      "Iteration 2772, loss = 0.03796474\n",
      "Iteration 2773, loss = 0.03796322\n",
      "Iteration 2774, loss = 0.03796170\n",
      "Iteration 2775, loss = 0.03796018\n",
      "Iteration 2776, loss = 0.03795866\n",
      "Iteration 2777, loss = 0.03795714\n",
      "Iteration 2778, loss = 0.03795563\n",
      "Iteration 2779, loss = 0.03795411\n",
      "Iteration 2780, loss = 0.03795260\n",
      "Iteration 2781, loss = 0.03795108\n",
      "Iteration 2782, loss = 0.03794957\n",
      "Iteration 2783, loss = 0.03794805\n",
      "Iteration 2784, loss = 0.03794654\n",
      "Iteration 2785, loss = 0.03794503\n",
      "Iteration 2786, loss = 0.03794352\n",
      "Iteration 2787, loss = 0.03794200\n",
      "Iteration 2788, loss = 0.03794049\n",
      "Iteration 2789, loss = 0.03793898\n",
      "Iteration 2790, loss = 0.03793747\n",
      "Iteration 2791, loss = 0.03793596\n",
      "Iteration 2792, loss = 0.03793445\n",
      "Iteration 2793, loss = 0.03793295\n",
      "Iteration 2794, loss = 0.03793144\n",
      "Iteration 2795, loss = 0.03792993\n",
      "Iteration 2796, loss = 0.03792842\n",
      "Iteration 2797, loss = 0.03792692\n",
      "Iteration 2798, loss = 0.03792541\n",
      "Iteration 2799, loss = 0.03792391\n",
      "Iteration 2800, loss = 0.03792240\n",
      "Iteration 2801, loss = 0.03792090\n",
      "Iteration 2802, loss = 0.03791939\n",
      "Iteration 2803, loss = 0.03791789\n",
      "Iteration 2804, loss = 0.03791639\n",
      "Iteration 2805, loss = 0.03791489\n",
      "Iteration 2806, loss = 0.03791338\n",
      "Iteration 2807, loss = 0.03791188\n",
      "Iteration 2808, loss = 0.03791038\n",
      "Iteration 2809, loss = 0.03790888\n",
      "Iteration 2810, loss = 0.03790738\n",
      "Iteration 2811, loss = 0.03790588\n",
      "Iteration 2812, loss = 0.03790438\n",
      "Iteration 2813, loss = 0.03790289\n",
      "Iteration 2814, loss = 0.03790139\n",
      "Iteration 2815, loss = 0.03789989\n",
      "Iteration 2816, loss = 0.03789839\n",
      "Iteration 2817, loss = 0.03789690\n",
      "Iteration 2818, loss = 0.03789540\n",
      "Iteration 2819, loss = 0.03789391\n",
      "Iteration 2820, loss = 0.03789241\n",
      "Iteration 2821, loss = 0.03789092\n",
      "Iteration 2822, loss = 0.03788942\n",
      "Iteration 2823, loss = 0.03788793\n",
      "Iteration 2824, loss = 0.03788644\n",
      "Iteration 2825, loss = 0.03788494\n",
      "Iteration 2826, loss = 0.03788345\n",
      "Iteration 2827, loss = 0.03788196\n",
      "Iteration 2828, loss = 0.03788047\n",
      "Iteration 2829, loss = 0.03787898\n",
      "Iteration 2830, loss = 0.03787749\n",
      "Iteration 2831, loss = 0.03787600\n",
      "Iteration 2832, loss = 0.03787451\n",
      "Iteration 2833, loss = 0.03787302\n",
      "Iteration 2834, loss = 0.03787153\n",
      "Iteration 2835, loss = 0.03787004\n",
      "Iteration 2836, loss = 0.03786855\n",
      "Iteration 2837, loss = 0.03786706\n",
      "Iteration 2838, loss = 0.03786558\n",
      "Iteration 2839, loss = 0.03786409\n",
      "Iteration 2840, loss = 0.03786260\n",
      "Iteration 2841, loss = 0.03786112\n",
      "Iteration 2842, loss = 0.03785963\n",
      "Iteration 2843, loss = 0.03785815\n",
      "Iteration 2844, loss = 0.03785666\n",
      "Iteration 2845, loss = 0.03785518\n",
      "Iteration 2846, loss = 0.03785369\n",
      "Iteration 2847, loss = 0.03785221\n",
      "Iteration 2848, loss = 0.03785073\n",
      "Iteration 2849, loss = 0.03784924\n",
      "Iteration 2850, loss = 0.03784776\n",
      "Iteration 2851, loss = 0.03784628\n",
      "Iteration 2852, loss = 0.03784480\n",
      "Iteration 2853, loss = 0.03784332\n",
      "Iteration 2854, loss = 0.03784183\n",
      "Iteration 2855, loss = 0.03784035\n",
      "Iteration 2856, loss = 0.03783887\n",
      "Iteration 2857, loss = 0.03783739\n",
      "Iteration 2858, loss = 0.03783591\n",
      "Iteration 2859, loss = 0.03783443\n",
      "Iteration 2860, loss = 0.03783296\n",
      "Iteration 2861, loss = 0.03783148\n",
      "Iteration 2862, loss = 0.03783000\n",
      "Iteration 2863, loss = 0.03782852\n",
      "Iteration 2864, loss = 0.03782704\n",
      "Iteration 2865, loss = 0.03782557\n",
      "Iteration 2866, loss = 0.03782409\n",
      "Iteration 2867, loss = 0.03782261\n",
      "Iteration 2868, loss = 0.03782114\n",
      "Iteration 2869, loss = 0.03781966\n",
      "Iteration 2870, loss = 0.03781818\n",
      "Iteration 2871, loss = 0.03781671\n",
      "Iteration 2872, loss = 0.03781523\n",
      "Iteration 2873, loss = 0.03781376\n",
      "Iteration 2874, loss = 0.03781229\n",
      "Iteration 2875, loss = 0.03781081\n",
      "Iteration 2876, loss = 0.03780934\n",
      "Iteration 2877, loss = 0.03780786\n",
      "Iteration 2878, loss = 0.03780639\n",
      "Iteration 2879, loss = 0.03780492\n",
      "Iteration 2880, loss = 0.03780345\n",
      "Iteration 2881, loss = 0.03780197\n",
      "Iteration 2882, loss = 0.03780050\n",
      "Iteration 2883, loss = 0.03779903\n",
      "Iteration 2884, loss = 0.03779756\n",
      "Iteration 2885, loss = 0.03779609\n",
      "Iteration 2886, loss = 0.03779462\n",
      "Iteration 2887, loss = 0.03779314\n",
      "Iteration 2888, loss = 0.03779167\n",
      "Iteration 2889, loss = 0.03779020\n",
      "Iteration 2890, loss = 0.03778873\n",
      "Iteration 2891, loss = 0.03778726\n",
      "Iteration 2892, loss = 0.03778579\n",
      "Iteration 2893, loss = 0.03778433\n",
      "Iteration 2894, loss = 0.03778286\n",
      "Iteration 2895, loss = 0.03778139\n",
      "Iteration 2896, loss = 0.03777992\n",
      "Iteration 2897, loss = 0.03777845\n",
      "Iteration 2898, loss = 0.03777698\n",
      "Iteration 2899, loss = 0.03777552\n",
      "Iteration 2900, loss = 0.03777405\n",
      "Iteration 2901, loss = 0.03777258\n",
      "Iteration 2902, loss = 0.03777111\n",
      "Iteration 2903, loss = 0.03776965\n",
      "Iteration 2904, loss = 0.03776818\n",
      "Iteration 2905, loss = 0.03776671\n",
      "Iteration 2906, loss = 0.03776525\n",
      "Iteration 2907, loss = 0.03776378\n",
      "Iteration 2908, loss = 0.03776231\n",
      "Iteration 2909, loss = 0.03776085\n",
      "Iteration 2910, loss = 0.03775938\n",
      "Iteration 2911, loss = 0.03775792\n",
      "Iteration 2912, loss = 0.03775645\n",
      "Iteration 2913, loss = 0.03775499\n",
      "Iteration 2914, loss = 0.03775352\n",
      "Iteration 2915, loss = 0.03775206\n",
      "Iteration 2916, loss = 0.03775059\n",
      "Iteration 2917, loss = 0.03774913\n",
      "Iteration 2918, loss = 0.03774766\n",
      "Iteration 2919, loss = 0.03774620\n",
      "Iteration 2920, loss = 0.03774474\n",
      "Iteration 2921, loss = 0.03774327\n",
      "Iteration 2922, loss = 0.03774181\n",
      "Iteration 2923, loss = 0.03774035\n",
      "Iteration 2924, loss = 0.03773888\n",
      "Iteration 2925, loss = 0.03773742\n",
      "Iteration 2926, loss = 0.03773596\n",
      "Iteration 2927, loss = 0.03773449\n",
      "Iteration 2928, loss = 0.03773303\n",
      "Iteration 2929, loss = 0.03773157\n",
      "Iteration 2930, loss = 0.03773011\n",
      "Iteration 2931, loss = 0.03772864\n",
      "Iteration 2932, loss = 0.03772718\n",
      "Iteration 2933, loss = 0.03772572\n",
      "Iteration 2934, loss = 0.03772426\n",
      "Iteration 2935, loss = 0.03772279\n",
      "Iteration 2936, loss = 0.03772133\n",
      "Iteration 2937, loss = 0.03771987\n",
      "Iteration 2938, loss = 0.03771841\n",
      "Iteration 2939, loss = 0.03771695\n",
      "Iteration 2940, loss = 0.03771549\n",
      "Iteration 2941, loss = 0.03771402\n",
      "Iteration 2942, loss = 0.03771256\n",
      "Iteration 2943, loss = 0.03771110\n",
      "Iteration 2944, loss = 0.03770964\n",
      "Iteration 2945, loss = 0.03770818\n",
      "Iteration 2946, loss = 0.03770672\n",
      "Iteration 2947, loss = 0.03770526\n",
      "Iteration 2948, loss = 0.03770379\n",
      "Iteration 2949, loss = 0.03770233\n",
      "Iteration 2950, loss = 0.03770087\n",
      "Iteration 2951, loss = 0.03769941\n",
      "Iteration 2952, loss = 0.03769795\n",
      "Iteration 2953, loss = 0.03769649\n",
      "Iteration 2954, loss = 0.03769503\n",
      "Iteration 2955, loss = 0.03769357\n",
      "Iteration 2956, loss = 0.03769211\n",
      "Iteration 2957, loss = 0.03769065\n",
      "Iteration 2958, loss = 0.03768919\n",
      "Iteration 2959, loss = 0.03768772\n",
      "Iteration 2960, loss = 0.03768626\n",
      "Iteration 2961, loss = 0.03768480\n",
      "Iteration 2962, loss = 0.03768334\n",
      "Iteration 2963, loss = 0.03768188\n",
      "Iteration 2964, loss = 0.03768042\n",
      "Iteration 2965, loss = 0.03767896\n",
      "Iteration 2966, loss = 0.03767750\n",
      "Iteration 2967, loss = 0.03767604\n",
      "Iteration 2968, loss = 0.03767458\n",
      "Iteration 2969, loss = 0.03767312\n",
      "Iteration 2970, loss = 0.03767165\n",
      "Iteration 2971, loss = 0.03767019\n",
      "Iteration 2972, loss = 0.03766873\n",
      "Iteration 2973, loss = 0.03766727\n",
      "Iteration 2974, loss = 0.03766581\n",
      "Iteration 2975, loss = 0.03766435\n",
      "Iteration 2976, loss = 0.03766289\n",
      "Iteration 2977, loss = 0.03766143\n",
      "Iteration 2978, loss = 0.03765996\n",
      "Iteration 2979, loss = 0.03765850\n",
      "Iteration 2980, loss = 0.03765704\n",
      "Iteration 2981, loss = 0.03765558\n",
      "Iteration 2982, loss = 0.03765412\n",
      "Iteration 2983, loss = 0.03765266\n",
      "Iteration 2984, loss = 0.03765119\n",
      "Iteration 2985, loss = 0.03764973\n",
      "Iteration 2986, loss = 0.03764827\n",
      "Iteration 2987, loss = 0.03764681\n",
      "Iteration 2988, loss = 0.03764535\n",
      "Iteration 2989, loss = 0.03764388\n",
      "Iteration 2990, loss = 0.03764242\n",
      "Iteration 2991, loss = 0.03764096\n",
      "Iteration 2992, loss = 0.03763949\n",
      "Iteration 2993, loss = 0.03763803\n",
      "Iteration 2994, loss = 0.03763657\n",
      "Iteration 2995, loss = 0.03763510\n",
      "Iteration 2996, loss = 0.03763364\n",
      "Iteration 2997, loss = 0.03763218\n",
      "Iteration 2998, loss = 0.03763071\n",
      "Iteration 2999, loss = 0.03762925\n",
      "Iteration 3000, loss = 0.03762779\n",
      "Iteration 3001, loss = 0.03762632\n",
      "Iteration 3002, loss = 0.03762486\n",
      "Iteration 3003, loss = 0.03762339\n",
      "Iteration 3004, loss = 0.03762193\n",
      "Iteration 3005, loss = 0.03762046\n",
      "Iteration 3006, loss = 0.03761900\n",
      "Iteration 3007, loss = 0.03761753\n",
      "Iteration 3008, loss = 0.03761607\n",
      "Iteration 3009, loss = 0.03761460\n",
      "Iteration 3010, loss = 0.03761314\n",
      "Iteration 3011, loss = 0.03761167\n",
      "Iteration 3012, loss = 0.03761020\n",
      "Iteration 3013, loss = 0.03760874\n",
      "Iteration 3014, loss = 0.03760727\n",
      "Iteration 3015, loss = 0.03760580\n",
      "Iteration 3016, loss = 0.03760433\n",
      "Iteration 3017, loss = 0.03760287\n",
      "Iteration 3018, loss = 0.03760140\n",
      "Iteration 3019, loss = 0.03759993\n",
      "Iteration 3020, loss = 0.03759846\n",
      "Iteration 3021, loss = 0.03759699\n",
      "Iteration 3022, loss = 0.03759553\n",
      "Iteration 3023, loss = 0.03759406\n",
      "Iteration 3024, loss = 0.03759259\n",
      "Iteration 3025, loss = 0.03759112\n",
      "Iteration 3026, loss = 0.03758965\n",
      "Iteration 3027, loss = 0.03758818\n",
      "Iteration 3028, loss = 0.03758671\n",
      "Iteration 3029, loss = 0.03758524\n",
      "Iteration 3030, loss = 0.03758376\n",
      "Iteration 3031, loss = 0.03758229\n",
      "Iteration 3032, loss = 0.03758082\n",
      "Iteration 3033, loss = 0.03757935\n",
      "Iteration 3034, loss = 0.03757788\n",
      "Iteration 3035, loss = 0.03757640\n",
      "Iteration 3036, loss = 0.03757493\n",
      "Iteration 3037, loss = 0.03757346\n",
      "Iteration 3038, loss = 0.03757198\n",
      "Iteration 3039, loss = 0.03757051\n",
      "Iteration 3040, loss = 0.03756903\n",
      "Iteration 3041, loss = 0.03756756\n",
      "Iteration 3042, loss = 0.03756608\n",
      "Iteration 3043, loss = 0.03756461\n",
      "Iteration 3044, loss = 0.03756313\n",
      "Iteration 3045, loss = 0.03756166\n",
      "Iteration 3046, loss = 0.03756018\n",
      "Iteration 3047, loss = 0.03755870\n",
      "Iteration 3048, loss = 0.03755723\n",
      "Iteration 3049, loss = 0.03755575\n",
      "Iteration 3050, loss = 0.03755427\n",
      "Iteration 3051, loss = 0.03755279\n",
      "Iteration 3052, loss = 0.03755131\n",
      "Iteration 3053, loss = 0.03754983\n",
      "Iteration 3054, loss = 0.03754835\n",
      "Iteration 3055, loss = 0.03754687\n",
      "Iteration 3056, loss = 0.03754539\n",
      "Iteration 3057, loss = 0.03754391\n",
      "Iteration 3058, loss = 0.03754243\n",
      "Iteration 3059, loss = 0.03754095\n",
      "Iteration 3060, loss = 0.03753946\n",
      "Iteration 3061, loss = 0.03753798\n",
      "Iteration 3062, loss = 0.03753650\n",
      "Iteration 3063, loss = 0.03753501\n",
      "Iteration 3064, loss = 0.03753353\n",
      "Iteration 3065, loss = 0.03753204\n",
      "Iteration 3066, loss = 0.03753056\n",
      "Iteration 3067, loss = 0.03752907\n",
      "Iteration 3068, loss = 0.03752758\n",
      "Iteration 3069, loss = 0.03752610\n",
      "Iteration 3070, loss = 0.03752461\n",
      "Iteration 3071, loss = 0.03752312\n",
      "Iteration 3072, loss = 0.03752163\n",
      "Iteration 3073, loss = 0.03752014\n",
      "Iteration 3074, loss = 0.03751865\n",
      "Iteration 3075, loss = 0.03751716\n",
      "Iteration 3076, loss = 0.03751567\n",
      "Iteration 3077, loss = 0.03751418\n",
      "Iteration 3078, loss = 0.03751269\n",
      "Iteration 3079, loss = 0.03751120\n",
      "Iteration 3080, loss = 0.03750971\n",
      "Iteration 3081, loss = 0.03750821\n",
      "Iteration 3082, loss = 0.03750672\n",
      "Iteration 3083, loss = 0.03750522\n",
      "Iteration 3084, loss = 0.03750373\n",
      "Iteration 3085, loss = 0.03750223\n",
      "Iteration 3086, loss = 0.03750074\n",
      "Iteration 3087, loss = 0.03749924\n",
      "Iteration 3088, loss = 0.03749774\n",
      "Iteration 3089, loss = 0.03749624\n",
      "Iteration 3090, loss = 0.03749474\n",
      "Iteration 3091, loss = 0.03749324\n",
      "Iteration 3092, loss = 0.03749174\n",
      "Iteration 3093, loss = 0.03749024\n",
      "Iteration 3094, loss = 0.03748874\n",
      "Iteration 3095, loss = 0.03748724\n",
      "Iteration 3096, loss = 0.03748574\n",
      "Iteration 3097, loss = 0.03748423\n",
      "Iteration 3098, loss = 0.03748273\n",
      "Iteration 3099, loss = 0.03748122\n",
      "Iteration 3100, loss = 0.03747972\n",
      "Iteration 3101, loss = 0.03747821\n",
      "Iteration 3102, loss = 0.03747671\n",
      "Iteration 3103, loss = 0.03747520\n",
      "Iteration 3104, loss = 0.03747369\n",
      "Iteration 3105, loss = 0.03747218\n",
      "Iteration 3106, loss = 0.03747067\n",
      "Iteration 3107, loss = 0.03746916\n",
      "Iteration 3108, loss = 0.03746765\n",
      "Iteration 3109, loss = 0.03746614\n",
      "Iteration 3110, loss = 0.03746462\n",
      "Iteration 3111, loss = 0.03746311\n",
      "Iteration 3112, loss = 0.03746160\n",
      "Iteration 3113, loss = 0.03746008\n",
      "Iteration 3114, loss = 0.03745857\n",
      "Iteration 3115, loss = 0.03745705\n",
      "Iteration 3116, loss = 0.03745553\n",
      "Iteration 3117, loss = 0.03745401\n",
      "Iteration 3118, loss = 0.03745250\n",
      "Iteration 3119, loss = 0.03745098\n",
      "Iteration 3120, loss = 0.03744946\n",
      "Iteration 3121, loss = 0.03744793\n",
      "Iteration 3122, loss = 0.03744641\n",
      "Iteration 3123, loss = 0.03744489\n",
      "Iteration 3124, loss = 0.03744336\n",
      "Iteration 3125, loss = 0.03744184\n",
      "Iteration 3126, loss = 0.03744031\n",
      "Iteration 3127, loss = 0.03743879\n",
      "Iteration 3128, loss = 0.03743726\n",
      "Iteration 3129, loss = 0.03743573\n",
      "Iteration 3130, loss = 0.03743420\n",
      "Iteration 3131, loss = 0.03743267\n",
      "Iteration 3132, loss = 0.03743114\n",
      "Iteration 3133, loss = 0.03742961\n",
      "Iteration 3134, loss = 0.03742808\n",
      "Iteration 3135, loss = 0.03742655\n",
      "Iteration 3136, loss = 0.03742501\n",
      "Iteration 3137, loss = 0.03742348\n",
      "Iteration 3138, loss = 0.03742194\n",
      "Iteration 3139, loss = 0.03742040\n",
      "Iteration 3140, loss = 0.03741887\n",
      "Iteration 3141, loss = 0.03741733\n",
      "Iteration 3142, loss = 0.03741579\n",
      "Iteration 3143, loss = 0.03741425\n",
      "Iteration 3144, loss = 0.03741270\n",
      "Iteration 3145, loss = 0.03741116\n",
      "Iteration 3146, loss = 0.03740962\n",
      "Iteration 3147, loss = 0.03740807\n",
      "Iteration 3148, loss = 0.03740653\n",
      "Iteration 3149, loss = 0.03740498\n",
      "Iteration 3150, loss = 0.03740343\n",
      "Iteration 3151, loss = 0.03740188\n",
      "Iteration 3152, loss = 0.03740033\n",
      "Iteration 3153, loss = 0.03739878\n",
      "Iteration 3154, loss = 0.03739723\n",
      "Iteration 3155, loss = 0.03739568\n",
      "Iteration 3156, loss = 0.03739412\n",
      "Iteration 3157, loss = 0.03739257\n",
      "Iteration 3158, loss = 0.03739101\n",
      "Iteration 3159, loss = 0.03738946\n",
      "Iteration 3160, loss = 0.03738790\n",
      "Iteration 3161, loss = 0.03738634\n",
      "Iteration 3162, loss = 0.03738478\n",
      "Iteration 3163, loss = 0.03738322\n",
      "Iteration 3164, loss = 0.03738165\n",
      "Iteration 3165, loss = 0.03738009\n",
      "Iteration 3166, loss = 0.03737852\n",
      "Iteration 3167, loss = 0.03737696\n",
      "Iteration 3168, loss = 0.03737539\n",
      "Iteration 3169, loss = 0.03737382\n",
      "Iteration 3170, loss = 0.03737225\n",
      "Iteration 3171, loss = 0.03737068\n",
      "Iteration 3172, loss = 0.03736911\n",
      "Iteration 3173, loss = 0.03736754\n",
      "Iteration 3174, loss = 0.03736596\n",
      "Iteration 3175, loss = 0.03736439\n",
      "Iteration 3176, loss = 0.03736281\n",
      "Iteration 3177, loss = 0.03736123\n",
      "Iteration 3178, loss = 0.03735965\n",
      "Iteration 3179, loss = 0.03735807\n",
      "Iteration 3180, loss = 0.03735649\n",
      "Iteration 3181, loss = 0.03735491\n",
      "Iteration 3182, loss = 0.03735333\n",
      "Iteration 3183, loss = 0.03735174\n",
      "Iteration 3184, loss = 0.03735015\n",
      "Iteration 3185, loss = 0.03734857\n",
      "Iteration 3186, loss = 0.03734698\n",
      "Iteration 3187, loss = 0.03734539\n",
      "Iteration 3188, loss = 0.03734379\n",
      "Iteration 3189, loss = 0.03734220\n",
      "Iteration 3190, loss = 0.03734061\n",
      "Iteration 3191, loss = 0.03733901\n",
      "Iteration 3192, loss = 0.03733742\n",
      "Iteration 3193, loss = 0.03733582\n",
      "Iteration 3194, loss = 0.03733422\n",
      "Iteration 3195, loss = 0.03733262\n",
      "Iteration 3196, loss = 0.03733101\n",
      "Iteration 3197, loss = 0.03732941\n",
      "Iteration 3198, loss = 0.03732781\n",
      "Iteration 3199, loss = 0.03732620\n",
      "Iteration 3200, loss = 0.03732459\n",
      "Iteration 3201, loss = 0.03732298\n",
      "Iteration 3202, loss = 0.03732137\n",
      "Iteration 3203, loss = 0.03731976\n",
      "Iteration 3204, loss = 0.03731815\n",
      "Iteration 3205, loss = 0.03731653\n",
      "Iteration 3206, loss = 0.03731492\n",
      "Iteration 3207, loss = 0.03731330\n",
      "Iteration 3208, loss = 0.03731168\n",
      "Iteration 3209, loss = 0.03731006\n",
      "Iteration 3210, loss = 0.03730844\n",
      "Iteration 3211, loss = 0.03730681\n",
      "Iteration 3212, loss = 0.03730519\n",
      "Iteration 3213, loss = 0.03730356\n",
      "Iteration 3214, loss = 0.03730193\n",
      "Iteration 3215, loss = 0.03730030\n",
      "Iteration 3216, loss = 0.03729867\n",
      "Iteration 3217, loss = 0.03729704\n",
      "Iteration 3218, loss = 0.03729540\n",
      "Iteration 3219, loss = 0.03729377\n",
      "Iteration 3220, loss = 0.03729213\n",
      "Iteration 3221, loss = 0.03729049\n",
      "Iteration 3222, loss = 0.03728885\n",
      "Iteration 3223, loss = 0.03728721\n",
      "Iteration 3224, loss = 0.03728557\n",
      "Iteration 3225, loss = 0.03728392\n",
      "Iteration 3226, loss = 0.03728227\n",
      "Iteration 3227, loss = 0.03728062\n",
      "Iteration 3228, loss = 0.03727897\n",
      "Iteration 3229, loss = 0.03727732\n",
      "Iteration 3230, loss = 0.03727567\n",
      "Iteration 3231, loss = 0.03727401\n",
      "Iteration 3232, loss = 0.03727236\n",
      "Iteration 3233, loss = 0.03727070\n",
      "Iteration 3234, loss = 0.03726904\n",
      "Iteration 3235, loss = 0.03726738\n",
      "Iteration 3236, loss = 0.03726571\n",
      "Iteration 3237, loss = 0.03726405\n",
      "Iteration 3238, loss = 0.03726238\n",
      "Iteration 3239, loss = 0.03726071\n",
      "Iteration 3240, loss = 0.03725904\n",
      "Iteration 3241, loss = 0.03725737\n",
      "Iteration 3242, loss = 0.03725570\n",
      "Iteration 3243, loss = 0.03725402\n",
      "Iteration 3244, loss = 0.03725234\n",
      "Iteration 3245, loss = 0.03725066\n",
      "Iteration 3246, loss = 0.03724898\n",
      "Iteration 3247, loss = 0.03724730\n",
      "Iteration 3248, loss = 0.03724561\n",
      "Iteration 3249, loss = 0.03724393\n",
      "Iteration 3250, loss = 0.03724224\n",
      "Iteration 3251, loss = 0.03724055\n",
      "Iteration 3252, loss = 0.03723886\n",
      "Iteration 3253, loss = 0.03723716\n",
      "Iteration 3254, loss = 0.03723547\n",
      "Iteration 3255, loss = 0.03723377\n",
      "Iteration 3256, loss = 0.03723207\n",
      "Iteration 3257, loss = 0.03723037\n",
      "Iteration 3258, loss = 0.03722866\n",
      "Iteration 3259, loss = 0.03722696\n",
      "Iteration 3260, loss = 0.03722525\n",
      "Iteration 3261, loss = 0.03722354\n",
      "Iteration 3262, loss = 0.03722183\n",
      "Iteration 3263, loss = 0.03722012\n",
      "Iteration 3264, loss = 0.03721840\n",
      "Iteration 3265, loss = 0.03721669\n",
      "Iteration 3266, loss = 0.03721497\n",
      "Iteration 3267, loss = 0.03721325\n",
      "Iteration 3268, loss = 0.03721152\n",
      "Iteration 3269, loss = 0.03720980\n",
      "Iteration 3270, loss = 0.03720807\n",
      "Iteration 3271, loss = 0.03720634\n",
      "Iteration 3272, loss = 0.03720461\n",
      "Iteration 3273, loss = 0.03720288\n",
      "Iteration 3274, loss = 0.03720114\n",
      "Iteration 3275, loss = 0.03719940\n",
      "Iteration 3276, loss = 0.03719766\n",
      "Iteration 3277, loss = 0.03719592\n",
      "Iteration 3278, loss = 0.03719418\n",
      "Iteration 3279, loss = 0.03719243\n",
      "Iteration 3280, loss = 0.03719069\n",
      "Iteration 3281, loss = 0.03718893\n",
      "Iteration 3282, loss = 0.03718718\n",
      "Iteration 3283, loss = 0.03718543\n",
      "Iteration 3284, loss = 0.03718367\n",
      "Iteration 3285, loss = 0.03718191\n",
      "Iteration 3286, loss = 0.03718015\n",
      "Iteration 3287, loss = 0.03717839\n",
      "Iteration 3288, loss = 0.03717662\n",
      "Iteration 3289, loss = 0.03717485\n",
      "Iteration 3290, loss = 0.03717308\n",
      "Iteration 3291, loss = 0.03717131\n",
      "Iteration 3292, loss = 0.03716954\n",
      "Iteration 3293, loss = 0.03716776\n",
      "Iteration 3294, loss = 0.03716598\n",
      "Iteration 3295, loss = 0.03716420\n",
      "Iteration 3296, loss = 0.03716242\n",
      "Iteration 3297, loss = 0.03716063\n",
      "Iteration 3298, loss = 0.03715884\n",
      "Iteration 3299, loss = 0.03715705\n",
      "Iteration 3300, loss = 0.03715526\n",
      "Iteration 3301, loss = 0.03715346\n",
      "Iteration 3302, loss = 0.03715167\n",
      "Iteration 3303, loss = 0.03714987\n",
      "Iteration 3304, loss = 0.03714806\n",
      "Iteration 3305, loss = 0.03714626\n",
      "Iteration 3306, loss = 0.03714445\n",
      "Iteration 3307, loss = 0.03714264\n",
      "Iteration 3308, loss = 0.03714083\n",
      "Iteration 3309, loss = 0.03713901\n",
      "Iteration 3310, loss = 0.03713720\n",
      "Iteration 3311, loss = 0.03713538\n",
      "Iteration 3312, loss = 0.03713355\n",
      "Iteration 3313, loss = 0.03713173\n",
      "Iteration 3314, loss = 0.03712990\n",
      "Iteration 3315, loss = 0.03712807\n",
      "Iteration 3316, loss = 0.03712624\n",
      "Iteration 3317, loss = 0.03712441\n",
      "Iteration 3318, loss = 0.03712257\n",
      "Iteration 3319, loss = 0.03712073\n",
      "Iteration 3320, loss = 0.03711889\n",
      "Iteration 3321, loss = 0.03711704\n",
      "Iteration 3322, loss = 0.03711519\n",
      "Iteration 3323, loss = 0.03711334\n",
      "Iteration 3324, loss = 0.03711149\n",
      "Iteration 3325, loss = 0.03710963\n",
      "Iteration 3326, loss = 0.03710778\n",
      "Iteration 3327, loss = 0.03710591\n",
      "Iteration 3328, loss = 0.03710405\n",
      "Iteration 3329, loss = 0.03710218\n",
      "Iteration 3330, loss = 0.03710032\n",
      "Iteration 3331, loss = 0.03709844\n",
      "Iteration 3332, loss = 0.03709657\n",
      "Iteration 3333, loss = 0.03709469\n",
      "Iteration 3334, loss = 0.03709281\n",
      "Iteration 3335, loss = 0.03709093\n",
      "Iteration 3336, loss = 0.03708904\n",
      "Iteration 3337, loss = 0.03708716\n",
      "Iteration 3338, loss = 0.03708527\n",
      "Iteration 3339, loss = 0.03708337\n",
      "Iteration 3340, loss = 0.03708147\n",
      "Iteration 3341, loss = 0.03707958\n",
      "Iteration 3342, loss = 0.03707767\n",
      "Iteration 3343, loss = 0.03707577\n",
      "Iteration 3344, loss = 0.03707386\n",
      "Iteration 3345, loss = 0.03707195\n",
      "Iteration 3346, loss = 0.03707003\n",
      "Iteration 3347, loss = 0.03706812\n",
      "Iteration 3348, loss = 0.03706620\n",
      "Iteration 3349, loss = 0.03706428\n",
      "Iteration 3350, loss = 0.03706235\n",
      "Iteration 3351, loss = 0.03706042\n",
      "Iteration 3352, loss = 0.03705849\n",
      "Iteration 3353, loss = 0.03705655\n",
      "Iteration 3354, loss = 0.03705462\n",
      "Iteration 3355, loss = 0.03705268\n",
      "Iteration 3356, loss = 0.03705073\n",
      "Iteration 3357, loss = 0.03704879\n",
      "Iteration 3358, loss = 0.03704684\n",
      "Iteration 3359, loss = 0.03704488\n",
      "Iteration 3360, loss = 0.03704293\n",
      "Iteration 3361, loss = 0.03704097\n",
      "Iteration 3362, loss = 0.03703901\n",
      "Iteration 3363, loss = 0.03703704\n",
      "Iteration 3364, loss = 0.03703507\n",
      "Iteration 3365, loss = 0.03703310\n",
      "Iteration 3366, loss = 0.03703113\n",
      "Iteration 3367, loss = 0.03702915\n",
      "Iteration 3368, loss = 0.03702717\n",
      "Iteration 3369, loss = 0.03702518\n",
      "Iteration 3370, loss = 0.03702320\n",
      "Iteration 3371, loss = 0.03702121\n",
      "Iteration 3372, loss = 0.03701921\n",
      "Iteration 3373, loss = 0.03701721\n",
      "Iteration 3374, loss = 0.03701521\n",
      "Iteration 3375, loss = 0.03701321\n",
      "Iteration 3376, loss = 0.03701120\n",
      "Iteration 3377, loss = 0.03700919\n",
      "Iteration 3378, loss = 0.03700718\n",
      "Iteration 3379, loss = 0.03700516\n",
      "Iteration 3380, loss = 0.03700314\n",
      "Iteration 3381, loss = 0.03700112\n",
      "Iteration 3382, loss = 0.03699909\n",
      "Iteration 3383, loss = 0.03699706\n",
      "Iteration 3384, loss = 0.03699503\n",
      "Iteration 3385, loss = 0.03699299\n",
      "Iteration 3386, loss = 0.03699095\n",
      "Iteration 3387, loss = 0.03698891\n",
      "Iteration 3388, loss = 0.03698686\n",
      "Iteration 3389, loss = 0.03698481\n",
      "Iteration 3390, loss = 0.03698275\n",
      "Iteration 3391, loss = 0.03698069\n",
      "Iteration 3392, loss = 0.03697863\n",
      "Iteration 3393, loss = 0.03697657\n",
      "Iteration 3394, loss = 0.03697450\n",
      "Iteration 3395, loss = 0.03697243\n",
      "Iteration 3396, loss = 0.03697035\n",
      "Iteration 3397, loss = 0.03696827\n",
      "Iteration 3398, loss = 0.03696619\n",
      "Iteration 3399, loss = 0.03696410\n",
      "Iteration 3400, loss = 0.03696201\n",
      "Iteration 3401, loss = 0.03695992\n",
      "Iteration 3402, loss = 0.03695782\n",
      "Iteration 3403, loss = 0.03695572\n",
      "Iteration 3404, loss = 0.03695361\n",
      "Iteration 3405, loss = 0.03695150\n",
      "Iteration 3406, loss = 0.03694939\n",
      "Iteration 3407, loss = 0.03694727\n",
      "Iteration 3408, loss = 0.03694515\n",
      "Iteration 3409, loss = 0.03694303\n",
      "Iteration 3410, loss = 0.03694090\n",
      "Iteration 3411, loss = 0.03693877\n",
      "Iteration 3412, loss = 0.03693664\n",
      "Iteration 3413, loss = 0.03693450\n",
      "Iteration 3414, loss = 0.03693235\n",
      "Iteration 3415, loss = 0.03693021\n",
      "Iteration 3416, loss = 0.03692806\n",
      "Iteration 3417, loss = 0.03692590\n",
      "Iteration 3418, loss = 0.03692374\n",
      "Iteration 3419, loss = 0.03692158\n",
      "Iteration 3420, loss = 0.03691942\n",
      "Iteration 3421, loss = 0.03691725\n",
      "Iteration 3422, loss = 0.03691507\n",
      "Iteration 3423, loss = 0.03691289\n",
      "Iteration 3424, loss = 0.03691071\n",
      "Iteration 3425, loss = 0.03690852\n",
      "Iteration 3426, loss = 0.03690633\n",
      "Iteration 3427, loss = 0.03690414\n",
      "Iteration 3428, loss = 0.03690194\n",
      "Iteration 3429, loss = 0.03689974\n",
      "Iteration 3430, loss = 0.03689753\n",
      "Iteration 3431, loss = 0.03689532\n",
      "Iteration 3432, loss = 0.03689311\n",
      "Iteration 3433, loss = 0.03689089\n",
      "Iteration 3434, loss = 0.03688867\n",
      "Iteration 3435, loss = 0.03688644\n",
      "Iteration 3436, loss = 0.03688421\n",
      "Iteration 3437, loss = 0.03688197\n",
      "Iteration 3438, loss = 0.03687973\n",
      "Iteration 3439, loss = 0.03687749\n",
      "Iteration 3440, loss = 0.03687524\n",
      "Iteration 3441, loss = 0.03687299\n",
      "Iteration 3442, loss = 0.03687073\n",
      "Iteration 3443, loss = 0.03686847\n",
      "Iteration 3444, loss = 0.03686620\n",
      "Iteration 3445, loss = 0.03686393\n",
      "Iteration 3446, loss = 0.03686166\n",
      "Iteration 3447, loss = 0.03685938\n",
      "Iteration 3448, loss = 0.03685710\n",
      "Iteration 3449, loss = 0.03685481\n",
      "Iteration 3450, loss = 0.03685252\n",
      "Iteration 3451, loss = 0.03685022\n",
      "Iteration 3452, loss = 0.03684792\n",
      "Iteration 3453, loss = 0.03684562\n",
      "Iteration 3454, loss = 0.03684331\n",
      "Iteration 3455, loss = 0.03684099\n",
      "Iteration 3456, loss = 0.03683867\n",
      "Iteration 3457, loss = 0.03683635\n",
      "Iteration 3458, loss = 0.03683402\n",
      "Iteration 3459, loss = 0.03683169\n",
      "Iteration 3460, loss = 0.03682935\n",
      "Iteration 3461, loss = 0.03682701\n",
      "Iteration 3462, loss = 0.03682467\n",
      "Iteration 3463, loss = 0.03682231\n",
      "Iteration 3464, loss = 0.03681996\n",
      "Iteration 3465, loss = 0.03681760\n",
      "Iteration 3466, loss = 0.03681523\n",
      "Iteration 3467, loss = 0.03681286\n",
      "Iteration 3468, loss = 0.03681049\n",
      "Iteration 3469, loss = 0.03680811\n",
      "Iteration 3470, loss = 0.03680573\n",
      "Iteration 3471, loss = 0.03680334\n",
      "Iteration 3472, loss = 0.03680095\n",
      "Iteration 3473, loss = 0.03679855\n",
      "Iteration 3474, loss = 0.03679615\n",
      "Iteration 3475, loss = 0.03679374\n",
      "Iteration 3476, loss = 0.03679133\n",
      "Iteration 3477, loss = 0.03678891\n",
      "Iteration 3478, loss = 0.03678649\n",
      "Iteration 3479, loss = 0.03678406\n",
      "Iteration 3480, loss = 0.03678163\n",
      "Iteration 3481, loss = 0.03677919\n",
      "Iteration 3482, loss = 0.03677675\n",
      "Iteration 3483, loss = 0.03677430\n",
      "Iteration 3484, loss = 0.03677185\n",
      "Iteration 3485, loss = 0.03676939\n",
      "Iteration 3486, loss = 0.03676693\n",
      "Iteration 3487, loss = 0.03676446\n",
      "Iteration 3488, loss = 0.03676199\n",
      "Iteration 3489, loss = 0.03675951\n",
      "Iteration 3490, loss = 0.03675703\n",
      "Iteration 3491, loss = 0.03675454\n",
      "Iteration 3492, loss = 0.03675205\n",
      "Iteration 3493, loss = 0.03674955\n",
      "Iteration 3494, loss = 0.03674705\n",
      "Iteration 3495, loss = 0.03674454\n",
      "Iteration 3496, loss = 0.03674203\n",
      "Iteration 3497, loss = 0.03673951\n",
      "Iteration 3498, loss = 0.03673698\n",
      "Iteration 3499, loss = 0.03673445\n",
      "Iteration 3500, loss = 0.03673192\n",
      "Iteration 3501, loss = 0.03672938\n",
      "Iteration 3502, loss = 0.03672683\n",
      "Iteration 3503, loss = 0.03672428\n",
      "Iteration 3504, loss = 0.03672173\n",
      "Iteration 3505, loss = 0.03671917\n",
      "Iteration 3506, loss = 0.03671660\n",
      "Iteration 3507, loss = 0.03671403\n",
      "Iteration 3508, loss = 0.03671145\n",
      "Iteration 3509, loss = 0.03670887\n",
      "Iteration 3510, loss = 0.03670628\n",
      "Iteration 3511, loss = 0.03670368\n",
      "Iteration 3512, loss = 0.03670108\n",
      "Iteration 3513, loss = 0.03669848\n",
      "Iteration 3514, loss = 0.03669587\n",
      "Iteration 3515, loss = 0.03669325\n",
      "Iteration 3516, loss = 0.03669063\n",
      "Iteration 3517, loss = 0.03668800\n",
      "Iteration 3518, loss = 0.03668537\n",
      "Iteration 3519, loss = 0.03668273\n",
      "Iteration 3520, loss = 0.03668009\n",
      "Iteration 3521, loss = 0.03667744\n",
      "Iteration 3522, loss = 0.03667478\n",
      "Iteration 3523, loss = 0.03667212\n",
      "Iteration 3524, loss = 0.03666945\n",
      "Iteration 3525, loss = 0.03666678\n",
      "Iteration 3526, loss = 0.03666410\n",
      "Iteration 3527, loss = 0.03666142\n",
      "Iteration 3528, loss = 0.03665873\n",
      "Iteration 3529, loss = 0.03665603\n",
      "Iteration 3530, loss = 0.03665333\n",
      "Iteration 3531, loss = 0.03665062\n",
      "Iteration 3532, loss = 0.03664791\n",
      "Iteration 3533, loss = 0.03664519\n",
      "Iteration 3534, loss = 0.03664246\n",
      "Iteration 3535, loss = 0.03663973\n",
      "Iteration 3536, loss = 0.03663699\n",
      "Iteration 3537, loss = 0.03663425\n",
      "Iteration 3538, loss = 0.03663150\n",
      "Iteration 3539, loss = 0.03662874\n",
      "Iteration 3540, loss = 0.03662598\n",
      "Iteration 3541, loss = 0.03662321\n",
      "Iteration 3542, loss = 0.03662044\n",
      "Iteration 3543, loss = 0.03661766\n",
      "Iteration 3544, loss = 0.03661488\n",
      "Iteration 3545, loss = 0.03661208\n",
      "Iteration 3546, loss = 0.03660928\n",
      "Iteration 3547, loss = 0.03660648\n",
      "Iteration 3548, loss = 0.03660367\n",
      "Iteration 3549, loss = 0.03660085\n",
      "Iteration 3550, loss = 0.03659803\n",
      "Iteration 3551, loss = 0.03659520\n",
      "Iteration 3552, loss = 0.03659236\n",
      "Iteration 3553, loss = 0.03658952\n",
      "Iteration 3554, loss = 0.03658667\n",
      "Iteration 3555, loss = 0.03658382\n",
      "Iteration 3556, loss = 0.03658096\n",
      "Iteration 3557, loss = 0.03657809\n",
      "Iteration 3558, loss = 0.03657521\n",
      "Iteration 3559, loss = 0.03657233\n",
      "Iteration 3560, loss = 0.03656945\n",
      "Iteration 3561, loss = 0.03656655\n",
      "Iteration 3562, loss = 0.03656365\n",
      "Iteration 3563, loss = 0.03656075\n",
      "Iteration 3564, loss = 0.03655783\n",
      "Iteration 3565, loss = 0.03655492\n",
      "Iteration 3566, loss = 0.03655199\n",
      "Iteration 3567, loss = 0.03654906\n",
      "Iteration 3568, loss = 0.03654612\n",
      "Iteration 3569, loss = 0.03654317\n",
      "Iteration 3570, loss = 0.03654022\n",
      "Iteration 3571, loss = 0.03653726\n",
      "Iteration 3572, loss = 0.03653429\n",
      "Iteration 3573, loss = 0.03653132\n",
      "Iteration 3574, loss = 0.03652834\n",
      "Iteration 3575, loss = 0.03652535\n",
      "Iteration 3576, loss = 0.03652236\n",
      "Iteration 3577, loss = 0.03651936\n",
      "Iteration 3578, loss = 0.03651636\n",
      "Iteration 3579, loss = 0.03651334\n",
      "Iteration 3580, loss = 0.03651032\n",
      "Iteration 3581, loss = 0.03650729\n",
      "Iteration 3582, loss = 0.03650426\n",
      "Iteration 3583, loss = 0.03650122\n",
      "Iteration 3584, loss = 0.03649817\n",
      "Iteration 3585, loss = 0.03649512\n",
      "Iteration 3586, loss = 0.03649205\n",
      "Iteration 3587, loss = 0.03648898\n",
      "Iteration 3588, loss = 0.03648591\n",
      "Iteration 3589, loss = 0.03648283\n",
      "Iteration 3590, loss = 0.03647974\n",
      "Iteration 3591, loss = 0.03647664\n",
      "Iteration 3592, loss = 0.03647353\n",
      "Iteration 3593, loss = 0.03647042\n",
      "Iteration 3594, loss = 0.03646730\n",
      "Iteration 3595, loss = 0.03646418\n",
      "Iteration 3596, loss = 0.03646104\n",
      "Iteration 3597, loss = 0.03645790\n",
      "Iteration 3598, loss = 0.03645476\n",
      "Iteration 3599, loss = 0.03645160\n",
      "Iteration 3600, loss = 0.03644844\n",
      "Iteration 3601, loss = 0.03644527\n",
      "Iteration 3602, loss = 0.03644209\n",
      "Iteration 3603, loss = 0.03643891\n",
      "Iteration 3604, loss = 0.03643572\n",
      "Iteration 3605, loss = 0.03643252\n",
      "Iteration 3606, loss = 0.03642931\n",
      "Iteration 3607, loss = 0.03642610\n",
      "Iteration 3608, loss = 0.03642288\n",
      "Iteration 3609, loss = 0.03641965\n",
      "Iteration 3610, loss = 0.03641641\n",
      "Iteration 3611, loss = 0.03641317\n",
      "Iteration 3612, loss = 0.03640992\n",
      "Iteration 3613, loss = 0.03640666\n",
      "Iteration 3614, loss = 0.03640339\n",
      "Iteration 3615, loss = 0.03640012\n",
      "Iteration 3616, loss = 0.03639684\n",
      "Iteration 3617, loss = 0.03639355\n",
      "Iteration 3618, loss = 0.03639025\n",
      "Iteration 3619, loss = 0.03638694\n",
      "Iteration 3620, loss = 0.03638363\n",
      "Iteration 3621, loss = 0.03638031\n",
      "Iteration 3622, loss = 0.03637698\n",
      "Iteration 3623, loss = 0.03637365\n",
      "Iteration 3624, loss = 0.03637030\n",
      "Iteration 3625, loss = 0.03636695\n",
      "Iteration 3626, loss = 0.03636359\n",
      "Iteration 3627, loss = 0.03636023\n",
      "Iteration 3628, loss = 0.03635685\n",
      "Iteration 3629, loss = 0.03635347\n",
      "Iteration 3630, loss = 0.03635008\n",
      "Iteration 3631, loss = 0.03634668\n",
      "Iteration 3632, loss = 0.03634327\n",
      "Iteration 3633, loss = 0.03633986\n",
      "Iteration 3634, loss = 0.03633643\n",
      "Iteration 3635, loss = 0.03633300\n",
      "Iteration 3636, loss = 0.03632956\n",
      "Iteration 3637, loss = 0.03632612\n",
      "Iteration 3638, loss = 0.03632266\n",
      "Iteration 3639, loss = 0.03631920\n",
      "Iteration 3640, loss = 0.03631573\n",
      "Iteration 3641, loss = 0.03631225\n",
      "Iteration 3642, loss = 0.03630876\n",
      "Iteration 3643, loss = 0.03630526\n",
      "Iteration 3644, loss = 0.03630176\n",
      "Iteration 3645, loss = 0.03629824\n",
      "Iteration 3646, loss = 0.03629472\n",
      "Iteration 3647, loss = 0.03629119\n",
      "Iteration 3648, loss = 0.03628766\n",
      "Iteration 3649, loss = 0.03628411\n",
      "Iteration 3650, loss = 0.03628056\n",
      "Iteration 3651, loss = 0.03627699\n",
      "Iteration 3652, loss = 0.03627342\n",
      "Iteration 3653, loss = 0.03626984\n",
      "Iteration 3654, loss = 0.03626625\n",
      "Iteration 3655, loss = 0.03626266\n",
      "Iteration 3656, loss = 0.03625905\n",
      "Iteration 3657, loss = 0.03625544\n",
      "Iteration 3658, loss = 0.03625182\n",
      "Iteration 3659, loss = 0.03624818\n",
      "Iteration 3660, loss = 0.03624454\n",
      "Iteration 3661, loss = 0.03624090\n",
      "Iteration 3662, loss = 0.03623724\n",
      "Iteration 3663, loss = 0.03623357\n",
      "Iteration 3664, loss = 0.03622990\n",
      "Iteration 3665, loss = 0.03622622\n",
      "Iteration 3666, loss = 0.03622253\n",
      "Iteration 3667, loss = 0.03621882\n",
      "Iteration 3668, loss = 0.03621512\n",
      "Iteration 3669, loss = 0.03621140\n",
      "Iteration 3670, loss = 0.03620767\n",
      "Iteration 3671, loss = 0.03620394\n",
      "Iteration 3672, loss = 0.03620019\n",
      "Iteration 3673, loss = 0.03619644\n",
      "Iteration 3674, loss = 0.03619267\n",
      "Iteration 3675, loss = 0.03618890\n",
      "Iteration 3676, loss = 0.03618512\n",
      "Iteration 3677, loss = 0.03618133\n",
      "Iteration 3678, loss = 0.03617754\n",
      "Iteration 3679, loss = 0.03617373\n",
      "Iteration 3680, loss = 0.03616991\n",
      "Iteration 3681, loss = 0.03616609\n",
      "Iteration 3682, loss = 0.03616225\n",
      "Iteration 3683, loss = 0.03615841\n",
      "Iteration 3684, loss = 0.03615456\n",
      "Iteration 3685, loss = 0.03615069\n",
      "Iteration 3686, loss = 0.03614682\n",
      "Iteration 3687, loss = 0.03614294\n",
      "Iteration 3688, loss = 0.03613905\n",
      "Iteration 3689, loss = 0.03613515\n",
      "Iteration 3690, loss = 0.03613125\n",
      "Iteration 3691, loss = 0.03612733\n",
      "Iteration 3692, loss = 0.03612340\n",
      "Iteration 3693, loss = 0.03611947\n",
      "Iteration 3694, loss = 0.03611552\n",
      "Iteration 3695, loss = 0.03611157\n",
      "Iteration 3696, loss = 0.03610760\n",
      "Iteration 3697, loss = 0.03610363\n",
      "Iteration 3698, loss = 0.03609964\n",
      "Iteration 3699, loss = 0.03609565\n",
      "Iteration 3700, loss = 0.03609165\n",
      "Iteration 3701, loss = 0.03608764\n",
      "Iteration 3702, loss = 0.03608362\n",
      "Iteration 3703, loss = 0.03607959\n",
      "Iteration 3704, loss = 0.03607555\n",
      "Iteration 3705, loss = 0.03607150\n",
      "Iteration 3706, loss = 0.03606744\n",
      "Iteration 3707, loss = 0.03606337\n",
      "Iteration 3708, loss = 0.03605929\n",
      "Iteration 3709, loss = 0.03605520\n",
      "Iteration 3710, loss = 0.03605110\n",
      "Iteration 3711, loss = 0.03604699\n",
      "Iteration 3712, loss = 0.03604288\n",
      "Iteration 3713, loss = 0.03603875\n",
      "Iteration 3714, loss = 0.03603461\n",
      "Iteration 3715, loss = 0.03603046\n",
      "Iteration 3716, loss = 0.03602631\n",
      "Iteration 3717, loss = 0.03602214\n",
      "Iteration 3718, loss = 0.03601796\n",
      "Iteration 3719, loss = 0.03601378\n",
      "Iteration 3720, loss = 0.03600958\n",
      "Iteration 3721, loss = 0.03600537\n",
      "Iteration 3722, loss = 0.03600116\n",
      "Iteration 3723, loss = 0.03599693\n",
      "Iteration 3724, loss = 0.03599269\n",
      "Iteration 3725, loss = 0.03598845\n",
      "Iteration 3726, loss = 0.03598419\n",
      "Iteration 3727, loss = 0.03597992\n",
      "Iteration 3728, loss = 0.03597564\n",
      "Iteration 3729, loss = 0.03597136\n",
      "Iteration 3730, loss = 0.03596706\n",
      "Iteration 3731, loss = 0.03596275\n",
      "Iteration 3732, loss = 0.03595843\n",
      "Iteration 3733, loss = 0.03595411\n",
      "Iteration 3734, loss = 0.03594977\n",
      "Iteration 3735, loss = 0.03594542\n",
      "Iteration 3736, loss = 0.03594106\n",
      "Iteration 3737, loss = 0.03593669\n",
      "Iteration 3738, loss = 0.03593231\n",
      "Iteration 3739, loss = 0.03592792\n",
      "Iteration 3740, loss = 0.03592352\n",
      "Iteration 3741, loss = 0.03591911\n",
      "Iteration 3742, loss = 0.03591469\n",
      "Iteration 3743, loss = 0.03591026\n",
      "Iteration 3744, loss = 0.03590582\n",
      "Iteration 3745, loss = 0.03590137\n",
      "Iteration 3746, loss = 0.03589690\n",
      "Iteration 3747, loss = 0.03589243\n",
      "Iteration 3748, loss = 0.03588795\n",
      "Iteration 3749, loss = 0.03588345\n",
      "Iteration 3750, loss = 0.03587895\n",
      "Iteration 3751, loss = 0.03587443\n",
      "Iteration 3752, loss = 0.03586991\n",
      "Iteration 3753, loss = 0.03586537\n",
      "Iteration 3754, loss = 0.03586082\n",
      "Iteration 3755, loss = 0.03585626\n",
      "Iteration 3756, loss = 0.03585169\n",
      "Iteration 3757, loss = 0.03584711\n",
      "Iteration 3758, loss = 0.03584252\n",
      "Iteration 3759, loss = 0.03583792\n",
      "Iteration 3760, loss = 0.03583331\n",
      "Iteration 3761, loss = 0.03582869\n",
      "Iteration 3762, loss = 0.03582405\n",
      "Iteration 3763, loss = 0.03581941\n",
      "Iteration 3764, loss = 0.03581475\n",
      "Iteration 3765, loss = 0.03581009\n",
      "Iteration 3766, loss = 0.03580541\n",
      "Iteration 3767, loss = 0.03580072\n",
      "Iteration 3768, loss = 0.03579602\n",
      "Iteration 3769, loss = 0.03579131\n",
      "Iteration 3770, loss = 0.03578659\n",
      "Iteration 3771, loss = 0.03578186\n",
      "Iteration 3772, loss = 0.03577712\n",
      "Iteration 3773, loss = 0.03577236\n",
      "Iteration 3774, loss = 0.03576760\n",
      "Iteration 3775, loss = 0.03576282\n",
      "Iteration 3776, loss = 0.03575803\n",
      "Iteration 3777, loss = 0.03575324\n",
      "Iteration 3778, loss = 0.03574843\n",
      "Iteration 3779, loss = 0.03574360\n",
      "Iteration 3780, loss = 0.03573877\n",
      "Iteration 3781, loss = 0.03573393\n",
      "Iteration 3782, loss = 0.03572907\n",
      "Iteration 3783, loss = 0.03572421\n",
      "Iteration 3784, loss = 0.03571933\n",
      "Iteration 3785, loss = 0.03571444\n",
      "Iteration 3786, loss = 0.03570954\n",
      "Iteration 3787, loss = 0.03570463\n",
      "Iteration 3788, loss = 0.03569971\n",
      "Iteration 3789, loss = 0.03569478\n",
      "Iteration 3790, loss = 0.03568983\n",
      "Iteration 3791, loss = 0.03568487\n",
      "Iteration 3792, loss = 0.03567991\n",
      "Iteration 3793, loss = 0.03567493\n",
      "Iteration 3794, loss = 0.03566993\n",
      "Iteration 3795, loss = 0.03566493\n",
      "Iteration 3796, loss = 0.03565992\n",
      "Iteration 3797, loss = 0.03565489\n",
      "Iteration 3798, loss = 0.03564985\n",
      "Iteration 3799, loss = 0.03564481\n",
      "Iteration 3800, loss = 0.03563975\n",
      "Iteration 3801, loss = 0.03563467\n",
      "Iteration 3802, loss = 0.03562959\n",
      "Iteration 3803, loss = 0.03562449\n",
      "Iteration 3804, loss = 0.03561939\n",
      "Iteration 3805, loss = 0.03561427\n",
      "Iteration 3806, loss = 0.03560914\n",
      "Iteration 3807, loss = 0.03560400\n",
      "Iteration 3808, loss = 0.03559884\n",
      "Iteration 3809, loss = 0.03559368\n",
      "Iteration 3810, loss = 0.03558850\n",
      "Iteration 3811, loss = 0.03558331\n",
      "Iteration 3812, loss = 0.03557811\n",
      "Iteration 3813, loss = 0.03557290\n",
      "Iteration 3814, loss = 0.03556767\n",
      "Iteration 3815, loss = 0.03556244\n",
      "Iteration 3816, loss = 0.03555719\n",
      "Iteration 3817, loss = 0.03555193\n",
      "Iteration 3818, loss = 0.03554666\n",
      "Iteration 3819, loss = 0.03554137\n",
      "Iteration 3820, loss = 0.03553608\n",
      "Iteration 3821, loss = 0.03553077\n",
      "Iteration 3822, loss = 0.03552545\n",
      "Iteration 3823, loss = 0.03552012\n",
      "Iteration 3824, loss = 0.03551477\n",
      "Iteration 3825, loss = 0.03550942\n",
      "Iteration 3826, loss = 0.03550405\n",
      "Iteration 3827, loss = 0.03549867\n",
      "Iteration 3828, loss = 0.03549327\n",
      "Iteration 3829, loss = 0.03548787\n",
      "Iteration 3830, loss = 0.03548245\n",
      "Iteration 3831, loss = 0.03547702\n",
      "Iteration 3832, loss = 0.03547158\n",
      "Iteration 3833, loss = 0.03546613\n",
      "Iteration 3834, loss = 0.03546067\n",
      "Iteration 3835, loss = 0.03545519\n",
      "Iteration 3836, loss = 0.03544970\n",
      "Iteration 3837, loss = 0.03544420\n",
      "Iteration 3838, loss = 0.03543868\n",
      "Iteration 3839, loss = 0.03543316\n",
      "Iteration 3840, loss = 0.03542762\n",
      "Iteration 3841, loss = 0.03542207\n",
      "Iteration 3842, loss = 0.03541650\n",
      "Iteration 3843, loss = 0.03541093\n",
      "Iteration 3844, loss = 0.03540534\n",
      "Iteration 3845, loss = 0.03539974\n",
      "Iteration 3846, loss = 0.03539413\n",
      "Iteration 3847, loss = 0.03538850\n",
      "Iteration 3848, loss = 0.03538286\n",
      "Iteration 3849, loss = 0.03537721\n",
      "Iteration 3850, loss = 0.03537155\n",
      "Iteration 3851, loss = 0.03536588\n",
      "Iteration 3852, loss = 0.03536019\n",
      "Iteration 3853, loss = 0.03535449\n",
      "Iteration 3854, loss = 0.03534878\n",
      "Iteration 3855, loss = 0.03534305\n",
      "Iteration 3856, loss = 0.03533731\n",
      "Iteration 3857, loss = 0.03533156\n",
      "Iteration 3858, loss = 0.03532580\n",
      "Iteration 3859, loss = 0.03532002\n",
      "Iteration 3860, loss = 0.03531424\n",
      "Iteration 3861, loss = 0.03530844\n",
      "Iteration 3862, loss = 0.03530262\n",
      "Iteration 3863, loss = 0.03529680\n",
      "Iteration 3864, loss = 0.03529096\n",
      "Iteration 3865, loss = 0.03528511\n",
      "Iteration 3866, loss = 0.03527924\n",
      "Iteration 3867, loss = 0.03527337\n",
      "Iteration 3868, loss = 0.03526748\n",
      "Iteration 3869, loss = 0.03526158\n",
      "Iteration 3870, loss = 0.03525566\n",
      "Iteration 3871, loss = 0.03524974\n",
      "Iteration 3872, loss = 0.03524380\n",
      "Iteration 3873, loss = 0.03523784\n",
      "Iteration 3874, loss = 0.03523188\n",
      "Iteration 3875, loss = 0.03522590\n",
      "Iteration 3876, loss = 0.03521991\n",
      "Iteration 3877, loss = 0.03521390\n",
      "Iteration 3878, loss = 0.03520789\n",
      "Iteration 3879, loss = 0.03520186\n",
      "Iteration 3880, loss = 0.03519581\n",
      "Iteration 3881, loss = 0.03518976\n",
      "Iteration 3882, loss = 0.03518369\n",
      "Iteration 3883, loss = 0.03517761\n",
      "Iteration 3884, loss = 0.03517151\n",
      "Iteration 3885, loss = 0.03516541\n",
      "Iteration 3886, loss = 0.03515929\n",
      "Iteration 3887, loss = 0.03515315\n",
      "Iteration 3888, loss = 0.03514701\n",
      "Iteration 3889, loss = 0.03514085\n",
      "Iteration 3890, loss = 0.03513468\n",
      "Iteration 3891, loss = 0.03512849\n",
      "Iteration 3892, loss = 0.03512229\n",
      "Iteration 3893, loss = 0.03511608\n",
      "Iteration 3894, loss = 0.03510986\n",
      "Iteration 3895, loss = 0.03510362\n",
      "Iteration 3896, loss = 0.03509737\n",
      "Iteration 3897, loss = 0.03509111\n",
      "Iteration 3898, loss = 0.03508483\n",
      "Iteration 3899, loss = 0.03507854\n",
      "Iteration 3900, loss = 0.03507224\n",
      "Iteration 3901, loss = 0.03506593\n",
      "Iteration 3902, loss = 0.03505960\n",
      "Iteration 3903, loss = 0.03505326\n",
      "Iteration 3904, loss = 0.03504690\n",
      "Iteration 3905, loss = 0.03504053\n",
      "Iteration 3906, loss = 0.03503415\n",
      "Iteration 3907, loss = 0.03502776\n",
      "Iteration 3908, loss = 0.03502135\n",
      "Iteration 3909, loss = 0.03501493\n",
      "Iteration 3910, loss = 0.03500850\n",
      "Iteration 3911, loss = 0.03500205\n",
      "Iteration 3912, loss = 0.03499559\n",
      "Iteration 3913, loss = 0.03498912\n",
      "Iteration 3914, loss = 0.03498263\n",
      "Iteration 3915, loss = 0.03497613\n",
      "Iteration 3916, loss = 0.03496962\n",
      "Iteration 3917, loss = 0.03496309\n",
      "Iteration 3918, loss = 0.03495655\n",
      "Iteration 3919, loss = 0.03495000\n",
      "Iteration 3920, loss = 0.03494343\n",
      "Iteration 3921, loss = 0.03493685\n",
      "Iteration 3922, loss = 0.03493026\n",
      "Iteration 3923, loss = 0.03492365\n",
      "Iteration 3924, loss = 0.03491703\n",
      "Iteration 3925, loss = 0.03491040\n",
      "Iteration 3926, loss = 0.03490376\n",
      "Iteration 3927, loss = 0.03489710\n",
      "Iteration 3928, loss = 0.03489042\n",
      "Iteration 3929, loss = 0.03488374\n",
      "Iteration 3930, loss = 0.03487704\n",
      "Iteration 3931, loss = 0.03487033\n",
      "Iteration 3932, loss = 0.03486360\n",
      "Iteration 3933, loss = 0.03485686\n",
      "Iteration 3934, loss = 0.03485011\n",
      "Iteration 3935, loss = 0.03484334\n",
      "Iteration 3936, loss = 0.03483656\n",
      "Iteration 3937, loss = 0.03482977\n",
      "Iteration 3938, loss = 0.03482296\n",
      "Iteration 3939, loss = 0.03481615\n",
      "Iteration 3940, loss = 0.03480931\n",
      "Iteration 3941, loss = 0.03480247\n",
      "Iteration 3942, loss = 0.03479561\n",
      "Iteration 3943, loss = 0.03478873\n",
      "Iteration 3944, loss = 0.03478185\n",
      "Iteration 3945, loss = 0.03477495\n",
      "Iteration 3946, loss = 0.03476803\n",
      "Iteration 3947, loss = 0.03476111\n",
      "Iteration 3948, loss = 0.03475416\n",
      "Iteration 3949, loss = 0.03474721\n",
      "Iteration 3950, loss = 0.03474024\n",
      "Iteration 3951, loss = 0.03473326\n",
      "Iteration 3952, loss = 0.03472627\n",
      "Iteration 3953, loss = 0.03471926\n",
      "Iteration 3954, loss = 0.03471224\n",
      "Iteration 3955, loss = 0.03470520\n",
      "Iteration 3956, loss = 0.03469816\n",
      "Iteration 3957, loss = 0.03469110\n",
      "Iteration 3958, loss = 0.03468402\n",
      "Iteration 3959, loss = 0.03467693\n",
      "Iteration 3960, loss = 0.03466983\n",
      "Iteration 3961, loss = 0.03466271\n",
      "Iteration 3962, loss = 0.03465558\n",
      "Iteration 3963, loss = 0.03464844\n",
      "Iteration 3964, loss = 0.03464129\n",
      "Iteration 3965, loss = 0.03463412\n",
      "Iteration 3966, loss = 0.03462693\n",
      "Iteration 3967, loss = 0.03461974\n",
      "Iteration 3968, loss = 0.03461253\n",
      "Iteration 3969, loss = 0.03460530\n",
      "Iteration 3970, loss = 0.03459807\n",
      "Iteration 3971, loss = 0.03459082\n",
      "Iteration 3972, loss = 0.03458355\n",
      "Iteration 3973, loss = 0.03457627\n",
      "Iteration 3974, loss = 0.03456898\n",
      "Iteration 3975, loss = 0.03456168\n",
      "Iteration 3976, loss = 0.03455436\n",
      "Iteration 3977, loss = 0.03454703\n",
      "Iteration 3978, loss = 0.03453968\n",
      "Iteration 3979, loss = 0.03453232\n",
      "Iteration 3980, loss = 0.03452495\n",
      "Iteration 3981, loss = 0.03451757\n",
      "Iteration 3982, loss = 0.03451017\n",
      "Iteration 3983, loss = 0.03450275\n",
      "Iteration 3984, loss = 0.03449533\n",
      "Iteration 3985, loss = 0.03448789\n",
      "Iteration 3986, loss = 0.03448043\n",
      "Iteration 3987, loss = 0.03447297\n",
      "Iteration 3988, loss = 0.03446549\n",
      "Iteration 3989, loss = 0.03445799\n",
      "Iteration 3990, loss = 0.03445048\n",
      "Iteration 3991, loss = 0.03444296\n",
      "Iteration 3992, loss = 0.03443543\n",
      "Iteration 3993, loss = 0.03442788\n",
      "Iteration 3994, loss = 0.03442032\n",
      "Iteration 3995, loss = 0.03441274\n",
      "Iteration 3996, loss = 0.03440516\n",
      "Iteration 3997, loss = 0.03439755\n",
      "Iteration 3998, loss = 0.03438994\n",
      "Iteration 3999, loss = 0.03438231\n",
      "Iteration 4000, loss = 0.03437467\n",
      "Iteration 4001, loss = 0.03436701\n",
      "Iteration 4002, loss = 0.03435934\n",
      "Iteration 4003, loss = 0.03435166\n",
      "Iteration 4004, loss = 0.03434396\n",
      "Iteration 4005, loss = 0.03433625\n",
      "Iteration 4006, loss = 0.03432853\n",
      "Iteration 4007, loss = 0.03432079\n",
      "Iteration 4008, loss = 0.03431304\n",
      "Iteration 4009, loss = 0.03430528\n",
      "Iteration 4010, loss = 0.03429750\n",
      "Iteration 4011, loss = 0.03428971\n",
      "Iteration 4012, loss = 0.03428190\n",
      "Iteration 4013, loss = 0.03427409\n",
      "Iteration 4014, loss = 0.03426626\n",
      "Iteration 4015, loss = 0.03425841\n",
      "Iteration 4016, loss = 0.03425055\n",
      "Iteration 4017, loss = 0.03424268\n",
      "Iteration 4018, loss = 0.03423480\n",
      "Iteration 4019, loss = 0.03422690\n",
      "Iteration 4020, loss = 0.03421899\n",
      "Iteration 4021, loss = 0.03421106\n",
      "Iteration 4022, loss = 0.03420312\n",
      "Iteration 4023, loss = 0.03419517\n",
      "Iteration 4024, loss = 0.03418721\n",
      "Iteration 4025, loss = 0.03417923\n",
      "Iteration 4026, loss = 0.03417123\n",
      "Iteration 4027, loss = 0.03416323\n",
      "Iteration 4028, loss = 0.03415521\n",
      "Iteration 4029, loss = 0.03414718\n",
      "Iteration 4030, loss = 0.03413913\n",
      "Iteration 4031, loss = 0.03413107\n",
      "Iteration 4032, loss = 0.03412300\n",
      "Iteration 4033, loss = 0.03411492\n",
      "Iteration 4034, loss = 0.03410682\n",
      "Iteration 4035, loss = 0.03409870\n",
      "Iteration 4036, loss = 0.03409058\n",
      "Iteration 4037, loss = 0.03408244\n",
      "Iteration 4038, loss = 0.03407429\n",
      "Iteration 4039, loss = 0.03406612\n",
      "Iteration 4040, loss = 0.03405794\n",
      "Iteration 4041, loss = 0.03404975\n",
      "Iteration 4042, loss = 0.03404155\n",
      "Iteration 4043, loss = 0.03403333\n",
      "Iteration 4044, loss = 0.03402509\n",
      "Iteration 4045, loss = 0.03401685\n",
      "Iteration 4046, loss = 0.03400859\n",
      "Iteration 4047, loss = 0.03400032\n",
      "Iteration 4048, loss = 0.03399204\n",
      "Iteration 4049, loss = 0.03398374\n",
      "Iteration 4050, loss = 0.03397543\n",
      "Iteration 4051, loss = 0.03396710\n",
      "Iteration 4052, loss = 0.03395876\n",
      "Iteration 4053, loss = 0.03395041\n",
      "Iteration 4054, loss = 0.03394205\n",
      "Iteration 4055, loss = 0.03393367\n",
      "Iteration 4056, loss = 0.03392528\n",
      "Iteration 4057, loss = 0.03391688\n",
      "Iteration 4058, loss = 0.03390846\n",
      "Iteration 4059, loss = 0.03390003\n",
      "Iteration 4060, loss = 0.03389159\n",
      "Iteration 4061, loss = 0.03388313\n",
      "Iteration 4062, loss = 0.03387467\n",
      "Iteration 4063, loss = 0.03386618\n",
      "Iteration 4064, loss = 0.03385769\n",
      "Iteration 4065, loss = 0.03384918\n",
      "Iteration 4066, loss = 0.03384066\n",
      "Iteration 4067, loss = 0.03383213\n",
      "Iteration 4068, loss = 0.03382358\n",
      "Iteration 4069, loss = 0.03381502\n",
      "Iteration 4070, loss = 0.03380645\n",
      "Iteration 4071, loss = 0.03379786\n",
      "Iteration 4072, loss = 0.03378926\n",
      "Iteration 4073, loss = 0.03378065\n",
      "Iteration 4074, loss = 0.03377203\n",
      "Iteration 4075, loss = 0.03376339\n",
      "Iteration 4076, loss = 0.03375474\n",
      "Iteration 4077, loss = 0.03374607\n",
      "Iteration 4078, loss = 0.03373740\n",
      "Iteration 4079, loss = 0.03372871\n",
      "Iteration 4080, loss = 0.03372001\n",
      "Iteration 4081, loss = 0.03371129\n",
      "Iteration 4082, loss = 0.03370256\n",
      "Iteration 4083, loss = 0.03369382\n",
      "Iteration 4084, loss = 0.03368507\n",
      "Iteration 4085, loss = 0.03367630\n",
      "Iteration 4086, loss = 0.03366753\n",
      "Iteration 4087, loss = 0.03365873\n",
      "Iteration 4088, loss = 0.03364993\n",
      "Iteration 4089, loss = 0.03364111\n",
      "Iteration 4090, loss = 0.03363228\n",
      "Iteration 4091, loss = 0.03362344\n",
      "Iteration 4092, loss = 0.03361459\n",
      "Iteration 4093, loss = 0.03360572\n",
      "Iteration 4094, loss = 0.03359684\n",
      "Iteration 4095, loss = 0.03358794\n",
      "Iteration 4096, loss = 0.03357904\n",
      "Iteration 4097, loss = 0.03357012\n",
      "Iteration 4098, loss = 0.03356119\n",
      "Iteration 4099, loss = 0.03355225\n",
      "Iteration 4100, loss = 0.03354329\n",
      "Iteration 4101, loss = 0.03353432\n",
      "Iteration 4102, loss = 0.03352534\n",
      "Iteration 4103, loss = 0.03351635\n",
      "Iteration 4104, loss = 0.03350734\n",
      "Iteration 4105, loss = 0.03349832\n",
      "Iteration 4106, loss = 0.03348929\n",
      "Iteration 4107, loss = 0.03348025\n",
      "Iteration 4108, loss = 0.03347119\n",
      "Iteration 4109, loss = 0.03346212\n",
      "Iteration 4110, loss = 0.03345304\n",
      "Iteration 4111, loss = 0.03344395\n",
      "Iteration 4112, loss = 0.03343484\n",
      "Iteration 4113, loss = 0.03342573\n",
      "Iteration 4114, loss = 0.03341660\n",
      "Iteration 4115, loss = 0.03340745\n",
      "Iteration 4116, loss = 0.03339830\n",
      "Iteration 4117, loss = 0.03338913\n",
      "Iteration 4118, loss = 0.03337995\n",
      "Iteration 4119, loss = 0.03337076\n",
      "Iteration 4120, loss = 0.03336156\n",
      "Iteration 4121, loss = 0.03335234\n",
      "Iteration 4122, loss = 0.03334311\n",
      "Iteration 4123, loss = 0.03333387\n",
      "Iteration 4124, loss = 0.03332462\n",
      "Iteration 4125, loss = 0.03331536\n",
      "Iteration 4126, loss = 0.03330608\n",
      "Iteration 4127, loss = 0.03329679\n",
      "Iteration 4128, loss = 0.03328749\n",
      "Iteration 4129, loss = 0.03327818\n",
      "Iteration 4130, loss = 0.03326886\n",
      "Iteration 4131, loss = 0.03325952\n",
      "Iteration 4132, loss = 0.03325017\n",
      "Iteration 4133, loss = 0.03324081\n",
      "Iteration 4134, loss = 0.03323144\n",
      "Iteration 4135, loss = 0.03322205\n",
      "Iteration 4136, loss = 0.03321266\n",
      "Iteration 4137, loss = 0.03320325\n",
      "Iteration 4138, loss = 0.03319383\n",
      "Iteration 4139, loss = 0.03318440\n",
      "Iteration 4140, loss = 0.03317495\n",
      "Iteration 4141, loss = 0.03316550\n",
      "Iteration 4142, loss = 0.03315603\n",
      "Iteration 4143, loss = 0.03314655\n",
      "Iteration 4144, loss = 0.03313706\n",
      "Iteration 4145, loss = 0.03312756\n",
      "Iteration 4146, loss = 0.03311804\n",
      "Iteration 4147, loss = 0.03310852\n",
      "Iteration 4148, loss = 0.03309898\n",
      "Iteration 4149, loss = 0.03308943\n",
      "Iteration 4150, loss = 0.03307987\n",
      "Iteration 4151, loss = 0.03307030\n",
      "Iteration 4152, loss = 0.03306071\n",
      "Iteration 4153, loss = 0.03305112\n",
      "Iteration 4154, loss = 0.03304151\n",
      "Iteration 4155, loss = 0.03303189\n",
      "Iteration 4156, loss = 0.03302226\n",
      "Iteration 4157, loss = 0.03301262\n",
      "Iteration 4158, loss = 0.03300297\n",
      "Iteration 4159, loss = 0.03299330\n",
      "Iteration 4160, loss = 0.03298363\n",
      "Iteration 4161, loss = 0.03297394\n",
      "Iteration 4162, loss = 0.03296424\n",
      "Iteration 4163, loss = 0.03295453\n",
      "Iteration 4164, loss = 0.03294481\n",
      "Iteration 4165, loss = 0.03293508\n",
      "Iteration 4166, loss = 0.03292534\n",
      "Iteration 4167, loss = 0.03291558\n",
      "Iteration 4168, loss = 0.03290582\n",
      "Iteration 4169, loss = 0.03289604\n",
      "Iteration 4170, loss = 0.03288625\n",
      "Iteration 4171, loss = 0.03287645\n",
      "Iteration 4172, loss = 0.03286664\n",
      "Iteration 4173, loss = 0.03285682\n",
      "Iteration 4174, loss = 0.03284699\n",
      "Iteration 4175, loss = 0.03283715\n",
      "Iteration 4176, loss = 0.03282729\n",
      "Iteration 4177, loss = 0.03281743\n",
      "Iteration 4178, loss = 0.03280755\n",
      "Iteration 4179, loss = 0.03279766\n",
      "Iteration 4180, loss = 0.03278777\n",
      "Iteration 4181, loss = 0.03277786\n",
      "Iteration 4182, loss = 0.03276794\n",
      "Iteration 4183, loss = 0.03275801\n",
      "Iteration 4184, loss = 0.03274806\n",
      "Iteration 4185, loss = 0.03273811\n",
      "Iteration 4186, loss = 0.03272815\n",
      "Iteration 4187, loss = 0.03271817\n",
      "Iteration 4188, loss = 0.03270819\n",
      "Iteration 4189, loss = 0.03269820\n",
      "Iteration 4190, loss = 0.03268819\n",
      "Iteration 4191, loss = 0.03267817\n",
      "Iteration 4192, loss = 0.03266815\n",
      "Iteration 4193, loss = 0.03265811\n",
      "Iteration 4194, loss = 0.03264806\n",
      "Iteration 4195, loss = 0.03263800\n",
      "Iteration 4196, loss = 0.03262793\n",
      "Iteration 4197, loss = 0.03261785\n",
      "Iteration 4198, loss = 0.03260776\n",
      "Iteration 4199, loss = 0.03259766\n",
      "Iteration 4200, loss = 0.03258755\n",
      "Iteration 4201, loss = 0.03257743\n",
      "Iteration 4202, loss = 0.03256730\n",
      "Iteration 4203, loss = 0.03255716\n",
      "Iteration 4204, loss = 0.03254701\n",
      "Iteration 4205, loss = 0.03253684\n",
      "Iteration 4206, loss = 0.03252667\n",
      "Iteration 4207, loss = 0.03251649\n",
      "Iteration 4208, loss = 0.03250630\n",
      "Iteration 4209, loss = 0.03249609\n",
      "Iteration 4210, loss = 0.03248588\n",
      "Iteration 4211, loss = 0.03247566\n",
      "Iteration 4212, loss = 0.03246542\n",
      "Iteration 4213, loss = 0.03245518\n",
      "Iteration 4214, loss = 0.03244493\n",
      "Iteration 4215, loss = 0.03243466\n",
      "Iteration 4216, loss = 0.03242439\n",
      "Iteration 4217, loss = 0.03241411\n",
      "Iteration 4218, loss = 0.03240381\n",
      "Iteration 4219, loss = 0.03239351\n",
      "Iteration 4220, loss = 0.03238320\n",
      "Iteration 4221, loss = 0.03237288\n",
      "Iteration 4222, loss = 0.03236254\n",
      "Iteration 4223, loss = 0.03235220\n",
      "Iteration 4224, loss = 0.03234185\n",
      "Iteration 4225, loss = 0.03233149\n",
      "Iteration 4226, loss = 0.03232112\n",
      "Iteration 4227, loss = 0.03231074\n",
      "Iteration 4228, loss = 0.03230035\n",
      "Iteration 4229, loss = 0.03228995\n",
      "Iteration 4230, loss = 0.03227954\n",
      "Iteration 4231, loss = 0.03226912\n",
      "Iteration 4232, loss = 0.03225869\n",
      "Iteration 4233, loss = 0.03224825\n",
      "Iteration 4234, loss = 0.03223781\n",
      "Iteration 4235, loss = 0.03222735\n",
      "Iteration 4236, loss = 0.03221688\n",
      "Iteration 4237, loss = 0.03220641\n",
      "Iteration 4238, loss = 0.03219592\n",
      "Iteration 4239, loss = 0.03218543\n",
      "Iteration 4240, loss = 0.03217493\n",
      "Iteration 4241, loss = 0.03216441\n",
      "Iteration 4242, loss = 0.03215389\n",
      "Iteration 4243, loss = 0.03214336\n",
      "Iteration 4244, loss = 0.03213282\n",
      "Iteration 4245, loss = 0.03212227\n",
      "Iteration 4246, loss = 0.03211171\n",
      "Iteration 4247, loss = 0.03210115\n",
      "Iteration 4248, loss = 0.03209057\n",
      "Iteration 4249, loss = 0.03207999\n",
      "Iteration 4250, loss = 0.03206939\n",
      "Iteration 4251, loss = 0.03205879\n",
      "Iteration 4252, loss = 0.03204818\n",
      "Iteration 4253, loss = 0.03203756\n",
      "Iteration 4254, loss = 0.03202693\n",
      "Iteration 4255, loss = 0.03201629\n",
      "Iteration 4256, loss = 0.03200564\n",
      "Iteration 4257, loss = 0.03199499\n",
      "Iteration 4258, loss = 0.03198432\n",
      "Iteration 4259, loss = 0.03197365\n",
      "Iteration 4260, loss = 0.03196297\n",
      "Iteration 4261, loss = 0.03195228\n",
      "Iteration 4262, loss = 0.03194158\n",
      "Iteration 4263, loss = 0.03193087\n",
      "Iteration 4264, loss = 0.03192015\n",
      "Iteration 4265, loss = 0.03190943\n",
      "Iteration 4266, loss = 0.03189870\n",
      "Iteration 4267, loss = 0.03188796\n",
      "Iteration 4268, loss = 0.03187721\n",
      "Iteration 4269, loss = 0.03186645\n",
      "Iteration 4270, loss = 0.03185568\n",
      "Iteration 4271, loss = 0.03184491\n",
      "Iteration 4272, loss = 0.03183412\n",
      "Iteration 4273, loss = 0.03182333\n",
      "Iteration 4274, loss = 0.03181253\n",
      "Iteration 4275, loss = 0.03180173\n",
      "Iteration 4276, loss = 0.03179091\n",
      "Iteration 4277, loss = 0.03178009\n",
      "Iteration 4278, loss = 0.03176926\n",
      "Iteration 4279, loss = 0.03175842\n",
      "Iteration 4280, loss = 0.03174757\n",
      "Iteration 4281, loss = 0.03173671\n",
      "Iteration 4282, loss = 0.03172585\n",
      "Iteration 4283, loss = 0.03171498\n",
      "Iteration 4284, loss = 0.03170410\n",
      "Iteration 4285, loss = 0.03169321\n",
      "Iteration 4286, loss = 0.03168231\n",
      "Iteration 4287, loss = 0.03167141\n",
      "Iteration 4288, loss = 0.03166050\n",
      "Iteration 4289, loss = 0.03164958\n",
      "Iteration 4290, loss = 0.03163866\n",
      "Iteration 4291, loss = 0.03162772\n",
      "Iteration 4292, loss = 0.03161678\n",
      "Iteration 4293, loss = 0.03160583\n",
      "Iteration 4294, loss = 0.03159488\n",
      "Iteration 4295, loss = 0.03158391\n",
      "Iteration 4296, loss = 0.03157294\n",
      "Iteration 4297, loss = 0.03156196\n",
      "Iteration 4298, loss = 0.03155098\n",
      "Iteration 4299, loss = 0.03153998\n",
      "Iteration 4300, loss = 0.03152898\n",
      "Iteration 4301, loss = 0.03151797\n",
      "Iteration 4302, loss = 0.03150696\n",
      "Iteration 4303, loss = 0.03149593\n",
      "Iteration 4304, loss = 0.03148490\n",
      "Iteration 4305, loss = 0.03147387\n",
      "Iteration 4306, loss = 0.03146282\n",
      "Iteration 4307, loss = 0.03145177\n",
      "Iteration 4308, loss = 0.03144071\n",
      "Iteration 4309, loss = 0.03142965\n",
      "Iteration 4310, loss = 0.03141857\n",
      "Iteration 4311, loss = 0.03140749\n",
      "Iteration 4312, loss = 0.03139641\n",
      "Iteration 4313, loss = 0.03138531\n",
      "Iteration 4314, loss = 0.03137421\n",
      "Iteration 4315, loss = 0.03136311\n",
      "Iteration 4316, loss = 0.03135199\n",
      "Iteration 4317, loss = 0.03134087\n",
      "Iteration 4318, loss = 0.03132974\n",
      "Iteration 4319, loss = 0.03131861\n",
      "Iteration 4320, loss = 0.03130747\n",
      "Iteration 4321, loss = 0.03129632\n",
      "Iteration 4322, loss = 0.03128517\n",
      "Iteration 4323, loss = 0.03127400\n",
      "Iteration 4324, loss = 0.03126284\n",
      "Iteration 4325, loss = 0.03125166\n",
      "Iteration 4326, loss = 0.03124048\n",
      "Iteration 4327, loss = 0.03122930\n",
      "Iteration 4328, loss = 0.03121810\n",
      "Iteration 4329, loss = 0.03120690\n",
      "Iteration 4330, loss = 0.03119570\n",
      "Iteration 4331, loss = 0.03118449\n",
      "Iteration 4332, loss = 0.03117327\n",
      "Iteration 4333, loss = 0.03116204\n",
      "Iteration 4334, loss = 0.03115081\n",
      "Iteration 4335, loss = 0.03113957\n",
      "Iteration 4336, loss = 0.03112833\n",
      "Iteration 4337, loss = 0.03111708\n",
      "Iteration 4338, loss = 0.03110583\n",
      "Iteration 4339, loss = 0.03109457\n",
      "Iteration 4340, loss = 0.03108330\n",
      "Iteration 4341, loss = 0.03107202\n",
      "Iteration 4342, loss = 0.03106075\n",
      "Iteration 4343, loss = 0.03104946\n",
      "Iteration 4344, loss = 0.03103817\n",
      "Iteration 4345, loss = 0.03102687\n",
      "Iteration 4346, loss = 0.03101557\n",
      "Iteration 4347, loss = 0.03100426\n",
      "Iteration 4348, loss = 0.03099295\n",
      "Iteration 4349, loss = 0.03098163\n",
      "Iteration 4350, loss = 0.03097030\n",
      "Iteration 4351, loss = 0.03095897\n",
      "Iteration 4352, loss = 0.03094763\n",
      "Iteration 4353, loss = 0.03093629\n",
      "Iteration 4354, loss = 0.03092494\n",
      "Iteration 4355, loss = 0.03091359\n",
      "Iteration 4356, loss = 0.03090223\n",
      "Iteration 4357, loss = 0.03089087\n",
      "Iteration 4358, loss = 0.03087950\n",
      "Iteration 4359, loss = 0.03086812\n",
      "Iteration 4360, loss = 0.03085674\n",
      "Iteration 4361, loss = 0.03084536\n",
      "Iteration 4362, loss = 0.03083397\n",
      "Iteration 4363, loss = 0.03082257\n",
      "Iteration 4364, loss = 0.03081117\n",
      "Iteration 4365, loss = 0.03079976\n",
      "Iteration 4366, loss = 0.03078835\n",
      "Iteration 4367, loss = 0.03077694\n",
      "Iteration 4368, loss = 0.03076552\n",
      "Iteration 4369, loss = 0.03075409\n",
      "Iteration 4370, loss = 0.03074266\n",
      "Iteration 4371, loss = 0.03073122\n",
      "Iteration 4372, loss = 0.03071978\n",
      "Iteration 4373, loss = 0.03070834\n",
      "Iteration 4374, loss = 0.03069688\n",
      "Iteration 4375, loss = 0.03068543\n",
      "Iteration 4376, loss = 0.03067397\n",
      "Iteration 4377, loss = 0.03066250\n",
      "Iteration 4378, loss = 0.03065103\n",
      "Iteration 4379, loss = 0.03063956\n",
      "Iteration 4380, loss = 0.03062808\n",
      "Iteration 4381, loss = 0.03061660\n",
      "Iteration 4382, loss = 0.03060511\n",
      "Iteration 4383, loss = 0.03059362\n",
      "Iteration 4384, loss = 0.03058212\n",
      "Iteration 4385, loss = 0.03057062\n",
      "Iteration 4386, loss = 0.03055911\n",
      "Iteration 4387, loss = 0.03054760\n",
      "Iteration 4388, loss = 0.03053609\n",
      "Iteration 4389, loss = 0.03052457\n",
      "Iteration 4390, loss = 0.03051305\n",
      "Iteration 4391, loss = 0.03050152\n",
      "Iteration 4392, loss = 0.03048999\n",
      "Iteration 4393, loss = 0.03047845\n",
      "Iteration 4394, loss = 0.03046691\n",
      "Iteration 4395, loss = 0.03045537\n",
      "Iteration 4396, loss = 0.03044382\n",
      "Iteration 4397, loss = 0.03043227\n",
      "Iteration 4398, loss = 0.03042071\n",
      "Iteration 4399, loss = 0.03040915\n",
      "Iteration 4400, loss = 0.03039759\n",
      "Iteration 4401, loss = 0.03038602\n",
      "Iteration 4402, loss = 0.03037445\n",
      "Iteration 4403, loss = 0.03036287\n",
      "Iteration 4404, loss = 0.03035129\n",
      "Iteration 4405, loss = 0.03033971\n",
      "Iteration 4406, loss = 0.03032813\n",
      "Iteration 4407, loss = 0.03031653\n",
      "Iteration 4408, loss = 0.03030494\n",
      "Iteration 4409, loss = 0.03029334\n",
      "Iteration 4410, loss = 0.03028174\n",
      "Iteration 4411, loss = 0.03027014\n",
      "Iteration 4412, loss = 0.03025853\n",
      "Iteration 4413, loss = 0.03024692\n",
      "Iteration 4414, loss = 0.03023530\n",
      "Iteration 4415, loss = 0.03022368\n",
      "Iteration 4416, loss = 0.03021206\n",
      "Iteration 4417, loss = 0.03020044\n",
      "Iteration 4418, loss = 0.03018881\n",
      "Iteration 4419, loss = 0.03017718\n",
      "Iteration 4420, loss = 0.03016554\n",
      "Iteration 4421, loss = 0.03015391\n",
      "Iteration 4422, loss = 0.03014226\n",
      "Iteration 4423, loss = 0.03013062\n",
      "Iteration 4424, loss = 0.03011897\n",
      "Iteration 4425, loss = 0.03010732\n",
      "Iteration 4426, loss = 0.03009567\n",
      "Iteration 4427, loss = 0.03008401\n",
      "Iteration 4428, loss = 0.03007235\n",
      "Iteration 4429, loss = 0.03006069\n",
      "Iteration 4430, loss = 0.03004903\n",
      "Iteration 4431, loss = 0.03003736\n",
      "Iteration 4432, loss = 0.03002569\n",
      "Iteration 4433, loss = 0.03001401\n",
      "Iteration 4434, loss = 0.03000234\n",
      "Iteration 4435, loss = 0.02999066\n",
      "Iteration 4436, loss = 0.02997898\n",
      "Iteration 4437, loss = 0.02996729\n",
      "Iteration 4438, loss = 0.02995560\n",
      "Iteration 4439, loss = 0.02994391\n",
      "Iteration 4440, loss = 0.02993222\n",
      "Iteration 4441, loss = 0.02992053\n",
      "Iteration 4442, loss = 0.02990883\n",
      "Iteration 4443, loss = 0.02989713\n",
      "Iteration 4444, loss = 0.02988543\n",
      "Iteration 4445, loss = 0.02987373\n",
      "Iteration 4446, loss = 0.02986202\n",
      "Iteration 4447, loss = 0.02985031\n",
      "Iteration 4448, loss = 0.02983860\n",
      "Iteration 4449, loss = 0.02982689\n",
      "Iteration 4450, loss = 0.02981517\n",
      "Iteration 4451, loss = 0.02980345\n",
      "Iteration 4452, loss = 0.02979173\n",
      "Iteration 4453, loss = 0.02978001\n",
      "Iteration 4454, loss = 0.02976829\n",
      "Iteration 4455, loss = 0.02975656\n",
      "Iteration 4456, loss = 0.02974483\n",
      "Iteration 4457, loss = 0.02973310\n",
      "Iteration 4458, loss = 0.02972137\n",
      "Iteration 4459, loss = 0.02970964\n",
      "Iteration 4460, loss = 0.02969790\n",
      "Iteration 4461, loss = 0.02968616\n",
      "Iteration 4462, loss = 0.02967442\n",
      "Iteration 4463, loss = 0.02966268\n",
      "Iteration 4464, loss = 0.02965094\n",
      "Iteration 4465, loss = 0.02963920\n",
      "Iteration 4466, loss = 0.02962745\n",
      "Iteration 4467, loss = 0.02961570\n",
      "Iteration 4468, loss = 0.02960395\n",
      "Iteration 4469, loss = 0.02959220\n",
      "Iteration 4470, loss = 0.02958045\n",
      "Iteration 4471, loss = 0.02956869\n",
      "Iteration 4472, loss = 0.02955694\n",
      "Iteration 4473, loss = 0.02954518\n",
      "Iteration 4474, loss = 0.02953342\n",
      "Iteration 4475, loss = 0.02952166\n",
      "Iteration 4476, loss = 0.02950990\n",
      "Iteration 4477, loss = 0.02949814\n",
      "Iteration 4478, loss = 0.02948638\n",
      "Iteration 4479, loss = 0.02947461\n",
      "Iteration 4480, loss = 0.02946285\n",
      "Iteration 4481, loss = 0.02945108\n",
      "Iteration 4482, loss = 0.02943931\n",
      "Iteration 4483, loss = 0.02942754\n",
      "Iteration 4484, loss = 0.02941577\n",
      "Iteration 4485, loss = 0.02940400\n",
      "Iteration 4486, loss = 0.02939223\n",
      "Iteration 4487, loss = 0.02938045\n",
      "Iteration 4488, loss = 0.02936868\n",
      "Iteration 4489, loss = 0.02935690\n",
      "Iteration 4490, loss = 0.02934513\n",
      "Iteration 4491, loss = 0.02933335\n",
      "Iteration 4492, loss = 0.02932157\n",
      "Iteration 4493, loss = 0.02930979\n",
      "Iteration 4494, loss = 0.02929801\n",
      "Iteration 4495, loss = 0.02928623\n",
      "Iteration 4496, loss = 0.02927445\n",
      "Iteration 4497, loss = 0.02926267\n",
      "Iteration 4498, loss = 0.02925089\n",
      "Iteration 4499, loss = 0.02923911\n",
      "Iteration 4500, loss = 0.02922732\n",
      "Iteration 4501, loss = 0.02921554\n",
      "Iteration 4502, loss = 0.02920375\n",
      "Iteration 4503, loss = 0.02919197\n",
      "Iteration 4504, loss = 0.02918018\n",
      "Iteration 4505, loss = 0.02916840\n",
      "Iteration 4506, loss = 0.02915661\n",
      "Iteration 4507, loss = 0.02914483\n",
      "Iteration 4508, loss = 0.02913304\n",
      "Iteration 4509, loss = 0.02912125\n",
      "Iteration 4510, loss = 0.02910946\n",
      "Iteration 4511, loss = 0.02909768\n",
      "Iteration 4512, loss = 0.02908589\n",
      "Iteration 4513, loss = 0.02907410\n",
      "Iteration 4514, loss = 0.02906231\n",
      "Iteration 4515, loss = 0.02905053\n",
      "Iteration 4516, loss = 0.02903874\n",
      "Iteration 4517, loss = 0.02902695\n",
      "Iteration 4518, loss = 0.02901516\n",
      "Iteration 4519, loss = 0.02900337\n",
      "Iteration 4520, loss = 0.02899159\n",
      "Iteration 4521, loss = 0.02897980\n",
      "Iteration 4522, loss = 0.02896801\n",
      "Iteration 4523, loss = 0.02895622\n",
      "Iteration 4524, loss = 0.02894443\n",
      "Iteration 4525, loss = 0.02893265\n",
      "Iteration 4526, loss = 0.02892086\n",
      "Iteration 4527, loss = 0.02890907\n",
      "Iteration 4528, loss = 0.02889729\n",
      "Iteration 4529, loss = 0.02888550\n",
      "Iteration 4530, loss = 0.02887371\n",
      "Iteration 4531, loss = 0.02886193\n",
      "Iteration 4532, loss = 0.02885014\n",
      "Iteration 4533, loss = 0.02883836\n",
      "Iteration 4534, loss = 0.02882658\n",
      "Iteration 4535, loss = 0.02881479\n",
      "Iteration 4536, loss = 0.02880301\n",
      "Iteration 4537, loss = 0.02879123\n",
      "Iteration 4538, loss = 0.02877944\n",
      "Iteration 4539, loss = 0.02876766\n",
      "Iteration 4540, loss = 0.02875588\n",
      "Iteration 4541, loss = 0.02874410\n",
      "Iteration 4542, loss = 0.02873232\n",
      "Iteration 4543, loss = 0.02872054\n",
      "Iteration 4544, loss = 0.02870877\n",
      "Iteration 4545, loss = 0.02869699\n",
      "Iteration 4546, loss = 0.02868521\n",
      "Iteration 4547, loss = 0.02867344\n",
      "Iteration 4548, loss = 0.02866166\n",
      "Iteration 4549, loss = 0.02864989\n",
      "Iteration 4550, loss = 0.02863812\n",
      "Iteration 4551, loss = 0.02862635\n",
      "Iteration 4552, loss = 0.02861458\n",
      "Iteration 4553, loss = 0.02860281\n",
      "Iteration 4554, loss = 0.02859104\n",
      "Iteration 4555, loss = 0.02857927\n",
      "Iteration 4556, loss = 0.02856750\n",
      "Iteration 4557, loss = 0.02855574\n",
      "Iteration 4558, loss = 0.02854398\n",
      "Iteration 4559, loss = 0.02853221\n",
      "Iteration 4560, loss = 0.02852045\n",
      "Iteration 4561, loss = 0.02850869\n",
      "Iteration 4562, loss = 0.02849693\n",
      "Iteration 4563, loss = 0.02848517\n",
      "Iteration 4564, loss = 0.02847342\n",
      "Iteration 4565, loss = 0.02846166\n",
      "Iteration 4566, loss = 0.02844991\n",
      "Iteration 4567, loss = 0.02843816\n",
      "Iteration 4568, loss = 0.02842641\n",
      "Iteration 4569, loss = 0.02841466\n",
      "Iteration 4570, loss = 0.02840291\n",
      "Iteration 4571, loss = 0.02839116\n",
      "Iteration 4572, loss = 0.02837942\n",
      "Iteration 4573, loss = 0.02836767\n",
      "Iteration 4574, loss = 0.02835593\n",
      "Iteration 4575, loss = 0.02834419\n",
      "Iteration 4576, loss = 0.02833245\n",
      "Iteration 4577, loss = 0.02832072\n",
      "Iteration 4578, loss = 0.02830898\n",
      "Iteration 4579, loss = 0.02829725\n",
      "Iteration 4580, loss = 0.02828552\n",
      "Iteration 4581, loss = 0.02827379\n",
      "Iteration 4582, loss = 0.02826206\n",
      "Iteration 4583, loss = 0.02825033\n",
      "Iteration 4584, loss = 0.02823861\n",
      "Iteration 4585, loss = 0.02822689\n",
      "Iteration 4586, loss = 0.02821517\n",
      "Iteration 4587, loss = 0.02820345\n",
      "Iteration 4588, loss = 0.02819173\n",
      "Iteration 4589, loss = 0.02818002\n",
      "Iteration 4590, loss = 0.02816831\n",
      "Iteration 4591, loss = 0.02815659\n",
      "Iteration 4592, loss = 0.02814489\n",
      "Iteration 4593, loss = 0.02813318\n",
      "Iteration 4594, loss = 0.02812148\n",
      "Iteration 4595, loss = 0.02810977\n",
      "Iteration 4596, loss = 0.02809807\n",
      "Iteration 4597, loss = 0.02808638\n",
      "Iteration 4598, loss = 0.02807468\n",
      "Iteration 4599, loss = 0.02806299\n",
      "Iteration 4600, loss = 0.02805130\n",
      "Iteration 4601, loss = 0.02803961\n",
      "Iteration 4602, loss = 0.02802792\n",
      "Iteration 4603, loss = 0.02801624\n",
      "Iteration 4604, loss = 0.02800456\n",
      "Iteration 4605, loss = 0.02799288\n",
      "Iteration 4606, loss = 0.02798120\n",
      "Iteration 4607, loss = 0.02796953\n",
      "Iteration 4608, loss = 0.02795785\n",
      "Iteration 4609, loss = 0.02794618\n",
      "Iteration 4610, loss = 0.02793452\n",
      "Iteration 4611, loss = 0.02792285\n",
      "Iteration 4612, loss = 0.02791119\n",
      "Iteration 4613, loss = 0.02789953\n",
      "Iteration 4614, loss = 0.02788787\n",
      "Iteration 4615, loss = 0.02787622\n",
      "Iteration 4616, loss = 0.02786457\n",
      "Iteration 4617, loss = 0.02785292\n",
      "Iteration 4618, loss = 0.02784127\n",
      "Iteration 4619, loss = 0.02782963\n",
      "Iteration 4620, loss = 0.02781799\n",
      "Iteration 4621, loss = 0.02780635\n",
      "Iteration 4622, loss = 0.02779472\n",
      "Iteration 4623, loss = 0.02778308\n",
      "Iteration 4624, loss = 0.02777145\n",
      "Iteration 4625, loss = 0.02775983\n",
      "Iteration 4626, loss = 0.02774820\n",
      "Iteration 4627, loss = 0.02773658\n",
      "Iteration 4628, loss = 0.02772496\n",
      "Iteration 4629, loss = 0.02771335\n",
      "Iteration 4630, loss = 0.02770174\n",
      "Iteration 4631, loss = 0.02769013\n",
      "Iteration 4632, loss = 0.02767852\n",
      "Iteration 4633, loss = 0.02766692\n",
      "Iteration 4634, loss = 0.02765532\n",
      "Iteration 4635, loss = 0.02764372\n",
      "Iteration 4636, loss = 0.02763213\n",
      "Iteration 4637, loss = 0.02762054\n",
      "Iteration 4638, loss = 0.02760895\n",
      "Iteration 4639, loss = 0.02759736\n",
      "Iteration 4640, loss = 0.02758578\n",
      "Iteration 4641, loss = 0.02757421\n",
      "Iteration 4642, loss = 0.02756263\n",
      "Iteration 4643, loss = 0.02755106\n",
      "Iteration 4644, loss = 0.02753949\n",
      "Iteration 4645, loss = 0.02752792\n",
      "Iteration 4646, loss = 0.02751636\n",
      "Iteration 4647, loss = 0.02750480\n",
      "Iteration 4648, loss = 0.02749325\n",
      "Iteration 4649, loss = 0.02748170\n",
      "Iteration 4650, loss = 0.02747015\n",
      "Iteration 4651, loss = 0.02745860\n",
      "Iteration 4652, loss = 0.02744706\n",
      "Iteration 4653, loss = 0.02743552\n",
      "Iteration 4654, loss = 0.02742399\n",
      "Iteration 4655, loss = 0.02741246\n",
      "Iteration 4656, loss = 0.02740093\n",
      "Iteration 4657, loss = 0.02738940\n",
      "Iteration 4658, loss = 0.02737788\n",
      "Iteration 4659, loss = 0.02736637\n",
      "Iteration 4660, loss = 0.02735485\n",
      "Iteration 4661, loss = 0.02734334\n",
      "Iteration 4662, loss = 0.02733184\n",
      "Iteration 4663, loss = 0.02732033\n",
      "Iteration 4664, loss = 0.02730883\n",
      "Iteration 4665, loss = 0.02729734\n",
      "Iteration 4666, loss = 0.02728585\n",
      "Iteration 4667, loss = 0.02727436\n",
      "Iteration 4668, loss = 0.02726287\n",
      "Iteration 4669, loss = 0.02725139\n",
      "Iteration 4670, loss = 0.02723992\n",
      "Iteration 4671, loss = 0.02722844\n",
      "Iteration 4672, loss = 0.02721697\n",
      "Iteration 4673, loss = 0.02720551\n",
      "Iteration 4674, loss = 0.02719405\n",
      "Iteration 4675, loss = 0.02718259\n",
      "Iteration 4676, loss = 0.02717113\n",
      "Iteration 4677, loss = 0.02715968\n",
      "Iteration 4678, loss = 0.02714824\n",
      "Iteration 4679, loss = 0.02713679\n",
      "Iteration 4680, loss = 0.02712536\n",
      "Iteration 4681, loss = 0.02711392\n",
      "Iteration 4682, loss = 0.02710249\n",
      "Iteration 4683, loss = 0.02709106\n",
      "Iteration 4684, loss = 0.02707964\n",
      "Iteration 4685, loss = 0.02706822\n",
      "Iteration 4686, loss = 0.02705681\n",
      "Iteration 4687, loss = 0.02704540\n",
      "Iteration 4688, loss = 0.02703399\n",
      "Iteration 4689, loss = 0.02702259\n",
      "Iteration 4690, loss = 0.02701119\n",
      "Iteration 4691, loss = 0.02699980\n",
      "Iteration 4692, loss = 0.02698841\n",
      "Iteration 4693, loss = 0.02697702\n",
      "Iteration 4694, loss = 0.02696564\n",
      "Iteration 4695, loss = 0.02695426\n",
      "Iteration 4696, loss = 0.02694289\n",
      "Iteration 4697, loss = 0.02693152\n",
      "Iteration 4698, loss = 0.02692015\n",
      "Iteration 4699, loss = 0.02690879\n",
      "Iteration 4700, loss = 0.02689744\n",
      "Iteration 4701, loss = 0.02688609\n",
      "Iteration 4702, loss = 0.02687474\n",
      "Iteration 4703, loss = 0.02686339\n",
      "Iteration 4704, loss = 0.02685206\n",
      "Iteration 4705, loss = 0.02684072\n",
      "Iteration 4706, loss = 0.02682939\n",
      "Iteration 4707, loss = 0.02681806\n",
      "Iteration 4708, loss = 0.02680674\n",
      "Iteration 4709, loss = 0.02679543\n",
      "Iteration 4710, loss = 0.02678411\n",
      "Iteration 4711, loss = 0.02677280\n",
      "Iteration 4712, loss = 0.02676150\n",
      "Iteration 4713, loss = 0.02675020\n",
      "Iteration 4714, loss = 0.02673890\n",
      "Iteration 4715, loss = 0.02672761\n",
      "Iteration 4716, loss = 0.02671633\n",
      "Iteration 4717, loss = 0.02670505\n",
      "Iteration 4718, loss = 0.02669377\n",
      "Iteration 4719, loss = 0.02668250\n",
      "Iteration 4720, loss = 0.02667123\n",
      "Iteration 4721, loss = 0.02665997\n",
      "Iteration 4722, loss = 0.02664871\n",
      "Iteration 4723, loss = 0.02663745\n",
      "Iteration 4724, loss = 0.02662620\n",
      "Iteration 4725, loss = 0.02661496\n",
      "Iteration 4726, loss = 0.02660372\n",
      "Iteration 4727, loss = 0.02659248\n",
      "Iteration 4728, loss = 0.02658125\n",
      "Iteration 4729, loss = 0.02657002\n",
      "Iteration 4730, loss = 0.02655880\n",
      "Iteration 4731, loss = 0.02654758\n",
      "Iteration 4732, loss = 0.02653637\n",
      "Iteration 4733, loss = 0.02652516\n",
      "Iteration 4734, loss = 0.02651396\n",
      "Iteration 4735, loss = 0.02650276\n",
      "Iteration 4736, loss = 0.02649157\n",
      "Iteration 4737, loss = 0.02648038\n",
      "Iteration 4738, loss = 0.02646920\n",
      "Iteration 4739, loss = 0.02645802\n",
      "Iteration 4740, loss = 0.02644684\n",
      "Iteration 4741, loss = 0.02643567\n",
      "Iteration 4742, loss = 0.02642451\n",
      "Iteration 4743, loss = 0.02641335\n",
      "Iteration 4744, loss = 0.02640219\n",
      "Iteration 4745, loss = 0.02639104\n",
      "Iteration 4746, loss = 0.02637990\n",
      "Iteration 4747, loss = 0.02636876\n",
      "Iteration 4748, loss = 0.02635762\n",
      "Iteration 4749, loss = 0.02634649\n",
      "Iteration 4750, loss = 0.02633537\n",
      "Iteration 4751, loss = 0.02632424\n",
      "Iteration 4752, loss = 0.02631313\n",
      "Iteration 4753, loss = 0.02630202\n",
      "Iteration 4754, loss = 0.02629091\n",
      "Iteration 4755, loss = 0.02627981\n",
      "Iteration 4756, loss = 0.02626871\n",
      "Iteration 4757, loss = 0.02625762\n",
      "Iteration 4758, loss = 0.02624654\n",
      "Iteration 4759, loss = 0.02623546\n",
      "Iteration 4760, loss = 0.02622438\n",
      "Iteration 4761, loss = 0.02621331\n",
      "Iteration 4762, loss = 0.02620224\n",
      "Iteration 4763, loss = 0.02619118\n",
      "Iteration 4764, loss = 0.02618013\n",
      "Iteration 4765, loss = 0.02616908\n",
      "Iteration 4766, loss = 0.02615803\n",
      "Iteration 4767, loss = 0.02614699\n",
      "Iteration 4768, loss = 0.02613595\n",
      "Iteration 4769, loss = 0.02612492\n",
      "Iteration 4770, loss = 0.02611390\n",
      "Iteration 4771, loss = 0.02610288\n",
      "Iteration 4772, loss = 0.02609186\n",
      "Iteration 4773, loss = 0.02608085\n",
      "Iteration 4774, loss = 0.02606985\n",
      "Iteration 4775, loss = 0.02605885\n",
      "Iteration 4776, loss = 0.02604785\n",
      "Iteration 4777, loss = 0.02603687\n",
      "Iteration 4778, loss = 0.02602588\n",
      "Iteration 4779, loss = 0.02601490\n",
      "Iteration 4780, loss = 0.02600393\n",
      "Iteration 4781, loss = 0.02599296\n",
      "Iteration 4782, loss = 0.02598200\n",
      "Iteration 4783, loss = 0.02597104\n",
      "Iteration 4784, loss = 0.02596009\n",
      "Iteration 4785, loss = 0.02594914\n",
      "Iteration 4786, loss = 0.02593820\n",
      "Iteration 4787, loss = 0.02592726\n",
      "Iteration 4788, loss = 0.02591633\n",
      "Iteration 4789, loss = 0.02590540\n",
      "Iteration 4790, loss = 0.02589448\n",
      "Iteration 4791, loss = 0.02588356\n",
      "Iteration 4792, loss = 0.02587265\n",
      "Iteration 4793, loss = 0.02586175\n",
      "Iteration 4794, loss = 0.02585085\n",
      "Iteration 4795, loss = 0.02583995\n",
      "Iteration 4796, loss = 0.02582906\n",
      "Iteration 4797, loss = 0.02581818\n",
      "Iteration 4798, loss = 0.02580730\n",
      "Iteration 4799, loss = 0.02579643\n",
      "Iteration 4800, loss = 0.02578556\n",
      "Iteration 4801, loss = 0.02577470\n",
      "Iteration 4802, loss = 0.02576384\n",
      "Iteration 4803, loss = 0.02575299\n",
      "Iteration 4804, loss = 0.02574214\n",
      "Iteration 4805, loss = 0.02573130\n",
      "Iteration 4806, loss = 0.02572047\n",
      "Iteration 4807, loss = 0.02570964\n",
      "Iteration 4808, loss = 0.02569881\n",
      "Iteration 4809, loss = 0.02568799\n",
      "Iteration 4810, loss = 0.02567718\n",
      "Iteration 4811, loss = 0.02566637\n",
      "Iteration 4812, loss = 0.02565556\n",
      "Iteration 4813, loss = 0.02564477\n",
      "Iteration 4814, loss = 0.02563397\n",
      "Iteration 4815, loss = 0.02562319\n",
      "Iteration 4816, loss = 0.02561241\n",
      "Iteration 4817, loss = 0.02560163\n",
      "Iteration 4818, loss = 0.02559086\n",
      "Iteration 4819, loss = 0.02558009\n",
      "Iteration 4820, loss = 0.02556933\n",
      "Iteration 4821, loss = 0.02555858\n",
      "Iteration 4822, loss = 0.02554783\n",
      "Iteration 4823, loss = 0.02553709\n",
      "Iteration 4824, loss = 0.02552635\n",
      "Iteration 4825, loss = 0.02551562\n",
      "Iteration 4826, loss = 0.02550489\n",
      "Iteration 4827, loss = 0.02549417\n",
      "Iteration 4828, loss = 0.02548346\n",
      "Iteration 4829, loss = 0.02547275\n",
      "Iteration 4830, loss = 0.02546204\n",
      "Iteration 4831, loss = 0.02545134\n",
      "Iteration 4832, loss = 0.02544065\n",
      "Iteration 4833, loss = 0.02542996\n",
      "Iteration 4834, loss = 0.02541928\n",
      "Iteration 4835, loss = 0.02540860\n",
      "Iteration 4836, loss = 0.02539793\n",
      "Iteration 4837, loss = 0.02538726\n",
      "Iteration 4838, loss = 0.02537660\n",
      "Iteration 4839, loss = 0.02536595\n",
      "Iteration 4840, loss = 0.02535530\n",
      "Iteration 4841, loss = 0.02534465\n",
      "Iteration 4842, loss = 0.02533402\n",
      "Iteration 4843, loss = 0.02532338\n",
      "Iteration 4844, loss = 0.02531276\n",
      "Iteration 4845, loss = 0.02530214\n",
      "Iteration 4846, loss = 0.02529152\n",
      "Iteration 4847, loss = 0.02528091\n",
      "Iteration 4848, loss = 0.02527031\n",
      "Iteration 4849, loss = 0.02525971\n",
      "Iteration 4850, loss = 0.02524911\n",
      "Iteration 4851, loss = 0.02523852\n",
      "Iteration 4852, loss = 0.02522794\n",
      "Iteration 4853, loss = 0.02521737\n",
      "Iteration 4854, loss = 0.02520679\n",
      "Iteration 4855, loss = 0.02519623\n",
      "Iteration 4856, loss = 0.02518567\n",
      "Iteration 4857, loss = 0.02517512\n",
      "Iteration 4858, loss = 0.02516457\n",
      "Iteration 4859, loss = 0.02515402\n",
      "Iteration 4860, loss = 0.02514349\n",
      "Iteration 4861, loss = 0.02513295\n",
      "Iteration 4862, loss = 0.02512243\n",
      "Iteration 4863, loss = 0.02511191\n",
      "Iteration 4864, loss = 0.02510139\n",
      "Iteration 4865, loss = 0.02509088\n",
      "Iteration 4866, loss = 0.02508038\n",
      "Iteration 4867, loss = 0.02506988\n",
      "Iteration 4868, loss = 0.02505939\n",
      "Iteration 4869, loss = 0.02504890\n",
      "Iteration 4870, loss = 0.02503842\n",
      "Iteration 4871, loss = 0.02502795\n",
      "Iteration 4872, loss = 0.02501748\n",
      "Iteration 4873, loss = 0.02500701\n",
      "Iteration 4874, loss = 0.02499655\n",
      "Iteration 4875, loss = 0.02498610\n",
      "Iteration 4876, loss = 0.02497565\n",
      "Iteration 4877, loss = 0.02496521\n",
      "Iteration 4878, loss = 0.02495478\n",
      "Iteration 4879, loss = 0.02494435\n",
      "Iteration 4880, loss = 0.02493392\n",
      "Iteration 4881, loss = 0.02492350\n",
      "Iteration 4882, loss = 0.02491309\n",
      "Iteration 4883, loss = 0.02490268\n",
      "Iteration 4884, loss = 0.02489228\n",
      "Iteration 4885, loss = 0.02488188\n",
      "Iteration 4886, loss = 0.02487149\n",
      "Iteration 4887, loss = 0.02486111\n",
      "Iteration 4888, loss = 0.02485073\n",
      "Iteration 4889, loss = 0.02484036\n",
      "Iteration 4890, loss = 0.02482999\n",
      "Iteration 4891, loss = 0.02481963\n",
      "Iteration 4892, loss = 0.02480927\n",
      "Iteration 4893, loss = 0.02479892\n",
      "Iteration 4894, loss = 0.02478857\n",
      "Iteration 4895, loss = 0.02477823\n",
      "Iteration 4896, loss = 0.02476790\n",
      "Iteration 4897, loss = 0.02475757\n",
      "Iteration 4898, loss = 0.02474725\n",
      "Iteration 4899, loss = 0.02473693\n",
      "Iteration 4900, loss = 0.02472662\n",
      "Iteration 4901, loss = 0.02471631\n",
      "Iteration 4902, loss = 0.02470601\n",
      "Iteration 4903, loss = 0.02469572\n",
      "Iteration 4904, loss = 0.02468543\n",
      "Iteration 4905, loss = 0.02467515\n",
      "Iteration 4906, loss = 0.02466487\n",
      "Iteration 4907, loss = 0.02465460\n",
      "Iteration 4908, loss = 0.02464433\n",
      "Iteration 4909, loss = 0.02463407\n",
      "Iteration 4910, loss = 0.02462382\n",
      "Iteration 4911, loss = 0.02461357\n",
      "Iteration 4912, loss = 0.02460333\n",
      "Iteration 4913, loss = 0.02459309\n",
      "Iteration 4914, loss = 0.02458286\n",
      "Iteration 4915, loss = 0.02457263\n",
      "Iteration 4916, loss = 0.02456241\n",
      "Iteration 4917, loss = 0.02455219\n",
      "Iteration 4918, loss = 0.02454199\n",
      "Iteration 4919, loss = 0.02453178\n",
      "Iteration 4920, loss = 0.02452158\n",
      "Iteration 4921, loss = 0.02451139\n",
      "Iteration 4922, loss = 0.02450120\n",
      "Iteration 4923, loss = 0.02449102\n",
      "Iteration 4924, loss = 0.02448085\n",
      "Iteration 4925, loss = 0.02447068\n",
      "Iteration 4926, loss = 0.02446051\n",
      "Iteration 4927, loss = 0.02445035\n",
      "Iteration 4928, loss = 0.02444020\n",
      "Iteration 4929, loss = 0.02443005\n",
      "Iteration 4930, loss = 0.02441991\n",
      "Iteration 4931, loss = 0.02440978\n",
      "Iteration 4932, loss = 0.02439965\n",
      "Iteration 4933, loss = 0.02438952\n",
      "Iteration 4934, loss = 0.02437940\n",
      "Iteration 4935, loss = 0.02436929\n",
      "Iteration 4936, loss = 0.02435918\n",
      "Iteration 4937, loss = 0.02434908\n",
      "Iteration 4938, loss = 0.02433898\n",
      "Iteration 4939, loss = 0.02432889\n",
      "Iteration 4940, loss = 0.02431880\n",
      "Iteration 4941, loss = 0.02430873\n",
      "Iteration 4942, loss = 0.02429865\n",
      "Iteration 4943, loss = 0.02428858\n",
      "Iteration 4944, loss = 0.02427852\n",
      "Iteration 4945, loss = 0.02426846\n",
      "Iteration 4946, loss = 0.02425841\n",
      "Iteration 4947, loss = 0.02424836\n",
      "Iteration 4948, loss = 0.02423832\n",
      "Iteration 4949, loss = 0.02422829\n",
      "Iteration 4950, loss = 0.02421826\n",
      "Iteration 4951, loss = 0.02420824\n",
      "Iteration 4952, loss = 0.02419822\n",
      "Iteration 4953, loss = 0.02418820\n",
      "Iteration 4954, loss = 0.02417820\n",
      "Iteration 4955, loss = 0.02416820\n",
      "Iteration 4956, loss = 0.02415820\n",
      "Iteration 4957, loss = 0.02414821\n",
      "Iteration 4958, loss = 0.02413822\n",
      "Iteration 4959, loss = 0.02412825\n",
      "Iteration 4960, loss = 0.02411827\n",
      "Iteration 4961, loss = 0.02410830\n",
      "Iteration 4962, loss = 0.02409834\n",
      "Iteration 4963, loss = 0.02408838\n",
      "Iteration 4964, loss = 0.02407843\n",
      "Iteration 4965, loss = 0.02406849\n",
      "Iteration 4966, loss = 0.02405855\n",
      "Iteration 4967, loss = 0.02404861\n",
      "Iteration 4968, loss = 0.02403868\n",
      "Iteration 4969, loss = 0.02402876\n",
      "Iteration 4970, loss = 0.02401884\n",
      "Iteration 4971, loss = 0.02400893\n",
      "Iteration 4972, loss = 0.02399902\n",
      "Iteration 4973, loss = 0.02398912\n",
      "Iteration 4974, loss = 0.02397922\n",
      "Iteration 4975, loss = 0.02396933\n",
      "Iteration 4976, loss = 0.02395945\n",
      "Iteration 4977, loss = 0.02394957\n",
      "Iteration 4978, loss = 0.02393969\n",
      "Iteration 4979, loss = 0.02392983\n",
      "Iteration 4980, loss = 0.02391996\n",
      "Iteration 4981, loss = 0.02391010\n",
      "Iteration 4982, loss = 0.02390025\n",
      "Iteration 4983, loss = 0.02389041\n",
      "Iteration 4984, loss = 0.02388056\n",
      "Iteration 4985, loss = 0.02387073\n",
      "Iteration 4986, loss = 0.02386090\n",
      "Iteration 4987, loss = 0.02385107\n",
      "Iteration 4988, loss = 0.02384125\n",
      "Iteration 4989, loss = 0.02383144\n",
      "Iteration 4990, loss = 0.02382163\n",
      "Iteration 4991, loss = 0.02381183\n",
      "Iteration 4992, loss = 0.02380203\n",
      "Iteration 4993, loss = 0.02379224\n",
      "Iteration 4994, loss = 0.02378245\n",
      "Iteration 4995, loss = 0.02377267\n",
      "Iteration 4996, loss = 0.02376290\n",
      "Iteration 4997, loss = 0.02375313\n",
      "Iteration 4998, loss = 0.02374336\n",
      "Iteration 4999, loss = 0.02373360\n",
      "Iteration 5000, loss = 0.02372385\n",
      "Iteration 5001, loss = 0.02371410\n",
      "Iteration 5002, loss = 0.02370436\n",
      "Iteration 5003, loss = 0.02369462\n",
      "Iteration 5004, loss = 0.02368489\n",
      "Iteration 5005, loss = 0.02367516\n",
      "Iteration 5006, loss = 0.02366544\n",
      "Iteration 5007, loss = 0.02365572\n",
      "Iteration 5008, loss = 0.02364601\n",
      "Iteration 5009, loss = 0.02363630\n",
      "Iteration 5010, loss = 0.02362660\n",
      "Iteration 5011, loss = 0.02361691\n",
      "Iteration 5012, loss = 0.02360722\n",
      "Iteration 5013, loss = 0.02359753\n",
      "Iteration 5014, loss = 0.02358786\n",
      "Iteration 5015, loss = 0.02357818\n",
      "Iteration 5016, loss = 0.02356851\n",
      "Iteration 5017, loss = 0.02355885\n",
      "Iteration 5018, loss = 0.02354919\n",
      "Iteration 5019, loss = 0.02353954\n",
      "Iteration 5020, loss = 0.02352989\n",
      "Iteration 5021, loss = 0.02352025\n",
      "Iteration 5022, loss = 0.02351062\n",
      "Iteration 5023, loss = 0.02350099\n",
      "Iteration 5024, loss = 0.02349136\n",
      "Iteration 5025, loss = 0.02348174\n",
      "Iteration 5026, loss = 0.02347212\n",
      "Iteration 5027, loss = 0.02346251\n",
      "Iteration 5028, loss = 0.02345291\n",
      "Iteration 5029, loss = 0.02344331\n",
      "Iteration 5030, loss = 0.02343372\n",
      "Iteration 5031, loss = 0.02342413\n",
      "Iteration 5032, loss = 0.02341454\n",
      "Iteration 5033, loss = 0.02340496\n",
      "Iteration 5034, loss = 0.02339539\n",
      "Iteration 5035, loss = 0.02338582\n",
      "Iteration 5036, loss = 0.02337626\n",
      "Iteration 5037, loss = 0.02336670\n",
      "Iteration 5038, loss = 0.02335715\n",
      "Iteration 5039, loss = 0.02334760\n",
      "Iteration 5040, loss = 0.02333806\n",
      "Iteration 5041, loss = 0.02332852\n",
      "Iteration 5042, loss = 0.02331899\n",
      "Iteration 5043, loss = 0.02330947\n",
      "Iteration 5044, loss = 0.02329995\n",
      "Iteration 5045, loss = 0.02329043\n",
      "Iteration 5046, loss = 0.02328092\n",
      "Iteration 5047, loss = 0.02327141\n",
      "Iteration 5048, loss = 0.02326191\n",
      "Iteration 5049, loss = 0.02325242\n",
      "Iteration 5050, loss = 0.02324293\n",
      "Iteration 5051, loss = 0.02323344\n",
      "Iteration 5052, loss = 0.02322396\n",
      "Iteration 5053, loss = 0.02321449\n",
      "Iteration 5054, loss = 0.02320502\n",
      "Iteration 5055, loss = 0.02319555\n",
      "Iteration 5056, loss = 0.02318609\n",
      "Iteration 5057, loss = 0.02317664\n",
      "Iteration 5058, loss = 0.02316719\n",
      "Iteration 5059, loss = 0.02315774\n",
      "Iteration 5060, loss = 0.02314830\n",
      "Iteration 5061, loss = 0.02313887\n",
      "Iteration 5062, loss = 0.02312944\n",
      "Iteration 5063, loss = 0.02312002\n",
      "Iteration 5064, loss = 0.02311060\n",
      "Iteration 5065, loss = 0.02310118\n",
      "Iteration 5066, loss = 0.02309177\n",
      "Iteration 5067, loss = 0.02308237\n",
      "Iteration 5068, loss = 0.02307297\n",
      "Iteration 5069, loss = 0.02306358\n",
      "Iteration 5070, loss = 0.02305419\n",
      "Iteration 5071, loss = 0.02304480\n",
      "Iteration 5072, loss = 0.02303543\n",
      "Iteration 5073, loss = 0.02302605\n",
      "Iteration 5074, loss = 0.02301668\n",
      "Iteration 5075, loss = 0.02300732\n",
      "Iteration 5076, loss = 0.02299796\n",
      "Iteration 5077, loss = 0.02298861\n",
      "Iteration 5078, loss = 0.02297926\n",
      "Iteration 5079, loss = 0.02296991\n",
      "Iteration 5080, loss = 0.02296057\n",
      "Iteration 5081, loss = 0.02295124\n",
      "Iteration 5082, loss = 0.02294191\n",
      "Iteration 5083, loss = 0.02293259\n",
      "Iteration 5084, loss = 0.02292327\n",
      "Iteration 5085, loss = 0.02291395\n",
      "Iteration 5086, loss = 0.02290464\n",
      "Iteration 5087, loss = 0.02289534\n",
      "Iteration 5088, loss = 0.02288604\n",
      "Iteration 5089, loss = 0.02287674\n",
      "Iteration 5090, loss = 0.02286745\n",
      "Iteration 5091, loss = 0.02285817\n",
      "Iteration 5092, loss = 0.02284889\n",
      "Iteration 5093, loss = 0.02283961\n",
      "Iteration 5094, loss = 0.02283034\n",
      "Iteration 5095, loss = 0.02282107\n",
      "Iteration 5096, loss = 0.02281181\n",
      "Iteration 5097, loss = 0.02280256\n",
      "Iteration 5098, loss = 0.02279331\n",
      "Iteration 5099, loss = 0.02278406\n",
      "Iteration 5100, loss = 0.02277482\n",
      "Iteration 5101, loss = 0.02276558\n",
      "Iteration 5102, loss = 0.02275635\n",
      "Iteration 5103, loss = 0.02274712\n",
      "Iteration 5104, loss = 0.02273790\n",
      "Iteration 5105, loss = 0.02272868\n",
      "Iteration 5106, loss = 0.02271947\n",
      "Iteration 5107, loss = 0.02271026\n",
      "Iteration 5108, loss = 0.02270105\n",
      "Iteration 5109, loss = 0.02269186\n",
      "Iteration 5110, loss = 0.02268266\n",
      "Iteration 5111, loss = 0.02267347\n",
      "Iteration 5112, loss = 0.02266429\n",
      "Iteration 5113, loss = 0.02265511\n",
      "Iteration 5114, loss = 0.02264593\n",
      "Iteration 5115, loss = 0.02263676\n",
      "Iteration 5116, loss = 0.02262760\n",
      "Iteration 5117, loss = 0.02261843\n",
      "Iteration 5118, loss = 0.02260928\n",
      "Iteration 5119, loss = 0.02260013\n",
      "Iteration 5120, loss = 0.02259098\n",
      "Iteration 5121, loss = 0.02258184\n",
      "Iteration 5122, loss = 0.02257270\n",
      "Iteration 5123, loss = 0.02256357\n",
      "Iteration 5124, loss = 0.02255444\n",
      "Iteration 5125, loss = 0.02254531\n",
      "Iteration 5126, loss = 0.02253619\n",
      "Iteration 5127, loss = 0.02252708\n",
      "Iteration 5128, loss = 0.02251797\n",
      "Iteration 5129, loss = 0.02250886\n",
      "Iteration 5130, loss = 0.02249976\n",
      "Iteration 5131, loss = 0.02249066\n",
      "Iteration 5132, loss = 0.02248157\n",
      "Iteration 5133, loss = 0.02247249\n",
      "Iteration 5134, loss = 0.02246340\n",
      "Iteration 5135, loss = 0.02245432\n",
      "Iteration 5136, loss = 0.02244525\n",
      "Iteration 5137, loss = 0.02243618\n",
      "Iteration 5138, loss = 0.02242712\n",
      "Iteration 5139, loss = 0.02241806\n",
      "Iteration 5140, loss = 0.02240900\n",
      "Iteration 5141, loss = 0.02239995\n",
      "Iteration 5142, loss = 0.02239090\n",
      "Iteration 5143, loss = 0.02238186\n",
      "Iteration 5144, loss = 0.02237282\n",
      "Iteration 5145, loss = 0.02236379\n",
      "Iteration 5146, loss = 0.02235476\n",
      "Iteration 5147, loss = 0.02234574\n",
      "Iteration 5148, loss = 0.02233672\n",
      "Iteration 5149, loss = 0.02232770\n",
      "Iteration 5150, loss = 0.02231869\n",
      "Iteration 5151, loss = 0.02230968\n",
      "Iteration 5152, loss = 0.02230068\n",
      "Iteration 5153, loss = 0.02229169\n",
      "Iteration 5154, loss = 0.02228269\n",
      "Iteration 5155, loss = 0.02227370\n",
      "Iteration 5156, loss = 0.02226472\n",
      "Iteration 5157, loss = 0.02225574\n",
      "Iteration 5158, loss = 0.02224676\n",
      "Iteration 5159, loss = 0.02223779\n",
      "Iteration 5160, loss = 0.02222882\n",
      "Iteration 5161, loss = 0.02221986\n",
      "Iteration 5162, loss = 0.02221090\n",
      "Iteration 5163, loss = 0.02220195\n",
      "Iteration 5164, loss = 0.02219300\n",
      "Iteration 5165, loss = 0.02218405\n",
      "Iteration 5166, loss = 0.02217511\n",
      "Iteration 5167, loss = 0.02216618\n",
      "Iteration 5168, loss = 0.02215724\n",
      "Iteration 5169, loss = 0.02214832\n",
      "Iteration 5170, loss = 0.02213939\n",
      "Iteration 5171, loss = 0.02213047\n",
      "Iteration 5172, loss = 0.02212156\n",
      "Iteration 5173, loss = 0.02211264\n",
      "Iteration 5174, loss = 0.02210374\n",
      "Iteration 5175, loss = 0.02209484\n",
      "Iteration 5176, loss = 0.02208594\n",
      "Iteration 5177, loss = 0.02207704\n",
      "Iteration 5178, loss = 0.02206815\n",
      "Iteration 5179, loss = 0.02205927\n",
      "Iteration 5180, loss = 0.02205039\n",
      "Iteration 5181, loss = 0.02204151\n",
      "Iteration 5182, loss = 0.02203263\n",
      "Iteration 5183, loss = 0.02202377\n",
      "Iteration 5184, loss = 0.02201490\n",
      "Iteration 5185, loss = 0.02200604\n",
      "Iteration 5186, loss = 0.02199718\n",
      "Iteration 5187, loss = 0.02198833\n",
      "Iteration 5188, loss = 0.02197948\n",
      "Iteration 5189, loss = 0.02197064\n",
      "Iteration 5190, loss = 0.02196180\n",
      "Iteration 5191, loss = 0.02195296\n",
      "Iteration 5192, loss = 0.02194413\n",
      "Iteration 5193, loss = 0.02193530\n",
      "Iteration 5194, loss = 0.02192648\n",
      "Iteration 5195, loss = 0.02191766\n",
      "Iteration 5196, loss = 0.02190884\n",
      "Iteration 5197, loss = 0.02190003\n",
      "Iteration 5198, loss = 0.02189122\n",
      "Iteration 5199, loss = 0.02188242\n",
      "Iteration 5200, loss = 0.02187362\n",
      "Iteration 5201, loss = 0.02186483\n",
      "Iteration 5202, loss = 0.02185604\n",
      "Iteration 5203, loss = 0.02184725\n",
      "Iteration 5204, loss = 0.02183846\n",
      "Iteration 5205, loss = 0.02182969\n",
      "Iteration 5206, loss = 0.02182091\n",
      "Iteration 5207, loss = 0.02181214\n",
      "Iteration 5208, loss = 0.02180337\n",
      "Iteration 5209, loss = 0.02179461\n",
      "Iteration 5210, loss = 0.02178585\n",
      "Iteration 5211, loss = 0.02177709\n",
      "Iteration 5212, loss = 0.02176834\n",
      "Iteration 5213, loss = 0.02175959\n",
      "Iteration 5214, loss = 0.02175085\n",
      "Iteration 5215, loss = 0.02174211\n",
      "Iteration 5216, loss = 0.02173337\n",
      "Iteration 5217, loss = 0.02172464\n",
      "Iteration 5218, loss = 0.02171591\n",
      "Iteration 5219, loss = 0.02170719\n",
      "Iteration 5220, loss = 0.02169847\n",
      "Iteration 5221, loss = 0.02168975\n",
      "Iteration 5222, loss = 0.02168104\n",
      "Iteration 5223, loss = 0.02167233\n",
      "Iteration 5224, loss = 0.02166363\n",
      "Iteration 5225, loss = 0.02165493\n",
      "Iteration 5226, loss = 0.02164623\n",
      "Iteration 5227, loss = 0.02163754\n",
      "Iteration 5228, loss = 0.02162885\n",
      "Iteration 5229, loss = 0.02162016\n",
      "Iteration 5230, loss = 0.02161148\n",
      "Iteration 5231, loss = 0.02160280\n",
      "Iteration 5232, loss = 0.02159413\n",
      "Iteration 5233, loss = 0.02158546\n",
      "Iteration 5234, loss = 0.02157679\n",
      "Iteration 5235, loss = 0.02156813\n",
      "Iteration 5236, loss = 0.02155947\n",
      "Iteration 5237, loss = 0.02155081\n",
      "Iteration 5238, loss = 0.02154216\n",
      "Iteration 5239, loss = 0.02153351\n",
      "Iteration 5240, loss = 0.02152487\n",
      "Iteration 5241, loss = 0.02151623\n",
      "Iteration 5242, loss = 0.02150759\n",
      "Iteration 5243, loss = 0.02149896\n",
      "Iteration 5244, loss = 0.02149033\n",
      "Iteration 5245, loss = 0.02148170\n",
      "Iteration 5246, loss = 0.02147308\n",
      "Iteration 5247, loss = 0.02146446\n",
      "Iteration 5248, loss = 0.02145585\n",
      "Iteration 5249, loss = 0.02144723\n",
      "Iteration 5250, loss = 0.02143863\n",
      "Iteration 5251, loss = 0.02143002\n",
      "Iteration 5252, loss = 0.02142142\n",
      "Iteration 5253, loss = 0.02141283\n",
      "Iteration 5254, loss = 0.02140423\n",
      "Iteration 5255, loss = 0.02139564\n",
      "Iteration 5256, loss = 0.02138706\n",
      "Iteration 5257, loss = 0.02137847\n",
      "Iteration 5258, loss = 0.02136990\n",
      "Iteration 5259, loss = 0.02136132\n",
      "Iteration 5260, loss = 0.02135275\n",
      "Iteration 5261, loss = 0.02134418\n",
      "Iteration 5262, loss = 0.02133562\n",
      "Iteration 5263, loss = 0.02132705\n",
      "Iteration 5264, loss = 0.02131850\n",
      "Iteration 5265, loss = 0.02130994\n",
      "Iteration 5266, loss = 0.02130139\n",
      "Iteration 5267, loss = 0.02129285\n",
      "Iteration 5268, loss = 0.02128430\n",
      "Iteration 5269, loss = 0.02127576\n",
      "Iteration 5270, loss = 0.02126723\n",
      "Iteration 5271, loss = 0.02125869\n",
      "Iteration 5272, loss = 0.02125016\n",
      "Iteration 5273, loss = 0.02124164\n",
      "Iteration 5274, loss = 0.02123311\n",
      "Iteration 5275, loss = 0.02122459\n",
      "Iteration 5276, loss = 0.02121608\n",
      "Iteration 5277, loss = 0.02120757\n",
      "Iteration 5278, loss = 0.02119906\n",
      "Iteration 5279, loss = 0.02119055\n",
      "Iteration 5280, loss = 0.02118205\n",
      "Iteration 5281, loss = 0.02117355\n",
      "Iteration 5282, loss = 0.02116506\n",
      "Iteration 5283, loss = 0.02115656\n",
      "Iteration 5284, loss = 0.02114807\n",
      "Iteration 5285, loss = 0.02113959\n",
      "Iteration 5286, loss = 0.02113111\n",
      "Iteration 5287, loss = 0.02112263\n",
      "Iteration 5288, loss = 0.02111415\n",
      "Iteration 5289, loss = 0.02110568\n",
      "Iteration 5290, loss = 0.02109721\n",
      "Iteration 5291, loss = 0.02108875\n",
      "Iteration 5292, loss = 0.02108029\n",
      "Iteration 5293, loss = 0.02107183\n",
      "Iteration 5294, loss = 0.02106337\n",
      "Iteration 5295, loss = 0.02105492\n",
      "Iteration 5296, loss = 0.02104647\n",
      "Iteration 5297, loss = 0.02103802\n",
      "Iteration 5298, loss = 0.02102958\n",
      "Iteration 5299, loss = 0.02102114\n",
      "Iteration 5300, loss = 0.02101271\n",
      "Iteration 5301, loss = 0.02100427\n",
      "Iteration 5302, loss = 0.02099585\n",
      "Iteration 5303, loss = 0.02098742\n",
      "Iteration 5304, loss = 0.02097900\n",
      "Iteration 5305, loss = 0.02097058\n",
      "Iteration 5306, loss = 0.02096216\n",
      "Iteration 5307, loss = 0.02095375\n",
      "Iteration 5308, loss = 0.02094534\n",
      "Iteration 5309, loss = 0.02093693\n",
      "Iteration 5310, loss = 0.02092853\n",
      "Iteration 5311, loss = 0.02092012\n",
      "Iteration 5312, loss = 0.02091173\n",
      "Iteration 5313, loss = 0.02090333\n",
      "Iteration 5314, loss = 0.02089494\n",
      "Iteration 5315, loss = 0.02088655\n",
      "Iteration 5316, loss = 0.02087817\n",
      "Iteration 5317, loss = 0.02086979\n",
      "Iteration 5318, loss = 0.02086141\n",
      "Iteration 5319, loss = 0.02085303\n",
      "Iteration 5320, loss = 0.02084466\n",
      "Iteration 5321, loss = 0.02083629\n",
      "Iteration 5322, loss = 0.02082792\n",
      "Iteration 5323, loss = 0.02081956\n",
      "Iteration 5324, loss = 0.02081120\n",
      "Iteration 5325, loss = 0.02080284\n",
      "Iteration 5326, loss = 0.02079449\n",
      "Iteration 5327, loss = 0.02078614\n",
      "Iteration 5328, loss = 0.02077779\n",
      "Iteration 5329, loss = 0.02076944\n",
      "Iteration 5330, loss = 0.02076110\n",
      "Iteration 5331, loss = 0.02075276\n",
      "Iteration 5332, loss = 0.02074443\n",
      "Iteration 5333, loss = 0.02073609\n",
      "Iteration 5334, loss = 0.02072776\n",
      "Iteration 5335, loss = 0.02071944\n",
      "Iteration 5336, loss = 0.02071111\n",
      "Iteration 5337, loss = 0.02070279\n",
      "Iteration 5338, loss = 0.02069447\n",
      "Iteration 5339, loss = 0.02068616\n",
      "Iteration 5340, loss = 0.02067784\n",
      "Iteration 5341, loss = 0.02066953\n",
      "Iteration 5342, loss = 0.02066123\n",
      "Iteration 5343, loss = 0.02065292\n",
      "Iteration 5344, loss = 0.02064462\n",
      "Iteration 5345, loss = 0.02063633\n",
      "Iteration 5346, loss = 0.02062803\n",
      "Iteration 5347, loss = 0.02061974\n",
      "Iteration 5348, loss = 0.02061145\n",
      "Iteration 5349, loss = 0.02060316\n",
      "Iteration 5350, loss = 0.02059488\n",
      "Iteration 5351, loss = 0.02058660\n",
      "Iteration 5352, loss = 0.02057832\n",
      "Iteration 5353, loss = 0.02057005\n",
      "Iteration 5354, loss = 0.02056177\n",
      "Iteration 5355, loss = 0.02055350\n",
      "Iteration 5356, loss = 0.02054524\n",
      "Iteration 5357, loss = 0.02053697\n",
      "Iteration 5358, loss = 0.02052871\n",
      "Iteration 5359, loss = 0.02052046\n",
      "Iteration 5360, loss = 0.02051220\n",
      "Iteration 5361, loss = 0.02050395\n",
      "Iteration 5362, loss = 0.02049570\n",
      "Iteration 5363, loss = 0.02048745\n",
      "Iteration 5364, loss = 0.02047921\n",
      "Iteration 5365, loss = 0.02047097\n",
      "Iteration 5366, loss = 0.02046273\n",
      "Iteration 5367, loss = 0.02045449\n",
      "Iteration 5368, loss = 0.02044626\n",
      "Iteration 5369, loss = 0.02043803\n",
      "Iteration 5370, loss = 0.02042980\n",
      "Iteration 5371, loss = 0.02042157\n",
      "Iteration 5372, loss = 0.02041335\n",
      "Iteration 5373, loss = 0.02040513\n",
      "Iteration 5374, loss = 0.02039692\n",
      "Iteration 5375, loss = 0.02038870\n",
      "Iteration 5376, loss = 0.02038049\n",
      "Iteration 5377, loss = 0.02037228\n",
      "Iteration 5378, loss = 0.02036407\n",
      "Iteration 5379, loss = 0.02035587\n",
      "Iteration 5380, loss = 0.02034767\n",
      "Iteration 5381, loss = 0.02033947\n",
      "Iteration 5382, loss = 0.02033128\n",
      "Iteration 5383, loss = 0.02032308\n",
      "Iteration 5384, loss = 0.02031489\n",
      "Iteration 5385, loss = 0.02030670\n",
      "Iteration 5386, loss = 0.02029852\n",
      "Iteration 5387, loss = 0.02029034\n",
      "Iteration 5388, loss = 0.02028216\n",
      "Iteration 5389, loss = 0.02027398\n",
      "Iteration 5390, loss = 0.02026580\n",
      "Iteration 5391, loss = 0.02025763\n",
      "Iteration 5392, loss = 0.02024946\n",
      "Iteration 5393, loss = 0.02024129\n",
      "Iteration 5394, loss = 0.02023313\n",
      "Iteration 5395, loss = 0.02022497\n",
      "Iteration 5396, loss = 0.02021681\n",
      "Iteration 5397, loss = 0.02020865\n",
      "Iteration 5398, loss = 0.02020050\n",
      "Iteration 5399, loss = 0.02019234\n",
      "Iteration 5400, loss = 0.02018419\n",
      "Iteration 5401, loss = 0.02017605\n",
      "Iteration 5402, loss = 0.02016790\n",
      "Iteration 5403, loss = 0.02015976\n",
      "Iteration 5404, loss = 0.02015162\n",
      "Iteration 5405, loss = 0.02014348\n",
      "Iteration 5406, loss = 0.02013535\n",
      "Iteration 5407, loss = 0.02012722\n",
      "Iteration 5408, loss = 0.02011909\n",
      "Iteration 5409, loss = 0.02011096\n",
      "Iteration 5410, loss = 0.02010283\n",
      "Iteration 5411, loss = 0.02009471\n",
      "Iteration 5412, loss = 0.02008659\n",
      "Iteration 5413, loss = 0.02007847\n",
      "Iteration 5414, loss = 0.02007036\n",
      "Iteration 5415, loss = 0.02006224\n",
      "Iteration 5416, loss = 0.02005413\n",
      "Iteration 5417, loss = 0.02004603\n",
      "Iteration 5418, loss = 0.02003792\n",
      "Iteration 5419, loss = 0.02002982\n",
      "Iteration 5420, loss = 0.02002172\n",
      "Iteration 5421, loss = 0.02001362\n",
      "Iteration 5422, loss = 0.02000552\n",
      "Iteration 5423, loss = 0.01999743\n",
      "Iteration 5424, loss = 0.01998934\n",
      "Iteration 5425, loss = 0.01998125\n",
      "Iteration 5426, loss = 0.01997316\n",
      "Iteration 5427, loss = 0.01996507\n",
      "Iteration 5428, loss = 0.01995699\n",
      "Iteration 5429, loss = 0.01994891\n",
      "Iteration 5430, loss = 0.01994083\n",
      "Iteration 5431, loss = 0.01993276\n",
      "Iteration 5432, loss = 0.01992469\n",
      "Iteration 5433, loss = 0.01991662\n",
      "Iteration 5434, loss = 0.01990855\n",
      "Iteration 5435, loss = 0.01990048\n",
      "Iteration 5436, loss = 0.01989242\n",
      "Iteration 5437, loss = 0.01988435\n",
      "Iteration 5438, loss = 0.01987630\n",
      "Iteration 5439, loss = 0.01986824\n",
      "Iteration 5440, loss = 0.01986018\n",
      "Iteration 5441, loss = 0.01985213\n",
      "Iteration 5442, loss = 0.01984408\n",
      "Iteration 5443, loss = 0.01983603\n",
      "Iteration 5444, loss = 0.01982798\n",
      "Iteration 5445, loss = 0.01981994\n",
      "Iteration 5446, loss = 0.01981190\n",
      "Iteration 5447, loss = 0.01980386\n",
      "Iteration 5448, loss = 0.01979582\n",
      "Iteration 5449, loss = 0.01978779\n",
      "Iteration 5450, loss = 0.01977975\n",
      "Iteration 5451, loss = 0.01977172\n",
      "Iteration 5452, loss = 0.01976369\n",
      "Iteration 5453, loss = 0.01975567\n",
      "Iteration 5454, loss = 0.01974764\n",
      "Iteration 5455, loss = 0.01973962\n",
      "Iteration 5456, loss = 0.01973160\n",
      "Iteration 5457, loss = 0.01972358\n",
      "Iteration 5458, loss = 0.01971557\n",
      "Iteration 5459, loss = 0.01970755\n",
      "Iteration 5460, loss = 0.01969954\n",
      "Iteration 5461, loss = 0.01969153\n",
      "Iteration 5462, loss = 0.01968352\n",
      "Iteration 5463, loss = 0.01967552\n",
      "Iteration 5464, loss = 0.01966752\n",
      "Iteration 5465, loss = 0.01965951\n",
      "Iteration 5466, loss = 0.01965151\n",
      "Iteration 5467, loss = 0.01964352\n",
      "Iteration 5468, loss = 0.01963552\n",
      "Iteration 5469, loss = 0.01962753\n",
      "Iteration 5470, loss = 0.01961954\n",
      "Iteration 5471, loss = 0.01961155\n",
      "Iteration 5472, loss = 0.01960356\n",
      "Iteration 5473, loss = 0.01959558\n",
      "Iteration 5474, loss = 0.01958760\n",
      "Iteration 5475, loss = 0.01957961\n",
      "Iteration 5476, loss = 0.01957164\n",
      "Iteration 5477, loss = 0.01956366\n",
      "Iteration 5478, loss = 0.01955568\n",
      "Iteration 5479, loss = 0.01954771\n",
      "Iteration 5480, loss = 0.01953974\n",
      "Iteration 5481, loss = 0.01953177\n",
      "Iteration 5482, loss = 0.01952380\n",
      "Iteration 5483, loss = 0.01951584\n",
      "Iteration 5484, loss = 0.01950788\n",
      "Iteration 5485, loss = 0.01949992\n",
      "Iteration 5486, loss = 0.01949196\n",
      "Iteration 5487, loss = 0.01948400\n",
      "Iteration 5488, loss = 0.01947604\n",
      "Iteration 5489, loss = 0.01946809\n",
      "Iteration 5490, loss = 0.01946014\n",
      "Iteration 5491, loss = 0.01945219\n",
      "Iteration 5492, loss = 0.01944424\n",
      "Iteration 5493, loss = 0.01943630\n",
      "Iteration 5494, loss = 0.01942835\n",
      "Iteration 5495, loss = 0.01942041\n",
      "Iteration 5496, loss = 0.01941247\n",
      "Iteration 5497, loss = 0.01940453\n",
      "Iteration 5498, loss = 0.01939660\n",
      "Iteration 5499, loss = 0.01938866\n",
      "Iteration 5500, loss = 0.01938073\n",
      "Iteration 5501, loss = 0.01937280\n",
      "Iteration 5502, loss = 0.01936487\n",
      "Iteration 5503, loss = 0.01935695\n",
      "Iteration 5504, loss = 0.01934902\n",
      "Iteration 5505, loss = 0.01934110\n",
      "Iteration 5506, loss = 0.01933318\n",
      "Iteration 5507, loss = 0.01932526\n",
      "Iteration 5508, loss = 0.01931734\n",
      "Iteration 5509, loss = 0.01930942\n",
      "Iteration 5510, loss = 0.01930151\n",
      "Iteration 5511, loss = 0.01929360\n",
      "Iteration 5512, loss = 0.01928569\n",
      "Iteration 5513, loss = 0.01927778\n",
      "Iteration 5514, loss = 0.01926987\n",
      "Iteration 5515, loss = 0.01926197\n",
      "Iteration 5516, loss = 0.01925406\n",
      "Iteration 5517, loss = 0.01924616\n",
      "Iteration 5518, loss = 0.01923826\n",
      "Iteration 5519, loss = 0.01923037\n",
      "Iteration 5520, loss = 0.01922247\n",
      "Iteration 5521, loss = 0.01921457\n",
      "Iteration 5522, loss = 0.01920668\n",
      "Iteration 5523, loss = 0.01919879\n",
      "Iteration 5524, loss = 0.01919090\n",
      "Iteration 5525, loss = 0.01918301\n",
      "Iteration 5526, loss = 0.01917513\n",
      "Iteration 5527, loss = 0.01916725\n",
      "Iteration 5528, loss = 0.01915936\n",
      "Iteration 5529, loss = 0.01915148\n",
      "Iteration 5530, loss = 0.01914360\n",
      "Iteration 5531, loss = 0.01913573\n",
      "Iteration 5532, loss = 0.01912785\n",
      "Iteration 5533, loss = 0.01911998\n",
      "Iteration 5534, loss = 0.01911211\n",
      "Iteration 5535, loss = 0.01910423\n",
      "Iteration 5536, loss = 0.01909637\n",
      "Iteration 5537, loss = 0.01908850\n",
      "Iteration 5538, loss = 0.01908063\n",
      "Iteration 5539, loss = 0.01907277\n",
      "Iteration 5540, loss = 0.01906491\n",
      "Iteration 5541, loss = 0.01905705\n",
      "Iteration 5542, loss = 0.01904919\n",
      "Iteration 5543, loss = 0.01904133\n",
      "Iteration 5544, loss = 0.01903348\n",
      "Iteration 5545, loss = 0.01902562\n",
      "Iteration 5546, loss = 0.01901777\n",
      "Iteration 5547, loss = 0.01900992\n",
      "Iteration 5548, loss = 0.01900207\n",
      "Iteration 5549, loss = 0.01899422\n",
      "Iteration 5550, loss = 0.01898638\n",
      "Iteration 5551, loss = 0.01897853\n",
      "Iteration 5552, loss = 0.01897069\n",
      "Iteration 5553, loss = 0.01896285\n",
      "Iteration 5554, loss = 0.01895501\n",
      "Iteration 5555, loss = 0.01894717\n",
      "Iteration 5556, loss = 0.01893934\n",
      "Iteration 5557, loss = 0.01893150\n",
      "Iteration 5558, loss = 0.01892367\n",
      "Iteration 5559, loss = 0.01891584\n",
      "Iteration 5560, loss = 0.01890801\n",
      "Iteration 5561, loss = 0.01890018\n",
      "Iteration 5562, loss = 0.01889235\n",
      "Iteration 5563, loss = 0.01888453\n",
      "Iteration 5564, loss = 0.01887670\n",
      "Iteration 5565, loss = 0.01886888\n",
      "Iteration 5566, loss = 0.01886106\n",
      "Iteration 5567, loss = 0.01885324\n",
      "Iteration 5568, loss = 0.01884542\n",
      "Iteration 5569, loss = 0.01883761\n",
      "Iteration 5570, loss = 0.01882979\n",
      "Iteration 5571, loss = 0.01882198\n",
      "Iteration 5572, loss = 0.01881417\n",
      "Iteration 5573, loss = 0.01880636\n",
      "Iteration 5574, loss = 0.01879855\n",
      "Iteration 5575, loss = 0.01879074\n",
      "Iteration 5576, loss = 0.01878293\n",
      "Iteration 5577, loss = 0.01877513\n",
      "Iteration 5578, loss = 0.01876733\n",
      "Iteration 5579, loss = 0.01875953\n",
      "Iteration 5580, loss = 0.01875173\n",
      "Iteration 5581, loss = 0.01874393\n",
      "Iteration 5582, loss = 0.01873613\n",
      "Iteration 5583, loss = 0.01872833\n",
      "Iteration 5584, loss = 0.01872054\n",
      "Iteration 5585, loss = 0.01871275\n",
      "Iteration 5586, loss = 0.01870496\n",
      "Iteration 5587, loss = 0.01869717\n",
      "Iteration 5588, loss = 0.01868938\n",
      "Iteration 5589, loss = 0.01868159\n",
      "Iteration 5590, loss = 0.01867381\n",
      "Iteration 5591, loss = 0.01866602\n",
      "Iteration 5592, loss = 0.01865824\n",
      "Iteration 5593, loss = 0.01865046\n",
      "Iteration 5594, loss = 0.01864268\n",
      "Iteration 5595, loss = 0.01863490\n",
      "Iteration 5596, loss = 0.01862712\n",
      "Iteration 5597, loss = 0.01861935\n",
      "Iteration 5598, loss = 0.01861157\n",
      "Iteration 5599, loss = 0.01860380\n",
      "Iteration 5600, loss = 0.01859603\n",
      "Iteration 5601, loss = 0.01858826\n",
      "Iteration 5602, loss = 0.01858049\n",
      "Iteration 5603, loss = 0.01857272\n",
      "Iteration 5604, loss = 0.01856495\n",
      "Iteration 5605, loss = 0.01855719\n",
      "Iteration 5606, loss = 0.01854943\n",
      "Iteration 5607, loss = 0.01854166\n",
      "Iteration 5608, loss = 0.01853390\n",
      "Iteration 5609, loss = 0.01852614\n",
      "Iteration 5610, loss = 0.01851838\n",
      "Iteration 5611, loss = 0.01851063\n",
      "Iteration 5612, loss = 0.01850287\n",
      "Iteration 5613, loss = 0.01849512\n",
      "Iteration 5614, loss = 0.01848737\n",
      "Iteration 5615, loss = 0.01847961\n",
      "Iteration 5616, loss = 0.01847186\n",
      "Iteration 5617, loss = 0.01846411\n",
      "Iteration 5618, loss = 0.01845637\n",
      "Iteration 5619, loss = 0.01844862\n",
      "Iteration 5620, loss = 0.01844088\n",
      "Iteration 5621, loss = 0.01843313\n",
      "Iteration 5622, loss = 0.01842539\n",
      "Iteration 5623, loss = 0.01841765\n",
      "Iteration 5624, loss = 0.01840991\n",
      "Iteration 5625, loss = 0.01840217\n",
      "Iteration 5626, loss = 0.01839443\n",
      "Iteration 5627, loss = 0.01838670\n",
      "Iteration 5628, loss = 0.01837896\n",
      "Iteration 5629, loss = 0.01837123\n",
      "Iteration 5630, loss = 0.01836350\n",
      "Iteration 5631, loss = 0.01835576\n",
      "Iteration 5632, loss = 0.01834803\n",
      "Iteration 5633, loss = 0.01834031\n",
      "Iteration 5634, loss = 0.01833258\n",
      "Iteration 5635, loss = 0.01832485\n",
      "Iteration 5636, loss = 0.01831713\n",
      "Iteration 5637, loss = 0.01830940\n",
      "Iteration 5638, loss = 0.01830168\n",
      "Iteration 5639, loss = 0.01829396\n",
      "Iteration 5640, loss = 0.01828624\n",
      "Iteration 5641, loss = 0.01827852\n",
      "Iteration 5642, loss = 0.01827080\n",
      "Iteration 5643, loss = 0.01826309\n",
      "Iteration 5644, loss = 0.01825537\n",
      "Iteration 5645, loss = 0.01824766\n",
      "Iteration 5646, loss = 0.01823995\n",
      "Iteration 5647, loss = 0.01823223\n",
      "Iteration 5648, loss = 0.01822452\n",
      "Iteration 5649, loss = 0.01821681\n",
      "Iteration 5650, loss = 0.01820911\n",
      "Iteration 5651, loss = 0.01820140\n",
      "Iteration 5652, loss = 0.01819369\n",
      "Iteration 5653, loss = 0.01818599\n",
      "Iteration 5654, loss = 0.01817829\n",
      "Iteration 5655, loss = 0.01817058\n",
      "Iteration 5656, loss = 0.01816288\n",
      "Iteration 5657, loss = 0.01815518\n",
      "Iteration 5658, loss = 0.01814749\n",
      "Iteration 5659, loss = 0.01813979\n",
      "Iteration 5660, loss = 0.01813209\n",
      "Iteration 5661, loss = 0.01812440\n",
      "Iteration 5662, loss = 0.01811670\n",
      "Iteration 5663, loss = 0.01810901\n",
      "Iteration 5664, loss = 0.01810132\n",
      "Iteration 5665, loss = 0.01809363\n",
      "Iteration 5666, loss = 0.01808594\n",
      "Iteration 5667, loss = 0.01807825\n",
      "Iteration 5668, loss = 0.01807056\n",
      "Iteration 5669, loss = 0.01806288\n",
      "Iteration 5670, loss = 0.01805519\n",
      "Iteration 5671, loss = 0.01804751\n",
      "Iteration 5672, loss = 0.01803982\n",
      "Iteration 5673, loss = 0.01803214\n",
      "Iteration 5674, loss = 0.01802446\n",
      "Iteration 5675, loss = 0.01801678\n",
      "Iteration 5676, loss = 0.01800910\n",
      "Iteration 5677, loss = 0.01800143\n",
      "Iteration 5678, loss = 0.01799375\n",
      "Iteration 5679, loss = 0.01798608\n",
      "Iteration 5680, loss = 0.01797840\n",
      "Iteration 5681, loss = 0.01797073\n",
      "Iteration 5682, loss = 0.01796306\n",
      "Iteration 5683, loss = 0.01795539\n",
      "Iteration 5684, loss = 0.01794772\n",
      "Iteration 5685, loss = 0.01794005\n",
      "Iteration 5686, loss = 0.01793238\n",
      "Iteration 5687, loss = 0.01792471\n",
      "Iteration 5688, loss = 0.01791705\n",
      "Iteration 5689, loss = 0.01790938\n",
      "Iteration 5690, loss = 0.01790172\n",
      "Iteration 5691, loss = 0.01789406\n",
      "Iteration 5692, loss = 0.01788640\n",
      "Iteration 5693, loss = 0.01787874\n",
      "Iteration 5694, loss = 0.01787108\n",
      "Iteration 5695, loss = 0.01786342\n",
      "Iteration 5696, loss = 0.01785576\n",
      "Iteration 5697, loss = 0.01784811\n",
      "Iteration 5698, loss = 0.01784045\n",
      "Iteration 5699, loss = 0.01783280\n",
      "Iteration 5700, loss = 0.01782515\n",
      "Iteration 5701, loss = 0.01781749\n",
      "Iteration 5702, loss = 0.01780984\n",
      "Iteration 5703, loss = 0.01780219\n",
      "Iteration 5704, loss = 0.01779454\n",
      "Iteration 5705, loss = 0.01778690\n",
      "Iteration 5706, loss = 0.01777925\n",
      "Iteration 5707, loss = 0.01777160\n",
      "Iteration 5708, loss = 0.01776396\n",
      "Iteration 5709, loss = 0.01775632\n",
      "Iteration 5710, loss = 0.01774867\n",
      "Iteration 5711, loss = 0.01774103\n",
      "Iteration 5712, loss = 0.01773339\n",
      "Iteration 5713, loss = 0.01772575\n",
      "Iteration 5714, loss = 0.01771811\n",
      "Iteration 5715, loss = 0.01771047\n",
      "Iteration 5716, loss = 0.01770284\n",
      "Iteration 5717, loss = 0.01769520\n",
      "Iteration 5718, loss = 0.01768757\n",
      "Iteration 5719, loss = 0.01767993\n",
      "Iteration 5720, loss = 0.01767230\n",
      "Iteration 5721, loss = 0.01766467\n",
      "Iteration 5722, loss = 0.01765704\n",
      "Iteration 5723, loss = 0.01764941\n",
      "Iteration 5724, loss = 0.01764178\n",
      "Iteration 5725, loss = 0.01763415\n",
      "Iteration 5726, loss = 0.01762653\n",
      "Iteration 5727, loss = 0.01761890\n",
      "Iteration 5728, loss = 0.01761127\n",
      "Iteration 5729, loss = 0.01760365\n",
      "Iteration 5730, loss = 0.01759603\n",
      "Iteration 5731, loss = 0.01758841\n",
      "Iteration 5732, loss = 0.01758078\n",
      "Iteration 5733, loss = 0.01757316\n",
      "Iteration 5734, loss = 0.01756555\n",
      "Iteration 5735, loss = 0.01755793\n",
      "Iteration 5736, loss = 0.01755031\n",
      "Iteration 5737, loss = 0.01754269\n",
      "Iteration 5738, loss = 0.01753508\n",
      "Iteration 5739, loss = 0.01752746\n",
      "Iteration 5740, loss = 0.01751985\n",
      "Iteration 5741, loss = 0.01751224\n",
      "Iteration 5742, loss = 0.01750463\n",
      "Iteration 5743, loss = 0.01749702\n",
      "Iteration 5744, loss = 0.01748941\n",
      "Iteration 5745, loss = 0.01748180\n",
      "Iteration 5746, loss = 0.01747419\n",
      "Iteration 5747, loss = 0.01746658\n",
      "Iteration 5748, loss = 0.01745898\n",
      "Iteration 5749, loss = 0.01745137\n",
      "Iteration 5750, loss = 0.01744377\n",
      "Iteration 5751, loss = 0.01743616\n",
      "Iteration 5752, loss = 0.01742856\n",
      "Iteration 5753, loss = 0.01742096\n",
      "Iteration 5754, loss = 0.01741336\n",
      "Iteration 5755, loss = 0.01740576\n",
      "Iteration 5756, loss = 0.01739816\n",
      "Iteration 5757, loss = 0.01739056\n",
      "Iteration 5758, loss = 0.01738297\n",
      "Iteration 5759, loss = 0.01737537\n",
      "Iteration 5760, loss = 0.01736778\n",
      "Iteration 5761, loss = 0.01736018\n",
      "Iteration 5762, loss = 0.01735259\n",
      "Iteration 5763, loss = 0.01734500\n",
      "Iteration 5764, loss = 0.01733740\n",
      "Iteration 5765, loss = 0.01732981\n",
      "Iteration 5766, loss = 0.01732222\n",
      "Iteration 5767, loss = 0.01731464\n",
      "Iteration 5768, loss = 0.01730705\n",
      "Iteration 5769, loss = 0.01729946\n",
      "Iteration 5770, loss = 0.01729187\n",
      "Iteration 5771, loss = 0.01728429\n",
      "Iteration 5772, loss = 0.01727671\n",
      "Iteration 5773, loss = 0.01726912\n",
      "Iteration 5774, loss = 0.01726154\n",
      "Iteration 5775, loss = 0.01725396\n",
      "Iteration 5776, loss = 0.01724638\n",
      "Iteration 5777, loss = 0.01723880\n",
      "Iteration 5778, loss = 0.01723122\n",
      "Iteration 5779, loss = 0.01722364\n",
      "Iteration 5780, loss = 0.01721606\n",
      "Iteration 5781, loss = 0.01720849\n",
      "Iteration 5782, loss = 0.01720091\n",
      "Iteration 5783, loss = 0.01719334\n",
      "Iteration 5784, loss = 0.01718576\n",
      "Iteration 5785, loss = 0.01717819\n",
      "Iteration 5786, loss = 0.01717062\n",
      "Iteration 5787, loss = 0.01716305\n",
      "Iteration 5788, loss = 0.01715548\n",
      "Iteration 5789, loss = 0.01714791\n",
      "Iteration 5790, loss = 0.01714034\n",
      "Iteration 5791, loss = 0.01713277\n",
      "Iteration 5792, loss = 0.01712520\n",
      "Iteration 5793, loss = 0.01711764\n",
      "Iteration 5794, loss = 0.01711007\n",
      "Iteration 5795, loss = 0.01710251\n",
      "Iteration 5796, loss = 0.01709494\n",
      "Iteration 5797, loss = 0.01708738\n",
      "Iteration 5798, loss = 0.01707982\n",
      "Iteration 5799, loss = 0.01707226\n",
      "Iteration 5800, loss = 0.01706470\n",
      "Iteration 5801, loss = 0.01705714\n",
      "Iteration 5802, loss = 0.01704958\n",
      "Iteration 5803, loss = 0.01704202\n",
      "Iteration 5804, loss = 0.01703447\n",
      "Iteration 5805, loss = 0.01702691\n",
      "Iteration 5806, loss = 0.01701936\n",
      "Iteration 5807, loss = 0.01701180\n",
      "Iteration 5808, loss = 0.01700425\n",
      "Iteration 5809, loss = 0.01699670\n",
      "Iteration 5810, loss = 0.01698915\n",
      "Iteration 5811, loss = 0.01698160\n",
      "Iteration 5812, loss = 0.01697405\n",
      "Iteration 5813, loss = 0.01696650\n",
      "Iteration 5814, loss = 0.01695895\n",
      "Iteration 5815, loss = 0.01695140\n",
      "Iteration 5816, loss = 0.01694385\n",
      "Iteration 5817, loss = 0.01693631\n",
      "Iteration 5818, loss = 0.01692876\n",
      "Iteration 5819, loss = 0.01692122\n",
      "Iteration 5820, loss = 0.01691368\n",
      "Iteration 5821, loss = 0.01690613\n",
      "Iteration 5822, loss = 0.01689859\n",
      "Iteration 5823, loss = 0.01689105\n",
      "Iteration 5824, loss = 0.01688351\n",
      "Iteration 5825, loss = 0.01687597\n",
      "Iteration 5826, loss = 0.01686844\n",
      "Iteration 5827, loss = 0.01686090\n",
      "Iteration 5828, loss = 0.01685336\n",
      "Iteration 5829, loss = 0.01684583\n",
      "Iteration 5830, loss = 0.01683829\n",
      "Iteration 5831, loss = 0.01683076\n",
      "Iteration 5832, loss = 0.01682322\n",
      "Iteration 5833, loss = 0.01681569\n",
      "Iteration 5834, loss = 0.01680816\n",
      "Iteration 5835, loss = 0.01680063\n",
      "Iteration 5836, loss = 0.01679310\n",
      "Iteration 5837, loss = 0.01678557\n",
      "Iteration 5838, loss = 0.01677804\n",
      "Iteration 5839, loss = 0.01677051\n",
      "Iteration 5840, loss = 0.01676299\n",
      "Iteration 5841, loss = 0.01675546\n",
      "Iteration 5842, loss = 0.01674794\n",
      "Iteration 5843, loss = 0.01674041\n",
      "Iteration 5844, loss = 0.01673289\n",
      "Iteration 5845, loss = 0.01672537\n",
      "Iteration 5846, loss = 0.01671785\n",
      "Iteration 5847, loss = 0.01671033\n",
      "Iteration 5848, loss = 0.01670281\n",
      "Iteration 5849, loss = 0.01669529\n",
      "Iteration 5850, loss = 0.01668777\n",
      "Iteration 5851, loss = 0.01668025\n",
      "Iteration 5852, loss = 0.01667273\n",
      "Iteration 5853, loss = 0.01666522\n",
      "Iteration 5854, loss = 0.01665770\n",
      "Iteration 5855, loss = 0.01665019\n",
      "Iteration 5856, loss = 0.01664267\n",
      "Iteration 5857, loss = 0.01663516\n",
      "Iteration 5858, loss = 0.01662765\n",
      "Iteration 5859, loss = 0.01662014\n",
      "Iteration 5860, loss = 0.01661263\n",
      "Iteration 5861, loss = 0.01660512\n",
      "Iteration 5862, loss = 0.01659761\n",
      "Iteration 5863, loss = 0.01659010\n",
      "Iteration 5864, loss = 0.01658260\n",
      "Iteration 5865, loss = 0.01657509\n",
      "Iteration 5866, loss = 0.01656759\n",
      "Iteration 5867, loss = 0.01656008\n",
      "Iteration 5868, loss = 0.01655258\n",
      "Iteration 5869, loss = 0.01654507\n",
      "Iteration 5870, loss = 0.01653757\n",
      "Iteration 5871, loss = 0.01653007\n",
      "Iteration 5872, loss = 0.01652257\n",
      "Iteration 5873, loss = 0.01651507\n",
      "Iteration 5874, loss = 0.01650757\n",
      "Iteration 5875, loss = 0.01650007\n",
      "Iteration 5876, loss = 0.01649258\n",
      "Iteration 5877, loss = 0.01648508\n",
      "Iteration 5878, loss = 0.01647759\n",
      "Iteration 5879, loss = 0.01647009\n",
      "Iteration 5880, loss = 0.01646260\n",
      "Iteration 5881, loss = 0.01645510\n",
      "Iteration 5882, loss = 0.01644761\n",
      "Iteration 5883, loss = 0.01644012\n",
      "Iteration 5884, loss = 0.01643263\n",
      "Iteration 5885, loss = 0.01642514\n",
      "Iteration 5886, loss = 0.01641765\n",
      "Iteration 5887, loss = 0.01641016\n",
      "Iteration 5888, loss = 0.01640268\n",
      "Iteration 5889, loss = 0.01639519\n",
      "Iteration 5890, loss = 0.01638770\n",
      "Iteration 5891, loss = 0.01638022\n",
      "Iteration 5892, loss = 0.01637273\n",
      "Iteration 5893, loss = 0.01636525\n",
      "Iteration 5894, loss = 0.01635777\n",
      "Iteration 5895, loss = 0.01635029\n",
      "Iteration 5896, loss = 0.01634281\n",
      "Iteration 5897, loss = 0.01633533\n",
      "Iteration 5898, loss = 0.01632785\n",
      "Iteration 5899, loss = 0.01632037\n",
      "Iteration 5900, loss = 0.01631289\n",
      "Iteration 5901, loss = 0.01630541\n",
      "Iteration 5902, loss = 0.01629794\n",
      "Iteration 5903, loss = 0.01629046\n",
      "Iteration 5904, loss = 0.01628299\n",
      "Iteration 5905, loss = 0.01627552\n",
      "Iteration 5906, loss = 0.01626804\n",
      "Iteration 5907, loss = 0.01626057\n",
      "Iteration 5908, loss = 0.01625310\n",
      "Iteration 5909, loss = 0.01624563\n",
      "Iteration 5910, loss = 0.01623816\n",
      "Iteration 5911, loss = 0.01623069\n",
      "Iteration 5912, loss = 0.01622323\n",
      "Iteration 5913, loss = 0.01621576\n",
      "Iteration 5914, loss = 0.01620829\n",
      "Iteration 5915, loss = 0.01620083\n",
      "Iteration 5916, loss = 0.01619336\n",
      "Iteration 5917, loss = 0.01618590\n",
      "Iteration 5918, loss = 0.01617844\n",
      "Iteration 5919, loss = 0.01617098\n",
      "Iteration 5920, loss = 0.01616351\n",
      "Iteration 5921, loss = 0.01615605\n",
      "Iteration 5922, loss = 0.01614859\n",
      "Iteration 5923, loss = 0.01614114\n",
      "Iteration 5924, loss = 0.01613368\n",
      "Iteration 5925, loss = 0.01612622\n",
      "Iteration 5926, loss = 0.01611877\n",
      "Iteration 5927, loss = 0.01611131\n",
      "Iteration 5928, loss = 0.01610386\n",
      "Iteration 5929, loss = 0.01609640\n",
      "Iteration 5930, loss = 0.01608895\n",
      "Iteration 5931, loss = 0.01608150\n",
      "Iteration 5932, loss = 0.01607405\n",
      "Iteration 5933, loss = 0.01606660\n",
      "Iteration 5934, loss = 0.01605915\n",
      "Iteration 5935, loss = 0.01605170\n",
      "Iteration 5936, loss = 0.01604425\n",
      "Iteration 5937, loss = 0.01603681\n",
      "Iteration 5938, loss = 0.01602936\n",
      "Iteration 5939, loss = 0.01602191\n",
      "Iteration 5940, loss = 0.01601447\n",
      "Iteration 5941, loss = 0.01600703\n",
      "Iteration 5942, loss = 0.01599958\n",
      "Iteration 5943, loss = 0.01599214\n",
      "Iteration 5944, loss = 0.01598470\n",
      "Iteration 5945, loss = 0.01597726\n",
      "Iteration 5946, loss = 0.01596982\n",
      "Iteration 5947, loss = 0.01596238\n",
      "Iteration 5948, loss = 0.01595495\n",
      "Iteration 5949, loss = 0.01594751\n",
      "Iteration 5950, loss = 0.01594007\n",
      "Iteration 5951, loss = 0.01593264\n",
      "Iteration 5952, loss = 0.01592520\n",
      "Iteration 5953, loss = 0.01591777\n",
      "Iteration 5954, loss = 0.01591034\n",
      "Iteration 5955, loss = 0.01590291\n",
      "Iteration 5956, loss = 0.01589548\n",
      "Iteration 5957, loss = 0.01588805\n",
      "Iteration 5958, loss = 0.01588062\n",
      "Iteration 5959, loss = 0.01587319\n",
      "Iteration 5960, loss = 0.01586576\n",
      "Iteration 5961, loss = 0.01585834\n",
      "Iteration 5962, loss = 0.01585091\n",
      "Iteration 5963, loss = 0.01584349\n",
      "Iteration 5964, loss = 0.01583606\n",
      "Iteration 5965, loss = 0.01582864\n",
      "Iteration 5966, loss = 0.01582122\n",
      "Iteration 5967, loss = 0.01581380\n",
      "Iteration 5968, loss = 0.01580638\n",
      "Iteration 5969, loss = 0.01579896\n",
      "Iteration 5970, loss = 0.01579154\n",
      "Iteration 5971, loss = 0.01578412\n",
      "Iteration 5972, loss = 0.01577670\n",
      "Iteration 5973, loss = 0.01576929\n",
      "Iteration 5974, loss = 0.01576187\n",
      "Iteration 5975, loss = 0.01575446\n",
      "Iteration 5976, loss = 0.01574705\n",
      "Iteration 5977, loss = 0.01573963\n",
      "Iteration 5978, loss = 0.01573222\n",
      "Iteration 5979, loss = 0.01572481\n",
      "Iteration 5980, loss = 0.01571740\n",
      "Iteration 5981, loss = 0.01570999\n",
      "Iteration 5982, loss = 0.01570258\n",
      "Iteration 5983, loss = 0.01569518\n",
      "Iteration 5984, loss = 0.01568777\n",
      "Iteration 5985, loss = 0.01568037\n",
      "Iteration 5986, loss = 0.01567296\n",
      "Iteration 5987, loss = 0.01566556\n",
      "Iteration 5988, loss = 0.01565816\n",
      "Iteration 5989, loss = 0.01565075\n",
      "Iteration 5990, loss = 0.01564335\n",
      "Iteration 5991, loss = 0.01563595\n",
      "Iteration 5992, loss = 0.01562855\n",
      "Iteration 5993, loss = 0.01562116\n",
      "Iteration 5994, loss = 0.01561376\n",
      "Iteration 5995, loss = 0.01560636\n",
      "Iteration 5996, loss = 0.01559897\n",
      "Iteration 5997, loss = 0.01559157\n",
      "Iteration 5998, loss = 0.01558418\n",
      "Iteration 5999, loss = 0.01557679\n",
      "Iteration 6000, loss = 0.01556939\n",
      "Iteration 6001, loss = 0.01556200\n",
      "Iteration 6002, loss = 0.01555461\n",
      "Iteration 6003, loss = 0.01554722\n",
      "Iteration 6004, loss = 0.01553984\n",
      "Iteration 6005, loss = 0.01553245\n",
      "Iteration 6006, loss = 0.01552506\n",
      "Iteration 6007, loss = 0.01551768\n",
      "Iteration 6008, loss = 0.01551029\n",
      "Iteration 6009, loss = 0.01550291\n",
      "Iteration 6010, loss = 0.01549553\n",
      "Iteration 6011, loss = 0.01548814\n",
      "Iteration 6012, loss = 0.01548076\n",
      "Iteration 6013, loss = 0.01547338\n",
      "Iteration 6014, loss = 0.01546601\n",
      "Iteration 6015, loss = 0.01545863\n",
      "Iteration 6016, loss = 0.01545125\n",
      "Iteration 6017, loss = 0.01544387\n",
      "Iteration 6018, loss = 0.01543650\n",
      "Iteration 6019, loss = 0.01542912\n",
      "Iteration 6020, loss = 0.01542175\n",
      "Iteration 6021, loss = 0.01541438\n",
      "Iteration 6022, loss = 0.01540701\n",
      "Iteration 6023, loss = 0.01539964\n",
      "Iteration 6024, loss = 0.01539227\n",
      "Iteration 6025, loss = 0.01538490\n",
      "Iteration 6026, loss = 0.01537753\n",
      "Iteration 6027, loss = 0.01537016\n",
      "Iteration 6028, loss = 0.01536280\n",
      "Iteration 6029, loss = 0.01535543\n",
      "Iteration 6030, loss = 0.01534807\n",
      "Iteration 6031, loss = 0.01534071\n",
      "Iteration 6032, loss = 0.01533334\n",
      "Iteration 6033, loss = 0.01532598\n",
      "Iteration 6034, loss = 0.01531862\n",
      "Iteration 6035, loss = 0.01531126\n",
      "Iteration 6036, loss = 0.01530391\n",
      "Iteration 6037, loss = 0.01529655\n",
      "Iteration 6038, loss = 0.01528919\n",
      "Iteration 6039, loss = 0.01528184\n",
      "Iteration 6040, loss = 0.01527448\n",
      "Iteration 6041, loss = 0.01526713\n",
      "Iteration 6042, loss = 0.01525978\n",
      "Iteration 6043, loss = 0.01525243\n",
      "Iteration 6044, loss = 0.01524508\n",
      "Iteration 6045, loss = 0.01523773\n",
      "Iteration 6046, loss = 0.01523038\n",
      "Iteration 6047, loss = 0.01522303\n",
      "Iteration 6048, loss = 0.01521568\n",
      "Iteration 6049, loss = 0.01520834\n",
      "Iteration 6050, loss = 0.01520099\n",
      "Iteration 6051, loss = 0.01519365\n",
      "Iteration 6052, loss = 0.01518631\n",
      "Iteration 6053, loss = 0.01517897\n",
      "Iteration 6054, loss = 0.01517163\n",
      "Iteration 6055, loss = 0.01516429\n",
      "Iteration 6056, loss = 0.01515695\n",
      "Iteration 6057, loss = 0.01514961\n",
      "Iteration 6058, loss = 0.01514227\n",
      "Iteration 6059, loss = 0.01513494\n",
      "Iteration 6060, loss = 0.01512760\n",
      "Iteration 6061, loss = 0.01512027\n",
      "Iteration 6062, loss = 0.01511294\n",
      "Iteration 6063, loss = 0.01510560\n",
      "Iteration 6064, loss = 0.01509827\n",
      "Iteration 6065, loss = 0.01509094\n",
      "Iteration 6066, loss = 0.01508362\n",
      "Iteration 6067, loss = 0.01507629\n",
      "Iteration 6068, loss = 0.01506896\n",
      "Iteration 6069, loss = 0.01506164\n",
      "Iteration 6070, loss = 0.01505431\n",
      "Iteration 6071, loss = 0.01504699\n",
      "Iteration 6072, loss = 0.01503967\n",
      "Iteration 6073, loss = 0.01503234\n",
      "Iteration 6074, loss = 0.01502502\n",
      "Iteration 6075, loss = 0.01501770\n",
      "Iteration 6076, loss = 0.01501039\n",
      "Iteration 6077, loss = 0.01500307\n",
      "Iteration 6078, loss = 0.01499575\n",
      "Iteration 6079, loss = 0.01498844\n",
      "Iteration 6080, loss = 0.01498112\n",
      "Iteration 6081, loss = 0.01497381\n",
      "Iteration 6082, loss = 0.01496650\n",
      "Iteration 6083, loss = 0.01495919\n",
      "Iteration 6084, loss = 0.01495188\n",
      "Iteration 6085, loss = 0.01494457\n",
      "Iteration 6086, loss = 0.01493726\n",
      "Iteration 6087, loss = 0.01492995\n",
      "Iteration 6088, loss = 0.01492265\n",
      "Iteration 6089, loss = 0.01491534\n",
      "Iteration 6090, loss = 0.01490804\n",
      "Iteration 6091, loss = 0.01490074\n",
      "Iteration 6092, loss = 0.01489343\n",
      "Iteration 6093, loss = 0.01488613\n",
      "Iteration 6094, loss = 0.01487883\n",
      "Iteration 6095, loss = 0.01487154\n",
      "Iteration 6096, loss = 0.01486424\n",
      "Iteration 6097, loss = 0.01485694\n",
      "Iteration 6098, loss = 0.01484965\n",
      "Iteration 6099, loss = 0.01484235\n",
      "Iteration 6100, loss = 0.01483506\n",
      "Iteration 6101, loss = 0.01482777\n",
      "Iteration 6102, loss = 0.01482048\n",
      "Iteration 6103, loss = 0.01481319\n",
      "Iteration 6104, loss = 0.01480590\n",
      "Iteration 6105, loss = 0.01479861\n",
      "Iteration 6106, loss = 0.01479132\n",
      "Iteration 6107, loss = 0.01478404\n",
      "Iteration 6108, loss = 0.01477675\n",
      "Iteration 6109, loss = 0.01476947\n",
      "Iteration 6110, loss = 0.01476219\n",
      "Iteration 6111, loss = 0.01475491\n",
      "Iteration 6112, loss = 0.01474763\n",
      "Iteration 6113, loss = 0.01474035\n",
      "Iteration 6114, loss = 0.01473307\n",
      "Iteration 6115, loss = 0.01472579\n",
      "Iteration 6116, loss = 0.01471852\n",
      "Iteration 6117, loss = 0.01471124\n",
      "Iteration 6118, loss = 0.01470397\n",
      "Iteration 6119, loss = 0.01469670\n",
      "Iteration 6120, loss = 0.01468943\n",
      "Iteration 6121, loss = 0.01468216\n",
      "Iteration 6122, loss = 0.01467489\n",
      "Iteration 6123, loss = 0.01466762\n",
      "Iteration 6124, loss = 0.01466035\n",
      "Iteration 6125, loss = 0.01465309\n",
      "Iteration 6126, loss = 0.01464582\n",
      "Iteration 6127, loss = 0.01463856\n",
      "Iteration 6128, loss = 0.01463130\n",
      "Iteration 6129, loss = 0.01462403\n",
      "Iteration 6130, loss = 0.01461677\n",
      "Iteration 6131, loss = 0.01460952\n",
      "Iteration 6132, loss = 0.01460226\n",
      "Iteration 6133, loss = 0.01459500\n",
      "Iteration 6134, loss = 0.01458775\n",
      "Iteration 6135, loss = 0.01458049\n",
      "Iteration 6136, loss = 0.01457324\n",
      "Iteration 6137, loss = 0.01456599\n",
      "Iteration 6138, loss = 0.01455874\n",
      "Iteration 6139, loss = 0.01455149\n",
      "Iteration 6140, loss = 0.01454424\n",
      "Iteration 6141, loss = 0.01453699\n",
      "Iteration 6142, loss = 0.01452974\n",
      "Iteration 6143, loss = 0.01452250\n",
      "Iteration 6144, loss = 0.01451525\n",
      "Iteration 6145, loss = 0.01450801\n",
      "Iteration 6146, loss = 0.01450077\n",
      "Iteration 6147, loss = 0.01449353\n",
      "Iteration 6148, loss = 0.01448629\n",
      "Iteration 6149, loss = 0.01447905\n",
      "Iteration 6150, loss = 0.01447181\n",
      "Iteration 6151, loss = 0.01446458\n",
      "Iteration 6152, loss = 0.01445734\n",
      "Iteration 6153, loss = 0.01445011\n",
      "Iteration 6154, loss = 0.01444288\n",
      "Iteration 6155, loss = 0.01443565\n",
      "Iteration 6156, loss = 0.01442842\n",
      "Iteration 6157, loss = 0.01442119\n",
      "Iteration 6158, loss = 0.01441396\n",
      "Iteration 6159, loss = 0.01440674\n",
      "Iteration 6160, loss = 0.01439951\n",
      "Iteration 6161, loss = 0.01439229\n",
      "Iteration 6162, loss = 0.01438506\n",
      "Iteration 6163, loss = 0.01437784\n",
      "Iteration 6164, loss = 0.01437062\n",
      "Iteration 6165, loss = 0.01436340\n",
      "Iteration 6166, loss = 0.01435618\n",
      "Iteration 6167, loss = 0.01434897\n",
      "Iteration 6168, loss = 0.01434175\n",
      "Iteration 6169, loss = 0.01433454\n",
      "Iteration 6170, loss = 0.01432732\n",
      "Iteration 6171, loss = 0.01432011\n",
      "Iteration 6172, loss = 0.01431290\n",
      "Iteration 6173, loss = 0.01430569\n",
      "Iteration 6174, loss = 0.01429848\n",
      "Iteration 6175, loss = 0.01429128\n",
      "Iteration 6176, loss = 0.01428407\n",
      "Iteration 6177, loss = 0.01427687\n",
      "Iteration 6178, loss = 0.01426966\n",
      "Iteration 6179, loss = 0.01426246\n",
      "Iteration 6180, loss = 0.01425526\n",
      "Iteration 6181, loss = 0.01424806\n",
      "Iteration 6182, loss = 0.01424086\n",
      "Iteration 6183, loss = 0.01423366\n",
      "Iteration 6184, loss = 0.01422647\n",
      "Iteration 6185, loss = 0.01421927\n",
      "Iteration 6186, loss = 0.01421208\n",
      "Iteration 6187, loss = 0.01420489\n",
      "Iteration 6188, loss = 0.01419770\n",
      "Iteration 6189, loss = 0.01419051\n",
      "Iteration 6190, loss = 0.01418332\n",
      "Iteration 6191, loss = 0.01417613\n",
      "Iteration 6192, loss = 0.01416895\n",
      "Iteration 6193, loss = 0.01416176\n",
      "Iteration 6194, loss = 0.01415458\n",
      "Iteration 6195, loss = 0.01414740\n",
      "Iteration 6196, loss = 0.01414021\n",
      "Iteration 6197, loss = 0.01413303\n",
      "Iteration 6198, loss = 0.01412586\n",
      "Iteration 6199, loss = 0.01411868\n",
      "Iteration 6200, loss = 0.01411150\n",
      "Iteration 6201, loss = 0.01410433\n",
      "Iteration 6202, loss = 0.01409716\n",
      "Iteration 6203, loss = 0.01408998\n",
      "Iteration 6204, loss = 0.01408281\n",
      "Iteration 6205, loss = 0.01407564\n",
      "Iteration 6206, loss = 0.01406848\n",
      "Iteration 6207, loss = 0.01406131\n",
      "Iteration 6208, loss = 0.01405414\n",
      "Iteration 6209, loss = 0.01404698\n",
      "Iteration 6210, loss = 0.01403982\n",
      "Iteration 6211, loss = 0.01403265\n",
      "Iteration 6212, loss = 0.01402549\n",
      "Iteration 6213, loss = 0.01401833\n",
      "Iteration 6214, loss = 0.01401118\n",
      "Iteration 6215, loss = 0.01400402\n",
      "Iteration 6216, loss = 0.01399686\n",
      "Iteration 6217, loss = 0.01398971\n",
      "Iteration 6218, loss = 0.01398256\n",
      "Iteration 6219, loss = 0.01397541\n",
      "Iteration 6220, loss = 0.01396826\n",
      "Iteration 6221, loss = 0.01396111\n",
      "Iteration 6222, loss = 0.01395396\n",
      "Iteration 6223, loss = 0.01394681\n",
      "Iteration 6224, loss = 0.01393967\n",
      "Iteration 6225, loss = 0.01393253\n",
      "Iteration 6226, loss = 0.01392538\n",
      "Iteration 6227, loss = 0.01391824\n",
      "Iteration 6228, loss = 0.01391110\n",
      "Iteration 6229, loss = 0.01390397\n",
      "Iteration 6230, loss = 0.01389683\n",
      "Iteration 6231, loss = 0.01388969\n",
      "Iteration 6232, loss = 0.01388256\n",
      "Iteration 6233, loss = 0.01387543\n",
      "Iteration 6234, loss = 0.01386830\n",
      "Iteration 6235, loss = 0.01386117\n",
      "Iteration 6236, loss = 0.01385404\n",
      "Iteration 6237, loss = 0.01384691\n",
      "Iteration 6238, loss = 0.01383978\n",
      "Iteration 6239, loss = 0.01383266\n",
      "Iteration 6240, loss = 0.01382554\n",
      "Iteration 6241, loss = 0.01381841\n",
      "Iteration 6242, loss = 0.01381129\n",
      "Iteration 6243, loss = 0.01380417\n",
      "Iteration 6244, loss = 0.01379706\n",
      "Iteration 6245, loss = 0.01378994\n",
      "Iteration 6246, loss = 0.01378282\n",
      "Iteration 6247, loss = 0.01377571\n",
      "Iteration 6248, loss = 0.01376860\n",
      "Iteration 6249, loss = 0.01376149\n",
      "Iteration 6250, loss = 0.01375438\n",
      "Iteration 6251, loss = 0.01374727\n",
      "Iteration 6252, loss = 0.01374016\n",
      "Iteration 6253, loss = 0.01373306\n",
      "Iteration 6254, loss = 0.01372595\n",
      "Iteration 6255, loss = 0.01371885\n",
      "Iteration 6256, loss = 0.01371175\n",
      "Iteration 6257, loss = 0.01370465\n",
      "Iteration 6258, loss = 0.01369755\n",
      "Iteration 6259, loss = 0.01369045\n",
      "Iteration 6260, loss = 0.01368336\n",
      "Iteration 6261, loss = 0.01367626\n",
      "Iteration 6262, loss = 0.01366917\n",
      "Iteration 6263, loss = 0.01366208\n",
      "Iteration 6264, loss = 0.01365499\n",
      "Iteration 6265, loss = 0.01364790\n",
      "Iteration 6266, loss = 0.01364081\n",
      "Iteration 6267, loss = 0.01363372\n",
      "Iteration 6268, loss = 0.01362664\n",
      "Iteration 6269, loss = 0.01361956\n",
      "Iteration 6270, loss = 0.01361247\n",
      "Iteration 6271, loss = 0.01360539\n",
      "Iteration 6272, loss = 0.01359831\n",
      "Iteration 6273, loss = 0.01359124\n",
      "Iteration 6274, loss = 0.01358416\n",
      "Iteration 6275, loss = 0.01357709\n",
      "Iteration 6276, loss = 0.01357001\n",
      "Iteration 6277, loss = 0.01356294\n",
      "Iteration 6278, loss = 0.01355587\n",
      "Iteration 6279, loss = 0.01354880\n",
      "Iteration 6280, loss = 0.01354173\n",
      "Iteration 6281, loss = 0.01353467\n",
      "Iteration 6282, loss = 0.01352760\n",
      "Iteration 6283, loss = 0.01352054\n",
      "Iteration 6284, loss = 0.01351348\n",
      "Iteration 6285, loss = 0.01350642\n",
      "Iteration 6286, loss = 0.01349936\n",
      "Iteration 6287, loss = 0.01349230\n",
      "Iteration 6288, loss = 0.01348524\n",
      "Iteration 6289, loss = 0.01347819\n",
      "Iteration 6290, loss = 0.01347114\n",
      "Iteration 6291, loss = 0.01346409\n",
      "Iteration 6292, loss = 0.01345704\n",
      "Iteration 6293, loss = 0.01344999\n",
      "Iteration 6294, loss = 0.01344294\n",
      "Iteration 6295, loss = 0.01343589\n",
      "Iteration 6296, loss = 0.01342885\n",
      "Iteration 6297, loss = 0.01342181\n",
      "Iteration 6298, loss = 0.01341476\n",
      "Iteration 6299, loss = 0.01340772\n",
      "Iteration 6300, loss = 0.01340069\n",
      "Iteration 6301, loss = 0.01339365\n",
      "Iteration 6302, loss = 0.01338661\n",
      "Iteration 6303, loss = 0.01337958\n",
      "Iteration 6304, loss = 0.01337255\n",
      "Iteration 6305, loss = 0.01336551\n",
      "Iteration 6306, loss = 0.01335848\n",
      "Iteration 6307, loss = 0.01335146\n",
      "Iteration 6308, loss = 0.01334443\n",
      "Iteration 6309, loss = 0.01333740\n",
      "Iteration 6310, loss = 0.01333038\n",
      "Iteration 6311, loss = 0.01332336\n",
      "Iteration 6312, loss = 0.01331634\n",
      "Iteration 6313, loss = 0.01330932\n",
      "Iteration 6314, loss = 0.01330230\n",
      "Iteration 6315, loss = 0.01329528\n",
      "Iteration 6316, loss = 0.01328827\n",
      "Iteration 6317, loss = 0.01328126\n",
      "Iteration 6318, loss = 0.01327424\n",
      "Iteration 6319, loss = 0.01326723\n",
      "Iteration 6320, loss = 0.01326022\n",
      "Iteration 6321, loss = 0.01325322\n",
      "Iteration 6322, loss = 0.01324621\n",
      "Iteration 6323, loss = 0.01323921\n",
      "Iteration 6324, loss = 0.01323220\n",
      "Iteration 6325, loss = 0.01322520\n",
      "Iteration 6326, loss = 0.01321820\n",
      "Iteration 6327, loss = 0.01321121\n",
      "Iteration 6328, loss = 0.01320421\n",
      "Iteration 6329, loss = 0.01319721\n",
      "Iteration 6330, loss = 0.01319022\n",
      "Iteration 6331, loss = 0.01318323\n",
      "Iteration 6332, loss = 0.01317624\n",
      "Iteration 6333, loss = 0.01316925\n",
      "Iteration 6334, loss = 0.01316226\n",
      "Iteration 6335, loss = 0.01315527\n",
      "Iteration 6336, loss = 0.01314829\n",
      "Iteration 6337, loss = 0.01314131\n",
      "Iteration 6338, loss = 0.01313433\n",
      "Iteration 6339, loss = 0.01312735\n",
      "Iteration 6340, loss = 0.01312037\n",
      "Iteration 6341, loss = 0.01311339\n",
      "Iteration 6342, loss = 0.01310642\n",
      "Iteration 6343, loss = 0.01309944\n",
      "Iteration 6344, loss = 0.01309247\n",
      "Iteration 6345, loss = 0.01308550\n",
      "Iteration 6346, loss = 0.01307853\n",
      "Iteration 6347, loss = 0.01307156\n",
      "Iteration 6348, loss = 0.01306460\n",
      "Iteration 6349, loss = 0.01305763\n",
      "Iteration 6350, loss = 0.01305067\n",
      "Iteration 6351, loss = 0.01304371\n",
      "Iteration 6352, loss = 0.01303675\n",
      "Iteration 6353, loss = 0.01302979\n",
      "Iteration 6354, loss = 0.01302283\n",
      "Iteration 6355, loss = 0.01301588\n",
      "Iteration 6356, loss = 0.01300892\n",
      "Iteration 6357, loss = 0.01300197\n",
      "Iteration 6358, loss = 0.01299502\n",
      "Iteration 6359, loss = 0.01298807\n",
      "Iteration 6360, loss = 0.01298113\n",
      "Iteration 6361, loss = 0.01297418\n",
      "Iteration 6362, loss = 0.01296724\n",
      "Iteration 6363, loss = 0.01296029\n",
      "Iteration 6364, loss = 0.01295335\n",
      "Iteration 6365, loss = 0.01294641\n",
      "Iteration 6366, loss = 0.01293948\n",
      "Iteration 6367, loss = 0.01293254\n",
      "Iteration 6368, loss = 0.01292561\n",
      "Iteration 6369, loss = 0.01291867\n",
      "Iteration 6370, loss = 0.01291174\n",
      "Iteration 6371, loss = 0.01290481\n",
      "Iteration 6372, loss = 0.01289788\n",
      "Iteration 6373, loss = 0.01289096\n",
      "Iteration 6374, loss = 0.01288403\n",
      "Iteration 6375, loss = 0.01287711\n",
      "Iteration 6376, loss = 0.01287019\n",
      "Iteration 6377, loss = 0.01286327\n",
      "Iteration 6378, loss = 0.01285635\n",
      "Iteration 6379, loss = 0.01284943\n",
      "Iteration 6380, loss = 0.01284252\n",
      "Iteration 6381, loss = 0.01283560\n",
      "Iteration 6382, loss = 0.01282869\n",
      "Iteration 6383, loss = 0.01282178\n",
      "Iteration 6384, loss = 0.01281487\n",
      "Iteration 6385, loss = 0.01280797\n",
      "Iteration 6386, loss = 0.01280106\n",
      "Iteration 6387, loss = 0.01279416\n",
      "Iteration 6388, loss = 0.01278725\n",
      "Iteration 6389, loss = 0.01278035\n",
      "Iteration 6390, loss = 0.01277345\n",
      "Iteration 6391, loss = 0.01276656\n",
      "Iteration 6392, loss = 0.01275966\n",
      "Iteration 6393, loss = 0.01275277\n",
      "Iteration 6394, loss = 0.01274587\n",
      "Iteration 6395, loss = 0.01273898\n",
      "Iteration 6396, loss = 0.01273209\n",
      "Iteration 6397, loss = 0.01272521\n",
      "Iteration 6398, loss = 0.01271832\n",
      "Iteration 6399, loss = 0.01271143\n",
      "Iteration 6400, loss = 0.01270455\n",
      "Iteration 6401, loss = 0.01269767\n",
      "Iteration 6402, loss = 0.01269079\n",
      "Iteration 6403, loss = 0.01268391\n",
      "Iteration 6404, loss = 0.01267704\n",
      "Iteration 6405, loss = 0.01267016\n",
      "Iteration 6406, loss = 0.01266329\n",
      "Iteration 6407, loss = 0.01265642\n",
      "Iteration 6408, loss = 0.01264955\n",
      "Iteration 6409, loss = 0.01264268\n",
      "Iteration 6410, loss = 0.01263582\n",
      "Iteration 6411, loss = 0.01262895\n",
      "Iteration 6412, loss = 0.01262209\n",
      "Iteration 6413, loss = 0.01261523\n",
      "Iteration 6414, loss = 0.01260837\n",
      "Iteration 6415, loss = 0.01260151\n",
      "Iteration 6416, loss = 0.01259465\n",
      "Iteration 6417, loss = 0.01258780\n",
      "Iteration 6418, loss = 0.01258094\n",
      "Iteration 6419, loss = 0.01257409\n",
      "Iteration 6420, loss = 0.01256724\n",
      "Iteration 6421, loss = 0.01256040\n",
      "Iteration 6422, loss = 0.01255355\n",
      "Iteration 6423, loss = 0.01254671\n",
      "Iteration 6424, loss = 0.01253986\n",
      "Iteration 6425, loss = 0.01253302\n",
      "Iteration 6426, loss = 0.01252618\n",
      "Iteration 6427, loss = 0.01251934\n",
      "Iteration 6428, loss = 0.01251251\n",
      "Iteration 6429, loss = 0.01250567\n",
      "Iteration 6430, loss = 0.01249884\n",
      "Iteration 6431, loss = 0.01249201\n",
      "Iteration 6432, loss = 0.01248518\n",
      "Iteration 6433, loss = 0.01247835\n",
      "Iteration 6434, loss = 0.01247153\n",
      "Iteration 6435, loss = 0.01246470\n",
      "Iteration 6436, loss = 0.01245788\n",
      "Iteration 6437, loss = 0.01245106\n",
      "Iteration 6438, loss = 0.01244424\n",
      "Iteration 6439, loss = 0.01243742\n",
      "Iteration 6440, loss = 0.01243061\n",
      "Iteration 6441, loss = 0.01242379\n",
      "Iteration 6442, loss = 0.01241698\n",
      "Iteration 6443, loss = 0.01241017\n",
      "Iteration 6444, loss = 0.01240336\n",
      "Iteration 6445, loss = 0.01239655\n",
      "Iteration 6446, loss = 0.01238975\n",
      "Iteration 6447, loss = 0.01238294\n",
      "Iteration 6448, loss = 0.01237614\n",
      "Iteration 6449, loss = 0.01236934\n",
      "Iteration 6450, loss = 0.01236254\n",
      "Iteration 6451, loss = 0.01235574\n",
      "Iteration 6452, loss = 0.01234895\n",
      "Iteration 6453, loss = 0.01234215\n",
      "Iteration 6454, loss = 0.01233536\n",
      "Iteration 6455, loss = 0.01232857\n",
      "Iteration 6456, loss = 0.01232178\n",
      "Iteration 6457, loss = 0.01231500\n",
      "Iteration 6458, loss = 0.01230821\n",
      "Iteration 6459, loss = 0.01230143\n",
      "Iteration 6460, loss = 0.01229465\n",
      "Iteration 6461, loss = 0.01228787\n",
      "Iteration 6462, loss = 0.01228109\n",
      "Iteration 6463, loss = 0.01227431\n",
      "Iteration 6464, loss = 0.01226754\n",
      "Iteration 6465, loss = 0.01226077\n",
      "Iteration 6466, loss = 0.01225399\n",
      "Iteration 6467, loss = 0.01224722\n",
      "Iteration 6468, loss = 0.01224046\n",
      "Iteration 6469, loss = 0.01223369\n",
      "Iteration 6470, loss = 0.01222693\n",
      "Iteration 6471, loss = 0.01222016\n",
      "Iteration 6472, loss = 0.01221340\n",
      "Iteration 6473, loss = 0.01220665\n",
      "Iteration 6474, loss = 0.01219989\n",
      "Iteration 6475, loss = 0.01219313\n",
      "Iteration 6476, loss = 0.01218638\n",
      "Iteration 6477, loss = 0.01217963\n",
      "Iteration 6478, loss = 0.01217288\n",
      "Iteration 6479, loss = 0.01216613\n",
      "Iteration 6480, loss = 0.01215938\n",
      "Iteration 6481, loss = 0.01215264\n",
      "Iteration 6482, loss = 0.01214589\n",
      "Iteration 6483, loss = 0.01213915\n",
      "Iteration 6484, loss = 0.01213241\n",
      "Iteration 6485, loss = 0.01212567\n",
      "Iteration 6486, loss = 0.01211894\n",
      "Iteration 6487, loss = 0.01211220\n",
      "Iteration 6488, loss = 0.01210547\n",
      "Iteration 6489, loss = 0.01209874\n",
      "Iteration 6490, loss = 0.01209201\n",
      "Iteration 6491, loss = 0.01208528\n",
      "Iteration 6492, loss = 0.01207856\n",
      "Iteration 6493, loss = 0.01207184\n",
      "Iteration 6494, loss = 0.01206511\n",
      "Iteration 6495, loss = 0.01205839\n",
      "Iteration 6496, loss = 0.01205167\n",
      "Iteration 6497, loss = 0.01204496\n",
      "Iteration 6498, loss = 0.01203824\n",
      "Iteration 6499, loss = 0.01203153\n",
      "Iteration 6500, loss = 0.01202482\n",
      "Iteration 6501, loss = 0.01201811\n",
      "Iteration 6502, loss = 0.01201140\n",
      "Iteration 6503, loss = 0.01200470\n",
      "Iteration 6504, loss = 0.01199799\n",
      "Iteration 6505, loss = 0.01199129\n",
      "Iteration 6506, loss = 0.01198459\n",
      "Iteration 6507, loss = 0.01197789\n",
      "Iteration 6508, loss = 0.01197119\n",
      "Iteration 6509, loss = 0.01196450\n",
      "Iteration 6510, loss = 0.01195780\n",
      "Iteration 6511, loss = 0.01195111\n",
      "Iteration 6512, loss = 0.01194442\n",
      "Iteration 6513, loss = 0.01193773\n",
      "Iteration 6514, loss = 0.01193105\n",
      "Iteration 6515, loss = 0.01192436\n",
      "Iteration 6516, loss = 0.01191768\n",
      "Iteration 6517, loss = 0.01191100\n",
      "Iteration 6518, loss = 0.01190432\n",
      "Iteration 6519, loss = 0.01189764\n",
      "Iteration 6520, loss = 0.01189097\n",
      "Iteration 6521, loss = 0.01188430\n",
      "Iteration 6522, loss = 0.01187762\n",
      "Iteration 6523, loss = 0.01187095\n",
      "Iteration 6524, loss = 0.01186429\n",
      "Iteration 6525, loss = 0.01185762\n",
      "Iteration 6526, loss = 0.01185095\n",
      "Iteration 6527, loss = 0.01184429\n",
      "Iteration 6528, loss = 0.01183763\n",
      "Iteration 6529, loss = 0.01183097\n",
      "Iteration 6530, loss = 0.01182431\n",
      "Iteration 6531, loss = 0.01181766\n",
      "Iteration 6532, loss = 0.01181101\n",
      "Iteration 6533, loss = 0.01180435\n",
      "Iteration 6534, loss = 0.01179770\n",
      "Iteration 6535, loss = 0.01179106\n",
      "Iteration 6536, loss = 0.01178441\n",
      "Iteration 6537, loss = 0.01177776\n",
      "Iteration 6538, loss = 0.01177112\n",
      "Iteration 6539, loss = 0.01176448\n",
      "Iteration 6540, loss = 0.01175784\n",
      "Iteration 6541, loss = 0.01175121\n",
      "Iteration 6542, loss = 0.01174457\n",
      "Iteration 6543, loss = 0.01173794\n",
      "Iteration 6544, loss = 0.01173130\n",
      "Iteration 6545, loss = 0.01172467\n",
      "Iteration 6546, loss = 0.01171805\n",
      "Iteration 6547, loss = 0.01171142\n",
      "Iteration 6548, loss = 0.01170480\n",
      "Iteration 6549, loss = 0.01169817\n",
      "Iteration 6550, loss = 0.01169155\n",
      "Iteration 6551, loss = 0.01168493\n",
      "Iteration 6552, loss = 0.01167832\n",
      "Iteration 6553, loss = 0.01167170\n",
      "Iteration 6554, loss = 0.01166509\n",
      "Iteration 6555, loss = 0.01165848\n",
      "Iteration 6556, loss = 0.01165187\n",
      "Iteration 6557, loss = 0.01164526\n",
      "Iteration 6558, loss = 0.01163865\n",
      "Iteration 6559, loss = 0.01163205\n",
      "Iteration 6560, loss = 0.01162545\n",
      "Iteration 6561, loss = 0.01161885\n",
      "Iteration 6562, loss = 0.01161225\n",
      "Iteration 6563, loss = 0.01160565\n",
      "Iteration 6564, loss = 0.01159906\n",
      "Iteration 6565, loss = 0.01159246\n",
      "Iteration 6566, loss = 0.01158587\n",
      "Iteration 6567, loss = 0.01157928\n",
      "Iteration 6568, loss = 0.01157270\n",
      "Iteration 6569, loss = 0.01156611\n",
      "Iteration 6570, loss = 0.01155953\n",
      "Iteration 6571, loss = 0.01155295\n",
      "Iteration 6572, loss = 0.01154637\n",
      "Iteration 6573, loss = 0.01153979\n",
      "Iteration 6574, loss = 0.01153321\n",
      "Iteration 6575, loss = 0.01152664\n",
      "Iteration 6576, loss = 0.01152007\n",
      "Iteration 6577, loss = 0.01151350\n",
      "Iteration 6578, loss = 0.01150693\n",
      "Iteration 6579, loss = 0.01150036\n",
      "Iteration 6580, loss = 0.01149379\n",
      "Iteration 6581, loss = 0.01148723\n",
      "Iteration 6582, loss = 0.01148067\n",
      "Iteration 6583, loss = 0.01147411\n",
      "Iteration 6584, loss = 0.01146755\n",
      "Iteration 6585, loss = 0.01146100\n",
      "Iteration 6586, loss = 0.01145444\n",
      "Iteration 6587, loss = 0.01144789\n",
      "Iteration 6588, loss = 0.01144134\n",
      "Iteration 6589, loss = 0.01143480\n",
      "Iteration 6590, loss = 0.01142825\n",
      "Iteration 6591, loss = 0.01142170\n",
      "Iteration 6592, loss = 0.01141516\n",
      "Iteration 6593, loss = 0.01140862\n",
      "Iteration 6594, loss = 0.01140208\n",
      "Iteration 6595, loss = 0.01139555\n",
      "Iteration 6596, loss = 0.01138901\n",
      "Iteration 6597, loss = 0.01138248\n",
      "Iteration 6598, loss = 0.01137595\n",
      "Iteration 6599, loss = 0.01136942\n",
      "Iteration 6600, loss = 0.01136289\n",
      "Iteration 6601, loss = 0.01135637\n",
      "Iteration 6602, loss = 0.01134984\n",
      "Iteration 6603, loss = 0.01134332\n",
      "Iteration 6604, loss = 0.01133680\n",
      "Iteration 6605, loss = 0.01133028\n",
      "Iteration 6606, loss = 0.01132377\n",
      "Iteration 6607, loss = 0.01131725\n",
      "Iteration 6608, loss = 0.01131074\n",
      "Iteration 6609, loss = 0.01130423\n",
      "Iteration 6610, loss = 0.01129772\n",
      "Iteration 6611, loss = 0.01129122\n",
      "Iteration 6612, loss = 0.01128471\n",
      "Iteration 6613, loss = 0.01127821\n",
      "Iteration 6614, loss = 0.01127171\n",
      "Iteration 6615, loss = 0.01126521\n",
      "Iteration 6616, loss = 0.01125872\n",
      "Iteration 6617, loss = 0.01125222\n",
      "Iteration 6618, loss = 0.01124573\n",
      "Iteration 6619, loss = 0.01123924\n",
      "Iteration 6620, loss = 0.01123275\n",
      "Iteration 6621, loss = 0.01122626\n",
      "Iteration 6622, loss = 0.01121978\n",
      "Iteration 6623, loss = 0.01121329\n",
      "Iteration 6624, loss = 0.01120681\n",
      "Iteration 6625, loss = 0.01120033\n",
      "Iteration 6626, loss = 0.01119385\n",
      "Iteration 6627, loss = 0.01118738\n",
      "Iteration 6628, loss = 0.01118090\n",
      "Iteration 6629, loss = 0.01117443\n",
      "Iteration 6630, loss = 0.01116796\n",
      "Iteration 6631, loss = 0.01116149\n",
      "Iteration 6632, loss = 0.01115503\n",
      "Iteration 6633, loss = 0.01114856\n",
      "Iteration 6634, loss = 0.01114210\n",
      "Iteration 6635, loss = 0.01113564\n",
      "Iteration 6636, loss = 0.01112918\n",
      "Iteration 6637, loss = 0.01112273\n",
      "Iteration 6638, loss = 0.01111627\n",
      "Iteration 6639, loss = 0.01110982\n",
      "Iteration 6640, loss = 0.01110337\n",
      "Iteration 6641, loss = 0.01109692\n",
      "Iteration 6642, loss = 0.01109047\n",
      "Iteration 6643, loss = 0.01108403\n",
      "Iteration 6644, loss = 0.01107759\n",
      "Iteration 6645, loss = 0.01107115\n",
      "Iteration 6646, loss = 0.01106471\n",
      "Iteration 6647, loss = 0.01105827\n",
      "Iteration 6648, loss = 0.01105184\n",
      "Iteration 6649, loss = 0.01104540\n",
      "Iteration 6650, loss = 0.01103897\n",
      "Iteration 6651, loss = 0.01103254\n",
      "Iteration 6652, loss = 0.01102611\n",
      "Iteration 6653, loss = 0.01101969\n",
      "Iteration 6654, loss = 0.01101327\n",
      "Iteration 6655, loss = 0.01100684\n",
      "Iteration 6656, loss = 0.01100043\n",
      "Iteration 6657, loss = 0.01099401\n",
      "Iteration 6658, loss = 0.01098759\n",
      "Iteration 6659, loss = 0.01098118\n",
      "Iteration 6660, loss = 0.01097477\n",
      "Iteration 6661, loss = 0.01096836\n",
      "Iteration 6662, loss = 0.01096195\n",
      "Iteration 6663, loss = 0.01095554\n",
      "Iteration 6664, loss = 0.01094914\n",
      "Iteration 6665, loss = 0.01094274\n",
      "Iteration 6666, loss = 0.01093634\n",
      "Iteration 6667, loss = 0.01092994\n",
      "Iteration 6668, loss = 0.01092354\n",
      "Iteration 6669, loss = 0.01091715\n",
      "Iteration 6670, loss = 0.01091076\n",
      "Iteration 6671, loss = 0.01090437\n",
      "Iteration 6672, loss = 0.01089798\n",
      "Iteration 6673, loss = 0.01089159\n",
      "Iteration 6674, loss = 0.01088521\n",
      "Iteration 6675, loss = 0.01087883\n",
      "Iteration 6676, loss = 0.01087245\n",
      "Iteration 6677, loss = 0.01086607\n",
      "Iteration 6678, loss = 0.01085969\n",
      "Iteration 6679, loss = 0.01085332\n",
      "Iteration 6680, loss = 0.01084695\n",
      "Iteration 6681, loss = 0.01084058\n",
      "Iteration 6682, loss = 0.01083421\n",
      "Iteration 6683, loss = 0.01082784\n",
      "Iteration 6684, loss = 0.01082148\n",
      "Iteration 6685, loss = 0.01081511\n",
      "Iteration 6686, loss = 0.01080875\n",
      "Iteration 6687, loss = 0.01080240\n",
      "Iteration 6688, loss = 0.01079604\n",
      "Iteration 6689, loss = 0.01078968\n",
      "Iteration 6690, loss = 0.01078333\n",
      "Iteration 6691, loss = 0.01077698\n",
      "Iteration 6692, loss = 0.01077063\n",
      "Iteration 6693, loss = 0.01076429\n",
      "Iteration 6694, loss = 0.01075794\n",
      "Iteration 6695, loss = 0.01075160\n",
      "Iteration 6696, loss = 0.01074526\n",
      "Iteration 6697, loss = 0.01073892\n",
      "Iteration 6698, loss = 0.01073258\n",
      "Iteration 6699, loss = 0.01072625\n",
      "Iteration 6700, loss = 0.01071992\n",
      "Iteration 6701, loss = 0.01071359\n",
      "Iteration 6702, loss = 0.01070726\n",
      "Iteration 6703, loss = 0.01070093\n",
      "Iteration 6704, loss = 0.01069461\n",
      "Iteration 6705, loss = 0.01068828\n",
      "Iteration 6706, loss = 0.01068196\n",
      "Iteration 6707, loss = 0.01067564\n",
      "Iteration 6708, loss = 0.01066933\n",
      "Iteration 6709, loss = 0.01066301\n",
      "Iteration 6710, loss = 0.01065670\n",
      "Iteration 6711, loss = 0.01065039\n",
      "Iteration 6712, loss = 0.01064408\n",
      "Iteration 6713, loss = 0.01063777\n",
      "Iteration 6714, loss = 0.01063147\n",
      "Iteration 6715, loss = 0.01062517\n",
      "Iteration 6716, loss = 0.01061887\n",
      "Iteration 6717, loss = 0.01061257\n",
      "Iteration 6718, loss = 0.01060627\n",
      "Iteration 6719, loss = 0.01059998\n",
      "Iteration 6720, loss = 0.01059368\n",
      "Iteration 6721, loss = 0.01058739\n",
      "Iteration 6722, loss = 0.01058110\n",
      "Iteration 6723, loss = 0.01057482\n",
      "Iteration 6724, loss = 0.01056853\n",
      "Iteration 6725, loss = 0.01056225\n",
      "Iteration 6726, loss = 0.01055597\n",
      "Iteration 6727, loss = 0.01054969\n",
      "Iteration 6728, loss = 0.01054342\n",
      "Iteration 6729, loss = 0.01053714\n",
      "Iteration 6730, loss = 0.01053087\n",
      "Iteration 6731, loss = 0.01052460\n",
      "Iteration 6732, loss = 0.01051833\n",
      "Iteration 6733, loss = 0.01051206\n",
      "Iteration 6734, loss = 0.01050580\n",
      "Iteration 6735, loss = 0.01049954\n",
      "Iteration 6736, loss = 0.01049328\n",
      "Iteration 6737, loss = 0.01048702\n",
      "Iteration 6738, loss = 0.01048076\n",
      "Iteration 6739, loss = 0.01047451\n",
      "Iteration 6740, loss = 0.01046826\n",
      "Iteration 6741, loss = 0.01046201\n",
      "Iteration 6742, loss = 0.01045576\n",
      "Iteration 6743, loss = 0.01044951\n",
      "Iteration 6744, loss = 0.01044327\n",
      "Iteration 6745, loss = 0.01043703\n",
      "Iteration 6746, loss = 0.01043079\n",
      "Iteration 6747, loss = 0.01042455\n",
      "Iteration 6748, loss = 0.01041831\n",
      "Iteration 6749, loss = 0.01041208\n",
      "Iteration 6750, loss = 0.01040585\n",
      "Iteration 6751, loss = 0.01039962\n",
      "Iteration 6752, loss = 0.01039339\n",
      "Iteration 6753, loss = 0.01038717\n",
      "Iteration 6754, loss = 0.01038094\n",
      "Iteration 6755, loss = 0.01037472\n",
      "Iteration 6756, loss = 0.01036850\n",
      "Iteration 6757, loss = 0.01036228\n",
      "Iteration 6758, loss = 0.01035607\n",
      "Iteration 6759, loss = 0.01034985\n",
      "Iteration 6760, loss = 0.01034364\n",
      "Iteration 6761, loss = 0.01033743\n",
      "Iteration 6762, loss = 0.01033123\n",
      "Iteration 6763, loss = 0.01032502\n",
      "Iteration 6764, loss = 0.01031882\n",
      "Iteration 6765, loss = 0.01031262\n",
      "Iteration 6766, loss = 0.01030642\n",
      "Iteration 6767, loss = 0.01030022\n",
      "Iteration 6768, loss = 0.01029403\n",
      "Iteration 6769, loss = 0.01028783\n",
      "Iteration 6770, loss = 0.01028164\n",
      "Iteration 6771, loss = 0.01027546\n",
      "Iteration 6772, loss = 0.01026927\n",
      "Iteration 6773, loss = 0.01026308\n",
      "Iteration 6774, loss = 0.01025690\n",
      "Iteration 6775, loss = 0.01025072\n",
      "Iteration 6776, loss = 0.01024454\n",
      "Iteration 6777, loss = 0.01023837\n",
      "Iteration 6778, loss = 0.01023219\n",
      "Iteration 6779, loss = 0.01022602\n",
      "Iteration 6780, loss = 0.01021985\n",
      "Iteration 6781, loss = 0.01021368\n",
      "Iteration 6782, loss = 0.01020752\n",
      "Iteration 6783, loss = 0.01020135\n",
      "Iteration 6784, loss = 0.01019519\n",
      "Iteration 6785, loss = 0.01018903\n",
      "Iteration 6786, loss = 0.01018287\n",
      "Iteration 6787, loss = 0.01017672\n",
      "Iteration 6788, loss = 0.01017057\n",
      "Iteration 6789, loss = 0.01016441\n",
      "Iteration 6790, loss = 0.01015826\n",
      "Iteration 6791, loss = 0.01015212\n",
      "Iteration 6792, loss = 0.01014597\n",
      "Iteration 6793, loss = 0.01013983\n",
      "Iteration 6794, loss = 0.01013369\n",
      "Iteration 6795, loss = 0.01012755\n",
      "Iteration 6796, loss = 0.01012141\n",
      "Iteration 6797, loss = 0.01011528\n",
      "Iteration 6798, loss = 0.01010915\n",
      "Iteration 6799, loss = 0.01010302\n",
      "Iteration 6800, loss = 0.01009689\n",
      "Iteration 6801, loss = 0.01009076\n",
      "Iteration 6802, loss = 0.01008464\n",
      "Iteration 6803, loss = 0.01007851\n",
      "Iteration 6804, loss = 0.01007239\n",
      "Iteration 6805, loss = 0.01006628\n",
      "Iteration 6806, loss = 0.01006016\n",
      "Iteration 6807, loss = 0.01005405\n",
      "Iteration 6808, loss = 0.01004794\n",
      "Iteration 6809, loss = 0.01004183\n",
      "Iteration 6810, loss = 0.01003572\n",
      "Iteration 6811, loss = 0.01002961\n",
      "Iteration 6812, loss = 0.01002351\n",
      "Iteration 6813, loss = 0.01001741\n",
      "Iteration 6814, loss = 0.01001131\n",
      "Iteration 6815, loss = 0.01000521\n",
      "Iteration 6816, loss = 0.00999912\n",
      "Iteration 6817, loss = 0.00999302\n",
      "Iteration 6818, loss = 0.00998693\n",
      "Iteration 6819, loss = 0.00998085\n",
      "Iteration 6820, loss = 0.00997476\n",
      "Iteration 6821, loss = 0.00996868\n",
      "Iteration 6822, loss = 0.00996259\n",
      "Iteration 6823, loss = 0.00995651\n",
      "Iteration 6824, loss = 0.00995044\n",
      "Iteration 6825, loss = 0.00994436\n",
      "Iteration 6826, loss = 0.00993829\n",
      "Iteration 6827, loss = 0.00993221\n",
      "Iteration 6828, loss = 0.00992614\n",
      "Iteration 6829, loss = 0.00992008\n",
      "Iteration 6830, loss = 0.00991401\n",
      "Iteration 6831, loss = 0.00990795\n",
      "Iteration 6832, loss = 0.00990189\n",
      "Iteration 6833, loss = 0.00989583\n",
      "Iteration 6834, loss = 0.00988977\n",
      "Iteration 6835, loss = 0.00988372\n",
      "Iteration 6836, loss = 0.00987767\n",
      "Iteration 6837, loss = 0.00987161\n",
      "Iteration 6838, loss = 0.00986557\n",
      "Iteration 6839, loss = 0.00985952\n",
      "Iteration 6840, loss = 0.00985348\n",
      "Iteration 6841, loss = 0.00984743\n",
      "Iteration 6842, loss = 0.00984140\n",
      "Iteration 6843, loss = 0.00983536\n",
      "Iteration 6844, loss = 0.00982932\n",
      "Iteration 6845, loss = 0.00982329\n",
      "Iteration 6846, loss = 0.00981726\n",
      "Iteration 6847, loss = 0.00981123\n",
      "Iteration 6848, loss = 0.00980520\n",
      "Iteration 6849, loss = 0.00979918\n",
      "Iteration 6850, loss = 0.00979315\n",
      "Iteration 6851, loss = 0.00978713\n",
      "Iteration 6852, loss = 0.00978112\n",
      "Iteration 6853, loss = 0.00977510\n",
      "Iteration 6854, loss = 0.00976909\n",
      "Iteration 6855, loss = 0.00976307\n",
      "Iteration 6856, loss = 0.00975706\n",
      "Iteration 6857, loss = 0.00975106\n",
      "Iteration 6858, loss = 0.00974505\n",
      "Iteration 6859, loss = 0.00973905\n",
      "Iteration 6860, loss = 0.00973305\n",
      "Iteration 6861, loss = 0.00972705\n",
      "Iteration 6862, loss = 0.00972105\n",
      "Iteration 6863, loss = 0.00971506\n",
      "Iteration 6864, loss = 0.00970906\n",
      "Iteration 6865, loss = 0.00970307\n",
      "Iteration 6866, loss = 0.00969708\n",
      "Iteration 6867, loss = 0.00969110\n",
      "Iteration 6868, loss = 0.00968511\n",
      "Iteration 6869, loss = 0.00967913\n",
      "Iteration 6870, loss = 0.00967315\n",
      "Iteration 6871, loss = 0.00966718\n",
      "Iteration 6872, loss = 0.00966120\n",
      "Iteration 6873, loss = 0.00965523\n",
      "Iteration 6874, loss = 0.00964926\n",
      "Iteration 6875, loss = 0.00964329\n",
      "Iteration 6876, loss = 0.00963732\n",
      "Iteration 6877, loss = 0.00963136\n",
      "Iteration 6878, loss = 0.00962540\n",
      "Iteration 6879, loss = 0.00961944\n",
      "Iteration 6880, loss = 0.00961348\n",
      "Iteration 6881, loss = 0.00960752\n",
      "Iteration 6882, loss = 0.00960157\n",
      "Iteration 6883, loss = 0.00959562\n",
      "Iteration 6884, loss = 0.00958967\n",
      "Iteration 6885, loss = 0.00958372\n",
      "Iteration 6886, loss = 0.00957778\n",
      "Iteration 6887, loss = 0.00957183\n",
      "Iteration 6888, loss = 0.00956589\n",
      "Iteration 6889, loss = 0.00955995\n",
      "Iteration 6890, loss = 0.00955402\n",
      "Iteration 6891, loss = 0.00954808\n",
      "Iteration 6892, loss = 0.00954215\n",
      "Iteration 6893, loss = 0.00953622\n",
      "Iteration 6894, loss = 0.00953030\n",
      "Iteration 6895, loss = 0.00952437\n",
      "Iteration 6896, loss = 0.00951845\n",
      "Iteration 6897, loss = 0.00951253\n",
      "Iteration 6898, loss = 0.00950661\n",
      "Iteration 6899, loss = 0.00950069\n",
      "Iteration 6900, loss = 0.00949478\n",
      "Iteration 6901, loss = 0.00948887\n",
      "Iteration 6902, loss = 0.00948296\n",
      "Iteration 6903, loss = 0.00947705\n",
      "Iteration 6904, loss = 0.00947114\n",
      "Iteration 6905, loss = 0.00946524\n",
      "Iteration 6906, loss = 0.00945934\n",
      "Iteration 6907, loss = 0.00945344\n",
      "Iteration 6908, loss = 0.00944754\n",
      "Iteration 6909, loss = 0.00944165\n",
      "Iteration 6910, loss = 0.00943576\n",
      "Iteration 6911, loss = 0.00942987\n",
      "Iteration 6912, loss = 0.00942398\n",
      "Iteration 6913, loss = 0.00941809\n",
      "Iteration 6914, loss = 0.00941221\n",
      "Iteration 6915, loss = 0.00940633\n",
      "Iteration 6916, loss = 0.00940045\n",
      "Iteration 6917, loss = 0.00939457\n",
      "Iteration 6918, loss = 0.00938870\n",
      "Iteration 6919, loss = 0.00938283\n",
      "Iteration 6920, loss = 0.00937696\n",
      "Iteration 6921, loss = 0.00937109\n",
      "Iteration 6922, loss = 0.00936522\n",
      "Iteration 6923, loss = 0.00935936\n",
      "Iteration 6924, loss = 0.00935350\n",
      "Iteration 6925, loss = 0.00934764\n",
      "Iteration 6926, loss = 0.00934178\n",
      "Iteration 6927, loss = 0.00933593\n",
      "Iteration 6928, loss = 0.00933007\n",
      "Iteration 6929, loss = 0.00932422\n",
      "Iteration 6930, loss = 0.00931838\n",
      "Iteration 6931, loss = 0.00931253\n",
      "Iteration 6932, loss = 0.00930669\n",
      "Iteration 6933, loss = 0.00930085\n",
      "Iteration 6934, loss = 0.00929501\n",
      "Iteration 6935, loss = 0.00928917\n",
      "Iteration 6936, loss = 0.00928334\n",
      "Iteration 6937, loss = 0.00927750\n",
      "Iteration 6938, loss = 0.00927167\n",
      "Iteration 6939, loss = 0.00926585\n",
      "Iteration 6940, loss = 0.00926002\n",
      "Iteration 6941, loss = 0.00925420\n",
      "Iteration 6942, loss = 0.00924838\n",
      "Iteration 6943, loss = 0.00924256\n",
      "Iteration 6944, loss = 0.00923674\n",
      "Iteration 6945, loss = 0.00923093\n",
      "Iteration 6946, loss = 0.00922511\n",
      "Iteration 6947, loss = 0.00921930\n",
      "Iteration 6948, loss = 0.00921350\n",
      "Iteration 6949, loss = 0.00920769\n",
      "Iteration 6950, loss = 0.00920189\n",
      "Iteration 6951, loss = 0.00919609\n",
      "Iteration 6952, loss = 0.00919029\n",
      "Iteration 6953, loss = 0.00918449\n",
      "Iteration 6954, loss = 0.00917870\n",
      "Iteration 6955, loss = 0.00917291\n",
      "Iteration 6956, loss = 0.00916712\n",
      "Iteration 6957, loss = 0.00916133\n",
      "Iteration 6958, loss = 0.00915554\n",
      "Iteration 6959, loss = 0.00914976\n",
      "Iteration 6960, loss = 0.00914398\n",
      "Iteration 6961, loss = 0.00913820\n",
      "Iteration 6962, loss = 0.00913243\n",
      "Iteration 6963, loss = 0.00912665\n",
      "Iteration 6964, loss = 0.00912088\n",
      "Iteration 6965, loss = 0.00911511\n",
      "Iteration 6966, loss = 0.00910935\n",
      "Iteration 6967, loss = 0.00910358\n",
      "Iteration 6968, loss = 0.00909782\n",
      "Iteration 6969, loss = 0.00909206\n",
      "Iteration 6970, loss = 0.00908630\n",
      "Iteration 6971, loss = 0.00908054\n",
      "Iteration 6972, loss = 0.00907479\n",
      "Iteration 6973, loss = 0.00906904\n",
      "Iteration 6974, loss = 0.00906329\n",
      "Iteration 6975, loss = 0.00905754\n",
      "Iteration 6976, loss = 0.00905180\n",
      "Iteration 6977, loss = 0.00904606\n",
      "Iteration 6978, loss = 0.00904032\n",
      "Iteration 6979, loss = 0.00903458\n",
      "Iteration 6980, loss = 0.00902885\n",
      "Iteration 6981, loss = 0.00902311\n",
      "Iteration 6982, loss = 0.00901738\n",
      "Iteration 6983, loss = 0.00901165\n",
      "Iteration 6984, loss = 0.00900593\n",
      "Iteration 6985, loss = 0.00900020\n",
      "Iteration 6986, loss = 0.00899448\n",
      "Iteration 6987, loss = 0.00898876\n",
      "Iteration 6988, loss = 0.00898305\n",
      "Iteration 6989, loss = 0.00897733\n",
      "Iteration 6990, loss = 0.00897162\n",
      "Iteration 6991, loss = 0.00896591\n",
      "Iteration 6992, loss = 0.00896020\n",
      "Iteration 6993, loss = 0.00895450\n",
      "Iteration 6994, loss = 0.00894879\n",
      "Iteration 6995, loss = 0.00894309\n",
      "Iteration 6996, loss = 0.00893740\n",
      "Iteration 6997, loss = 0.00893170\n",
      "Iteration 6998, loss = 0.00892601\n",
      "Iteration 6999, loss = 0.00892031\n",
      "Iteration 7000, loss = 0.00891462\n",
      "Iteration 7001, loss = 0.00890894\n",
      "Iteration 7002, loss = 0.00890325\n",
      "Iteration 7003, loss = 0.00889757\n",
      "Iteration 7004, loss = 0.00889189\n",
      "Iteration 7005, loss = 0.00888621\n",
      "Iteration 7006, loss = 0.00888054\n",
      "Iteration 7007, loss = 0.00887487\n",
      "Iteration 7008, loss = 0.00886919\n",
      "Iteration 7009, loss = 0.00886353\n",
      "Iteration 7010, loss = 0.00885786\n",
      "Iteration 7011, loss = 0.00885220\n",
      "Iteration 7012, loss = 0.00884653\n",
      "Iteration 7013, loss = 0.00884088\n",
      "Iteration 7014, loss = 0.00883522\n",
      "Iteration 7015, loss = 0.00882956\n",
      "Iteration 7016, loss = 0.00882391\n",
      "Iteration 7017, loss = 0.00881826\n",
      "Iteration 7018, loss = 0.00881261\n",
      "Iteration 7019, loss = 0.00880697\n",
      "Iteration 7020, loss = 0.00880133\n",
      "Iteration 7021, loss = 0.00879569\n",
      "Iteration 7022, loss = 0.00879005\n",
      "Iteration 7023, loss = 0.00878441\n",
      "Iteration 7024, loss = 0.00877878\n",
      "Iteration 7025, loss = 0.00877315\n",
      "Iteration 7026, loss = 0.00876752\n",
      "Iteration 7027, loss = 0.00876189\n",
      "Iteration 7028, loss = 0.00875627\n",
      "Iteration 7029, loss = 0.00875065\n",
      "Iteration 7030, loss = 0.00874503\n",
      "Iteration 7031, loss = 0.00873941\n",
      "Iteration 7032, loss = 0.00873379\n",
      "Iteration 7033, loss = 0.00872818\n",
      "Iteration 7034, loss = 0.00872257\n",
      "Iteration 7035, loss = 0.00871696\n",
      "Iteration 7036, loss = 0.00871136\n",
      "Iteration 7037, loss = 0.00870576\n",
      "Iteration 7038, loss = 0.00870016\n",
      "Iteration 7039, loss = 0.00869456\n",
      "Iteration 7040, loss = 0.00868896\n",
      "Iteration 7041, loss = 0.00868337\n",
      "Iteration 7042, loss = 0.00867778\n",
      "Iteration 7043, loss = 0.00867219\n",
      "Iteration 7044, loss = 0.00866660\n",
      "Iteration 7045, loss = 0.00866102\n",
      "Iteration 7046, loss = 0.00865543\n",
      "Iteration 7047, loss = 0.00864985\n",
      "Iteration 7048, loss = 0.00864428\n",
      "Iteration 7049, loss = 0.00863870\n",
      "Iteration 7050, loss = 0.00863313\n",
      "Iteration 7051, loss = 0.00862756\n",
      "Iteration 7052, loss = 0.00862199\n",
      "Iteration 7053, loss = 0.00861643\n",
      "Iteration 7054, loss = 0.00861086\n",
      "Iteration 7055, loss = 0.00860530\n",
      "Iteration 7056, loss = 0.00859975\n",
      "Iteration 7057, loss = 0.00859419\n",
      "Iteration 7058, loss = 0.00858864\n",
      "Iteration 7059, loss = 0.00858309\n",
      "Iteration 7060, loss = 0.00857754\n",
      "Iteration 7061, loss = 0.00857199\n",
      "Iteration 7062, loss = 0.00856645\n",
      "Iteration 7063, loss = 0.00856091\n",
      "Iteration 7064, loss = 0.00855537\n",
      "Iteration 7065, loss = 0.00854983\n",
      "Iteration 7066, loss = 0.00854429\n",
      "Iteration 7067, loss = 0.00853876\n",
      "Iteration 7068, loss = 0.00853323\n",
      "Iteration 7069, loss = 0.00852771\n",
      "Iteration 7070, loss = 0.00852218\n",
      "Iteration 7071, loss = 0.00851666\n",
      "Iteration 7072, loss = 0.00851114\n",
      "Iteration 7073, loss = 0.00850562\n",
      "Iteration 7074, loss = 0.00850011\n",
      "Iteration 7075, loss = 0.00849459\n",
      "Iteration 7076, loss = 0.00848908\n",
      "Iteration 7077, loss = 0.00848357\n",
      "Iteration 7078, loss = 0.00847807\n",
      "Iteration 7079, loss = 0.00847257\n",
      "Iteration 7080, loss = 0.00846706\n",
      "Iteration 7081, loss = 0.00846157\n",
      "Iteration 7082, loss = 0.00845607\n",
      "Iteration 7083, loss = 0.00845058\n",
      "Iteration 7084, loss = 0.00844508\n",
      "Iteration 7085, loss = 0.00843960\n",
      "Iteration 7086, loss = 0.00843411\n",
      "Iteration 7087, loss = 0.00842863\n",
      "Iteration 7088, loss = 0.00842314\n",
      "Iteration 7089, loss = 0.00841766\n",
      "Iteration 7090, loss = 0.00841219\n",
      "Iteration 7091, loss = 0.00840671\n",
      "Iteration 7092, loss = 0.00840124\n",
      "Iteration 7093, loss = 0.00839577\n",
      "Iteration 7094, loss = 0.00839030\n",
      "Iteration 7095, loss = 0.00838484\n",
      "Iteration 7096, loss = 0.00837938\n",
      "Iteration 7097, loss = 0.00837392\n",
      "Iteration 7098, loss = 0.00836846\n",
      "Iteration 7099, loss = 0.00836300\n",
      "Iteration 7100, loss = 0.00835755\n",
      "Iteration 7101, loss = 0.00835210\n",
      "Iteration 7102, loss = 0.00834665\n",
      "Iteration 7103, loss = 0.00834121\n",
      "Iteration 7104, loss = 0.00833577\n",
      "Iteration 7105, loss = 0.00833033\n",
      "Iteration 7106, loss = 0.00832489\n",
      "Iteration 7107, loss = 0.00831945\n",
      "Iteration 7108, loss = 0.00831402\n",
      "Iteration 7109, loss = 0.00830859\n",
      "Iteration 7110, loss = 0.00830316\n",
      "Iteration 7111, loss = 0.00829773\n",
      "Iteration 7112, loss = 0.00829231\n",
      "Iteration 7113, loss = 0.00828689\n",
      "Iteration 7114, loss = 0.00828147\n",
      "Iteration 7115, loss = 0.00827605\n",
      "Iteration 7116, loss = 0.00827064\n",
      "Iteration 7117, loss = 0.00826523\n",
      "Iteration 7118, loss = 0.00825982\n",
      "Iteration 7119, loss = 0.00825441\n",
      "Iteration 7120, loss = 0.00824901\n",
      "Iteration 7121, loss = 0.00824361\n",
      "Iteration 7122, loss = 0.00823821\n",
      "Iteration 7123, loss = 0.00823281\n",
      "Iteration 7124, loss = 0.00822742\n",
      "Iteration 7125, loss = 0.00822203\n",
      "Iteration 7126, loss = 0.00821664\n",
      "Iteration 7127, loss = 0.00821125\n",
      "Iteration 7128, loss = 0.00820587\n",
      "Iteration 7129, loss = 0.00820048\n",
      "Iteration 7130, loss = 0.00819511\n",
      "Iteration 7131, loss = 0.00818973\n",
      "Iteration 7132, loss = 0.00818435\n",
      "Iteration 7133, loss = 0.00817898\n",
      "Iteration 7134, loss = 0.00817361\n",
      "Iteration 7135, loss = 0.00816825\n",
      "Iteration 7136, loss = 0.00816288\n",
      "Iteration 7137, loss = 0.00815752\n",
      "Iteration 7138, loss = 0.00815216\n",
      "Iteration 7139, loss = 0.00814680\n",
      "Iteration 7140, loss = 0.00814145\n",
      "Iteration 7141, loss = 0.00813610\n",
      "Iteration 7142, loss = 0.00813075\n",
      "Iteration 7143, loss = 0.00812540\n",
      "Iteration 7144, loss = 0.00812005\n",
      "Iteration 7145, loss = 0.00811471\n",
      "Iteration 7146, loss = 0.00810937\n",
      "Iteration 7147, loss = 0.00810403\n",
      "Iteration 7148, loss = 0.00809870\n",
      "Iteration 7149, loss = 0.00809337\n",
      "Iteration 7150, loss = 0.00808804\n",
      "Iteration 7151, loss = 0.00808271\n",
      "Iteration 7152, loss = 0.00807738\n",
      "Iteration 7153, loss = 0.00807206\n",
      "Iteration 7154, loss = 0.00806674\n",
      "Iteration 7155, loss = 0.00806142\n",
      "Iteration 7156, loss = 0.00805611\n",
      "Iteration 7157, loss = 0.00805080\n",
      "Iteration 7158, loss = 0.00804549\n",
      "Iteration 7159, loss = 0.00804018\n",
      "Iteration 7160, loss = 0.00803487\n",
      "Iteration 7161, loss = 0.00802957\n",
      "Iteration 7162, loss = 0.00802427\n",
      "Iteration 7163, loss = 0.00801897\n",
      "Iteration 7164, loss = 0.00801368\n",
      "Iteration 7165, loss = 0.00800839\n",
      "Iteration 7166, loss = 0.00800310\n",
      "Iteration 7167, loss = 0.00799781\n",
      "Iteration 7168, loss = 0.00799252\n",
      "Iteration 7169, loss = 0.00798724\n",
      "Iteration 7170, loss = 0.00798196\n",
      "Iteration 7171, loss = 0.00797668\n",
      "Iteration 7172, loss = 0.00797141\n",
      "Iteration 7173, loss = 0.00796614\n",
      "Iteration 7174, loss = 0.00796087\n",
      "Iteration 7175, loss = 0.00795560\n",
      "Iteration 7176, loss = 0.00795033\n",
      "Iteration 7177, loss = 0.00794507\n",
      "Iteration 7178, loss = 0.00793981\n",
      "Iteration 7179, loss = 0.00793455\n",
      "Iteration 7180, loss = 0.00792930\n",
      "Iteration 7181, loss = 0.00792405\n",
      "Iteration 7182, loss = 0.00791880\n",
      "Iteration 7183, loss = 0.00791355\n",
      "Iteration 7184, loss = 0.00790830\n",
      "Iteration 7185, loss = 0.00790306\n",
      "Iteration 7186, loss = 0.00789782\n",
      "Iteration 7187, loss = 0.00789258\n",
      "Iteration 7188, loss = 0.00788735\n",
      "Iteration 7189, loss = 0.00788212\n",
      "Iteration 7190, loss = 0.00787689\n",
      "Iteration 7191, loss = 0.00787166\n",
      "Iteration 7192, loss = 0.00786644\n",
      "Iteration 7193, loss = 0.00786121\n",
      "Iteration 7194, loss = 0.00785599\n",
      "Iteration 7195, loss = 0.00785078\n",
      "Iteration 7196, loss = 0.00784556\n",
      "Iteration 7197, loss = 0.00784035\n",
      "Iteration 7198, loss = 0.00783514\n",
      "Iteration 7199, loss = 0.00782993\n",
      "Iteration 7200, loss = 0.00782473\n",
      "Iteration 7201, loss = 0.00781953\n",
      "Iteration 7202, loss = 0.00781433\n",
      "Iteration 7203, loss = 0.00780913\n",
      "Iteration 7204, loss = 0.00780394\n",
      "Iteration 7205, loss = 0.00779875\n",
      "Iteration 7206, loss = 0.00779356\n",
      "Iteration 7207, loss = 0.00778837\n",
      "Iteration 7208, loss = 0.00778319\n",
      "Iteration 7209, loss = 0.00777800\n",
      "Iteration 7210, loss = 0.00777283\n",
      "Iteration 7211, loss = 0.00776765\n",
      "Iteration 7212, loss = 0.00776248\n",
      "Iteration 7213, loss = 0.00775730\n",
      "Iteration 7214, loss = 0.00775214\n",
      "Iteration 7215, loss = 0.00774697\n",
      "Iteration 7216, loss = 0.00774181\n",
      "Iteration 7217, loss = 0.00773664\n",
      "Iteration 7218, loss = 0.00773149\n",
      "Iteration 7219, loss = 0.00772633\n",
      "Iteration 7220, loss = 0.00772118\n",
      "Iteration 7221, loss = 0.00771603\n",
      "Iteration 7222, loss = 0.00771088\n",
      "Iteration 7223, loss = 0.00770573\n",
      "Iteration 7224, loss = 0.00770059\n",
      "Iteration 7225, loss = 0.00769545\n",
      "Iteration 7226, loss = 0.00769031\n",
      "Iteration 7227, loss = 0.00768518\n",
      "Iteration 7228, loss = 0.00768004\n",
      "Iteration 7229, loss = 0.00767491\n",
      "Iteration 7230, loss = 0.00766978\n",
      "Iteration 7231, loss = 0.00766466\n",
      "Iteration 7232, loss = 0.00765954\n",
      "Iteration 7233, loss = 0.00765442\n",
      "Iteration 7234, loss = 0.00764930\n",
      "Iteration 7235, loss = 0.00764418\n",
      "Iteration 7236, loss = 0.00763907\n",
      "Iteration 7237, loss = 0.00763396\n",
      "Iteration 7238, loss = 0.00762886\n",
      "Iteration 7239, loss = 0.00762375\n",
      "Iteration 7240, loss = 0.00761865\n",
      "Iteration 7241, loss = 0.00761355\n",
      "Iteration 7242, loss = 0.00760845\n",
      "Iteration 7243, loss = 0.00760336\n",
      "Iteration 7244, loss = 0.00759827\n",
      "Iteration 7245, loss = 0.00759318\n",
      "Iteration 7246, loss = 0.00758809\n",
      "Iteration 7247, loss = 0.00758301\n",
      "Iteration 7248, loss = 0.00757793\n",
      "Iteration 7249, loss = 0.00757285\n",
      "Iteration 7250, loss = 0.00756777\n",
      "Iteration 7251, loss = 0.00756270\n",
      "Iteration 7252, loss = 0.00755763\n",
      "Iteration 7253, loss = 0.00755256\n",
      "Iteration 7254, loss = 0.00754749\n",
      "Iteration 7255, loss = 0.00754243\n",
      "Iteration 7256, loss = 0.00753737\n",
      "Iteration 7257, loss = 0.00753231\n",
      "Iteration 7258, loss = 0.00752726\n",
      "Iteration 7259, loss = 0.00752221\n",
      "Iteration 7260, loss = 0.00751716\n",
      "Iteration 7261, loss = 0.00751211\n",
      "Iteration 7262, loss = 0.00750706\n",
      "Iteration 7263, loss = 0.00750202\n",
      "Iteration 7264, loss = 0.00749698\n",
      "Iteration 7265, loss = 0.00749194\n",
      "Iteration 7266, loss = 0.00748691\n",
      "Iteration 7267, loss = 0.00748188\n",
      "Iteration 7268, loss = 0.00747685\n",
      "Iteration 7269, loss = 0.00747182\n",
      "Iteration 7270, loss = 0.00746680\n",
      "Iteration 7271, loss = 0.00746178\n",
      "Iteration 7272, loss = 0.00745676\n",
      "Iteration 7273, loss = 0.00745174\n",
      "Iteration 7274, loss = 0.00744673\n",
      "Iteration 7275, loss = 0.00744172\n",
      "Iteration 7276, loss = 0.00743671\n",
      "Iteration 7277, loss = 0.00743170\n",
      "Iteration 7278, loss = 0.00742670\n",
      "Iteration 7279, loss = 0.00742170\n",
      "Iteration 7280, loss = 0.00741670\n",
      "Iteration 7281, loss = 0.00741171\n",
      "Iteration 7282, loss = 0.00740671\n",
      "Iteration 7283, loss = 0.00740172\n",
      "Iteration 7284, loss = 0.00739674\n",
      "Iteration 7285, loss = 0.00739175\n",
      "Iteration 7286, loss = 0.00738677\n",
      "Iteration 7287, loss = 0.00738179\n",
      "Iteration 7288, loss = 0.00737681\n",
      "Iteration 7289, loss = 0.00737184\n",
      "Iteration 7290, loss = 0.00736687\n",
      "Iteration 7291, loss = 0.00736190\n",
      "Iteration 7292, loss = 0.00735693\n",
      "Iteration 7293, loss = 0.00735197\n",
      "Iteration 7294, loss = 0.00734701\n",
      "Iteration 7295, loss = 0.00734205\n",
      "Iteration 7296, loss = 0.00733710\n",
      "Iteration 7297, loss = 0.00733214\n",
      "Iteration 7298, loss = 0.00732719\n",
      "Iteration 7299, loss = 0.00732224\n",
      "Iteration 7300, loss = 0.00731730\n",
      "Iteration 7301, loss = 0.00731236\n",
      "Iteration 7302, loss = 0.00730742\n",
      "Iteration 7303, loss = 0.00730248\n",
      "Iteration 7304, loss = 0.00729755\n",
      "Iteration 7305, loss = 0.00729261\n",
      "Iteration 7306, loss = 0.00728768\n",
      "Iteration 7307, loss = 0.00728276\n",
      "Iteration 7308, loss = 0.00727783\n",
      "Iteration 7309, loss = 0.00727291\n",
      "Iteration 7310, loss = 0.00726799\n",
      "Iteration 7311, loss = 0.00726308\n",
      "Iteration 7312, loss = 0.00725816\n",
      "Iteration 7313, loss = 0.00725325\n",
      "Iteration 7314, loss = 0.00724835\n",
      "Iteration 7315, loss = 0.00724344\n",
      "Iteration 7316, loss = 0.00723854\n",
      "Iteration 7317, loss = 0.00723364\n",
      "Iteration 7318, loss = 0.00722874\n",
      "Iteration 7319, loss = 0.00722385\n",
      "Iteration 7320, loss = 0.00721895\n",
      "Iteration 7321, loss = 0.00721406\n",
      "Iteration 7322, loss = 0.00720918\n",
      "Iteration 7323, loss = 0.00720429\n",
      "Iteration 7324, loss = 0.00719941\n",
      "Iteration 7325, loss = 0.00719453\n",
      "Iteration 7326, loss = 0.00718966\n",
      "Iteration 7327, loss = 0.00718478\n",
      "Iteration 7328, loss = 0.00717991\n",
      "Iteration 7329, loss = 0.00717504\n",
      "Iteration 7330, loss = 0.00717018\n",
      "Iteration 7331, loss = 0.00716532\n",
      "Iteration 7332, loss = 0.00716046\n",
      "Iteration 7333, loss = 0.00715560\n",
      "Iteration 7334, loss = 0.00715074\n",
      "Iteration 7335, loss = 0.00714589\n",
      "Iteration 7336, loss = 0.00714104\n",
      "Iteration 7337, loss = 0.00713620\n",
      "Iteration 7338, loss = 0.00713135\n",
      "Iteration 7339, loss = 0.00712651\n",
      "Iteration 7340, loss = 0.00712167\n",
      "Iteration 7341, loss = 0.00711683\n",
      "Iteration 7342, loss = 0.00711200\n",
      "Iteration 7343, loss = 0.00710717\n",
      "Iteration 7344, loss = 0.00710234\n",
      "Iteration 7345, loss = 0.00709752\n",
      "Iteration 7346, loss = 0.00709269\n",
      "Iteration 7347, loss = 0.00708788\n",
      "Iteration 7348, loss = 0.00708306\n",
      "Iteration 7349, loss = 0.00707824\n",
      "Iteration 7350, loss = 0.00707343\n",
      "Iteration 7351, loss = 0.00706862\n",
      "Iteration 7352, loss = 0.00706382\n",
      "Iteration 7353, loss = 0.00705901\n",
      "Iteration 7354, loss = 0.00705421\n",
      "Iteration 7355, loss = 0.00704941\n",
      "Iteration 7356, loss = 0.00704462\n",
      "Iteration 7357, loss = 0.00703982\n",
      "Iteration 7358, loss = 0.00703503\n",
      "Iteration 7359, loss = 0.00703025\n",
      "Iteration 7360, loss = 0.00702546\n",
      "Iteration 7361, loss = 0.00702068\n",
      "Iteration 7362, loss = 0.00701590\n",
      "Iteration 7363, loss = 0.00701112\n",
      "Iteration 7364, loss = 0.00700635\n",
      "Iteration 7365, loss = 0.00700158\n",
      "Iteration 7366, loss = 0.00699681\n",
      "Iteration 7367, loss = 0.00699204\n",
      "Iteration 7368, loss = 0.00698728\n",
      "Iteration 7369, loss = 0.00698252\n",
      "Iteration 7370, loss = 0.00697776\n",
      "Iteration 7371, loss = 0.00697300\n",
      "Iteration 7372, loss = 0.00696825\n",
      "Iteration 7373, loss = 0.00696350\n",
      "Iteration 7374, loss = 0.00695875\n",
      "Iteration 7375, loss = 0.00695401\n",
      "Iteration 7376, loss = 0.00694927\n",
      "Iteration 7377, loss = 0.00694453\n",
      "Iteration 7378, loss = 0.00693979\n",
      "Iteration 7379, loss = 0.00693506\n",
      "Iteration 7380, loss = 0.00693033\n",
      "Iteration 7381, loss = 0.00692560\n",
      "Iteration 7382, loss = 0.00692087\n",
      "Iteration 7383, loss = 0.00691615\n",
      "Iteration 7384, loss = 0.00691143\n",
      "Iteration 7385, loss = 0.00690671\n",
      "Iteration 7386, loss = 0.00690200\n",
      "Iteration 7387, loss = 0.00689729\n",
      "Iteration 7388, loss = 0.00689258\n",
      "Iteration 7389, loss = 0.00688787\n",
      "Iteration 7390, loss = 0.00688317\n",
      "Iteration 7391, loss = 0.00687847\n",
      "Iteration 7392, loss = 0.00687377\n",
      "Iteration 7393, loss = 0.00686907\n",
      "Iteration 7394, loss = 0.00686438\n",
      "Iteration 7395, loss = 0.00685969\n",
      "Iteration 7396, loss = 0.00685500\n",
      "Iteration 7397, loss = 0.00685032\n",
      "Iteration 7398, loss = 0.00684563\n",
      "Iteration 7399, loss = 0.00684095\n",
      "Iteration 7400, loss = 0.00683628\n",
      "Iteration 7401, loss = 0.00683160\n",
      "Iteration 7402, loss = 0.00682693\n",
      "Iteration 7403, loss = 0.00682226\n",
      "Iteration 7404, loss = 0.00681760\n",
      "Iteration 7405, loss = 0.00681293\n",
      "Iteration 7406, loss = 0.00680827\n",
      "Iteration 7407, loss = 0.00680362\n",
      "Iteration 7408, loss = 0.00679896\n",
      "Iteration 7409, loss = 0.00679431\n",
      "Iteration 7410, loss = 0.00678966\n",
      "Iteration 7411, loss = 0.00678501\n",
      "Iteration 7412, loss = 0.00678037\n",
      "Iteration 7413, loss = 0.00677573\n",
      "Iteration 7414, loss = 0.00677109\n",
      "Iteration 7415, loss = 0.00676645\n",
      "Iteration 7416, loss = 0.00676182\n",
      "Iteration 7417, loss = 0.00675719\n",
      "Iteration 7418, loss = 0.00675256\n",
      "Iteration 7419, loss = 0.00674794\n",
      "Iteration 7420, loss = 0.00674332\n",
      "Iteration 7421, loss = 0.00673870\n",
      "Iteration 7422, loss = 0.00673408\n",
      "Iteration 7423, loss = 0.00672947\n",
      "Iteration 7424, loss = 0.00672486\n",
      "Iteration 7425, loss = 0.00672025\n",
      "Iteration 7426, loss = 0.00671564\n",
      "Iteration 7427, loss = 0.00671104\n",
      "Iteration 7428, loss = 0.00670644\n",
      "Iteration 7429, loss = 0.00670184\n",
      "Iteration 7430, loss = 0.00669725\n",
      "Iteration 7431, loss = 0.00669265\n",
      "Iteration 7432, loss = 0.00668807\n",
      "Iteration 7433, loss = 0.00668348\n",
      "Iteration 7434, loss = 0.00667890\n",
      "Iteration 7435, loss = 0.00667431\n",
      "Iteration 7436, loss = 0.00666974\n",
      "Iteration 7437, loss = 0.00666516\n",
      "Iteration 7438, loss = 0.00666059\n",
      "Iteration 7439, loss = 0.00665602\n",
      "Iteration 7440, loss = 0.00665145\n",
      "Iteration 7441, loss = 0.00664689\n",
      "Iteration 7442, loss = 0.00664232\n",
      "Iteration 7443, loss = 0.00663777\n",
      "Iteration 7444, loss = 0.00663321\n",
      "Iteration 7445, loss = 0.00662866\n",
      "Iteration 7446, loss = 0.00662410\n",
      "Iteration 7447, loss = 0.00661956\n",
      "Iteration 7448, loss = 0.00661501\n",
      "Iteration 7449, loss = 0.00661047\n",
      "Iteration 7450, loss = 0.00660593\n",
      "Iteration 7451, loss = 0.00660139\n",
      "Iteration 7452, loss = 0.00659686\n",
      "Iteration 7453, loss = 0.00659233\n",
      "Iteration 7454, loss = 0.00658780\n",
      "Iteration 7455, loss = 0.00658327\n",
      "Iteration 7456, loss = 0.00657875\n",
      "Iteration 7457, loss = 0.00657423\n",
      "Iteration 7458, loss = 0.00656971\n",
      "Iteration 7459, loss = 0.00656520\n",
      "Iteration 7460, loss = 0.00656068\n",
      "Iteration 7461, loss = 0.00655617\n",
      "Iteration 7462, loss = 0.00655167\n",
      "Iteration 7463, loss = 0.00654716\n",
      "Iteration 7464, loss = 0.00654266\n",
      "Iteration 7465, loss = 0.00653816\n",
      "Iteration 7466, loss = 0.00653367\n",
      "Iteration 7467, loss = 0.00652917\n",
      "Iteration 7468, loss = 0.00652468\n",
      "Iteration 7469, loss = 0.00652020\n",
      "Iteration 7470, loss = 0.00651571\n",
      "Iteration 7471, loss = 0.00651123\n",
      "Iteration 7472, loss = 0.00650675\n",
      "Iteration 7473, loss = 0.00650228\n",
      "Iteration 7474, loss = 0.00649780\n",
      "Iteration 7475, loss = 0.00649333\n",
      "Iteration 7476, loss = 0.00648886\n",
      "Iteration 7477, loss = 0.00648440\n",
      "Iteration 7478, loss = 0.00647994\n",
      "Iteration 7479, loss = 0.00647548\n",
      "Iteration 7480, loss = 0.00647102\n",
      "Iteration 7481, loss = 0.00646656\n",
      "Iteration 7482, loss = 0.00646211\n",
      "Iteration 7483, loss = 0.00645766\n",
      "Iteration 7484, loss = 0.00645322\n",
      "Iteration 7485, loss = 0.00644878\n",
      "Iteration 7486, loss = 0.00644433\n",
      "Iteration 7487, loss = 0.00643990\n",
      "Iteration 7488, loss = 0.00643546\n",
      "Iteration 7489, loss = 0.00643103\n",
      "Iteration 7490, loss = 0.00642660\n",
      "Iteration 7491, loss = 0.00642217\n",
      "Iteration 7492, loss = 0.00641775\n",
      "Iteration 7493, loss = 0.00641333\n",
      "Iteration 7494, loss = 0.00640891\n",
      "Iteration 7495, loss = 0.00640450\n",
      "Iteration 7496, loss = 0.00640008\n",
      "Iteration 7497, loss = 0.00639567\n",
      "Iteration 7498, loss = 0.00639127\n",
      "Iteration 7499, loss = 0.00638686\n",
      "Iteration 7500, loss = 0.00638246\n",
      "Iteration 7501, loss = 0.00637806\n",
      "Iteration 7502, loss = 0.00637367\n",
      "Iteration 7503, loss = 0.00636927\n",
      "Iteration 7504, loss = 0.00636488\n",
      "Iteration 7505, loss = 0.00636049\n",
      "Iteration 7506, loss = 0.00635611\n",
      "Iteration 7507, loss = 0.00635173\n",
      "Iteration 7508, loss = 0.00634735\n",
      "Iteration 7509, loss = 0.00634297\n",
      "Iteration 7510, loss = 0.00633860\n",
      "Iteration 7511, loss = 0.00633423\n",
      "Iteration 7512, loss = 0.00632986\n",
      "Iteration 7513, loss = 0.00632549\n",
      "Iteration 7514, loss = 0.00632113\n",
      "Iteration 7515, loss = 0.00631677\n",
      "Iteration 7516, loss = 0.00631241\n",
      "Iteration 7517, loss = 0.00630806\n",
      "Iteration 7518, loss = 0.00630371\n",
      "Iteration 7519, loss = 0.00629936\n",
      "Iteration 7520, loss = 0.00629501\n",
      "Iteration 7521, loss = 0.00629067\n",
      "Iteration 7522, loss = 0.00628633\n",
      "Iteration 7523, loss = 0.00628199\n",
      "Iteration 7524, loss = 0.00627765\n",
      "Iteration 7525, loss = 0.00627332\n",
      "Iteration 7526, loss = 0.00626899\n",
      "Iteration 7527, loss = 0.00626467\n",
      "Iteration 7528, loss = 0.00626034\n",
      "Iteration 7529, loss = 0.00625602\n",
      "Iteration 7530, loss = 0.00625170\n",
      "Iteration 7531, loss = 0.00624739\n",
      "Iteration 7532, loss = 0.00624307\n",
      "Iteration 7533, loss = 0.00623877\n",
      "Iteration 7534, loss = 0.00623446\n",
      "Iteration 7535, loss = 0.00623015\n",
      "Iteration 7536, loss = 0.00622585\n",
      "Iteration 7537, loss = 0.00622155\n",
      "Iteration 7538, loss = 0.00621726\n",
      "Iteration 7539, loss = 0.00621296\n",
      "Iteration 7540, loss = 0.00620867\n",
      "Iteration 7541, loss = 0.00620439\n",
      "Iteration 7542, loss = 0.00620010\n",
      "Iteration 7543, loss = 0.00619582\n",
      "Iteration 7544, loss = 0.00619154\n",
      "Iteration 7545, loss = 0.00618726\n",
      "Iteration 7546, loss = 0.00618299\n",
      "Iteration 7547, loss = 0.00617872\n",
      "Iteration 7548, loss = 0.00617445\n",
      "Iteration 7549, loss = 0.00617018\n",
      "Iteration 7550, loss = 0.00616592\n",
      "Iteration 7551, loss = 0.00616166\n",
      "Iteration 7552, loss = 0.00615741\n",
      "Iteration 7553, loss = 0.00615315\n",
      "Iteration 7554, loss = 0.00614890\n",
      "Iteration 7555, loss = 0.00614465\n",
      "Iteration 7556, loss = 0.00614041\n",
      "Iteration 7557, loss = 0.00613616\n",
      "Iteration 7558, loss = 0.00613192\n",
      "Iteration 7559, loss = 0.00612768\n",
      "Iteration 7560, loss = 0.00612345\n",
      "Iteration 7561, loss = 0.00611922\n",
      "Iteration 7562, loss = 0.00611499\n",
      "Iteration 7563, loss = 0.00611076\n",
      "Iteration 7564, loss = 0.00610654\n",
      "Iteration 7565, loss = 0.00610232\n",
      "Iteration 7566, loss = 0.00609810\n",
      "Iteration 7567, loss = 0.00609388\n",
      "Iteration 7568, loss = 0.00608967\n",
      "Iteration 7569, loss = 0.00608546\n",
      "Iteration 7570, loss = 0.00608126\n",
      "Iteration 7571, loss = 0.00607705\n",
      "Iteration 7572, loss = 0.00607285\n",
      "Iteration 7573, loss = 0.00606865\n",
      "Iteration 7574, loss = 0.00606446\n",
      "Iteration 7575, loss = 0.00606026\n",
      "Iteration 7576, loss = 0.00605607\n",
      "Iteration 7577, loss = 0.00605189\n",
      "Iteration 7578, loss = 0.00604770\n",
      "Iteration 7579, loss = 0.00604352\n",
      "Iteration 7580, loss = 0.00603934\n",
      "Iteration 7581, loss = 0.00603516\n",
      "Iteration 7582, loss = 0.00603099\n",
      "Iteration 7583, loss = 0.00602682\n",
      "Iteration 7584, loss = 0.00602265\n",
      "Iteration 7585, loss = 0.00601849\n",
      "Iteration 7586, loss = 0.00601432\n",
      "Iteration 7587, loss = 0.00601017\n",
      "Iteration 7588, loss = 0.00600601\n",
      "Iteration 7589, loss = 0.00600185\n",
      "Iteration 7590, loss = 0.00599770\n",
      "Iteration 7591, loss = 0.00599356\n",
      "Iteration 7592, loss = 0.00598941\n",
      "Iteration 7593, loss = 0.00598527\n",
      "Iteration 7594, loss = 0.00598113\n",
      "Iteration 7595, loss = 0.00597699\n",
      "Iteration 7596, loss = 0.00597286\n",
      "Iteration 7597, loss = 0.00596872\n",
      "Iteration 7598, loss = 0.00596460\n",
      "Iteration 7599, loss = 0.00596047\n",
      "Iteration 7600, loss = 0.00595635\n",
      "Iteration 7601, loss = 0.00595223\n",
      "Iteration 7602, loss = 0.00594811\n",
      "Iteration 7603, loss = 0.00594399\n",
      "Iteration 7604, loss = 0.00593988\n",
      "Iteration 7605, loss = 0.00593577\n",
      "Iteration 7606, loss = 0.00593167\n",
      "Iteration 7607, loss = 0.00592756\n",
      "Iteration 7608, loss = 0.00592346\n",
      "Iteration 7609, loss = 0.00591936\n",
      "Iteration 7610, loss = 0.00591527\n",
      "Iteration 7611, loss = 0.00591118\n",
      "Iteration 7612, loss = 0.00590709\n",
      "Iteration 7613, loss = 0.00590300\n",
      "Iteration 7614, loss = 0.00589891\n",
      "Iteration 7615, loss = 0.00589483\n",
      "Iteration 7616, loss = 0.00589075\n",
      "Iteration 7617, loss = 0.00588668\n",
      "Iteration 7618, loss = 0.00588261\n",
      "Iteration 7619, loss = 0.00587854\n",
      "Iteration 7620, loss = 0.00587447\n",
      "Iteration 7621, loss = 0.00587040\n",
      "Iteration 7622, loss = 0.00586634\n",
      "Iteration 7623, loss = 0.00586228\n",
      "Iteration 7624, loss = 0.00585823\n",
      "Iteration 7625, loss = 0.00585417\n",
      "Iteration 7626, loss = 0.00585012\n",
      "Iteration 7627, loss = 0.00584607\n",
      "Iteration 7628, loss = 0.00584203\n",
      "Iteration 7629, loss = 0.00583799\n",
      "Iteration 7630, loss = 0.00583395\n",
      "Iteration 7631, loss = 0.00582991\n",
      "Iteration 7632, loss = 0.00582588\n",
      "Iteration 7633, loss = 0.00582185\n",
      "Iteration 7634, loss = 0.00581782\n",
      "Iteration 7635, loss = 0.00581379\n",
      "Iteration 7636, loss = 0.00580977\n",
      "Iteration 7637, loss = 0.00580575\n",
      "Iteration 7638, loss = 0.00580173\n",
      "Iteration 7639, loss = 0.00579772\n",
      "Iteration 7640, loss = 0.00579370\n",
      "Iteration 7641, loss = 0.00578970\n",
      "Iteration 7642, loss = 0.00578569\n",
      "Iteration 7643, loss = 0.00578169\n",
      "Iteration 7644, loss = 0.00577769\n",
      "Iteration 7645, loss = 0.00577369\n",
      "Iteration 7646, loss = 0.00576969\n",
      "Iteration 7647, loss = 0.00576570\n",
      "Iteration 7648, loss = 0.00576171\n",
      "Iteration 7649, loss = 0.00575772\n",
      "Iteration 7650, loss = 0.00575374\n",
      "Iteration 7651, loss = 0.00574976\n",
      "Iteration 7652, loss = 0.00574578\n",
      "Iteration 7653, loss = 0.00574181\n",
      "Iteration 7654, loss = 0.00573783\n",
      "Iteration 7655, loss = 0.00573386\n",
      "Iteration 7656, loss = 0.00572990\n",
      "Iteration 7657, loss = 0.00572593\n",
      "Iteration 7658, loss = 0.00572197\n",
      "Iteration 7659, loss = 0.00571801\n",
      "Iteration 7660, loss = 0.00571406\n",
      "Iteration 7661, loss = 0.00571010\n",
      "Iteration 7662, loss = 0.00570615\n",
      "Iteration 7663, loss = 0.00570220\n",
      "Iteration 7664, loss = 0.00569826\n",
      "Iteration 7665, loss = 0.00569432\n",
      "Iteration 7666, loss = 0.00569038\n",
      "Iteration 7667, loss = 0.00568644\n",
      "Iteration 7668, loss = 0.00568251\n",
      "Iteration 7669, loss = 0.00567858\n",
      "Iteration 7670, loss = 0.00567465\n",
      "Iteration 7671, loss = 0.00567072\n",
      "Iteration 7672, loss = 0.00566680\n",
      "Iteration 7673, loss = 0.00566288\n",
      "Iteration 7674, loss = 0.00565896\n",
      "Iteration 7675, loss = 0.00565505\n",
      "Iteration 7676, loss = 0.00565114\n",
      "Iteration 7677, loss = 0.00564723\n",
      "Iteration 7678, loss = 0.00564332\n",
      "Iteration 7679, loss = 0.00563942\n",
      "Iteration 7680, loss = 0.00563552\n",
      "Iteration 7681, loss = 0.00563162\n",
      "Iteration 7682, loss = 0.00562773\n",
      "Iteration 7683, loss = 0.00562383\n",
      "Iteration 7684, loss = 0.00561994\n",
      "Iteration 7685, loss = 0.00561606\n",
      "Iteration 7686, loss = 0.00561217\n",
      "Iteration 7687, loss = 0.00560829\n",
      "Iteration 7688, loss = 0.00560441\n",
      "Iteration 7689, loss = 0.00560054\n",
      "Iteration 7690, loss = 0.00559667\n",
      "Iteration 7691, loss = 0.00559280\n",
      "Iteration 7692, loss = 0.00558893\n",
      "Iteration 7693, loss = 0.00558506\n",
      "Iteration 7694, loss = 0.00558120\n",
      "Iteration 7695, loss = 0.00557734\n",
      "Iteration 7696, loss = 0.00557349\n",
      "Iteration 7697, loss = 0.00556963\n",
      "Iteration 7698, loss = 0.00556578\n",
      "Iteration 7699, loss = 0.00556194\n",
      "Iteration 7700, loss = 0.00555809\n",
      "Iteration 7701, loss = 0.00555425\n",
      "Iteration 7702, loss = 0.00555041\n",
      "Iteration 7703, loss = 0.00554657\n",
      "Iteration 7704, loss = 0.00554274\n",
      "Iteration 7705, loss = 0.00553891\n",
      "Iteration 7706, loss = 0.00553508\n",
      "Iteration 7707, loss = 0.00553125\n",
      "Iteration 7708, loss = 0.00552743\n",
      "Iteration 7709, loss = 0.00552361\n",
      "Iteration 7710, loss = 0.00551979\n",
      "Iteration 7711, loss = 0.00551598\n",
      "Iteration 7712, loss = 0.00551217\n",
      "Iteration 7713, loss = 0.00550836\n",
      "Iteration 7714, loss = 0.00550455\n",
      "Iteration 7715, loss = 0.00550075\n",
      "Iteration 7716, loss = 0.00549695\n",
      "Iteration 7717, loss = 0.00549315\n",
      "Iteration 7718, loss = 0.00548935\n",
      "Iteration 7719, loss = 0.00548556\n",
      "Iteration 7720, loss = 0.00548177\n",
      "Iteration 7721, loss = 0.00547798\n",
      "Iteration 7722, loss = 0.00547420\n",
      "Iteration 7723, loss = 0.00547042\n",
      "Iteration 7724, loss = 0.00546664\n",
      "Iteration 7725, loss = 0.00546286\n",
      "Iteration 7726, loss = 0.00545909\n",
      "Iteration 7727, loss = 0.00545532\n",
      "Iteration 7728, loss = 0.00545155\n",
      "Iteration 7729, loss = 0.00544779\n",
      "Iteration 7730, loss = 0.00544403\n",
      "Iteration 7731, loss = 0.00544027\n",
      "Iteration 7732, loss = 0.00543651\n",
      "Iteration 7733, loss = 0.00543276\n",
      "Iteration 7734, loss = 0.00542900\n",
      "Iteration 7735, loss = 0.00542526\n",
      "Iteration 7736, loss = 0.00542151\n",
      "Iteration 7737, loss = 0.00541777\n",
      "Iteration 7738, loss = 0.00541403\n",
      "Iteration 7739, loss = 0.00541029\n",
      "Iteration 7740, loss = 0.00540656\n",
      "Iteration 7741, loss = 0.00540282\n",
      "Iteration 7742, loss = 0.00539909\n",
      "Iteration 7743, loss = 0.00539537\n",
      "Iteration 7744, loss = 0.00539164\n",
      "Iteration 7745, loss = 0.00538792\n",
      "Iteration 7746, loss = 0.00538421\n",
      "Iteration 7747, loss = 0.00538049\n",
      "Iteration 7748, loss = 0.00537678\n",
      "Iteration 7749, loss = 0.00537307\n",
      "Iteration 7750, loss = 0.00536936\n",
      "Iteration 7751, loss = 0.00536566\n",
      "Iteration 7752, loss = 0.00536195\n",
      "Iteration 7753, loss = 0.00535826\n",
      "Iteration 7754, loss = 0.00535456\n",
      "Iteration 7755, loss = 0.00535087\n",
      "Iteration 7756, loss = 0.00534718\n",
      "Iteration 7757, loss = 0.00534349\n",
      "Iteration 7758, loss = 0.00533980\n",
      "Iteration 7759, loss = 0.00533612\n",
      "Iteration 7760, loss = 0.00533244\n",
      "Iteration 7761, loss = 0.00532876\n",
      "Iteration 7762, loss = 0.00532509\n",
      "Iteration 7763, loss = 0.00532142\n",
      "Iteration 7764, loss = 0.00531775\n",
      "Iteration 7765, loss = 0.00531408\n",
      "Iteration 7766, loss = 0.00531042\n",
      "Iteration 7767, loss = 0.00530676\n",
      "Iteration 7768, loss = 0.00530310\n",
      "Iteration 7769, loss = 0.00529945\n",
      "Iteration 7770, loss = 0.00529579\n",
      "Iteration 7771, loss = 0.00529214\n",
      "Iteration 7772, loss = 0.00528850\n",
      "Iteration 7773, loss = 0.00528485\n",
      "Iteration 7774, loss = 0.00528121\n",
      "Iteration 7775, loss = 0.00527757\n",
      "Iteration 7776, loss = 0.00527394\n",
      "Iteration 7777, loss = 0.00527030\n",
      "Iteration 7778, loss = 0.00526667\n",
      "Iteration 7779, loss = 0.00526305\n",
      "Iteration 7780, loss = 0.00525942\n",
      "Iteration 7781, loss = 0.00525580\n",
      "Iteration 7782, loss = 0.00525218\n",
      "Iteration 7783, loss = 0.00524856\n",
      "Iteration 7784, loss = 0.00524495\n",
      "Iteration 7785, loss = 0.00524134\n",
      "Iteration 7786, loss = 0.00523773\n",
      "Iteration 7787, loss = 0.00523412\n",
      "Iteration 7788, loss = 0.00523052\n",
      "Iteration 7789, loss = 0.00522692\n",
      "Iteration 7790, loss = 0.00522332\n",
      "Iteration 7791, loss = 0.00521972\n",
      "Iteration 7792, loss = 0.00521613\n",
      "Iteration 7793, loss = 0.00521254\n",
      "Iteration 7794, loss = 0.00520896\n",
      "Iteration 7795, loss = 0.00520537\n",
      "Iteration 7796, loss = 0.00520179\n",
      "Iteration 7797, loss = 0.00519821\n",
      "Iteration 7798, loss = 0.00519463\n",
      "Iteration 7799, loss = 0.00519106\n",
      "Iteration 7800, loss = 0.00518749\n",
      "Iteration 7801, loss = 0.00518392\n",
      "Iteration 7802, loss = 0.00518036\n",
      "Iteration 7803, loss = 0.00517680\n",
      "Iteration 7804, loss = 0.00517324\n",
      "Iteration 7805, loss = 0.00516968\n",
      "Iteration 7806, loss = 0.00516612\n",
      "Iteration 7807, loss = 0.00516257\n",
      "Iteration 7808, loss = 0.00515902\n",
      "Iteration 7809, loss = 0.00515548\n",
      "Iteration 7810, loss = 0.00515193\n",
      "Iteration 7811, loss = 0.00514839\n",
      "Iteration 7812, loss = 0.00514485\n",
      "Iteration 7813, loss = 0.00514132\n",
      "Iteration 7814, loss = 0.00513779\n",
      "Iteration 7815, loss = 0.00513426\n",
      "Iteration 7816, loss = 0.00513073\n",
      "Iteration 7817, loss = 0.00512720\n",
      "Iteration 7818, loss = 0.00512368\n",
      "Iteration 7819, loss = 0.00512016\n",
      "Iteration 7820, loss = 0.00511665\n",
      "Iteration 7821, loss = 0.00511313\n",
      "Iteration 7822, loss = 0.00510962\n",
      "Iteration 7823, loss = 0.00510611\n",
      "Iteration 7824, loss = 0.00510261\n",
      "Iteration 7825, loss = 0.00509910\n",
      "Iteration 7826, loss = 0.00509560\n",
      "Iteration 7827, loss = 0.00509211\n",
      "Iteration 7828, loss = 0.00508861\n",
      "Iteration 7829, loss = 0.00508512\n",
      "Iteration 7830, loss = 0.00508163\n",
      "Iteration 7831, loss = 0.00507814\n",
      "Iteration 7832, loss = 0.00507466\n",
      "Iteration 7833, loss = 0.00507118\n",
      "Iteration 7834, loss = 0.00506770\n",
      "Iteration 7835, loss = 0.00506422\n",
      "Iteration 7836, loss = 0.00506075\n",
      "Iteration 7837, loss = 0.00505728\n",
      "Iteration 7838, loss = 0.00505381\n",
      "Iteration 7839, loss = 0.00505035\n",
      "Iteration 7840, loss = 0.00504688\n",
      "Iteration 7841, loss = 0.00504342\n",
      "Iteration 7842, loss = 0.00503997\n",
      "Iteration 7843, loss = 0.00503651\n",
      "Iteration 7844, loss = 0.00503306\n",
      "Iteration 7845, loss = 0.00502961\n",
      "Iteration 7846, loss = 0.00502616\n",
      "Iteration 7847, loss = 0.00502272\n",
      "Iteration 7848, loss = 0.00501928\n",
      "Iteration 7849, loss = 0.00501584\n",
      "Iteration 7850, loss = 0.00501240\n",
      "Iteration 7851, loss = 0.00500897\n",
      "Iteration 7852, loss = 0.00500554\n",
      "Iteration 7853, loss = 0.00500211\n",
      "Iteration 7854, loss = 0.00499869\n",
      "Iteration 7855, loss = 0.00499526\n",
      "Iteration 7856, loss = 0.00499184\n",
      "Iteration 7857, loss = 0.00498843\n",
      "Iteration 7858, loss = 0.00498501\n",
      "Iteration 7859, loss = 0.00498160\n",
      "Iteration 7860, loss = 0.00497819\n",
      "Iteration 7861, loss = 0.00497478\n",
      "Iteration 7862, loss = 0.00497138\n",
      "Iteration 7863, loss = 0.00496798\n",
      "Iteration 7864, loss = 0.00496458\n",
      "Iteration 7865, loss = 0.00496119\n",
      "Iteration 7866, loss = 0.00495779\n",
      "Iteration 7867, loss = 0.00495440\n",
      "Iteration 7868, loss = 0.00495101\n",
      "Iteration 7869, loss = 0.00494763\n",
      "Iteration 7870, loss = 0.00494425\n",
      "Iteration 7871, loss = 0.00494087\n",
      "Iteration 7872, loss = 0.00493749\n",
      "Iteration 7873, loss = 0.00493411\n",
      "Iteration 7874, loss = 0.00493074\n",
      "Iteration 7875, loss = 0.00492737\n",
      "Iteration 7876, loss = 0.00492401\n",
      "Iteration 7877, loss = 0.00492064\n",
      "Iteration 7878, loss = 0.00491728\n",
      "Iteration 7879, loss = 0.00491392\n",
      "Iteration 7880, loss = 0.00491057\n",
      "Iteration 7881, loss = 0.00490721\n",
      "Iteration 7882, loss = 0.00490386\n",
      "Iteration 7883, loss = 0.00490051\n",
      "Iteration 7884, loss = 0.00489717\n",
      "Iteration 7885, loss = 0.00489383\n",
      "Iteration 7886, loss = 0.00489049\n",
      "Iteration 7887, loss = 0.00488715\n",
      "Iteration 7888, loss = 0.00488381\n",
      "Iteration 7889, loss = 0.00488048\n",
      "Iteration 7890, loss = 0.00487715\n",
      "Iteration 7891, loss = 0.00487382\n",
      "Iteration 7892, loss = 0.00487050\n",
      "Iteration 7893, loss = 0.00486718\n",
      "Iteration 7894, loss = 0.00486386\n",
      "Iteration 7895, loss = 0.00486054\n",
      "Iteration 7896, loss = 0.00485723\n",
      "Iteration 7897, loss = 0.00485392\n",
      "Iteration 7898, loss = 0.00485061\n",
      "Iteration 7899, loss = 0.00484730\n",
      "Iteration 7900, loss = 0.00484400\n",
      "Iteration 7901, loss = 0.00484070\n",
      "Iteration 7902, loss = 0.00483740\n",
      "Iteration 7903, loss = 0.00483411\n",
      "Iteration 7904, loss = 0.00483081\n",
      "Iteration 7905, loss = 0.00482752\n",
      "Iteration 7906, loss = 0.00482424\n",
      "Iteration 7907, loss = 0.00482095\n",
      "Iteration 7908, loss = 0.00481767\n",
      "Iteration 7909, loss = 0.00481439\n",
      "Iteration 7910, loss = 0.00481111\n",
      "Iteration 7911, loss = 0.00480784\n",
      "Iteration 7912, loss = 0.00480457\n",
      "Iteration 7913, loss = 0.00480130\n",
      "Iteration 7914, loss = 0.00479803\n",
      "Iteration 7915, loss = 0.00479477\n",
      "Iteration 7916, loss = 0.00479150\n",
      "Iteration 7917, loss = 0.00478825\n",
      "Iteration 7918, loss = 0.00478499\n",
      "Iteration 7919, loss = 0.00478174\n",
      "Iteration 7920, loss = 0.00477849\n",
      "Iteration 7921, loss = 0.00477524\n",
      "Iteration 7922, loss = 0.00477199\n",
      "Iteration 7923, loss = 0.00476875\n",
      "Iteration 7924, loss = 0.00476551\n",
      "Iteration 7925, loss = 0.00476227\n",
      "Iteration 7926, loss = 0.00475903\n",
      "Iteration 7927, loss = 0.00475580\n",
      "Iteration 7928, loss = 0.00475257\n",
      "Iteration 7929, loss = 0.00474934\n",
      "Iteration 7930, loss = 0.00474612\n",
      "Iteration 7931, loss = 0.00474290\n",
      "Iteration 7932, loss = 0.00473968\n",
      "Iteration 7933, loss = 0.00473646\n",
      "Iteration 7934, loss = 0.00473325\n",
      "Iteration 7935, loss = 0.00473003\n",
      "Iteration 7936, loss = 0.00472683\n",
      "Iteration 7937, loss = 0.00472362\n",
      "Iteration 7938, loss = 0.00472041\n",
      "Iteration 7939, loss = 0.00471721\n",
      "Iteration 7940, loss = 0.00471401\n",
      "Iteration 7941, loss = 0.00471082\n",
      "Iteration 7942, loss = 0.00470762\n",
      "Iteration 7943, loss = 0.00470443\n",
      "Iteration 7944, loss = 0.00470124\n",
      "Iteration 7945, loss = 0.00469806\n",
      "Iteration 7946, loss = 0.00469487\n",
      "Iteration 7947, loss = 0.00469169\n",
      "Iteration 7948, loss = 0.00468852\n",
      "Iteration 7949, loss = 0.00468534\n",
      "Iteration 7950, loss = 0.00468217\n",
      "Iteration 7951, loss = 0.00467900\n",
      "Iteration 7952, loss = 0.00467583\n",
      "Iteration 7953, loss = 0.00467266\n",
      "Iteration 7954, loss = 0.00466950\n",
      "Iteration 7955, loss = 0.00466634\n",
      "Iteration 7956, loss = 0.00466318\n",
      "Iteration 7957, loss = 0.00466003\n",
      "Iteration 7958, loss = 0.00465687\n",
      "Iteration 7959, loss = 0.00465372\n",
      "Iteration 7960, loss = 0.00465058\n",
      "Iteration 7961, loss = 0.00464743\n",
      "Iteration 7962, loss = 0.00464429\n",
      "Iteration 7963, loss = 0.00464115\n",
      "Iteration 7964, loss = 0.00463801\n",
      "Iteration 7965, loss = 0.00463488\n",
      "Iteration 7966, loss = 0.00463175\n",
      "Iteration 7967, loss = 0.00462862\n",
      "Iteration 7968, loss = 0.00462549\n",
      "Iteration 7969, loss = 0.00462237\n",
      "Iteration 7970, loss = 0.00461924\n",
      "Iteration 7971, loss = 0.00461613\n",
      "Iteration 7972, loss = 0.00461301\n",
      "Iteration 7973, loss = 0.00460989\n",
      "Iteration 7974, loss = 0.00460678\n",
      "Iteration 7975, loss = 0.00460367\n",
      "Iteration 7976, loss = 0.00460057\n",
      "Iteration 7977, loss = 0.00459746\n",
      "Iteration 7978, loss = 0.00459436\n",
      "Iteration 7979, loss = 0.00459126\n",
      "Iteration 7980, loss = 0.00458817\n",
      "Iteration 7981, loss = 0.00458507\n",
      "Iteration 7982, loss = 0.00458198\n",
      "Iteration 7983, loss = 0.00457889\n",
      "Iteration 7984, loss = 0.00457581\n",
      "Iteration 7985, loss = 0.00457272\n",
      "Iteration 7986, loss = 0.00456964\n",
      "Iteration 7987, loss = 0.00456656\n",
      "Iteration 7988, loss = 0.00456349\n",
      "Iteration 7989, loss = 0.00456041\n",
      "Iteration 7990, loss = 0.00455734\n",
      "Iteration 7991, loss = 0.00455427\n",
      "Iteration 7992, loss = 0.00455121\n",
      "Iteration 7993, loss = 0.00454815\n",
      "Iteration 7994, loss = 0.00454508\n",
      "Iteration 7995, loss = 0.00454203\n",
      "Iteration 7996, loss = 0.00453897\n",
      "Iteration 7997, loss = 0.00453592\n",
      "Iteration 7998, loss = 0.00453287\n",
      "Iteration 7999, loss = 0.00452982\n",
      "Iteration 8000, loss = 0.00452677\n",
      "Iteration 8001, loss = 0.00452373\n",
      "Iteration 8002, loss = 0.00452069\n",
      "Iteration 8003, loss = 0.00451765\n",
      "Iteration 8004, loss = 0.00451462\n",
      "Iteration 8005, loss = 0.00451158\n",
      "Iteration 8006, loss = 0.00450855\n",
      "Iteration 8007, loss = 0.00450552\n",
      "Iteration 8008, loss = 0.00450250\n",
      "Iteration 8009, loss = 0.00449948\n",
      "Iteration 8010, loss = 0.00449645\n",
      "Iteration 8011, loss = 0.00449344\n",
      "Iteration 8012, loss = 0.00449042\n",
      "Iteration 8013, loss = 0.00448741\n",
      "Iteration 8014, loss = 0.00448440\n",
      "Iteration 8015, loss = 0.00448139\n",
      "Iteration 8016, loss = 0.00447838\n",
      "Iteration 8017, loss = 0.00447538\n",
      "Iteration 8018, loss = 0.00447238\n",
      "Iteration 8019, loss = 0.00446938\n",
      "Iteration 8020, loss = 0.00446639\n",
      "Iteration 8021, loss = 0.00446339\n",
      "Iteration 8022, loss = 0.00446040\n",
      "Iteration 8023, loss = 0.00445742\n",
      "Iteration 8024, loss = 0.00445443\n",
      "Iteration 8025, loss = 0.00445145\n",
      "Iteration 8026, loss = 0.00444847\n",
      "Iteration 8027, loss = 0.00444549\n",
      "Iteration 8028, loss = 0.00444251\n",
      "Iteration 8029, loss = 0.00443954\n",
      "Iteration 8030, loss = 0.00443657\n",
      "Iteration 8031, loss = 0.00443360\n",
      "Iteration 8032, loss = 0.00443063\n",
      "Iteration 8033, loss = 0.00442767\n",
      "Iteration 8034, loss = 0.00442471\n",
      "Iteration 8035, loss = 0.00442175\n",
      "Iteration 8036, loss = 0.00441880\n",
      "Iteration 8037, loss = 0.00441584\n",
      "Iteration 8038, loss = 0.00441289\n",
      "Iteration 8039, loss = 0.00440994\n",
      "Iteration 8040, loss = 0.00440700\n",
      "Iteration 8041, loss = 0.00440405\n",
      "Iteration 8042, loss = 0.00440111\n",
      "Iteration 8043, loss = 0.00439818\n",
      "Iteration 8044, loss = 0.00439524\n",
      "Iteration 8045, loss = 0.00439231\n",
      "Iteration 8046, loss = 0.00438937\n",
      "Iteration 8047, loss = 0.00438645\n",
      "Iteration 8048, loss = 0.00438352\n",
      "Iteration 8049, loss = 0.00438060\n",
      "Iteration 8050, loss = 0.00437768\n",
      "Iteration 8051, loss = 0.00437476\n",
      "Iteration 8052, loss = 0.00437184\n",
      "Iteration 8053, loss = 0.00436893\n",
      "Iteration 8054, loss = 0.00436601\n",
      "Iteration 8055, loss = 0.00436311\n",
      "Iteration 8056, loss = 0.00436020\n",
      "Iteration 8057, loss = 0.00435730\n",
      "Iteration 8058, loss = 0.00435439\n",
      "Iteration 8059, loss = 0.00435149\n",
      "Iteration 8060, loss = 0.00434860\n",
      "Iteration 8061, loss = 0.00434570\n",
      "Iteration 8062, loss = 0.00434281\n",
      "Iteration 8063, loss = 0.00433992\n",
      "Iteration 8064, loss = 0.00433704\n",
      "Iteration 8065, loss = 0.00433415\n",
      "Iteration 8066, loss = 0.00433127\n",
      "Iteration 8067, loss = 0.00432839\n",
      "Iteration 8068, loss = 0.00432551\n",
      "Iteration 8069, loss = 0.00432264\n",
      "Iteration 8070, loss = 0.00431976\n",
      "Iteration 8071, loss = 0.00431689\n",
      "Iteration 8072, loss = 0.00431403\n",
      "Iteration 8073, loss = 0.00431116\n",
      "Iteration 8074, loss = 0.00430830\n",
      "Iteration 8075, loss = 0.00430544\n",
      "Iteration 8076, loss = 0.00430258\n",
      "Iteration 8077, loss = 0.00429973\n",
      "Iteration 8078, loss = 0.00429687\n",
      "Iteration 8079, loss = 0.00429402\n",
      "Iteration 8080, loss = 0.00429117\n",
      "Iteration 8081, loss = 0.00428833\n",
      "Iteration 8082, loss = 0.00428549\n",
      "Iteration 8083, loss = 0.00428264\n",
      "Iteration 8084, loss = 0.00427981\n",
      "Iteration 8085, loss = 0.00427697\n",
      "Iteration 8086, loss = 0.00427414\n",
      "Iteration 8087, loss = 0.00427131\n",
      "Iteration 8088, loss = 0.00426848\n",
      "Iteration 8089, loss = 0.00426565\n",
      "Iteration 8090, loss = 0.00426283\n",
      "Iteration 8091, loss = 0.00426000\n",
      "Iteration 8092, loss = 0.00425719\n",
      "Iteration 8093, loss = 0.00425437\n",
      "Iteration 8094, loss = 0.00425155\n",
      "Iteration 8095, loss = 0.00424874\n",
      "Iteration 8096, loss = 0.00424593\n",
      "Iteration 8097, loss = 0.00424313\n",
      "Iteration 8098, loss = 0.00424032\n",
      "Iteration 8099, loss = 0.00423752\n",
      "Iteration 8100, loss = 0.00423472\n",
      "Iteration 8101, loss = 0.00423192\n",
      "Iteration 8102, loss = 0.00422913\n",
      "Iteration 8103, loss = 0.00422633\n",
      "Iteration 8104, loss = 0.00422354\n",
      "Iteration 8105, loss = 0.00422075\n",
      "Iteration 8106, loss = 0.00421797\n",
      "Iteration 8107, loss = 0.00421518\n",
      "Iteration 8108, loss = 0.00421240\n",
      "Iteration 8109, loss = 0.00420963\n",
      "Iteration 8110, loss = 0.00420685\n",
      "Iteration 8111, loss = 0.00420408\n",
      "Iteration 8112, loss = 0.00420130\n",
      "Iteration 8113, loss = 0.00419854\n",
      "Iteration 8114, loss = 0.00419577\n",
      "Iteration 8115, loss = 0.00419300\n",
      "Iteration 8116, loss = 0.00419024\n",
      "Iteration 8117, loss = 0.00418748\n",
      "Iteration 8118, loss = 0.00418473\n",
      "Iteration 8119, loss = 0.00418197\n",
      "Iteration 8120, loss = 0.00417922\n",
      "Iteration 8121, loss = 0.00417647\n",
      "Iteration 8122, loss = 0.00417372\n",
      "Iteration 8123, loss = 0.00417097\n",
      "Iteration 8124, loss = 0.00416823\n",
      "Iteration 8125, loss = 0.00416549\n",
      "Iteration 8126, loss = 0.00416275\n",
      "Iteration 8127, loss = 0.00416002\n",
      "Iteration 8128, loss = 0.00415728\n",
      "Iteration 8129, loss = 0.00415455\n",
      "Iteration 8130, loss = 0.00415182\n",
      "Iteration 8131, loss = 0.00414910\n",
      "Iteration 8132, loss = 0.00414637\n",
      "Iteration 8133, loss = 0.00414365\n",
      "Iteration 8134, loss = 0.00414093\n",
      "Iteration 8135, loss = 0.00413821\n",
      "Iteration 8136, loss = 0.00413550\n",
      "Iteration 8137, loss = 0.00413279\n",
      "Iteration 8138, loss = 0.00413008\n",
      "Iteration 8139, loss = 0.00412737\n",
      "Iteration 8140, loss = 0.00412466\n",
      "Iteration 8141, loss = 0.00412196\n",
      "Iteration 8142, loss = 0.00411926\n",
      "Iteration 8143, loss = 0.00411656\n",
      "Iteration 8144, loss = 0.00411387\n",
      "Iteration 8145, loss = 0.00411117\n",
      "Iteration 8146, loss = 0.00410848\n",
      "Iteration 8147, loss = 0.00410579\n",
      "Iteration 8148, loss = 0.00410310\n",
      "Iteration 8149, loss = 0.00410042\n",
      "Iteration 8150, loss = 0.00409774\n",
      "Iteration 8151, loss = 0.00409506\n",
      "Iteration 8152, loss = 0.00409238\n",
      "Iteration 8153, loss = 0.00408970\n",
      "Iteration 8154, loss = 0.00408703\n",
      "Iteration 8155, loss = 0.00408436\n",
      "Iteration 8156, loss = 0.00408169\n",
      "Iteration 8157, loss = 0.00407903\n",
      "Iteration 8158, loss = 0.00407636\n",
      "Iteration 8159, loss = 0.00407370\n",
      "Iteration 8160, loss = 0.00407104\n",
      "Iteration 8161, loss = 0.00406839\n",
      "Iteration 8162, loss = 0.00406573\n",
      "Iteration 8163, loss = 0.00406308\n",
      "Iteration 8164, loss = 0.00406043\n",
      "Iteration 8165, loss = 0.00405778\n",
      "Iteration 8166, loss = 0.00405514\n",
      "Iteration 8167, loss = 0.00405249\n",
      "Iteration 8168, loss = 0.00404985\n",
      "Iteration 8169, loss = 0.00404722\n",
      "Iteration 8170, loss = 0.00404458\n",
      "Iteration 8171, loss = 0.00404195\n",
      "Iteration 8172, loss = 0.00403931\n",
      "Iteration 8173, loss = 0.00403669\n",
      "Iteration 8174, loss = 0.00403406\n",
      "Iteration 8175, loss = 0.00403143\n",
      "Iteration 8176, loss = 0.00402881\n",
      "Iteration 8177, loss = 0.00402619\n",
      "Iteration 8178, loss = 0.00402357\n",
      "Iteration 8179, loss = 0.00402096\n",
      "Iteration 8180, loss = 0.00401835\n",
      "Iteration 8181, loss = 0.00401573\n",
      "Iteration 8182, loss = 0.00401313\n",
      "Iteration 8183, loss = 0.00401052\n",
      "Iteration 8184, loss = 0.00400792\n",
      "Iteration 8185, loss = 0.00400531\n",
      "Iteration 8186, loss = 0.00400271\n",
      "Iteration 8187, loss = 0.00400012\n",
      "Iteration 8188, loss = 0.00399752\n",
      "Iteration 8189, loss = 0.00399493\n",
      "Iteration 8190, loss = 0.00399234\n",
      "Iteration 8191, loss = 0.00398975\n",
      "Iteration 8192, loss = 0.00398717\n",
      "Iteration 8193, loss = 0.00398458\n",
      "Iteration 8194, loss = 0.00398200\n",
      "Iteration 8195, loss = 0.00397942\n",
      "Iteration 8196, loss = 0.00397684\n",
      "Iteration 8197, loss = 0.00397427\n",
      "Iteration 8198, loss = 0.00397170\n",
      "Iteration 8199, loss = 0.00396913\n",
      "Iteration 8200, loss = 0.00396656\n",
      "Iteration 8201, loss = 0.00396399\n",
      "Iteration 8202, loss = 0.00396143\n",
      "Iteration 8203, loss = 0.00395887\n",
      "Iteration 8204, loss = 0.00395631\n",
      "Iteration 8205, loss = 0.00395375\n",
      "Iteration 8206, loss = 0.00395120\n",
      "Iteration 8207, loss = 0.00394865\n",
      "Iteration 8208, loss = 0.00394610\n",
      "Iteration 8209, loss = 0.00394355\n",
      "Iteration 8210, loss = 0.00394101\n",
      "Iteration 8211, loss = 0.00393846\n",
      "Iteration 8212, loss = 0.00393592\n",
      "Iteration 8213, loss = 0.00393338\n",
      "Iteration 8214, loss = 0.00393085\n",
      "Iteration 8215, loss = 0.00392831\n",
      "Iteration 8216, loss = 0.00392578\n",
      "Iteration 8217, loss = 0.00392325\n",
      "Iteration 8218, loss = 0.00392072\n",
      "Iteration 8219, loss = 0.00391820\n",
      "Iteration 8220, loss = 0.00391567\n",
      "Iteration 8221, loss = 0.00391315\n",
      "Iteration 8222, loss = 0.00391063\n",
      "Iteration 8223, loss = 0.00390812\n",
      "Iteration 8224, loss = 0.00390560\n",
      "Iteration 8225, loss = 0.00390309\n",
      "Iteration 8226, loss = 0.00390058\n",
      "Iteration 8227, loss = 0.00389808\n",
      "Iteration 8228, loss = 0.00389557\n",
      "Iteration 8229, loss = 0.00389307\n",
      "Iteration 8230, loss = 0.00389057\n",
      "Iteration 8231, loss = 0.00388807\n",
      "Iteration 8232, loss = 0.00388557\n",
      "Iteration 8233, loss = 0.00388308\n",
      "Iteration 8234, loss = 0.00388058\n",
      "Iteration 8235, loss = 0.00387809\n",
      "Iteration 8236, loss = 0.00387561\n",
      "Iteration 8237, loss = 0.00387312\n",
      "Iteration 8238, loss = 0.00387064\n",
      "Iteration 8239, loss = 0.00386816\n",
      "Iteration 8240, loss = 0.00386568\n",
      "Iteration 8241, loss = 0.00386320\n",
      "Iteration 8242, loss = 0.00386073\n",
      "Iteration 8243, loss = 0.00385825\n",
      "Iteration 8244, loss = 0.00385578\n",
      "Iteration 8245, loss = 0.00385332\n",
      "Iteration 8246, loss = 0.00385085\n",
      "Iteration 8247, loss = 0.00384839\n",
      "Iteration 8248, loss = 0.00384592\n",
      "Iteration 8249, loss = 0.00384347\n",
      "Iteration 8250, loss = 0.00384101\n",
      "Iteration 8251, loss = 0.00383855\n",
      "Iteration 8252, loss = 0.00383610\n",
      "Iteration 8253, loss = 0.00383365\n",
      "Iteration 8254, loss = 0.00383120\n",
      "Iteration 8255, loss = 0.00382876\n",
      "Iteration 8256, loss = 0.00382631\n",
      "Iteration 8257, loss = 0.00382387\n",
      "Iteration 8258, loss = 0.00382143\n",
      "Iteration 8259, loss = 0.00381899\n",
      "Iteration 8260, loss = 0.00381656\n",
      "Iteration 8261, loss = 0.00381412\n",
      "Iteration 8262, loss = 0.00381169\n",
      "Iteration 8263, loss = 0.00380926\n",
      "Iteration 8264, loss = 0.00380684\n",
      "Iteration 8265, loss = 0.00380441\n",
      "Iteration 8266, loss = 0.00380199\n",
      "Iteration 8267, loss = 0.00379957\n",
      "Iteration 8268, loss = 0.00379715\n",
      "Iteration 8269, loss = 0.00379474\n",
      "Iteration 8270, loss = 0.00379232\n",
      "Iteration 8271, loss = 0.00378991\n",
      "Iteration 8272, loss = 0.00378750\n",
      "Iteration 8273, loss = 0.00378509\n",
      "Iteration 8274, loss = 0.00378269\n",
      "Iteration 8275, loss = 0.00378029\n",
      "Iteration 8276, loss = 0.00377788\n",
      "Iteration 8277, loss = 0.00377549\n",
      "Iteration 8278, loss = 0.00377309\n",
      "Iteration 8279, loss = 0.00377069\n",
      "Iteration 8280, loss = 0.00376830\n",
      "Iteration 8281, loss = 0.00376591\n",
      "Iteration 8282, loss = 0.00376352\n",
      "Iteration 8283, loss = 0.00376114\n",
      "Iteration 8284, loss = 0.00375875\n",
      "Iteration 8285, loss = 0.00375637\n",
      "Iteration 8286, loss = 0.00375399\n",
      "Iteration 8287, loss = 0.00375161\n",
      "Iteration 8288, loss = 0.00374924\n",
      "Iteration 8289, loss = 0.00374687\n",
      "Iteration 8290, loss = 0.00374449\n",
      "Iteration 8291, loss = 0.00374213\n",
      "Iteration 8292, loss = 0.00373976\n",
      "Iteration 8293, loss = 0.00373739\n",
      "Iteration 8294, loss = 0.00373503\n",
      "Iteration 8295, loss = 0.00373267\n",
      "Iteration 8296, loss = 0.00373031\n",
      "Iteration 8297, loss = 0.00372796\n",
      "Iteration 8298, loss = 0.00372560\n",
      "Iteration 8299, loss = 0.00372325\n",
      "Iteration 8300, loss = 0.00372090\n",
      "Iteration 8301, loss = 0.00371855\n",
      "Iteration 8302, loss = 0.00371621\n",
      "Iteration 8303, loss = 0.00371386\n",
      "Iteration 8304, loss = 0.00371152\n",
      "Iteration 8305, loss = 0.00370918\n",
      "Iteration 8306, loss = 0.00370684\n",
      "Iteration 8307, loss = 0.00370451\n",
      "Iteration 8308, loss = 0.00370218\n",
      "Iteration 8309, loss = 0.00369984\n",
      "Iteration 8310, loss = 0.00369751\n",
      "Iteration 8311, loss = 0.00369519\n",
      "Iteration 8312, loss = 0.00369286\n",
      "Iteration 8313, loss = 0.00369054\n",
      "Iteration 8314, loss = 0.00368822\n",
      "Iteration 8315, loss = 0.00368590\n",
      "Iteration 8316, loss = 0.00368358\n",
      "Iteration 8317, loss = 0.00368127\n",
      "Iteration 8318, loss = 0.00367896\n",
      "Iteration 8319, loss = 0.00367665\n",
      "Iteration 8320, loss = 0.00367434\n",
      "Iteration 8321, loss = 0.00367203\n",
      "Iteration 8322, loss = 0.00366973\n",
      "Iteration 8323, loss = 0.00366743\n",
      "Iteration 8324, loss = 0.00366513\n",
      "Iteration 8325, loss = 0.00366283\n",
      "Iteration 8326, loss = 0.00366053\n",
      "Iteration 8327, loss = 0.00365824\n",
      "Iteration 8328, loss = 0.00365595\n",
      "Iteration 8329, loss = 0.00365366\n",
      "Iteration 8330, loss = 0.00365137\n",
      "Iteration 8331, loss = 0.00364908\n",
      "Iteration 8332, loss = 0.00364680\n",
      "Iteration 8333, loss = 0.00364452\n",
      "Iteration 8334, loss = 0.00364224\n",
      "Iteration 8335, loss = 0.00363996\n",
      "Iteration 8336, loss = 0.00363769\n",
      "Iteration 8337, loss = 0.00363541\n",
      "Iteration 8338, loss = 0.00363314\n",
      "Iteration 8339, loss = 0.00363087\n",
      "Iteration 8340, loss = 0.00362861\n",
      "Iteration 8341, loss = 0.00362634\n",
      "Iteration 8342, loss = 0.00362408\n",
      "Iteration 8343, loss = 0.00362182\n",
      "Iteration 8344, loss = 0.00361956\n",
      "Iteration 8345, loss = 0.00361730\n",
      "Iteration 8346, loss = 0.00361505\n",
      "Iteration 8347, loss = 0.00361279\n",
      "Iteration 8348, loss = 0.00361054\n",
      "Iteration 8349, loss = 0.00360830\n",
      "Iteration 8350, loss = 0.00360605\n",
      "Iteration 8351, loss = 0.00360380\n",
      "Iteration 8352, loss = 0.00360156\n",
      "Iteration 8353, loss = 0.00359932\n",
      "Iteration 8354, loss = 0.00359708\n",
      "Iteration 8355, loss = 0.00359485\n",
      "Iteration 8356, loss = 0.00359261\n",
      "Iteration 8357, loss = 0.00359038\n",
      "Iteration 8358, loss = 0.00358815\n",
      "Iteration 8359, loss = 0.00358592\n",
      "Iteration 8360, loss = 0.00358369\n",
      "Iteration 8361, loss = 0.00358147\n",
      "Iteration 8362, loss = 0.00357925\n",
      "Iteration 8363, loss = 0.00357703\n",
      "Iteration 8364, loss = 0.00357481\n",
      "Iteration 8365, loss = 0.00357259\n",
      "Iteration 8366, loss = 0.00357038\n",
      "Iteration 8367, loss = 0.00356816\n",
      "Iteration 8368, loss = 0.00356595\n",
      "Iteration 8369, loss = 0.00356375\n",
      "Iteration 8370, loss = 0.00356154\n",
      "Iteration 8371, loss = 0.00355933\n",
      "Iteration 8372, loss = 0.00355713\n",
      "Iteration 8373, loss = 0.00355493\n",
      "Iteration 8374, loss = 0.00355273\n",
      "Iteration 8375, loss = 0.00355054\n",
      "Iteration 8376, loss = 0.00354834\n",
      "Iteration 8377, loss = 0.00354615\n",
      "Iteration 8378, loss = 0.00354396\n",
      "Iteration 8379, loss = 0.00354177\n",
      "Iteration 8380, loss = 0.00353958\n",
      "Iteration 8381, loss = 0.00353740\n",
      "Iteration 8382, loss = 0.00353522\n",
      "Iteration 8383, loss = 0.00353304\n",
      "Iteration 8384, loss = 0.00353086\n",
      "Iteration 8385, loss = 0.00352868\n",
      "Iteration 8386, loss = 0.00352651\n",
      "Iteration 8387, loss = 0.00352433\n",
      "Iteration 8388, loss = 0.00352216\n",
      "Iteration 8389, loss = 0.00352000\n",
      "Iteration 8390, loss = 0.00351783\n",
      "Iteration 8391, loss = 0.00351566\n",
      "Iteration 8392, loss = 0.00351350\n",
      "Iteration 8393, loss = 0.00351134\n",
      "Iteration 8394, loss = 0.00350918\n",
      "Iteration 8395, loss = 0.00350702\n",
      "Iteration 8396, loss = 0.00350487\n",
      "Iteration 8397, loss = 0.00350272\n",
      "Iteration 8398, loss = 0.00350057\n",
      "Iteration 8399, loss = 0.00349842\n",
      "Iteration 8400, loss = 0.00349627\n",
      "Iteration 8401, loss = 0.00349412\n",
      "Iteration 8402, loss = 0.00349198\n",
      "Iteration 8403, loss = 0.00348984\n",
      "Iteration 8404, loss = 0.00348770\n",
      "Iteration 8405, loss = 0.00348556\n",
      "Iteration 8406, loss = 0.00348343\n",
      "Iteration 8407, loss = 0.00348129\n",
      "Iteration 8408, loss = 0.00347916\n",
      "Iteration 8409, loss = 0.00347703\n",
      "Iteration 8410, loss = 0.00347491\n",
      "Iteration 8411, loss = 0.00347278\n",
      "Iteration 8412, loss = 0.00347066\n",
      "Iteration 8413, loss = 0.00346853\n",
      "Iteration 8414, loss = 0.00346641\n",
      "Iteration 8415, loss = 0.00346430\n",
      "Iteration 8416, loss = 0.00346218\n",
      "Iteration 8417, loss = 0.00346007\n",
      "Iteration 8418, loss = 0.00345795\n",
      "Iteration 8419, loss = 0.00345584\n",
      "Iteration 8420, loss = 0.00345374\n",
      "Iteration 8421, loss = 0.00345163\n",
      "Iteration 8422, loss = 0.00344952\n",
      "Iteration 8423, loss = 0.00344742\n",
      "Iteration 8424, loss = 0.00344532\n",
      "Iteration 8425, loss = 0.00344322\n",
      "Iteration 8426, loss = 0.00344113\n",
      "Iteration 8427, loss = 0.00343903\n",
      "Iteration 8428, loss = 0.00343694\n",
      "Iteration 8429, loss = 0.00343485\n",
      "Iteration 8430, loss = 0.00343276\n",
      "Iteration 8431, loss = 0.00343067\n",
      "Iteration 8432, loss = 0.00342859\n",
      "Iteration 8433, loss = 0.00342650\n",
      "Iteration 8434, loss = 0.00342442\n",
      "Iteration 8435, loss = 0.00342234\n",
      "Iteration 8436, loss = 0.00342026\n",
      "Iteration 8437, loss = 0.00341819\n",
      "Iteration 8438, loss = 0.00341611\n",
      "Iteration 8439, loss = 0.00341404\n",
      "Iteration 8440, loss = 0.00341197\n",
      "Iteration 8441, loss = 0.00340990\n",
      "Iteration 8442, loss = 0.00340784\n",
      "Iteration 8443, loss = 0.00340577\n",
      "Iteration 8444, loss = 0.00340371\n",
      "Iteration 8445, loss = 0.00340165\n",
      "Iteration 8446, loss = 0.00339959\n",
      "Iteration 8447, loss = 0.00339753\n",
      "Iteration 8448, loss = 0.00339548\n",
      "Iteration 8449, loss = 0.00339343\n",
      "Iteration 8450, loss = 0.00339137\n",
      "Iteration 8451, loss = 0.00338932\n",
      "Iteration 8452, loss = 0.00338728\n",
      "Iteration 8453, loss = 0.00338523\n",
      "Iteration 8454, loss = 0.00338319\n",
      "Iteration 8455, loss = 0.00338115\n",
      "Iteration 8456, loss = 0.00337911\n",
      "Iteration 8457, loss = 0.00337707\n",
      "Iteration 8458, loss = 0.00337503\n",
      "Iteration 8459, loss = 0.00337300\n",
      "Iteration 8460, loss = 0.00337096\n",
      "Iteration 8461, loss = 0.00336893\n",
      "Iteration 8462, loss = 0.00336691\n",
      "Iteration 8463, loss = 0.00336488\n",
      "Iteration 8464, loss = 0.00336285\n",
      "Iteration 8465, loss = 0.00336083\n",
      "Iteration 8466, loss = 0.00335881\n",
      "Iteration 8467, loss = 0.00335679\n",
      "Iteration 8468, loss = 0.00335477\n",
      "Iteration 8469, loss = 0.00335276\n",
      "Iteration 8470, loss = 0.00335074\n",
      "Iteration 8471, loss = 0.00334873\n",
      "Iteration 8472, loss = 0.00334672\n",
      "Iteration 8473, loss = 0.00334471\n",
      "Iteration 8474, loss = 0.00334270\n",
      "Iteration 8475, loss = 0.00334070\n",
      "Iteration 8476, loss = 0.00333870\n",
      "Iteration 8477, loss = 0.00333670\n",
      "Iteration 8478, loss = 0.00333470\n",
      "Iteration 8479, loss = 0.00333270\n",
      "Iteration 8480, loss = 0.00333070\n",
      "Iteration 8481, loss = 0.00332871\n",
      "Iteration 8482, loss = 0.00332672\n",
      "Iteration 8483, loss = 0.00332473\n",
      "Iteration 8484, loss = 0.00332274\n",
      "Iteration 8485, loss = 0.00332075\n",
      "Iteration 8486, loss = 0.00331877\n",
      "Iteration 8487, loss = 0.00331679\n",
      "Iteration 8488, loss = 0.00331481\n",
      "Iteration 8489, loss = 0.00331283\n",
      "Iteration 8490, loss = 0.00331085\n",
      "Iteration 8491, loss = 0.00330887\n",
      "Iteration 8492, loss = 0.00330690\n",
      "Iteration 8493, loss = 0.00330493\n",
      "Iteration 8494, loss = 0.00330296\n",
      "Iteration 8495, loss = 0.00330099\n",
      "Iteration 8496, loss = 0.00329902\n",
      "Iteration 8497, loss = 0.00329706\n",
      "Iteration 8498, loss = 0.00329510\n",
      "Iteration 8499, loss = 0.00329314\n",
      "Iteration 8500, loss = 0.00329118\n",
      "Iteration 8501, loss = 0.00328922\n",
      "Iteration 8502, loss = 0.00328726\n",
      "Iteration 8503, loss = 0.00328531\n",
      "Iteration 8504, loss = 0.00328336\n",
      "Iteration 8505, loss = 0.00328141\n",
      "Iteration 8506, loss = 0.00327946\n",
      "Iteration 8507, loss = 0.00327751\n",
      "Iteration 8508, loss = 0.00327557\n",
      "Iteration 8509, loss = 0.00327363\n",
      "Iteration 8510, loss = 0.00327169\n",
      "Iteration 8511, loss = 0.00326975\n",
      "Iteration 8512, loss = 0.00326781\n",
      "Iteration 8513, loss = 0.00326587\n",
      "Iteration 8514, loss = 0.00326394\n",
      "Iteration 8515, loss = 0.00326201\n",
      "Iteration 8516, loss = 0.00326008\n",
      "Iteration 8517, loss = 0.00325815\n",
      "Iteration 8518, loss = 0.00325622\n",
      "Iteration 8519, loss = 0.00325430\n",
      "Iteration 8520, loss = 0.00325237\n",
      "Iteration 8521, loss = 0.00325045\n",
      "Iteration 8522, loss = 0.00324853\n",
      "Iteration 8523, loss = 0.00324661\n",
      "Iteration 8524, loss = 0.00324470\n",
      "Iteration 8525, loss = 0.00324278\n",
      "Iteration 8526, loss = 0.00324087\n",
      "Iteration 8527, loss = 0.00323896\n",
      "Iteration 8528, loss = 0.00323705\n",
      "Iteration 8529, loss = 0.00323514\n",
      "Iteration 8530, loss = 0.00323324\n",
      "Iteration 8531, loss = 0.00323133\n",
      "Iteration 8532, loss = 0.00322943\n",
      "Iteration 8533, loss = 0.00322753\n",
      "Iteration 8534, loss = 0.00322563\n",
      "Iteration 8535, loss = 0.00322374\n",
      "Iteration 8536, loss = 0.00322184\n",
      "Iteration 8537, loss = 0.00321995\n",
      "Iteration 8538, loss = 0.00321806\n",
      "Iteration 8539, loss = 0.00321617\n",
      "Iteration 8540, loss = 0.00321428\n",
      "Iteration 8541, loss = 0.00321239\n",
      "Iteration 8542, loss = 0.00321051\n",
      "Iteration 8543, loss = 0.00320863\n",
      "Iteration 8544, loss = 0.00320675\n",
      "Iteration 8545, loss = 0.00320487\n",
      "Iteration 8546, loss = 0.00320299\n",
      "Iteration 8547, loss = 0.00320111\n",
      "Iteration 8548, loss = 0.00319924\n",
      "Iteration 8549, loss = 0.00319737\n",
      "Iteration 8550, loss = 0.00319550\n",
      "Iteration 8551, loss = 0.00319363\n",
      "Iteration 8552, loss = 0.00319176\n",
      "Iteration 8553, loss = 0.00318989\n",
      "Iteration 8554, loss = 0.00318803\n",
      "Iteration 8555, loss = 0.00318617\n",
      "Iteration 8556, loss = 0.00318431\n",
      "Iteration 8557, loss = 0.00318245\n",
      "Iteration 8558, loss = 0.00318059\n",
      "Iteration 8559, loss = 0.00317874\n",
      "Iteration 8560, loss = 0.00317689\n",
      "Iteration 8561, loss = 0.00317503\n",
      "Iteration 8562, loss = 0.00317318\n",
      "Iteration 8563, loss = 0.00317134\n",
      "Iteration 8564, loss = 0.00316949\n",
      "Iteration 8565, loss = 0.00316765\n",
      "Iteration 8566, loss = 0.00316580\n",
      "Iteration 8567, loss = 0.00316396\n",
      "Iteration 8568, loss = 0.00316212\n",
      "Iteration 8569, loss = 0.00316028\n",
      "Iteration 8570, loss = 0.00315845\n",
      "Iteration 8571, loss = 0.00315661\n",
      "Iteration 8572, loss = 0.00315478\n",
      "Iteration 8573, loss = 0.00315295\n",
      "Iteration 8574, loss = 0.00315112\n",
      "Iteration 8575, loss = 0.00314929\n",
      "Iteration 8576, loss = 0.00314747\n",
      "Iteration 8577, loss = 0.00314564\n",
      "Iteration 8578, loss = 0.00314382\n",
      "Iteration 8579, loss = 0.00314200\n",
      "Iteration 8580, loss = 0.00314018\n",
      "Iteration 8581, loss = 0.00313836\n",
      "Iteration 8582, loss = 0.00313655\n",
      "Iteration 8583, loss = 0.00313473\n",
      "Iteration 8584, loss = 0.00313292\n",
      "Iteration 8585, loss = 0.00313111\n",
      "Iteration 8586, loss = 0.00312930\n",
      "Iteration 8587, loss = 0.00312749\n",
      "Iteration 8588, loss = 0.00312569\n",
      "Iteration 8589, loss = 0.00312388\n",
      "Iteration 8590, loss = 0.00312208\n",
      "Iteration 8591, loss = 0.00312028\n",
      "Iteration 8592, loss = 0.00311848\n",
      "Iteration 8593, loss = 0.00311668\n",
      "Iteration 8594, loss = 0.00311489\n",
      "Iteration 8595, loss = 0.00311309\n",
      "Iteration 8596, loss = 0.00311130\n",
      "Iteration 8597, loss = 0.00310951\n",
      "Iteration 8598, loss = 0.00310772\n",
      "Iteration 8599, loss = 0.00310593\n",
      "Iteration 8600, loss = 0.00310415\n",
      "Iteration 8601, loss = 0.00310236\n",
      "Iteration 8602, loss = 0.00310058\n",
      "Iteration 8603, loss = 0.00309880\n",
      "Iteration 8604, loss = 0.00309702\n",
      "Iteration 8605, loss = 0.00309525\n",
      "Iteration 8606, loss = 0.00309347\n",
      "Iteration 8607, loss = 0.00309170\n",
      "Iteration 8608, loss = 0.00308992\n",
      "Iteration 8609, loss = 0.00308815\n",
      "Iteration 8610, loss = 0.00308638\n",
      "Iteration 8611, loss = 0.00308462\n",
      "Iteration 8612, loss = 0.00308285\n",
      "Iteration 8613, loss = 0.00308109\n",
      "Iteration 8614, loss = 0.00307932\n",
      "Iteration 8615, loss = 0.00307756\n",
      "Iteration 8616, loss = 0.00307580\n",
      "Iteration 8617, loss = 0.00307405\n",
      "Iteration 8618, loss = 0.00307229\n",
      "Iteration 8619, loss = 0.00307054\n",
      "Iteration 8620, loss = 0.00306878\n",
      "Iteration 8621, loss = 0.00306703\n",
      "Iteration 8622, loss = 0.00306528\n",
      "Iteration 8623, loss = 0.00306353\n",
      "Iteration 8624, loss = 0.00306179\n",
      "Iteration 8625, loss = 0.00306004\n",
      "Iteration 8626, loss = 0.00305830\n",
      "Iteration 8627, loss = 0.00305656\n",
      "Iteration 8628, loss = 0.00305482\n",
      "Iteration 8629, loss = 0.00305308\n",
      "Iteration 8630, loss = 0.00305135\n",
      "Iteration 8631, loss = 0.00304961\n",
      "Iteration 8632, loss = 0.00304788\n",
      "Iteration 8633, loss = 0.00304615\n",
      "Iteration 8634, loss = 0.00304442\n",
      "Iteration 8635, loss = 0.00304269\n",
      "Iteration 8636, loss = 0.00304096\n",
      "Iteration 8637, loss = 0.00303924\n",
      "Iteration 8638, loss = 0.00303751\n",
      "Iteration 8639, loss = 0.00303579\n",
      "Iteration 8640, loss = 0.00303407\n",
      "Iteration 8641, loss = 0.00303235\n",
      "Iteration 8642, loss = 0.00303064\n",
      "Iteration 8643, loss = 0.00302892\n",
      "Iteration 8644, loss = 0.00302721\n",
      "Iteration 8645, loss = 0.00302549\n",
      "Iteration 8646, loss = 0.00302378\n",
      "Iteration 8647, loss = 0.00302207\n",
      "Iteration 8648, loss = 0.00302037\n",
      "Iteration 8649, loss = 0.00301866\n",
      "Iteration 8650, loss = 0.00301696\n",
      "Iteration 8651, loss = 0.00301525\n",
      "Iteration 8652, loss = 0.00301355\n",
      "Iteration 8653, loss = 0.00301185\n",
      "Iteration 8654, loss = 0.00301016\n",
      "Iteration 8655, loss = 0.00300846\n",
      "Iteration 8656, loss = 0.00300676\n",
      "Iteration 8657, loss = 0.00300507\n",
      "Iteration 8658, loss = 0.00300338\n",
      "Iteration 8659, loss = 0.00300169\n",
      "Iteration 8660, loss = 0.00300000\n",
      "Iteration 8661, loss = 0.00299831\n",
      "Iteration 8662, loss = 0.00299663\n",
      "Iteration 8663, loss = 0.00299495\n",
      "Iteration 8664, loss = 0.00299326\n",
      "Iteration 8665, loss = 0.00299158\n",
      "Iteration 8666, loss = 0.00298990\n",
      "Iteration 8667, loss = 0.00298823\n",
      "Iteration 8668, loss = 0.00298655\n",
      "Iteration 8669, loss = 0.00298488\n",
      "Iteration 8670, loss = 0.00298320\n",
      "Iteration 8671, loss = 0.00298153\n",
      "Iteration 8672, loss = 0.00297986\n",
      "Iteration 8673, loss = 0.00297820\n",
      "Iteration 8674, loss = 0.00297653\n",
      "Iteration 8675, loss = 0.00297486\n",
      "Iteration 8676, loss = 0.00297320\n",
      "Iteration 8677, loss = 0.00297154\n",
      "Iteration 8678, loss = 0.00296988\n",
      "Iteration 8679, loss = 0.00296822\n",
      "Iteration 8680, loss = 0.00296656\n",
      "Iteration 8681, loss = 0.00296491\n",
      "Iteration 8682, loss = 0.00296325\n",
      "Iteration 8683, loss = 0.00296160\n",
      "Iteration 8684, loss = 0.00295995\n",
      "Iteration 8685, loss = 0.00295830\n",
      "Iteration 8686, loss = 0.00295665\n",
      "Iteration 8687, loss = 0.00295501\n",
      "Iteration 8688, loss = 0.00295336\n",
      "Iteration 8689, loss = 0.00295172\n",
      "Iteration 8690, loss = 0.00295008\n",
      "Iteration 8691, loss = 0.00294844\n",
      "Iteration 8692, loss = 0.00294680\n",
      "Iteration 8693, loss = 0.00294516\n",
      "Iteration 8694, loss = 0.00294353\n",
      "Iteration 8695, loss = 0.00294189\n",
      "Iteration 8696, loss = 0.00294026\n",
      "Iteration 8697, loss = 0.00293863\n",
      "Iteration 8698, loss = 0.00293700\n",
      "Iteration 8699, loss = 0.00293537\n",
      "Iteration 8700, loss = 0.00293375\n",
      "Iteration 8701, loss = 0.00293212\n",
      "Iteration 8702, loss = 0.00293050\n",
      "Iteration 8703, loss = 0.00292888\n",
      "Iteration 8704, loss = 0.00292726\n",
      "Iteration 8705, loss = 0.00292564\n",
      "Iteration 8706, loss = 0.00292402\n",
      "Iteration 8707, loss = 0.00292241\n",
      "Iteration 8708, loss = 0.00292079\n",
      "Iteration 8709, loss = 0.00291918\n",
      "Iteration 8710, loss = 0.00291757\n",
      "Iteration 8711, loss = 0.00291596\n",
      "Iteration 8712, loss = 0.00291435\n",
      "Iteration 8713, loss = 0.00291274\n",
      "Iteration 8714, loss = 0.00291114\n",
      "Iteration 8715, loss = 0.00290953\n",
      "Iteration 8716, loss = 0.00290793\n",
      "Iteration 8717, loss = 0.00290633\n",
      "Iteration 8718, loss = 0.00290473\n",
      "Iteration 8719, loss = 0.00290314\n",
      "Iteration 8720, loss = 0.00290154\n",
      "Iteration 8721, loss = 0.00289994\n",
      "Iteration 8722, loss = 0.00289835\n",
      "Iteration 8723, loss = 0.00289676\n",
      "Iteration 8724, loss = 0.00289517\n",
      "Iteration 8725, loss = 0.00289358\n",
      "Iteration 8726, loss = 0.00289199\n",
      "Iteration 8727, loss = 0.00289041\n",
      "Iteration 8728, loss = 0.00288883\n",
      "Iteration 8729, loss = 0.00288724\n",
      "Iteration 8730, loss = 0.00288566\n",
      "Iteration 8731, loss = 0.00288408\n",
      "Iteration 8732, loss = 0.00288250\n",
      "Iteration 8733, loss = 0.00288093\n",
      "Iteration 8734, loss = 0.00287935\n",
      "Iteration 8735, loss = 0.00287778\n",
      "Iteration 8736, loss = 0.00287621\n",
      "Iteration 8737, loss = 0.00287463\n",
      "Iteration 8738, loss = 0.00287307\n",
      "Iteration 8739, loss = 0.00287150\n",
      "Iteration 8740, loss = 0.00286993\n",
      "Iteration 8741, loss = 0.00286837\n",
      "Iteration 8742, loss = 0.00286680\n",
      "Iteration 8743, loss = 0.00286524\n",
      "Iteration 8744, loss = 0.00286368\n",
      "Iteration 8745, loss = 0.00286212\n",
      "Iteration 8746, loss = 0.00286056\n",
      "Iteration 8747, loss = 0.00285901\n",
      "Iteration 8748, loss = 0.00285745\n",
      "Iteration 8749, loss = 0.00285590\n",
      "Iteration 8750, loss = 0.00285435\n",
      "Iteration 8751, loss = 0.00285280\n",
      "Iteration 8752, loss = 0.00285125\n",
      "Iteration 8753, loss = 0.00284970\n",
      "Iteration 8754, loss = 0.00284816\n",
      "Iteration 8755, loss = 0.00284661\n",
      "Iteration 8756, loss = 0.00284507\n",
      "Iteration 8757, loss = 0.00284353\n",
      "Iteration 8758, loss = 0.00284199\n",
      "Iteration 8759, loss = 0.00284045\n",
      "Iteration 8760, loss = 0.00283891\n",
      "Iteration 8761, loss = 0.00283738\n",
      "Iteration 8762, loss = 0.00283584\n",
      "Iteration 8763, loss = 0.00283431\n",
      "Iteration 8764, loss = 0.00283278\n",
      "Iteration 8765, loss = 0.00283125\n",
      "Iteration 8766, loss = 0.00282972\n",
      "Iteration 8767, loss = 0.00282819\n",
      "Iteration 8768, loss = 0.00282667\n",
      "Iteration 8769, loss = 0.00282514\n",
      "Iteration 8770, loss = 0.00282362\n",
      "Iteration 8771, loss = 0.00282210\n",
      "Iteration 8772, loss = 0.00282058\n",
      "Iteration 8773, loss = 0.00281906\n",
      "Iteration 8774, loss = 0.00281754\n",
      "Iteration 8775, loss = 0.00281603\n",
      "Iteration 8776, loss = 0.00281451\n",
      "Iteration 8777, loss = 0.00281300\n",
      "Iteration 8778, loss = 0.00281149\n",
      "Iteration 8779, loss = 0.00280998\n",
      "Iteration 8780, loss = 0.00280847\n",
      "Iteration 8781, loss = 0.00280696\n",
      "Iteration 8782, loss = 0.00280546\n",
      "Iteration 8783, loss = 0.00280395\n",
      "Iteration 8784, loss = 0.00280245\n",
      "Iteration 8785, loss = 0.00280095\n",
      "Iteration 8786, loss = 0.00279945\n",
      "Iteration 8787, loss = 0.00279795\n",
      "Iteration 8788, loss = 0.00279645\n",
      "Iteration 8789, loss = 0.00279496\n",
      "Iteration 8790, loss = 0.00279346\n",
      "Iteration 8791, loss = 0.00279197\n",
      "Iteration 8792, loss = 0.00279048\n",
      "Iteration 8793, loss = 0.00278899\n",
      "Iteration 8794, loss = 0.00278750\n",
      "Iteration 8795, loss = 0.00278601\n",
      "Iteration 8796, loss = 0.00278453\n",
      "Iteration 8797, loss = 0.00278304\n",
      "Iteration 8798, loss = 0.00278156\n",
      "Iteration 8799, loss = 0.00278008\n",
      "Iteration 8800, loss = 0.00277860\n",
      "Iteration 8801, loss = 0.00277712\n",
      "Iteration 8802, loss = 0.00277564\n",
      "Iteration 8803, loss = 0.00277417\n",
      "Iteration 8804, loss = 0.00277269\n",
      "Iteration 8805, loss = 0.00277122\n",
      "Iteration 8806, loss = 0.00276975\n",
      "Iteration 8807, loss = 0.00276827\n",
      "Iteration 8808, loss = 0.00276681\n",
      "Iteration 8809, loss = 0.00276534\n",
      "Iteration 8810, loss = 0.00276387\n",
      "Iteration 8811, loss = 0.00276241\n",
      "Iteration 8812, loss = 0.00276094\n",
      "Iteration 8813, loss = 0.00275948\n",
      "Iteration 8814, loss = 0.00275802\n",
      "Iteration 8815, loss = 0.00275656\n",
      "Iteration 8816, loss = 0.00275510\n",
      "Iteration 8817, loss = 0.00275364\n",
      "Iteration 8818, loss = 0.00275219\n",
      "Iteration 8819, loss = 0.00275074\n",
      "Iteration 8820, loss = 0.00274928\n",
      "Iteration 8821, loss = 0.00274783\n",
      "Iteration 8822, loss = 0.00274638\n",
      "Iteration 8823, loss = 0.00274493\n",
      "Iteration 8824, loss = 0.00274349\n",
      "Iteration 8825, loss = 0.00274204\n",
      "Iteration 8826, loss = 0.00274060\n",
      "Iteration 8827, loss = 0.00273915\n",
      "Iteration 8828, loss = 0.00273771\n",
      "Iteration 8829, loss = 0.00273627\n",
      "Iteration 8830, loss = 0.00273483\n",
      "Iteration 8831, loss = 0.00273339\n",
      "Iteration 8832, loss = 0.00273196\n",
      "Iteration 8833, loss = 0.00273052\n",
      "Iteration 8834, loss = 0.00272909\n",
      "Iteration 8835, loss = 0.00272766\n",
      "Iteration 8836, loss = 0.00272623\n",
      "Iteration 8837, loss = 0.00272480\n",
      "Iteration 8838, loss = 0.00272337\n",
      "Iteration 8839, loss = 0.00272194\n",
      "Iteration 8840, loss = 0.00272052\n",
      "Iteration 8841, loss = 0.00271909\n",
      "Iteration 8842, loss = 0.00271767\n",
      "Iteration 8843, loss = 0.00271625\n",
      "Iteration 8844, loss = 0.00271483\n",
      "Iteration 8845, loss = 0.00271341\n",
      "Iteration 8846, loss = 0.00271199\n",
      "Iteration 8847, loss = 0.00271057\n",
      "Iteration 8848, loss = 0.00270916\n",
      "Iteration 8849, loss = 0.00270775\n",
      "Iteration 8850, loss = 0.00270633\n",
      "Iteration 8851, loss = 0.00270492\n",
      "Iteration 8852, loss = 0.00270351\n",
      "Iteration 8853, loss = 0.00270211\n",
      "Iteration 8854, loss = 0.00270070\n",
      "Iteration 8855, loss = 0.00269929\n",
      "Iteration 8856, loss = 0.00269789\n",
      "Iteration 8857, loss = 0.00269649\n",
      "Iteration 8858, loss = 0.00269509\n",
      "Iteration 8859, loss = 0.00269368\n",
      "Iteration 8860, loss = 0.00269229\n",
      "Iteration 8861, loss = 0.00269089\n",
      "Iteration 8862, loss = 0.00268949\n",
      "Iteration 8863, loss = 0.00268810\n",
      "Iteration 8864, loss = 0.00268670\n",
      "Iteration 8865, loss = 0.00268531\n",
      "Iteration 8866, loss = 0.00268392\n",
      "Iteration 8867, loss = 0.00268253\n",
      "Iteration 8868, loss = 0.00268114\n",
      "Iteration 8869, loss = 0.00267976\n",
      "Iteration 8870, loss = 0.00267837\n",
      "Iteration 8871, loss = 0.00267699\n",
      "Iteration 8872, loss = 0.00267560\n",
      "Iteration 8873, loss = 0.00267422\n",
      "Iteration 8874, loss = 0.00267284\n",
      "Iteration 8875, loss = 0.00267146\n",
      "Iteration 8876, loss = 0.00267008\n",
      "Iteration 8877, loss = 0.00266871\n",
      "Iteration 8878, loss = 0.00266733\n",
      "Iteration 8879, loss = 0.00266596\n",
      "Iteration 8880, loss = 0.00266458\n",
      "Iteration 8881, loss = 0.00266321\n",
      "Iteration 8882, loss = 0.00266184\n",
      "Iteration 8883, loss = 0.00266047\n",
      "Iteration 8884, loss = 0.00265911\n",
      "Iteration 8885, loss = 0.00265774\n",
      "Iteration 8886, loss = 0.00265638\n",
      "Iteration 8887, loss = 0.00265501\n",
      "Iteration 8888, loss = 0.00265365\n",
      "Iteration 8889, loss = 0.00265229\n",
      "Iteration 8890, loss = 0.00265093\n",
      "Iteration 8891, loss = 0.00264957\n",
      "Iteration 8892, loss = 0.00264821\n",
      "Iteration 8893, loss = 0.00264686\n",
      "Iteration 8894, loss = 0.00264550\n",
      "Iteration 8895, loss = 0.00264415\n",
      "Iteration 8896, loss = 0.00264280\n",
      "Iteration 8897, loss = 0.00264145\n",
      "Iteration 8898, loss = 0.00264010\n",
      "Iteration 8899, loss = 0.00263875\n",
      "Iteration 8900, loss = 0.00263740\n",
      "Iteration 8901, loss = 0.00263606\n",
      "Iteration 8902, loss = 0.00263471\n",
      "Iteration 8903, loss = 0.00263337\n",
      "Iteration 8904, loss = 0.00263203\n",
      "Iteration 8905, loss = 0.00263069\n",
      "Iteration 8906, loss = 0.00262935\n",
      "Iteration 8907, loss = 0.00262801\n",
      "Iteration 8908, loss = 0.00262667\n",
      "Iteration 8909, loss = 0.00262534\n",
      "Iteration 8910, loss = 0.00262400\n",
      "Iteration 8911, loss = 0.00262267\n",
      "Iteration 8912, loss = 0.00262134\n",
      "Iteration 8913, loss = 0.00262001\n",
      "Iteration 8914, loss = 0.00261868\n",
      "Iteration 8915, loss = 0.00261735\n",
      "Iteration 8916, loss = 0.00261602\n",
      "Iteration 8917, loss = 0.00261470\n",
      "Iteration 8918, loss = 0.00261337\n",
      "Iteration 8919, loss = 0.00261205\n",
      "Iteration 8920, loss = 0.00261073\n",
      "Iteration 8921, loss = 0.00260941\n",
      "Iteration 8922, loss = 0.00260809\n",
      "Iteration 8923, loss = 0.00260677\n",
      "Iteration 8924, loss = 0.00260545\n",
      "Iteration 8925, loss = 0.00260414\n",
      "Iteration 8926, loss = 0.00260282\n",
      "Iteration 8927, loss = 0.00260151\n",
      "Iteration 8928, loss = 0.00260020\n",
      "Iteration 8929, loss = 0.00259889\n",
      "Iteration 8930, loss = 0.00259758\n",
      "Iteration 8931, loss = 0.00259627\n",
      "Iteration 8932, loss = 0.00259497\n",
      "Iteration 8933, loss = 0.00259366\n",
      "Iteration 8934, loss = 0.00259236\n",
      "Iteration 8935, loss = 0.00259105\n",
      "Iteration 8936, loss = 0.00258975\n",
      "Iteration 8937, loss = 0.00258845\n",
      "Iteration 8938, loss = 0.00258715\n",
      "Iteration 8939, loss = 0.00258585\n",
      "Iteration 8940, loss = 0.00258456\n",
      "Iteration 8941, loss = 0.00258326\n",
      "Iteration 8942, loss = 0.00258196\n",
      "Iteration 8943, loss = 0.00258067\n",
      "Iteration 8944, loss = 0.00257938\n",
      "Iteration 8945, loss = 0.00257809\n",
      "Iteration 8946, loss = 0.00257680\n",
      "Iteration 8947, loss = 0.00257551\n",
      "Iteration 8948, loss = 0.00257422\n",
      "Iteration 8949, loss = 0.00257294\n",
      "Iteration 8950, loss = 0.00257165\n",
      "Iteration 8951, loss = 0.00257037\n",
      "Iteration 8952, loss = 0.00256909\n",
      "Iteration 8953, loss = 0.00256781\n",
      "Iteration 8954, loss = 0.00256653\n",
      "Iteration 8955, loss = 0.00256525\n",
      "Iteration 8956, loss = 0.00256397\n",
      "Iteration 8957, loss = 0.00256269\n",
      "Iteration 8958, loss = 0.00256142\n",
      "Iteration 8959, loss = 0.00256014\n",
      "Iteration 8960, loss = 0.00255887\n",
      "Iteration 8961, loss = 0.00255760\n",
      "Iteration 8962, loss = 0.00255633\n",
      "Iteration 8963, loss = 0.00255506\n",
      "Iteration 8964, loss = 0.00255379\n",
      "Iteration 8965, loss = 0.00255253\n",
      "Iteration 8966, loss = 0.00255126\n",
      "Iteration 8967, loss = 0.00255000\n",
      "Iteration 8968, loss = 0.00254873\n",
      "Iteration 8969, loss = 0.00254747\n",
      "Iteration 8970, loss = 0.00254621\n",
      "Iteration 8971, loss = 0.00254495\n",
      "Iteration 8972, loss = 0.00254369\n",
      "Iteration 8973, loss = 0.00254244\n",
      "Iteration 8974, loss = 0.00254118\n",
      "Iteration 8975, loss = 0.00253993\n",
      "Iteration 8976, loss = 0.00253867\n",
      "Iteration 8977, loss = 0.00253742\n",
      "Iteration 8978, loss = 0.00253617\n",
      "Iteration 8979, loss = 0.00253492\n",
      "Iteration 8980, loss = 0.00253367\n",
      "Iteration 8981, loss = 0.00253242\n",
      "Iteration 8982, loss = 0.00253118\n",
      "Iteration 8983, loss = 0.00252993\n",
      "Iteration 8984, loss = 0.00252869\n",
      "Iteration 8985, loss = 0.00252744\n",
      "Iteration 8986, loss = 0.00252620\n",
      "Iteration 8987, loss = 0.00252496\n",
      "Iteration 8988, loss = 0.00252372\n",
      "Iteration 8989, loss = 0.00252248\n",
      "Iteration 8990, loss = 0.00252125\n",
      "Iteration 8991, loss = 0.00252001\n",
      "Iteration 8992, loss = 0.00251878\n",
      "Iteration 8993, loss = 0.00251754\n",
      "Iteration 8994, loss = 0.00251631\n",
      "Iteration 8995, loss = 0.00251508\n",
      "Iteration 8996, loss = 0.00251385\n",
      "Iteration 8997, loss = 0.00251262\n",
      "Iteration 8998, loss = 0.00251139\n",
      "Iteration 8999, loss = 0.00251017\n",
      "Iteration 9000, loss = 0.00250894\n",
      "Iteration 9001, loss = 0.00250772\n",
      "Iteration 9002, loss = 0.00250649\n",
      "Iteration 9003, loss = 0.00250527\n",
      "Iteration 9004, loss = 0.00250405\n",
      "Iteration 9005, loss = 0.00250283\n",
      "Iteration 9006, loss = 0.00250161\n",
      "Iteration 9007, loss = 0.00250040\n",
      "Iteration 9008, loss = 0.00249918\n",
      "Iteration 9009, loss = 0.00249796\n",
      "Iteration 9010, loss = 0.00249675\n",
      "Iteration 9011, loss = 0.00249554\n",
      "Iteration 9012, loss = 0.00249433\n",
      "Iteration 9013, loss = 0.00249312\n",
      "Iteration 9014, loss = 0.00249191\n",
      "Iteration 9015, loss = 0.00249070\n",
      "Iteration 9016, loss = 0.00248949\n",
      "Iteration 9017, loss = 0.00248829\n",
      "Iteration 9018, loss = 0.00248708\n",
      "Iteration 9019, loss = 0.00248588\n",
      "Iteration 9020, loss = 0.00248468\n",
      "Iteration 9021, loss = 0.00248347\n",
      "Iteration 9022, loss = 0.00248227\n",
      "Iteration 9023, loss = 0.00248108\n",
      "Iteration 9024, loss = 0.00247988\n",
      "Iteration 9025, loss = 0.00247868\n",
      "Iteration 9026, loss = 0.00247749\n",
      "Iteration 9027, loss = 0.00247629\n",
      "Iteration 9028, loss = 0.00247510\n",
      "Iteration 9029, loss = 0.00247391\n",
      "Iteration 9030, loss = 0.00247271\n",
      "Iteration 9031, loss = 0.00247152\n",
      "Iteration 9032, loss = 0.00247034\n",
      "Iteration 9033, loss = 0.00246915\n",
      "Iteration 9034, loss = 0.00246796\n",
      "Iteration 9035, loss = 0.00246678\n",
      "Iteration 9036, loss = 0.00246559\n",
      "Iteration 9037, loss = 0.00246441\n",
      "Iteration 9038, loss = 0.00246323\n",
      "Iteration 9039, loss = 0.00246205\n",
      "Iteration 9040, loss = 0.00246087\n",
      "Iteration 9041, loss = 0.00245969\n",
      "Iteration 9042, loss = 0.00245851\n",
      "Iteration 9043, loss = 0.00245733\n",
      "Iteration 9044, loss = 0.00245616\n",
      "Iteration 9045, loss = 0.00245498\n",
      "Iteration 9046, loss = 0.00245381\n",
      "Iteration 9047, loss = 0.00245264\n",
      "Iteration 9048, loss = 0.00245147\n",
      "Iteration 9049, loss = 0.00245030\n",
      "Iteration 9050, loss = 0.00244913\n",
      "Iteration 9051, loss = 0.00244796\n",
      "Iteration 9052, loss = 0.00244679\n",
      "Iteration 9053, loss = 0.00244563\n",
      "Iteration 9054, loss = 0.00244446\n",
      "Iteration 9055, loss = 0.00244330\n",
      "Iteration 9056, loss = 0.00244214\n",
      "Iteration 9057, loss = 0.00244098\n",
      "Iteration 9058, loss = 0.00243982\n",
      "Iteration 9059, loss = 0.00243866\n",
      "Iteration 9060, loss = 0.00243750\n",
      "Iteration 9061, loss = 0.00243635\n",
      "Iteration 9062, loss = 0.00243519\n",
      "Iteration 9063, loss = 0.00243404\n",
      "Iteration 9064, loss = 0.00243288\n",
      "Iteration 9065, loss = 0.00243173\n",
      "Iteration 9066, loss = 0.00243058\n",
      "Iteration 9067, loss = 0.00242943\n",
      "Iteration 9068, loss = 0.00242828\n",
      "Iteration 9069, loss = 0.00242713\n",
      "Iteration 9070, loss = 0.00242599\n",
      "Iteration 9071, loss = 0.00242484\n",
      "Iteration 9072, loss = 0.00242370\n",
      "Iteration 9073, loss = 0.00242255\n",
      "Iteration 9074, loss = 0.00242141\n",
      "Iteration 9075, loss = 0.00242027\n",
      "Iteration 9076, loss = 0.00241913\n",
      "Iteration 9077, loss = 0.00241799\n",
      "Iteration 9078, loss = 0.00241685\n",
      "Iteration 9079, loss = 0.00241571\n",
      "Iteration 9080, loss = 0.00241458\n",
      "Iteration 9081, loss = 0.00241344\n",
      "Iteration 9082, loss = 0.00241231\n",
      "Iteration 9083, loss = 0.00241118\n",
      "Iteration 9084, loss = 0.00241004\n",
      "Iteration 9085, loss = 0.00240891\n",
      "Iteration 9086, loss = 0.00240778\n",
      "Iteration 9087, loss = 0.00240666\n",
      "Iteration 9088, loss = 0.00240553\n",
      "Iteration 9089, loss = 0.00240440\n",
      "Iteration 9090, loss = 0.00240328\n",
      "Iteration 9091, loss = 0.00240215\n",
      "Iteration 9092, loss = 0.00240103\n",
      "Iteration 9093, loss = 0.00239991\n",
      "Iteration 9094, loss = 0.00239879\n",
      "Iteration 9095, loss = 0.00239767\n",
      "Iteration 9096, loss = 0.00239655\n",
      "Iteration 9097, loss = 0.00239543\n",
      "Iteration 9098, loss = 0.00239431\n",
      "Iteration 9099, loss = 0.00239320\n",
      "Iteration 9100, loss = 0.00239208\n",
      "Iteration 9101, loss = 0.00239097\n",
      "Iteration 9102, loss = 0.00238986\n",
      "Iteration 9103, loss = 0.00238874\n",
      "Iteration 9104, loss = 0.00238763\n",
      "Iteration 9105, loss = 0.00238652\n",
      "Iteration 9106, loss = 0.00238542\n",
      "Iteration 9107, loss = 0.00238431\n",
      "Iteration 9108, loss = 0.00238320\n",
      "Iteration 9109, loss = 0.00238210\n",
      "Iteration 9110, loss = 0.00238099\n",
      "Iteration 9111, loss = 0.00237989\n",
      "Iteration 9112, loss = 0.00237879\n",
      "Iteration 9113, loss = 0.00237769\n",
      "Iteration 9114, loss = 0.00237659\n",
      "Iteration 9115, loss = 0.00237549\n",
      "Iteration 9116, loss = 0.00237439\n",
      "Iteration 9117, loss = 0.00237329\n",
      "Iteration 9118, loss = 0.00237220\n",
      "Iteration 9119, loss = 0.00237110\n",
      "Iteration 9120, loss = 0.00237001\n",
      "Iteration 9121, loss = 0.00236892\n",
      "Iteration 9122, loss = 0.00236782\n",
      "Iteration 9123, loss = 0.00236673\n",
      "Iteration 9124, loss = 0.00236564\n",
      "Iteration 9125, loss = 0.00236456\n",
      "Iteration 9126, loss = 0.00236347\n",
      "Iteration 9127, loss = 0.00236238\n",
      "Iteration 9128, loss = 0.00236130\n",
      "Iteration 9129, loss = 0.00236021\n",
      "Iteration 9130, loss = 0.00235913\n",
      "Iteration 9131, loss = 0.00235805\n",
      "Iteration 9132, loss = 0.00235696\n",
      "Iteration 9133, loss = 0.00235588\n",
      "Iteration 9134, loss = 0.00235480\n",
      "Iteration 9135, loss = 0.00235373\n",
      "Iteration 9136, loss = 0.00235265\n",
      "Iteration 9137, loss = 0.00235157\n",
      "Iteration 9138, loss = 0.00235050\n",
      "Iteration 9139, loss = 0.00234942\n",
      "Iteration 9140, loss = 0.00234835\n",
      "Iteration 9141, loss = 0.00234728\n",
      "Iteration 9142, loss = 0.00234621\n",
      "Iteration 9143, loss = 0.00234514\n",
      "Iteration 9144, loss = 0.00234407\n",
      "Iteration 9145, loss = 0.00234300\n",
      "Iteration 9146, loss = 0.00234193\n",
      "Iteration 9147, loss = 0.00234086\n",
      "Iteration 9148, loss = 0.00233980\n",
      "Iteration 9149, loss = 0.00233874\n",
      "Iteration 9150, loss = 0.00233767\n",
      "Iteration 9151, loss = 0.00233661\n",
      "Iteration 9152, loss = 0.00233555\n",
      "Iteration 9153, loss = 0.00233449\n",
      "Iteration 9154, loss = 0.00233343\n",
      "Iteration 9155, loss = 0.00233237\n",
      "Iteration 9156, loss = 0.00233131\n",
      "Iteration 9157, loss = 0.00233026\n",
      "Iteration 9158, loss = 0.00232920\n",
      "Iteration 9159, loss = 0.00232815\n",
      "Iteration 9160, loss = 0.00232710\n",
      "Iteration 9161, loss = 0.00232604\n",
      "Iteration 9162, loss = 0.00232499\n",
      "Iteration 9163, loss = 0.00232394\n",
      "Iteration 9164, loss = 0.00232289\n",
      "Iteration 9165, loss = 0.00232185\n",
      "Iteration 9166, loss = 0.00232080\n",
      "Iteration 9167, loss = 0.00231975\n",
      "Iteration 9168, loss = 0.00231871\n",
      "Iteration 9169, loss = 0.00231766\n",
      "Iteration 9170, loss = 0.00231662\n",
      "Iteration 9171, loss = 0.00231558\n",
      "Iteration 9172, loss = 0.00231454\n",
      "Iteration 9173, loss = 0.00231350\n",
      "Iteration 9174, loss = 0.00231246\n",
      "Iteration 9175, loss = 0.00231142\n",
      "Iteration 9176, loss = 0.00231038\n",
      "Iteration 9177, loss = 0.00230934\n",
      "Iteration 9178, loss = 0.00230831\n",
      "Iteration 9179, loss = 0.00230727\n",
      "Iteration 9180, loss = 0.00230624\n",
      "Iteration 9181, loss = 0.00230521\n",
      "Iteration 9182, loss = 0.00230418\n",
      "Iteration 9183, loss = 0.00230315\n",
      "Iteration 9184, loss = 0.00230212\n",
      "Iteration 9185, loss = 0.00230109\n",
      "Iteration 9186, loss = 0.00230006\n",
      "Iteration 9187, loss = 0.00229903\n",
      "Iteration 9188, loss = 0.00229801\n",
      "Iteration 9189, loss = 0.00229698\n",
      "Iteration 9190, loss = 0.00229596\n",
      "Iteration 9191, loss = 0.00229494\n",
      "Iteration 9192, loss = 0.00229392\n",
      "Iteration 9193, loss = 0.00229290\n",
      "Iteration 9194, loss = 0.00229188\n",
      "Iteration 9195, loss = 0.00229086\n",
      "Iteration 9196, loss = 0.00228984\n",
      "Iteration 9197, loss = 0.00228882\n",
      "Iteration 9198, loss = 0.00228781\n",
      "Iteration 9199, loss = 0.00228679\n",
      "Iteration 9200, loss = 0.00228578\n",
      "Iteration 9201, loss = 0.00228476\n",
      "Iteration 9202, loss = 0.00228375\n",
      "Iteration 9203, loss = 0.00228274\n",
      "Iteration 9204, loss = 0.00228173\n",
      "Iteration 9205, loss = 0.00228072\n",
      "Iteration 9206, loss = 0.00227971\n",
      "Iteration 9207, loss = 0.00227871\n",
      "Iteration 9208, loss = 0.00227770\n",
      "Iteration 9209, loss = 0.00227669\n",
      "Iteration 9210, loss = 0.00227569\n",
      "Iteration 9211, loss = 0.00227469\n",
      "Iteration 9212, loss = 0.00227368\n",
      "Iteration 9213, loss = 0.00227268\n",
      "Iteration 9214, loss = 0.00227168\n",
      "Iteration 9215, loss = 0.00227068\n",
      "Iteration 9216, loss = 0.00226968\n",
      "Iteration 9217, loss = 0.00226868\n",
      "Iteration 9218, loss = 0.00226769\n",
      "Iteration 9219, loss = 0.00226669\n",
      "Iteration 9220, loss = 0.00226570\n",
      "Iteration 9221, loss = 0.00226470\n",
      "Iteration 9222, loss = 0.00226371\n",
      "Iteration 9223, loss = 0.00226272\n",
      "Iteration 9224, loss = 0.00226173\n",
      "Iteration 9225, loss = 0.00226074\n",
      "Training loss did not improve more than tol=0.000001 for 10 consecutive epochs. Stopping.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.82      1.00      0.90        14\n",
      "           2       0.94      0.85      0.89        20\n",
      "           3       0.95      0.90      0.92        20\n",
      "\n",
      "    accuracy                           0.91        54\n",
      "   macro avg       0.91      0.92      0.91        54\n",
      "weighted avg       0.91      0.91      0.91        54\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "hidden_layers = (512,)\n",
    "activation = 'logistic'\n",
    "opts = dict(hidden_layer_sizes = hidden_layers,verbose = True,activation = activation,tol = 1e-6,max_iter = int(1e6))\n",
    "solver = 'adam'\n",
    "clf_MLP = MLPClassifier(solver = solver, **opts)\n",
    "clf_MLP.fit(Z_train,y_train)\n",
    "predictions = clf_MLP.predict(Z_test)\n",
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00        45\n",
      "           2       1.00      1.00      1.00        51\n",
      "           3       1.00      1.00      1.00        28\n",
      "\n",
      "    accuracy                           1.00       124\n",
      "   macro avg       1.00      1.00      1.00       124\n",
      "weighted avg       1.00      1.00      1.00       124\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred = clf_MLP.predict(Z_train)\n",
    "print(classification_report(y_train,pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 結論:\n",
    "### 1. 當成分比例採0.4時，準確率還不錯，為94%\n",
    "### 2. hidden layers = (30,) 跟 hidden layers = (512,) PCA的結果差不多，所以可以選擇一層30個神經元即可"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
