{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 作業三: 分類器的原理與評比實驗\n",
    "## 資料來源: 來自 Yale Face 38 人的人臉影像共 2410 張，每張大小 192×168\n",
    "## 目標: \n",
    "## 計畫執行這篇講義描述的分類器比較，即採用三種分類器分別對三組資料進行分類學習與測試。其中分類器包括： \n",
    "## 1.多元羅吉斯回歸 2.支援向量機 3.神經網路\n",
    "## 影像資料處理: \n",
    "## 1.原始資料 2.進行PCA主成分分析\n",
    "## 分類方法: \n",
    "- ## Logistic Regression\n",
    "- ## SVM\n",
    "- ## Neural Network\n",
    "### 姓名: 鄭欣莉\n",
    "### 學號: 410877039"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 標準化後原始資料 + 羅吉斯迴歸"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- solver = 'lbfgs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io \n",
    "\n",
    "D = scipy.io.loadmat('allFaces.mat')\n",
    "X = D['faces'].T\n",
    "Y = np.ndarray.flatten(D['nfaces']) #轉換成[64個0, 62個1, .....]\n",
    "y = []\n",
    "for i in range(len(Y)): #len(Y)\n",
    "    for j in range(Y[i]):\n",
    "        y.append(i)\n",
    "y = np.array(y)\n",
    "m = int(D['m']) #168\n",
    "n = int(D['n']) #192\n",
    "n_persons = int(D['person']) #38人\n",
    "test_size = 0.30 #多少比較好\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size)\n",
    "#標準化\n",
    "scalar = StandardScaler()\n",
    "X_train_ = scalar.fit_transform(X_train)\n",
    "X_test_ = scalar.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def clf_LR(solver):\n",
    "    opts = dict(tol = 1e-6, max_iter = int(1e6), verbose = 1)\n",
    "    clf_LR = LogisticRegression(solver = solver, **opts)\n",
    "    clf_LR.fit(X_train_,y_train)\n",
    "    y_pred = clf_LR.predict(X_test_)\n",
    "    print(f\"{clf_LR.score(X_test_, y_test):.2%}\\n\")\n",
    "    print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed: 45.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95.44%\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.75      0.84        28\n",
      "           1       0.95      1.00      0.97        19\n",
      "           2       0.93      1.00      0.96        25\n",
      "           3       0.89      1.00      0.94        17\n",
      "           4       1.00      1.00      1.00        19\n",
      "           5       1.00      1.00      1.00        14\n",
      "           6       1.00      1.00      1.00        23\n",
      "           7       0.94      1.00      0.97        17\n",
      "           8       0.85      1.00      0.92        17\n",
      "           9       0.90      0.95      0.93        20\n",
      "          10       1.00      0.92      0.96        24\n",
      "          11       1.00      1.00      1.00        19\n",
      "          12       0.95      1.00      0.97        18\n",
      "          13       1.00      0.79      0.88        19\n",
      "          14       0.94      0.94      0.94        17\n",
      "          15       1.00      0.89      0.94         9\n",
      "          16       1.00      1.00      1.00        18\n",
      "          17       1.00      1.00      1.00        19\n",
      "          18       0.85      1.00      0.92        17\n",
      "          19       1.00      0.89      0.94        18\n",
      "          20       1.00      0.95      0.97        19\n",
      "          21       0.95      1.00      0.98        21\n",
      "          22       1.00      0.95      0.98        21\n",
      "          23       0.86      1.00      0.92        12\n",
      "          24       0.96      0.96      0.96        24\n",
      "          25       1.00      0.94      0.97        18\n",
      "          26       1.00      1.00      1.00        18\n",
      "          27       1.00      0.95      0.97        19\n",
      "          28       1.00      0.92      0.96        13\n",
      "          29       1.00      1.00      1.00        18\n",
      "          30       0.93      0.97      0.95        29\n",
      "          31       0.84      1.00      0.91        16\n",
      "          32       0.94      1.00      0.97        17\n",
      "          33       0.94      0.88      0.91        17\n",
      "          34       1.00      0.85      0.92        26\n",
      "          35       1.00      0.95      0.98        21\n",
      "          36       0.94      0.94      0.94        18\n",
      "          37       0.83      1.00      0.90        19\n",
      "\n",
      "    accuracy                           0.95       723\n",
      "   macro avg       0.96      0.96      0.95       723\n",
      "weighted avg       0.96      0.95      0.95       723\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf_LR(solver = 'lbfgs')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- solver = 'liblinear'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]97.93%\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.89      0.94        28\n",
      "           1       0.95      0.95      0.95        19\n",
      "           2       0.93      1.00      0.96        25\n",
      "           3       1.00      1.00      1.00        17\n",
      "           4       1.00      1.00      1.00        19\n",
      "           5       1.00      1.00      1.00        14\n",
      "           6       1.00      1.00      1.00        23\n",
      "           7       1.00      1.00      1.00        17\n",
      "           8       0.89      1.00      0.94        17\n",
      "           9       1.00      0.95      0.97        20\n",
      "          10       1.00      0.96      0.98        24\n",
      "          11       1.00      1.00      1.00        19\n",
      "          12       0.95      1.00      0.97        18\n",
      "          13       1.00      0.84      0.91        19\n",
      "          14       1.00      1.00      1.00        17\n",
      "          15       1.00      1.00      1.00         9\n",
      "          16       1.00      1.00      1.00        18\n",
      "          17       1.00      1.00      1.00        19\n",
      "          18       1.00      1.00      1.00        17\n",
      "          19       1.00      0.94      0.97        18\n",
      "          20       1.00      1.00      1.00        19\n",
      "          21       1.00      1.00      1.00        21\n",
      "          22       1.00      1.00      1.00        21\n",
      "          23       1.00      1.00      1.00        12\n",
      "          24       1.00      0.96      0.98        24\n",
      "          25       0.95      1.00      0.97        18\n",
      "          26       0.95      1.00      0.97        18\n",
      "          27       1.00      0.95      0.97        19\n",
      "          28       1.00      1.00      1.00        13\n",
      "          29       0.95      1.00      0.97        18\n",
      "          30       1.00      0.97      0.98        29\n",
      "          31       1.00      1.00      1.00        16\n",
      "          32       1.00      1.00      1.00        17\n",
      "          33       0.85      1.00      0.92        17\n",
      "          34       1.00      0.96      0.98        26\n",
      "          35       1.00      0.95      0.98        21\n",
      "          36       0.86      1.00      0.92        18\n",
      "          37       1.00      1.00      1.00        19\n",
      "\n",
      "    accuracy                           0.98       723\n",
      "   macro avg       0.98      0.98      0.98       723\n",
      "weighted avg       0.98      0.98      0.98       723\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf_LR(solver = 'liblinear')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- solver = 'newton-cg'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 結論：\n",
    "### 1. Classification Report 的衡量指標個別代表的意思:\n",
    "- ### 精確率(Precision) 為預測為真的樣本有幾個預測正確\n",
    "- ### 召回率(Recall) 為事實為真的樣本中有幾個是預測正確的\n",
    "- ### F1 score 為精確率和召回率的加權平均數\n",
    "### 2. 以不同solver的第一筆資料為例:\n",
    "- ### solver = 'lbfgs' 預測為真且預測正確的比例為95%，solver = 'liblinear'為100%\n",
    "- ### solver = 'lbfgs 在事實為真的樣本中預測正確率只有75%，solver = 'liblinear'為89%\n",
    "- ### solver = 'lbfgs'精確率和召回率的調和平均為84%，solver = 'liblinear'為94%\n",
    "### 3. 標準化過後的AT&T資料，solver = 'lbfgs' 跟 'newton-cg' 表現均差不多，準確率均在90%以上，solver = 'liblinear' 準確率最高98%，對於38筆資料來說表現絕對不差。\n",
    "### 4. solver = 'lbfgs' 需要跑的時間比較少"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA主成分分析(成分比例採0.95) + 羅吉斯迴歸"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- solver = 'lbfgs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def PCA_LR(n_components, solver):\n",
    "    pca = PCA(n_components = n_components).fit(X_train_)\n",
    "    Z_train = pca.transform(X_train_)\n",
    "    Z_test = pca.transform(X_test_)\n",
    "    opts = dict(tol = 1e-6, max_iter = int(1e6), verbose = 1)\n",
    "    clf_PCA = LogisticRegression(solver = solver, **opts)\n",
    "    clf_PCA.fit(Z_train, y_train)\n",
    "    y_pred = clf_PCA.predict(Z_test)\n",
    "    print(f\"{clf_PCA.score(Z_test, y_test):.2%}\\n\")\n",
    "    print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87.14%\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.64      0.78        28\n",
      "           1       1.00      0.89      0.94        19\n",
      "           2       0.92      0.88      0.90        25\n",
      "           3       0.94      1.00      0.97        17\n",
      "           4       0.95      1.00      0.97        19\n",
      "           5       0.82      1.00      0.90        14\n",
      "           6       1.00      0.87      0.93        23\n",
      "           7       0.76      0.76      0.76        17\n",
      "           8       0.79      0.88      0.83        17\n",
      "           9       0.94      0.80      0.86        20\n",
      "          10       1.00      0.75      0.86        24\n",
      "          11       0.81      0.89      0.85        19\n",
      "          12       1.00      0.83      0.91        18\n",
      "          13       1.00      0.84      0.91        19\n",
      "          14       0.79      0.88      0.83        17\n",
      "          15       0.89      0.89      0.89         9\n",
      "          16       0.85      0.94      0.89        18\n",
      "          17       0.76      1.00      0.86        19\n",
      "          18       0.59      0.94      0.73        17\n",
      "          19       0.93      0.78      0.85        18\n",
      "          20       0.86      0.95      0.90        19\n",
      "          21       0.82      0.86      0.84        21\n",
      "          22       0.88      0.71      0.79        21\n",
      "          23       0.62      0.83      0.71        12\n",
      "          24       0.95      0.88      0.91        24\n",
      "          25       0.94      0.83      0.88        18\n",
      "          26       0.95      1.00      0.97        18\n",
      "          27       0.85      0.89      0.87        19\n",
      "          28       1.00      0.92      0.96        13\n",
      "          29       0.94      0.89      0.91        18\n",
      "          30       0.96      0.90      0.93        29\n",
      "          31       0.70      1.00      0.82        16\n",
      "          32       0.94      1.00      0.97        17\n",
      "          33       0.72      0.76      0.74        17\n",
      "          34       0.91      0.81      0.86        26\n",
      "          35       0.89      0.81      0.85        21\n",
      "          36       0.84      0.89      0.86        18\n",
      "          37       0.82      0.95      0.88        19\n",
      "\n",
      "    accuracy                           0.87       723\n",
      "   macro avg       0.88      0.88      0.87       723\n",
      "weighted avg       0.89      0.87      0.87       723\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   30.0s finished\n"
     ]
    }
   ],
   "source": [
    "PCA_LR(n_components = 0.9, solver = 'lbfgs')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- solver = 'liblinear'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]79.39%\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.61      0.74        28\n",
      "           1       0.60      0.79      0.68        19\n",
      "           2       0.95      0.84      0.89        25\n",
      "           3       0.73      0.94      0.82        17\n",
      "           4       0.90      0.95      0.92        19\n",
      "           5       0.68      0.93      0.79        14\n",
      "           6       0.87      0.57      0.68        23\n",
      "           7       0.50      0.65      0.56        17\n",
      "           8       0.88      0.88      0.88        17\n",
      "           9       1.00      0.70      0.82        20\n",
      "          10       0.95      0.75      0.84        24\n",
      "          11       0.84      0.84      0.84        19\n",
      "          12       0.94      0.94      0.94        18\n",
      "          13       1.00      0.74      0.85        19\n",
      "          14       0.87      0.76      0.81        17\n",
      "          15       0.50      0.78      0.61         9\n",
      "          16       0.84      0.89      0.86        18\n",
      "          17       0.86      0.95      0.90        19\n",
      "          18       0.55      0.94      0.70        17\n",
      "          19       0.74      0.78      0.76        18\n",
      "          20       1.00      0.68      0.81        19\n",
      "          21       0.89      0.81      0.85        21\n",
      "          22       0.88      0.71      0.79        21\n",
      "          23       0.83      0.83      0.83        12\n",
      "          24       0.95      0.88      0.91        24\n",
      "          25       0.86      0.67      0.75        18\n",
      "          26       0.89      0.94      0.92        18\n",
      "          27       0.80      0.84      0.82        19\n",
      "          28       0.61      0.85      0.71        13\n",
      "          29       0.74      0.94      0.83        18\n",
      "          30       1.00      0.62      0.77        29\n",
      "          31       0.73      0.69      0.71        16\n",
      "          32       0.88      0.82      0.85        17\n",
      "          33       0.34      0.71      0.46        17\n",
      "          34       1.00      0.65      0.79        26\n",
      "          35       0.81      0.81      0.81        21\n",
      "          36       0.73      0.89      0.80        18\n",
      "          37       0.86      0.95      0.90        19\n",
      "\n",
      "    accuracy                           0.79       723\n",
      "   macro avg       0.81      0.80      0.80       723\n",
      "weighted avg       0.83      0.79      0.80       723\n",
      "\n"
     ]
    }
   ],
   "source": [
    "PCA_LR(n_components = 0.9, solver = 'liblinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94.05%\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.71      0.83        28\n",
      "           1       1.00      0.95      0.97        19\n",
      "           2       1.00      1.00      1.00        25\n",
      "           3       0.85      1.00      0.92        17\n",
      "           4       1.00      1.00      1.00        19\n",
      "           5       0.88      1.00      0.93        14\n",
      "           6       0.96      1.00      0.98        23\n",
      "           7       0.89      1.00      0.94        17\n",
      "           8       0.94      1.00      0.97        17\n",
      "           9       0.85      0.85      0.85        20\n",
      "          10       1.00      0.88      0.93        24\n",
      "          11       1.00      1.00      1.00        19\n",
      "          12       1.00      0.89      0.94        18\n",
      "          13       1.00      0.79      0.88        19\n",
      "          14       0.89      0.94      0.91        17\n",
      "          15       0.89      0.89      0.89         9\n",
      "          16       1.00      1.00      1.00        18\n",
      "          17       1.00      1.00      1.00        19\n",
      "          18       0.74      1.00      0.85        17\n",
      "          19       1.00      0.89      0.94        18\n",
      "          20       0.94      0.89      0.92        19\n",
      "          21       0.91      0.95      0.93        21\n",
      "          22       0.91      0.95      0.93        21\n",
      "          23       1.00      1.00      1.00        12\n",
      "          24       0.96      0.96      0.96        24\n",
      "          25       1.00      0.94      0.97        18\n",
      "          26       1.00      1.00      1.00        18\n",
      "          27       1.00      0.84      0.91        19\n",
      "          28       1.00      0.92      0.96        13\n",
      "          29       1.00      1.00      1.00        18\n",
      "          30       0.94      1.00      0.97        29\n",
      "          31       0.76      1.00      0.86        16\n",
      "          32       1.00      1.00      1.00        17\n",
      "          33       1.00      0.88      0.94        17\n",
      "          34       1.00      0.85      0.92        26\n",
      "          35       0.95      0.90      0.93        21\n",
      "          36       0.82      1.00      0.90        18\n",
      "          37       0.83      1.00      0.90        19\n",
      "\n",
      "    accuracy                           0.94       723\n",
      "   macro avg       0.94      0.94      0.94       723\n",
      "weighted avg       0.95      0.94      0.94       723\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   18.7s finished\n"
     ]
    }
   ],
   "source": [
    "PCA_LR(n_components = 0.95, solver = 'lbfgs')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 結論:\n",
    "### PCA的AT&T資料，當成分比例採<=0.9時，準確率均在90%以下，solver = 'lbfgs' 的準確率為87%，solver = 'liblinear' 準確率為79%，因此表示無法在降低維度的同時保留重要資訊；當成分比例採0.95時，準確率94%，所以下面主成分比例將採0.95。 "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 標準化後原始資料+SVC"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### one vs one"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- kernel = 'linear'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC,LinearSVC\n",
    "\n",
    "def clf_SVC(C, opts, clf_svm):\n",
    "    clf_svm.fit(X_train_,y_train)\n",
    "    predictions = clf_svm.predict(X_test_)\n",
    "    print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.75      0.86        28\n",
      "           1       0.81      0.89      0.85        19\n",
      "           2       0.89      0.96      0.92        25\n",
      "           3       0.74      1.00      0.85        17\n",
      "           4       0.73      1.00      0.84        19\n",
      "           5       0.88      1.00      0.93        14\n",
      "           6       0.96      1.00      0.98        23\n",
      "           7       0.68      1.00      0.81        17\n",
      "           8       0.88      0.88      0.88        17\n",
      "           9       0.71      0.85      0.77        20\n",
      "          10       1.00      0.88      0.93        24\n",
      "          11       0.95      1.00      0.97        19\n",
      "          12       0.88      0.83      0.86        18\n",
      "          13       1.00      0.89      0.94        19\n",
      "          14       0.94      0.94      0.94        17\n",
      "          15       1.00      0.89      0.94         9\n",
      "          16       1.00      1.00      1.00        18\n",
      "          17       1.00      1.00      1.00        19\n",
      "          18       0.85      1.00      0.92        17\n",
      "          19       0.94      0.89      0.91        18\n",
      "          20       1.00      0.84      0.91        19\n",
      "          21       0.95      1.00      0.98        21\n",
      "          22       1.00      0.90      0.95        21\n",
      "          23       0.86      1.00      0.92        12\n",
      "          24       0.96      0.96      0.96        24\n",
      "          25       1.00      0.89      0.94        18\n",
      "          26       1.00      1.00      1.00        18\n",
      "          27       1.00      0.95      0.97        19\n",
      "          28       0.92      0.92      0.92        13\n",
      "          29       0.95      1.00      0.97        18\n",
      "          30       1.00      0.76      0.86        29\n",
      "          31       0.84      1.00      0.91        16\n",
      "          32       1.00      0.88      0.94        17\n",
      "          33       0.86      0.71      0.77        17\n",
      "          34       0.90      0.69      0.78        26\n",
      "          35       0.90      0.90      0.90        21\n",
      "          36       0.94      0.83      0.88        18\n",
      "          37       0.89      0.89      0.89        19\n",
      "\n",
      "    accuracy                           0.91       723\n",
      "   macro avg       0.92      0.92      0.91       723\n",
      "weighted avg       0.92      0.91      0.91       723\n",
      "\n"
     ]
    }
   ],
   "source": [
    "C = 1\n",
    "opts = dict(C = C, decision_function_shape = 'ovo', \\\n",
    "             tol = 1e-6, max_iter = int(1e6))\n",
    "clf_svm = SVC(kernel = 'linear', **opts)\n",
    "clf_SVC(C = C, opts = opts, clf_svm = clf_svm)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- kernel = 'sigmoid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.18      0.29        28\n",
      "           1       0.38      0.32      0.34        19\n",
      "           2       1.00      0.08      0.15        25\n",
      "           3       0.59      0.59      0.59        17\n",
      "           4       0.79      0.58      0.67        19\n",
      "           5       0.80      0.29      0.42        14\n",
      "           6       0.58      0.30      0.40        23\n",
      "           7       0.50      0.82      0.62        17\n",
      "           8       0.75      0.88      0.81        17\n",
      "           9       0.15      0.20      0.17        20\n",
      "          10       0.85      0.46      0.59        24\n",
      "          11       1.00      0.42      0.59        19\n",
      "          12       0.03      0.44      0.06        18\n",
      "          13       0.50      0.16      0.24        19\n",
      "          14       0.50      0.06      0.11        17\n",
      "          15       0.10      0.11      0.11         9\n",
      "          16       0.67      0.22      0.33        18\n",
      "          17       1.00      0.32      0.48        19\n",
      "          18       0.71      0.29      0.42        17\n",
      "          19       1.00      0.50      0.67        18\n",
      "          20       0.85      0.58      0.69        19\n",
      "          21       0.40      0.10      0.15        21\n",
      "          22       1.00      0.14      0.25        21\n",
      "          23       0.57      0.67      0.62        12\n",
      "          24       0.70      0.29      0.41        24\n",
      "          25       1.00      0.11      0.20        18\n",
      "          26       0.71      0.56      0.63        18\n",
      "          27       0.20      0.37      0.26        19\n",
      "          28       0.36      0.38      0.37        13\n",
      "          29       1.00      0.33      0.50        18\n",
      "          30       1.00      0.52      0.68        29\n",
      "          31       0.27      0.19      0.22        16\n",
      "          32       1.00      0.24      0.38        17\n",
      "          33       0.16      1.00      0.27        17\n",
      "          34       1.00      0.04      0.07        26\n",
      "          35       1.00      0.29      0.44        21\n",
      "          36       1.00      0.61      0.76        18\n",
      "          37       1.00      0.32      0.48        19\n",
      "\n",
      "    accuracy                           0.36       723\n",
      "   macro avg       0.68      0.37      0.41       723\n",
      "weighted avg       0.70      0.36      0.41       723\n",
      "\n"
     ]
    }
   ],
   "source": [
    "C = 1\n",
    "opts = dict(C = C, decision_function_shape = 'ovo', \\\n",
    "             tol = 1e-6, max_iter = int(1e6))\n",
    "clf_svm = SVC(kernel = 'sigmoid', **opts)\n",
    "clf_SVC(C = C, opts = opts, clf_svm = clf_svm)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 結論:\n",
    "### 在決策函數中選擇一對一(one vs one)的模式，可以看到當 kernel 選擇 'linear' 準確率有91%，kernel 選擇 'sigmoid' 準確率卻只有36%，表現得非常不好"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA主成分分析(成分比例採0.95) + SVM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### one vs one"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- kernel = 'linear'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.75      0.81        28\n",
      "           1       0.80      0.84      0.82        19\n",
      "           2       0.91      0.84      0.87        25\n",
      "           3       0.81      1.00      0.89        17\n",
      "           4       0.76      1.00      0.86        19\n",
      "           5       0.88      1.00      0.93        14\n",
      "           6       0.79      0.96      0.86        23\n",
      "           7       0.71      0.88      0.79        17\n",
      "           8       0.88      0.88      0.88        17\n",
      "           9       0.81      0.85      0.83        20\n",
      "          10       1.00      0.88      0.93        24\n",
      "          11       0.90      1.00      0.95        19\n",
      "          12       1.00      0.83      0.91        18\n",
      "          13       0.88      0.79      0.83        19\n",
      "          14       0.88      0.88      0.88        17\n",
      "          15       0.82      1.00      0.90         9\n",
      "          16       0.90      1.00      0.95        18\n",
      "          17       0.83      1.00      0.90        19\n",
      "          18       0.94      0.94      0.94        17\n",
      "          19       0.94      0.89      0.91        18\n",
      "          20       0.95      0.95      0.95        19\n",
      "          21       0.84      1.00      0.91        21\n",
      "          22       0.95      0.86      0.90        21\n",
      "          23       0.80      1.00      0.89        12\n",
      "          24       0.89      1.00      0.94        24\n",
      "          25       1.00      0.83      0.91        18\n",
      "          26       1.00      1.00      1.00        18\n",
      "          27       0.95      0.95      0.95        19\n",
      "          28       0.86      0.92      0.89        13\n",
      "          29       0.95      1.00      0.97        18\n",
      "          30       1.00      0.76      0.86        29\n",
      "          31       0.83      0.94      0.88        16\n",
      "          32       1.00      0.88      0.94        17\n",
      "          33       0.92      0.65      0.76        17\n",
      "          34       0.94      0.65      0.77        26\n",
      "          35       0.90      0.90      0.90        21\n",
      "          36       1.00      0.83      0.91        18\n",
      "          37       0.88      0.79      0.83        19\n",
      "\n",
      "    accuracy                           0.89       723\n",
      "   macro avg       0.89      0.90      0.89       723\n",
      "weighted avg       0.90      0.89      0.89       723\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=0.95).fit(X_train_)\n",
    "Z_train = pca.transform(X_train_) #降維\n",
    "Z_test = pca.transform(X_test_)\n",
    "C = 1\n",
    "opts = dict(C=C,tol = 1e-6,max_iter = int(1e6))\n",
    "clf_svm = SVC(kernel = 'linear',**opts)\n",
    "clf_svm.fit(Z_train,y_train)\n",
    "predictions = clf_svm.predict(Z_test)\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### one vs the rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.75      0.81        28\n",
      "           1       0.80      0.84      0.82        19\n",
      "           2       0.91      0.84      0.87        25\n",
      "           3       0.81      1.00      0.89        17\n",
      "           4       0.76      1.00      0.86        19\n",
      "           5       0.88      1.00      0.93        14\n",
      "           6       0.79      0.96      0.86        23\n",
      "           7       0.71      0.88      0.79        17\n",
      "           8       0.88      0.88      0.88        17\n",
      "           9       0.81      0.85      0.83        20\n",
      "          10       1.00      0.88      0.93        24\n",
      "          11       0.90      1.00      0.95        19\n",
      "          12       1.00      0.83      0.91        18\n",
      "          13       0.88      0.79      0.83        19\n",
      "          14       0.88      0.88      0.88        17\n",
      "          15       0.82      1.00      0.90         9\n",
      "          16       0.90      1.00      0.95        18\n",
      "          17       0.83      1.00      0.90        19\n",
      "          18       0.94      0.94      0.94        17\n",
      "          19       0.94      0.89      0.91        18\n",
      "          20       0.95      0.95      0.95        19\n",
      "          21       0.84      1.00      0.91        21\n",
      "          22       0.95      0.86      0.90        21\n",
      "          23       0.80      1.00      0.89        12\n",
      "          24       0.89      1.00      0.94        24\n",
      "          25       1.00      0.83      0.91        18\n",
      "          26       1.00      1.00      1.00        18\n",
      "          27       0.95      0.95      0.95        19\n",
      "          28       0.86      0.92      0.89        13\n",
      "          29       0.95      1.00      0.97        18\n",
      "          30       1.00      0.76      0.86        29\n",
      "          31       0.83      0.94      0.88        16\n",
      "          32       1.00      0.88      0.94        17\n",
      "          33       0.92      0.65      0.76        17\n",
      "          34       0.94      0.65      0.77        26\n",
      "          35       0.90      0.90      0.90        21\n",
      "          36       1.00      0.83      0.91        18\n",
      "          37       0.88      0.79      0.83        19\n",
      "\n",
      "    accuracy                           0.89       723\n",
      "   macro avg       0.89      0.90      0.89       723\n",
      "weighted avg       0.90      0.89      0.89       723\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=0.95).fit(X_train_)\n",
    "Z_train = pca.transform(X_train_) #降維\n",
    "Z_test = pca.transform(X_test_)\n",
    "C = 1\n",
    "opts = dict(C=C,tol = 1e-6,max_iter = int(1e6))\n",
    "clf_svm = SVC(kernel = 'linear',**opts)\n",
    "clf_svm.fit(Z_train,y_train)\n",
    "predictions = clf_svm.predict(Z_test)\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 結論:\n",
    "### 當成分比例採0.95時，不管選擇一對一或一對其他準確率皆為89%，原資料的準確率表現較好，但是成分比例採0.95時，所需要跑的時間減少了很多，因此或許準確率89%是很不錯的"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 標準化後原始資料 + 神經網路NN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- hidden layers = (30,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 3.67443798\n",
      "Iteration 2, loss = 3.58665463\n",
      "Iteration 3, loss = 3.53058939\n",
      "Iteration 4, loss = 3.47661425\n",
      "Iteration 5, loss = 3.42735088\n",
      "Iteration 6, loss = 3.37680664\n",
      "Iteration 7, loss = 3.32126342\n",
      "Iteration 8, loss = 3.26326692\n",
      "Iteration 9, loss = 3.19550576\n",
      "Iteration 10, loss = 3.10940056\n",
      "Iteration 11, loss = 3.03183927\n",
      "Iteration 12, loss = 2.94673218\n",
      "Iteration 13, loss = 2.87777310\n",
      "Iteration 14, loss = 2.79979739\n",
      "Iteration 15, loss = 2.69814833\n",
      "Iteration 16, loss = 2.61701581\n",
      "Iteration 17, loss = 2.52570641\n",
      "Iteration 18, loss = 2.44533189\n",
      "Iteration 19, loss = 2.37073854\n",
      "Iteration 20, loss = 2.28154694\n",
      "Iteration 21, loss = 2.20986532\n",
      "Iteration 22, loss = 2.12091287\n",
      "Iteration 23, loss = 2.06549788\n",
      "Iteration 24, loss = 1.98986842\n",
      "Iteration 25, loss = 1.92020395\n",
      "Iteration 26, loss = 1.84137532\n",
      "Iteration 27, loss = 1.74861831\n",
      "Iteration 28, loss = 1.67951273\n",
      "Iteration 29, loss = 1.64100552\n",
      "Iteration 30, loss = 1.58924052\n",
      "Iteration 31, loss = 1.52528314\n",
      "Iteration 32, loss = 1.44860066\n",
      "Iteration 33, loss = 1.38491917\n",
      "Iteration 34, loss = 1.32590332\n",
      "Iteration 35, loss = 1.26518770\n",
      "Iteration 36, loss = 1.22616913\n",
      "Iteration 37, loss = 1.17329664\n",
      "Iteration 38, loss = 1.14097251\n",
      "Iteration 39, loss = 1.07824820\n",
      "Iteration 40, loss = 1.03246530\n",
      "Iteration 41, loss = 0.97712276\n",
      "Iteration 42, loss = 0.93853614\n",
      "Iteration 43, loss = 0.89557524\n",
      "Iteration 44, loss = 0.85094866\n",
      "Iteration 45, loss = 0.83912540\n",
      "Iteration 46, loss = 0.80026885\n",
      "Iteration 47, loss = 0.75830898\n",
      "Iteration 48, loss = 0.76136280\n",
      "Iteration 49, loss = 0.72544877\n",
      "Iteration 50, loss = 0.67165521\n",
      "Iteration 51, loss = 0.66112671\n",
      "Iteration 52, loss = 0.63620647\n",
      "Iteration 53, loss = 0.60113496\n",
      "Iteration 54, loss = 0.59415626\n",
      "Iteration 55, loss = 0.56739740\n",
      "Iteration 56, loss = 0.57479371\n",
      "Iteration 57, loss = 0.54960487\n",
      "Iteration 58, loss = 0.53274260\n",
      "Iteration 59, loss = 0.51860326\n",
      "Iteration 60, loss = 0.48741383\n",
      "Iteration 61, loss = 0.49193899\n",
      "Iteration 62, loss = 0.47315939\n",
      "Iteration 63, loss = 0.46271003\n",
      "Iteration 64, loss = 0.45387879\n",
      "Iteration 65, loss = 0.44775432\n",
      "Iteration 66, loss = 0.43737497\n",
      "Iteration 67, loss = 0.40558025\n",
      "Iteration 68, loss = 0.39848928\n",
      "Iteration 69, loss = 0.37857587\n",
      "Iteration 70, loss = 0.38026275\n",
      "Iteration 71, loss = 0.35828484\n",
      "Iteration 72, loss = 0.32426194\n",
      "Iteration 73, loss = 0.32771094\n",
      "Iteration 74, loss = 0.34568635\n",
      "Iteration 75, loss = 0.33590466\n",
      "Iteration 76, loss = 0.32829550\n",
      "Iteration 77, loss = 0.30095237\n",
      "Iteration 78, loss = 0.28334683\n",
      "Iteration 79, loss = 0.30584802\n",
      "Iteration 80, loss = 0.30892588\n",
      "Iteration 81, loss = 0.34004567\n",
      "Iteration 82, loss = 0.32395073\n",
      "Iteration 83, loss = 0.29870590\n",
      "Iteration 84, loss = 0.26947277\n",
      "Iteration 85, loss = 0.24814327\n",
      "Iteration 86, loss = 0.24059632\n",
      "Iteration 87, loss = 0.23909749\n",
      "Iteration 88, loss = 0.23103290\n",
      "Iteration 89, loss = 0.23493457\n",
      "Iteration 90, loss = 0.22711473\n",
      "Iteration 91, loss = 0.22278540\n",
      "Iteration 92, loss = 0.21765334\n",
      "Iteration 93, loss = 0.22546781\n",
      "Iteration 94, loss = 0.23303223\n",
      "Iteration 95, loss = 0.21035689\n",
      "Iteration 96, loss = 0.21703896\n",
      "Iteration 97, loss = 0.19897998\n",
      "Iteration 98, loss = 0.19391371\n",
      "Iteration 99, loss = 0.21684112\n",
      "Iteration 100, loss = 0.19019328\n",
      "Iteration 101, loss = 0.22500332\n",
      "Iteration 102, loss = 0.21932626\n",
      "Iteration 103, loss = 0.20948922\n",
      "Iteration 104, loss = 0.19958446\n",
      "Iteration 105, loss = 0.19717826\n",
      "Iteration 106, loss = 0.21314124\n",
      "Iteration 107, loss = 0.19973325\n",
      "Iteration 108, loss = 0.17818725\n",
      "Iteration 109, loss = 0.17779200\n",
      "Iteration 110, loss = 0.16889432\n",
      "Iteration 111, loss = 0.19792171\n",
      "Iteration 112, loss = 0.20802737\n",
      "Iteration 113, loss = 0.21799449\n",
      "Iteration 114, loss = 0.17160240\n",
      "Iteration 115, loss = 0.16253914\n",
      "Iteration 116, loss = 0.15728069\n",
      "Iteration 117, loss = 0.16313929\n",
      "Iteration 118, loss = 0.16081843\n",
      "Iteration 119, loss = 0.15872552\n",
      "Iteration 120, loss = 0.15805662\n",
      "Iteration 121, loss = 0.16307849\n",
      "Iteration 122, loss = 0.14833434\n",
      "Iteration 123, loss = 0.14925943\n",
      "Iteration 124, loss = 0.14022368\n",
      "Iteration 125, loss = 0.15184231\n",
      "Iteration 126, loss = 0.14817106\n",
      "Iteration 127, loss = 0.13693824\n",
      "Iteration 128, loss = 0.12699119\n",
      "Iteration 129, loss = 0.13216965\n",
      "Iteration 130, loss = 0.13078943\n",
      "Iteration 131, loss = 0.14428669\n",
      "Iteration 132, loss = 0.14001288\n",
      "Iteration 133, loss = 0.13535940\n",
      "Iteration 134, loss = 0.14815532\n",
      "Iteration 135, loss = 0.15685708\n",
      "Iteration 136, loss = 0.13998998\n",
      "Iteration 137, loss = 0.12912149\n",
      "Iteration 138, loss = 0.12365631\n",
      "Iteration 139, loss = 0.12481963\n",
      "Iteration 140, loss = 0.13380916\n",
      "Iteration 141, loss = 0.14011581\n",
      "Iteration 142, loss = 0.11279385\n",
      "Iteration 143, loss = 0.10470390\n",
      "Iteration 144, loss = 0.13826309\n",
      "Iteration 145, loss = 0.14300251\n",
      "Iteration 146, loss = 0.12868733\n",
      "Iteration 147, loss = 0.14268201\n",
      "Iteration 148, loss = 0.11756748\n",
      "Iteration 149, loss = 0.11360443\n",
      "Iteration 150, loss = 0.12214379\n",
      "Iteration 151, loss = 0.10913532\n",
      "Iteration 152, loss = 0.10504509\n",
      "Iteration 153, loss = 0.13057991\n",
      "Iteration 154, loss = 0.11705161\n",
      "Training loss did not improve more than tol=0.000001 for 10 consecutive epochs. Stopping.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.75      0.86        28\n",
      "           1       1.00      0.89      0.94        19\n",
      "           2       1.00      1.00      1.00        25\n",
      "           3       0.89      1.00      0.94        17\n",
      "           4       1.00      1.00      1.00        19\n",
      "           5       1.00      1.00      1.00        14\n",
      "           6       0.96      0.96      0.96        23\n",
      "           7       1.00      0.88      0.94        17\n",
      "           8       0.49      1.00      0.65        17\n",
      "           9       0.95      0.95      0.95        20\n",
      "          10       1.00      0.88      0.93        24\n",
      "          11       0.94      0.89      0.92        19\n",
      "          12       0.94      0.94      0.94        18\n",
      "          13       0.95      0.95      0.95        19\n",
      "          14       0.94      0.88      0.91        17\n",
      "          15       0.82      1.00      0.90         9\n",
      "          16       0.95      1.00      0.97        18\n",
      "          17       0.90      1.00      0.95        19\n",
      "          18       0.89      1.00      0.94        17\n",
      "          19       0.94      0.89      0.91        18\n",
      "          20       1.00      0.89      0.94        19\n",
      "          21       0.95      0.95      0.95        21\n",
      "          22       0.95      0.95      0.95        21\n",
      "          23       0.86      1.00      0.92        12\n",
      "          24       0.95      0.88      0.91        24\n",
      "          25       0.94      0.94      0.94        18\n",
      "          26       0.95      1.00      0.97        18\n",
      "          27       1.00      0.95      0.97        19\n",
      "          28       1.00      1.00      1.00        13\n",
      "          29       0.95      1.00      0.97        18\n",
      "          30       0.96      0.83      0.89        29\n",
      "          31       0.74      0.88      0.80        16\n",
      "          32       0.94      0.94      0.94        17\n",
      "          33       1.00      0.76      0.87        17\n",
      "          34       1.00      0.88      0.94        26\n",
      "          35       1.00      0.90      0.95        21\n",
      "          36       0.89      0.94      0.92        18\n",
      "          37       1.00      1.00      1.00        19\n",
      "\n",
      "    accuracy                           0.93       723\n",
      "   macro avg       0.94      0.94      0.93       723\n",
      "weighted avg       0.94      0.93      0.93       723\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "hidden_layers = (30,)\n",
    "activation = 'logistic'\n",
    "opts = dict(hidden_layer_sizes = hidden_layers,verbose = True,activation = activation,tol = 1e-6,max_iter = int(1e6))\n",
    "solver = 'adam'\n",
    "clf_MLP = MLPClassifier(solver = solver, **opts)\n",
    "clf_MLP.fit(X_train_,y_train)\n",
    "predictions = clf_MLP.predict(X_test_)\n",
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- hidden layers = (512,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 3.62200575\n",
      "Iteration 2, loss = 2.93244924\n",
      "Iteration 3, loss = 2.56313388\n",
      "Iteration 4, loss = 2.24841519\n",
      "Iteration 5, loss = 1.90345348\n",
      "Iteration 6, loss = 1.59988835\n",
      "Iteration 7, loss = 1.32108053\n",
      "Iteration 8, loss = 1.09378386\n",
      "Iteration 9, loss = 0.92433359\n",
      "Iteration 10, loss = 0.78460455\n",
      "Iteration 11, loss = 0.67130396\n",
      "Iteration 12, loss = 0.56298065\n",
      "Iteration 13, loss = 0.49679017\n",
      "Iteration 14, loss = 0.43873536\n",
      "Iteration 15, loss = 0.39398754\n",
      "Iteration 16, loss = 0.37499996\n",
      "Iteration 17, loss = 0.35666856\n",
      "Iteration 18, loss = 0.32242074\n",
      "Iteration 19, loss = 0.30268779\n",
      "Iteration 20, loss = 0.27134169\n",
      "Iteration 21, loss = 0.25116113\n",
      "Iteration 22, loss = 0.23589284\n",
      "Iteration 23, loss = 0.22829532\n",
      "Iteration 24, loss = 0.21889326\n",
      "Iteration 25, loss = 0.20618449\n",
      "Iteration 26, loss = 0.19285980\n",
      "Iteration 27, loss = 0.17468511\n",
      "Iteration 28, loss = 0.16629536\n",
      "Iteration 29, loss = 0.17334362\n",
      "Iteration 30, loss = 0.17942741\n",
      "Iteration 31, loss = 0.17119307\n",
      "Iteration 32, loss = 0.16102485\n",
      "Iteration 33, loss = 0.15970406\n",
      "Iteration 34, loss = 0.15449547\n",
      "Iteration 35, loss = 0.15521063\n",
      "Iteration 36, loss = 0.14838334\n",
      "Iteration 37, loss = 0.16105044\n",
      "Iteration 38, loss = 0.16439935\n",
      "Iteration 39, loss = 0.15620072\n",
      "Iteration 40, loss = 0.14373543\n",
      "Iteration 41, loss = 0.15158941\n",
      "Iteration 42, loss = 0.15998904\n",
      "Iteration 43, loss = 0.15980753\n",
      "Iteration 44, loss = 0.14823347\n",
      "Iteration 45, loss = 0.15229324\n",
      "Iteration 46, loss = 0.14380264\n",
      "Iteration 47, loss = 0.13461716\n",
      "Iteration 48, loss = 0.13162432\n",
      "Iteration 49, loss = 0.12799703\n",
      "Iteration 50, loss = 0.12896646\n",
      "Iteration 51, loss = 0.13592230\n",
      "Iteration 52, loss = 0.13296883\n",
      "Iteration 53, loss = 0.13379950\n",
      "Iteration 54, loss = 0.13729750\n",
      "Iteration 55, loss = 0.14773453\n",
      "Iteration 56, loss = 0.15337694\n",
      "Iteration 57, loss = 0.14116933\n",
      "Iteration 58, loss = 0.13892861\n",
      "Iteration 59, loss = 0.14288170\n",
      "Iteration 60, loss = 0.13833373\n",
      "Training loss did not improve more than tol=0.000001 for 10 consecutive epochs. Stopping.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.79      0.88        28\n",
      "           1       1.00      0.95      0.97        19\n",
      "           2       0.96      1.00      0.98        25\n",
      "           3       0.85      1.00      0.92        17\n",
      "           4       1.00      0.95      0.97        19\n",
      "           5       0.76      0.93      0.84        14\n",
      "           6       1.00      1.00      1.00        23\n",
      "           7       0.94      1.00      0.97        17\n",
      "           8       0.94      1.00      0.97        17\n",
      "           9       0.95      0.90      0.92        20\n",
      "          10       1.00      0.92      0.96        24\n",
      "          11       1.00      0.95      0.97        19\n",
      "          12       0.95      1.00      0.97        18\n",
      "          13       1.00      0.95      0.97        19\n",
      "          14       0.94      0.94      0.94        17\n",
      "          15       0.90      1.00      0.95         9\n",
      "          16       0.95      1.00      0.97        18\n",
      "          17       1.00      1.00      1.00        19\n",
      "          18       0.81      1.00      0.89        17\n",
      "          19       0.94      0.89      0.91        18\n",
      "          20       1.00      0.79      0.88        19\n",
      "          21       0.84      1.00      0.91        21\n",
      "          22       1.00      0.95      0.98        21\n",
      "          23       1.00      0.92      0.96        12\n",
      "          24       1.00      0.92      0.96        24\n",
      "          25       1.00      0.94      0.97        18\n",
      "          26       0.95      1.00      0.97        18\n",
      "          27       1.00      0.84      0.91        19\n",
      "          28       0.92      0.92      0.92        13\n",
      "          29       1.00      1.00      1.00        18\n",
      "          30       0.96      0.90      0.93        29\n",
      "          31       0.94      0.94      0.94        16\n",
      "          32       1.00      0.88      0.94        17\n",
      "          33       0.73      0.94      0.82        17\n",
      "          34       1.00      0.92      0.96        26\n",
      "          35       0.95      0.95      0.95        21\n",
      "          36       0.75      1.00      0.86        18\n",
      "          37       1.00      1.00      1.00        19\n",
      "\n",
      "    accuracy                           0.94       723\n",
      "   macro avg       0.95      0.95      0.94       723\n",
      "weighted avg       0.95      0.94      0.94       723\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "hidden_layers = (512,)\n",
    "activation = 'logistic'\n",
    "opts = dict(hidden_layer_sizes = hidden_layers,verbose = True,activation = activation,tol = 1e-6,max_iter = int(1e6))\n",
    "solver = 'adam'\n",
    "clf_MLP = MLPClassifier(solver = solver, **opts)\n",
    "clf_MLP.fit(X_train_,y_train)\n",
    "predictions = clf_MLP.predict(X_test_)\n",
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 結論:\n",
    "### 1. 因為前兩組資料選擇不同的activation 跟 solver ，效果並沒有差太多，因此只對於hidden layers = (30,) 跟 hidden layers = (512,) 做比較\n",
    "### 2. 因 hidden layers = (512,) 跟 hidden layers = (60,60,60) 或 hidden layers = (512,512,512)，差別並沒有太大，只用一層512個神經元就可以達到跟三層60個神經元或是三層512個神經元的效果，所以只比較 hidden layers = (30,) 跟 hidden layers = (512,) \n",
    "### 3. hidden layers = (30,) 的準確率為93% ， hidden layers = (512,) 的準確率為94%，表現均不錯 "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA主成分分析(成分比例採0.95) + 神經網路NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 3.68186379\n",
      "Iteration 2, loss = 3.62970329\n",
      "Iteration 3, loss = 3.58747715\n",
      "Iteration 4, loss = 3.54822854\n",
      "Iteration 5, loss = 3.51050639\n",
      "Iteration 6, loss = 3.47079332\n",
      "Iteration 7, loss = 3.43047724\n",
      "Iteration 8, loss = 3.38711651\n",
      "Iteration 9, loss = 3.34105788\n",
      "Iteration 10, loss = 3.29267801\n",
      "Iteration 11, loss = 3.24205058\n",
      "Iteration 12, loss = 3.18776879\n",
      "Iteration 13, loss = 3.13178065\n",
      "Iteration 14, loss = 3.07436468\n",
      "Iteration 15, loss = 3.01507833\n",
      "Iteration 16, loss = 2.95194607\n",
      "Iteration 17, loss = 2.88618106\n",
      "Iteration 18, loss = 2.81776393\n",
      "Iteration 19, loss = 2.74835676\n",
      "Iteration 20, loss = 2.67912425\n",
      "Iteration 21, loss = 2.61037599\n",
      "Iteration 22, loss = 2.54210000\n",
      "Iteration 23, loss = 2.47496461\n",
      "Iteration 24, loss = 2.40804952\n",
      "Iteration 25, loss = 2.34195478\n",
      "Iteration 26, loss = 2.27624321\n",
      "Iteration 27, loss = 2.21211917\n",
      "Iteration 28, loss = 2.14857630\n",
      "Iteration 29, loss = 2.08589285\n",
      "Iteration 30, loss = 2.02532318\n",
      "Iteration 31, loss = 1.96610706\n",
      "Iteration 32, loss = 1.90959379\n",
      "Iteration 33, loss = 1.85363188\n",
      "Iteration 34, loss = 1.79937219\n",
      "Iteration 35, loss = 1.74691156\n",
      "Iteration 36, loss = 1.69581738\n",
      "Iteration 37, loss = 1.64682660\n",
      "Iteration 38, loss = 1.60004462\n",
      "Iteration 39, loss = 1.55398305\n",
      "Iteration 40, loss = 1.50987023\n",
      "Iteration 41, loss = 1.46746057\n",
      "Iteration 42, loss = 1.42621741\n",
      "Iteration 43, loss = 1.38630583\n",
      "Iteration 44, loss = 1.34768024\n",
      "Iteration 45, loss = 1.31081195\n",
      "Iteration 46, loss = 1.27480872\n",
      "Iteration 47, loss = 1.24149704\n",
      "Iteration 48, loss = 1.20668279\n",
      "Iteration 49, loss = 1.17489464\n",
      "Iteration 50, loss = 1.14414735\n",
      "Iteration 51, loss = 1.11383575\n",
      "Iteration 52, loss = 1.08490054\n",
      "Iteration 53, loss = 1.05641318\n",
      "Iteration 54, loss = 1.02996877\n",
      "Iteration 55, loss = 1.00462242\n",
      "Iteration 56, loss = 0.97951981\n",
      "Iteration 57, loss = 0.95560029\n",
      "Iteration 58, loss = 0.93309963\n",
      "Iteration 59, loss = 0.91049236\n",
      "Iteration 60, loss = 0.88954635\n",
      "Iteration 61, loss = 0.86863718\n",
      "Iteration 62, loss = 0.84893452\n",
      "Iteration 63, loss = 0.83063406\n",
      "Iteration 64, loss = 0.81168874\n",
      "Iteration 65, loss = 0.79382750\n",
      "Iteration 66, loss = 0.77633918\n",
      "Iteration 67, loss = 0.75989376\n",
      "Iteration 68, loss = 0.74440962\n",
      "Iteration 69, loss = 0.72858591\n",
      "Iteration 70, loss = 0.71342926\n",
      "Iteration 71, loss = 0.69943592\n",
      "Iteration 72, loss = 0.68506256\n",
      "Iteration 73, loss = 0.67202851\n",
      "Iteration 74, loss = 0.65861990\n",
      "Iteration 75, loss = 0.64669247\n",
      "Iteration 76, loss = 0.63403337\n",
      "Iteration 77, loss = 0.62210311\n",
      "Iteration 78, loss = 0.61172824\n",
      "Iteration 79, loss = 0.59926325\n",
      "Iteration 80, loss = 0.58874653\n",
      "Iteration 81, loss = 0.57797032\n",
      "Iteration 82, loss = 0.56780339\n",
      "Iteration 83, loss = 0.55781120\n",
      "Iteration 84, loss = 0.54860675\n",
      "Iteration 85, loss = 0.53866488\n",
      "Iteration 86, loss = 0.52938824\n",
      "Iteration 87, loss = 0.52110119\n",
      "Iteration 88, loss = 0.51201688\n",
      "Iteration 89, loss = 0.50388123\n",
      "Iteration 90, loss = 0.49560618\n",
      "Iteration 91, loss = 0.48759194\n",
      "Iteration 92, loss = 0.47976031\n",
      "Iteration 93, loss = 0.47250638\n",
      "Iteration 94, loss = 0.46475600\n",
      "Iteration 95, loss = 0.45812037\n",
      "Iteration 96, loss = 0.45083176\n",
      "Iteration 97, loss = 0.44410291\n",
      "Iteration 98, loss = 0.43791834\n",
      "Iteration 99, loss = 0.43098956\n",
      "Iteration 100, loss = 0.42458565\n",
      "Iteration 101, loss = 0.41854386\n",
      "Iteration 102, loss = 0.41225371\n",
      "Iteration 103, loss = 0.40648997\n",
      "Iteration 104, loss = 0.40070966\n",
      "Iteration 105, loss = 0.39499728\n",
      "Iteration 106, loss = 0.38957800\n",
      "Iteration 107, loss = 0.38389457\n",
      "Iteration 108, loss = 0.37880393\n",
      "Iteration 109, loss = 0.37379473\n",
      "Iteration 110, loss = 0.36833406\n",
      "Iteration 111, loss = 0.36313244\n",
      "Iteration 112, loss = 0.35849268\n",
      "Iteration 113, loss = 0.35367497\n",
      "Iteration 114, loss = 0.34895420\n",
      "Iteration 115, loss = 0.34420831\n",
      "Iteration 116, loss = 0.34011742\n",
      "Iteration 117, loss = 0.33570904\n",
      "Iteration 118, loss = 0.33118199\n",
      "Iteration 119, loss = 0.32699205\n",
      "Iteration 120, loss = 0.32306073\n",
      "Iteration 121, loss = 0.31938490\n",
      "Iteration 122, loss = 0.31488915\n",
      "Iteration 123, loss = 0.31122778\n",
      "Iteration 124, loss = 0.30772581\n",
      "Iteration 125, loss = 0.30404688\n",
      "Iteration 126, loss = 0.30072024\n",
      "Iteration 127, loss = 0.29635415\n",
      "Iteration 128, loss = 0.29279704\n",
      "Iteration 129, loss = 0.28980216\n",
      "Iteration 130, loss = 0.28589170\n",
      "Iteration 131, loss = 0.28334219\n",
      "Iteration 132, loss = 0.27940760\n",
      "Iteration 133, loss = 0.27702285\n",
      "Iteration 134, loss = 0.27401075\n",
      "Iteration 135, loss = 0.27045380\n",
      "Iteration 136, loss = 0.26724826\n",
      "Iteration 137, loss = 0.26425548\n",
      "Iteration 138, loss = 0.26128969\n",
      "Iteration 139, loss = 0.25865383\n",
      "Iteration 140, loss = 0.25578213\n",
      "Iteration 141, loss = 0.25268962\n",
      "Iteration 142, loss = 0.25018653\n",
      "Iteration 143, loss = 0.24768836\n",
      "Iteration 144, loss = 0.24498141\n",
      "Iteration 145, loss = 0.24247448\n",
      "Iteration 146, loss = 0.23998758\n",
      "Iteration 147, loss = 0.23744972\n",
      "Iteration 148, loss = 0.23482689\n",
      "Iteration 149, loss = 0.23235612\n",
      "Iteration 150, loss = 0.22973238\n",
      "Iteration 151, loss = 0.22738123\n",
      "Iteration 152, loss = 0.22506402\n",
      "Iteration 153, loss = 0.22307674\n",
      "Iteration 154, loss = 0.22064413\n",
      "Iteration 155, loss = 0.21817894\n",
      "Iteration 156, loss = 0.21600138\n",
      "Iteration 157, loss = 0.21391954\n",
      "Iteration 158, loss = 0.21182496\n",
      "Iteration 159, loss = 0.20932716\n",
      "Iteration 160, loss = 0.20738389\n",
      "Iteration 161, loss = 0.20542563\n",
      "Iteration 162, loss = 0.20339398\n",
      "Iteration 163, loss = 0.20136162\n",
      "Iteration 164, loss = 0.19908268\n",
      "Iteration 165, loss = 0.19775750\n",
      "Iteration 166, loss = 0.19557562\n",
      "Iteration 167, loss = 0.19385073\n",
      "Iteration 168, loss = 0.19186523\n",
      "Iteration 169, loss = 0.19032614\n",
      "Iteration 170, loss = 0.18907121\n",
      "Iteration 171, loss = 0.18614596\n",
      "Iteration 172, loss = 0.18493959\n",
      "Iteration 173, loss = 0.18305795\n",
      "Iteration 174, loss = 0.18126169\n",
      "Iteration 175, loss = 0.17974952\n",
      "Iteration 176, loss = 0.17774482\n",
      "Iteration 177, loss = 0.17657004\n",
      "Iteration 178, loss = 0.17514286\n",
      "Iteration 179, loss = 0.17304468\n",
      "Iteration 180, loss = 0.17140601\n",
      "Iteration 181, loss = 0.17017411\n",
      "Iteration 182, loss = 0.16883988\n",
      "Iteration 183, loss = 0.16695157\n",
      "Iteration 184, loss = 0.16601971\n",
      "Iteration 185, loss = 0.16464334\n",
      "Iteration 186, loss = 0.16266028\n",
      "Iteration 187, loss = 0.16142068\n",
      "Iteration 188, loss = 0.16015970\n",
      "Iteration 189, loss = 0.15845241\n",
      "Iteration 190, loss = 0.15713530\n",
      "Iteration 191, loss = 0.15589322\n",
      "Iteration 192, loss = 0.15435165\n",
      "Iteration 193, loss = 0.15305782\n",
      "Iteration 194, loss = 0.15216639\n",
      "Iteration 195, loss = 0.15047403\n",
      "Iteration 196, loss = 0.14942511\n",
      "Iteration 197, loss = 0.14803397\n",
      "Iteration 198, loss = 0.14787843\n",
      "Iteration 199, loss = 0.14546264\n",
      "Iteration 200, loss = 0.14479145\n",
      "Iteration 201, loss = 0.14360713\n",
      "Iteration 202, loss = 0.14209888\n",
      "Iteration 203, loss = 0.14092487\n",
      "Iteration 204, loss = 0.13968823\n",
      "Iteration 205, loss = 0.13896942\n",
      "Iteration 206, loss = 0.13754668\n",
      "Iteration 207, loss = 0.13639010\n",
      "Iteration 208, loss = 0.13524526\n",
      "Iteration 209, loss = 0.13442272\n",
      "Iteration 210, loss = 0.13316574\n",
      "Iteration 211, loss = 0.13210815\n",
      "Iteration 212, loss = 0.13091546\n",
      "Iteration 213, loss = 0.12966445\n",
      "Iteration 214, loss = 0.12902510\n",
      "Iteration 215, loss = 0.12781089\n",
      "Iteration 216, loss = 0.12741260\n",
      "Iteration 217, loss = 0.12628399\n",
      "Iteration 218, loss = 0.12510726\n",
      "Iteration 219, loss = 0.12437357\n",
      "Iteration 220, loss = 0.12400743\n",
      "Iteration 221, loss = 0.12218106\n",
      "Iteration 222, loss = 0.12137985\n",
      "Iteration 223, loss = 0.11967295\n",
      "Iteration 224, loss = 0.11941403\n",
      "Iteration 225, loss = 0.11842833\n",
      "Iteration 226, loss = 0.11799953\n",
      "Iteration 227, loss = 0.11705105\n",
      "Iteration 228, loss = 0.11563733\n",
      "Iteration 229, loss = 0.11477829\n",
      "Iteration 230, loss = 0.11383519\n",
      "Iteration 231, loss = 0.11312951\n",
      "Iteration 232, loss = 0.11204201\n",
      "Iteration 233, loss = 0.11172354\n",
      "Iteration 234, loss = 0.11081834\n",
      "Iteration 235, loss = 0.10970368\n",
      "Iteration 236, loss = 0.10939080\n",
      "Iteration 237, loss = 0.10841619\n",
      "Iteration 238, loss = 0.10762553\n",
      "Iteration 239, loss = 0.10660126\n",
      "Iteration 240, loss = 0.10582013\n",
      "Iteration 241, loss = 0.10508241\n",
      "Iteration 242, loss = 0.10395521\n",
      "Iteration 243, loss = 0.10352758\n",
      "Iteration 244, loss = 0.10309574\n",
      "Iteration 245, loss = 0.10226288\n",
      "Iteration 246, loss = 0.10147402\n",
      "Iteration 247, loss = 0.10076618\n",
      "Iteration 248, loss = 0.09985769\n",
      "Iteration 249, loss = 0.09941371\n",
      "Iteration 250, loss = 0.09865740\n",
      "Iteration 251, loss = 0.09789122\n",
      "Iteration 252, loss = 0.09690557\n",
      "Iteration 253, loss = 0.09668348\n",
      "Iteration 254, loss = 0.09598621\n",
      "Iteration 255, loss = 0.09535522\n",
      "Iteration 256, loss = 0.09435368\n",
      "Iteration 257, loss = 0.09388130\n",
      "Iteration 258, loss = 0.09337933\n",
      "Iteration 259, loss = 0.09260779\n",
      "Iteration 260, loss = 0.09213203\n",
      "Iteration 261, loss = 0.09164995\n",
      "Iteration 262, loss = 0.09059695\n",
      "Iteration 263, loss = 0.09004035\n",
      "Iteration 264, loss = 0.08965152\n",
      "Iteration 265, loss = 0.08886654\n",
      "Iteration 266, loss = 0.08804220\n",
      "Iteration 267, loss = 0.08735787\n",
      "Iteration 268, loss = 0.08673325\n",
      "Iteration 269, loss = 0.08618166\n",
      "Iteration 270, loss = 0.08596453\n",
      "Iteration 271, loss = 0.08536182\n",
      "Iteration 272, loss = 0.08463573\n",
      "Iteration 273, loss = 0.08383013\n",
      "Iteration 274, loss = 0.08400639\n",
      "Iteration 275, loss = 0.08296290\n",
      "Iteration 276, loss = 0.08251174\n",
      "Iteration 277, loss = 0.08204859\n",
      "Iteration 278, loss = 0.08106685\n",
      "Iteration 279, loss = 0.08081501\n",
      "Iteration 280, loss = 0.08066168\n",
      "Iteration 281, loss = 0.08005145\n",
      "Iteration 282, loss = 0.07898126\n",
      "Iteration 283, loss = 0.07835623\n",
      "Iteration 284, loss = 0.07808542\n",
      "Iteration 285, loss = 0.07784732\n",
      "Iteration 286, loss = 0.07747439\n",
      "Iteration 287, loss = 0.07636917\n",
      "Iteration 288, loss = 0.07608930\n",
      "Iteration 289, loss = 0.07553601\n",
      "Iteration 290, loss = 0.07551729\n",
      "Iteration 291, loss = 0.07486753\n",
      "Iteration 292, loss = 0.07465648\n",
      "Iteration 293, loss = 0.07382024\n",
      "Iteration 294, loss = 0.07318514\n",
      "Iteration 295, loss = 0.07277707\n",
      "Iteration 296, loss = 0.07282691\n",
      "Iteration 297, loss = 0.07230532\n",
      "Iteration 298, loss = 0.07180008\n",
      "Iteration 299, loss = 0.07107416\n",
      "Iteration 300, loss = 0.07067902\n",
      "Iteration 301, loss = 0.07036114\n",
      "Iteration 302, loss = 0.07012158\n",
      "Iteration 303, loss = 0.06950467\n",
      "Iteration 304, loss = 0.06911992\n",
      "Iteration 305, loss = 0.06873377\n",
      "Iteration 306, loss = 0.06821534\n",
      "Iteration 307, loss = 0.06738403\n",
      "Iteration 308, loss = 0.06743600\n",
      "Iteration 309, loss = 0.06714583\n",
      "Iteration 310, loss = 0.06638037\n",
      "Iteration 311, loss = 0.06586452\n",
      "Iteration 312, loss = 0.06557278\n",
      "Iteration 313, loss = 0.06540622\n",
      "Iteration 314, loss = 0.06461181\n",
      "Iteration 315, loss = 0.06439247\n",
      "Iteration 316, loss = 0.06374849\n",
      "Iteration 317, loss = 0.06380545\n",
      "Iteration 318, loss = 0.06334808\n",
      "Iteration 319, loss = 0.06311677\n",
      "Iteration 320, loss = 0.06245779\n",
      "Iteration 321, loss = 0.06220235\n",
      "Iteration 322, loss = 0.06156156\n",
      "Iteration 323, loss = 0.06144619\n",
      "Iteration 324, loss = 0.06080758\n",
      "Iteration 325, loss = 0.06106350\n",
      "Iteration 326, loss = 0.06048581\n",
      "Iteration 327, loss = 0.06019379\n",
      "Iteration 328, loss = 0.05938148\n",
      "Iteration 329, loss = 0.05955575\n",
      "Iteration 330, loss = 0.06015724\n",
      "Iteration 331, loss = 0.05911721\n",
      "Iteration 332, loss = 0.05894692\n",
      "Iteration 333, loss = 0.05809830\n",
      "Iteration 334, loss = 0.05744931\n",
      "Iteration 335, loss = 0.05716969\n",
      "Iteration 336, loss = 0.05710358\n",
      "Iteration 337, loss = 0.05663733\n",
      "Iteration 338, loss = 0.05654442\n",
      "Iteration 339, loss = 0.05620659\n",
      "Iteration 340, loss = 0.05553694\n",
      "Iteration 341, loss = 0.05563862\n",
      "Iteration 342, loss = 0.05556920\n",
      "Iteration 343, loss = 0.05484251\n",
      "Iteration 344, loss = 0.05470509\n",
      "Iteration 345, loss = 0.05436213\n",
      "Iteration 346, loss = 0.05418748\n",
      "Iteration 347, loss = 0.05415375\n",
      "Iteration 348, loss = 0.05343618\n",
      "Iteration 349, loss = 0.05337611\n",
      "Iteration 350, loss = 0.05299118\n",
      "Iteration 351, loss = 0.05220140\n",
      "Iteration 352, loss = 0.05218692\n",
      "Iteration 353, loss = 0.05176489\n",
      "Iteration 354, loss = 0.05173716\n",
      "Iteration 355, loss = 0.05108175\n",
      "Iteration 356, loss = 0.05120246\n",
      "Iteration 357, loss = 0.05128999\n",
      "Iteration 358, loss = 0.05111364\n",
      "Iteration 359, loss = 0.04988866\n",
      "Iteration 360, loss = 0.05031034\n",
      "Iteration 361, loss = 0.04966643\n",
      "Iteration 362, loss = 0.04968699\n",
      "Iteration 363, loss = 0.04896240\n",
      "Iteration 364, loss = 0.04917973\n",
      "Iteration 365, loss = 0.04874874\n",
      "Iteration 366, loss = 0.04833203\n",
      "Iteration 367, loss = 0.04826618\n",
      "Iteration 368, loss = 0.04798500\n",
      "Iteration 369, loss = 0.04743847\n",
      "Iteration 370, loss = 0.04748715\n",
      "Iteration 371, loss = 0.04738001\n",
      "Iteration 372, loss = 0.04725023\n",
      "Iteration 373, loss = 0.04664730\n",
      "Iteration 374, loss = 0.04618510\n",
      "Iteration 375, loss = 0.04596635\n",
      "Iteration 376, loss = 0.04630553\n",
      "Iteration 377, loss = 0.04573457\n",
      "Iteration 378, loss = 0.04600058\n",
      "Iteration 379, loss = 0.04514107\n",
      "Iteration 380, loss = 0.04490571\n",
      "Iteration 381, loss = 0.04507855\n",
      "Iteration 382, loss = 0.04456931\n",
      "Iteration 383, loss = 0.04445968\n",
      "Iteration 384, loss = 0.04399764\n",
      "Iteration 385, loss = 0.04366843\n",
      "Iteration 386, loss = 0.04366966\n",
      "Iteration 387, loss = 0.04297731\n",
      "Iteration 388, loss = 0.04293714\n",
      "Iteration 389, loss = 0.04269613\n",
      "Iteration 390, loss = 0.04267124\n",
      "Iteration 391, loss = 0.04233804\n",
      "Iteration 392, loss = 0.04187298\n",
      "Iteration 393, loss = 0.04203406\n",
      "Iteration 394, loss = 0.04202192\n",
      "Iteration 395, loss = 0.04173617\n",
      "Iteration 396, loss = 0.04098281\n",
      "Iteration 397, loss = 0.04139663\n",
      "Iteration 398, loss = 0.04098084\n",
      "Iteration 399, loss = 0.04106987\n",
      "Iteration 400, loss = 0.04036130\n",
      "Iteration 401, loss = 0.04036697\n",
      "Iteration 402, loss = 0.04057900\n",
      "Iteration 403, loss = 0.03986591\n",
      "Iteration 404, loss = 0.03979870\n",
      "Iteration 405, loss = 0.04015703\n",
      "Iteration 406, loss = 0.03926930\n",
      "Iteration 407, loss = 0.03904910\n",
      "Iteration 408, loss = 0.03896874\n",
      "Iteration 409, loss = 0.03896141\n",
      "Iteration 410, loss = 0.03872254\n",
      "Iteration 411, loss = 0.03817962\n",
      "Iteration 412, loss = 0.03835874\n",
      "Iteration 413, loss = 0.03769129\n",
      "Iteration 414, loss = 0.03769336\n",
      "Iteration 415, loss = 0.03741104\n",
      "Iteration 416, loss = 0.03736765\n",
      "Iteration 417, loss = 0.03701173\n",
      "Iteration 418, loss = 0.03711540\n",
      "Iteration 419, loss = 0.03682701\n",
      "Iteration 420, loss = 0.03705096\n",
      "Iteration 421, loss = 0.03634454\n",
      "Iteration 422, loss = 0.03638087\n",
      "Iteration 423, loss = 0.03623939\n",
      "Iteration 424, loss = 0.03616724\n",
      "Iteration 425, loss = 0.03560639\n",
      "Iteration 426, loss = 0.03549297\n",
      "Iteration 427, loss = 0.03515534\n",
      "Iteration 428, loss = 0.03539935\n",
      "Iteration 429, loss = 0.03572531\n",
      "Iteration 430, loss = 0.03437852\n",
      "Iteration 431, loss = 0.03518253\n",
      "Iteration 432, loss = 0.03472103\n",
      "Iteration 433, loss = 0.03444906\n",
      "Iteration 434, loss = 0.03411314\n",
      "Iteration 435, loss = 0.03389740\n",
      "Iteration 436, loss = 0.03415528\n",
      "Iteration 437, loss = 0.03354762\n",
      "Iteration 438, loss = 0.03386877\n",
      "Iteration 439, loss = 0.03361332\n",
      "Iteration 440, loss = 0.03345455\n",
      "Iteration 441, loss = 0.03313568\n",
      "Iteration 442, loss = 0.03305908\n",
      "Iteration 443, loss = 0.03299347\n",
      "Iteration 444, loss = 0.03297601\n",
      "Iteration 445, loss = 0.03257165\n",
      "Iteration 446, loss = 0.03222242\n",
      "Iteration 447, loss = 0.03214134\n",
      "Iteration 448, loss = 0.03240503\n",
      "Iteration 449, loss = 0.03192536\n",
      "Iteration 450, loss = 0.03168505\n",
      "Iteration 451, loss = 0.03208196\n",
      "Iteration 452, loss = 0.03147732\n",
      "Iteration 453, loss = 0.03115794\n",
      "Iteration 454, loss = 0.03104419\n",
      "Iteration 455, loss = 0.03151289\n",
      "Iteration 456, loss = 0.03100492\n",
      "Iteration 457, loss = 0.03106693\n",
      "Iteration 458, loss = 0.03112378\n",
      "Iteration 459, loss = 0.03124416\n",
      "Iteration 460, loss = 0.03097811\n",
      "Iteration 461, loss = 0.03047615\n",
      "Iteration 462, loss = 0.03029006\n",
      "Iteration 463, loss = 0.03018929\n",
      "Iteration 464, loss = 0.03024609\n",
      "Iteration 465, loss = 0.02986794\n",
      "Iteration 466, loss = 0.02979707\n",
      "Iteration 467, loss = 0.03040454\n",
      "Iteration 468, loss = 0.02986912\n",
      "Iteration 469, loss = 0.02914562\n",
      "Iteration 470, loss = 0.02912194\n",
      "Iteration 471, loss = 0.02912383\n",
      "Iteration 472, loss = 0.02909354\n",
      "Iteration 473, loss = 0.02828145\n",
      "Iteration 474, loss = 0.02861569\n",
      "Iteration 475, loss = 0.02923033\n",
      "Iteration 476, loss = 0.02869223\n",
      "Iteration 477, loss = 0.02874687\n",
      "Iteration 478, loss = 0.02813261\n",
      "Iteration 479, loss = 0.02815664\n",
      "Iteration 480, loss = 0.02779105\n",
      "Iteration 481, loss = 0.02802992\n",
      "Iteration 482, loss = 0.02862758\n",
      "Iteration 483, loss = 0.02794394\n",
      "Iteration 484, loss = 0.02720674\n",
      "Iteration 485, loss = 0.02719376\n",
      "Iteration 486, loss = 0.02729404\n",
      "Iteration 487, loss = 0.02687780\n",
      "Iteration 488, loss = 0.02712331\n",
      "Iteration 489, loss = 0.02730141\n",
      "Iteration 490, loss = 0.02644967\n",
      "Iteration 491, loss = 0.02702234\n",
      "Iteration 492, loss = 0.02630197\n",
      "Iteration 493, loss = 0.02773782\n",
      "Iteration 494, loss = 0.02617674\n",
      "Iteration 495, loss = 0.02690595\n",
      "Iteration 496, loss = 0.02813632\n",
      "Iteration 497, loss = 0.02644626\n",
      "Iteration 498, loss = 0.02605193\n",
      "Iteration 499, loss = 0.02539599\n",
      "Iteration 500, loss = 0.02561668\n",
      "Iteration 501, loss = 0.02526442\n",
      "Iteration 502, loss = 0.02524864\n",
      "Iteration 503, loss = 0.02535080\n",
      "Iteration 504, loss = 0.02507227\n",
      "Iteration 505, loss = 0.02505800\n",
      "Iteration 506, loss = 0.02505971\n",
      "Iteration 507, loss = 0.02481980\n",
      "Iteration 508, loss = 0.02485187\n",
      "Iteration 509, loss = 0.02452508\n",
      "Iteration 510, loss = 0.02454097\n",
      "Iteration 511, loss = 0.02442724\n",
      "Iteration 512, loss = 0.02426738\n",
      "Iteration 513, loss = 0.02402931\n",
      "Iteration 514, loss = 0.02390100\n",
      "Iteration 515, loss = 0.02427732\n",
      "Iteration 516, loss = 0.02495580\n",
      "Iteration 517, loss = 0.02439761\n",
      "Iteration 518, loss = 0.02408887\n",
      "Iteration 519, loss = 0.02398654\n",
      "Iteration 520, loss = 0.02348251\n",
      "Iteration 521, loss = 0.02353149\n",
      "Iteration 522, loss = 0.02352317\n",
      "Iteration 523, loss = 0.02329881\n",
      "Iteration 524, loss = 0.02321228\n",
      "Iteration 525, loss = 0.02292321\n",
      "Iteration 526, loss = 0.02292098\n",
      "Iteration 527, loss = 0.02260369\n",
      "Iteration 528, loss = 0.02310262\n",
      "Iteration 529, loss = 0.02296702\n",
      "Iteration 530, loss = 0.02244230\n",
      "Iteration 531, loss = 0.02270111\n",
      "Iteration 532, loss = 0.02311111\n",
      "Iteration 533, loss = 0.02219818\n",
      "Iteration 534, loss = 0.02230522\n",
      "Iteration 535, loss = 0.02184102\n",
      "Iteration 536, loss = 0.02240196\n",
      "Iteration 537, loss = 0.02199682\n",
      "Iteration 538, loss = 0.02190821\n",
      "Iteration 539, loss = 0.02227958\n",
      "Iteration 540, loss = 0.02202813\n",
      "Iteration 541, loss = 0.02278311\n",
      "Iteration 542, loss = 0.02101983\n",
      "Iteration 543, loss = 0.02213282\n",
      "Iteration 544, loss = 0.02159751\n",
      "Iteration 545, loss = 0.02172045\n",
      "Iteration 546, loss = 0.02146139\n",
      "Iteration 547, loss = 0.02125254\n",
      "Iteration 548, loss = 0.02137107\n",
      "Iteration 549, loss = 0.02070128\n",
      "Iteration 550, loss = 0.02082354\n",
      "Iteration 551, loss = 0.02069292\n",
      "Iteration 552, loss = 0.02080291\n",
      "Iteration 553, loss = 0.02096643\n",
      "Iteration 554, loss = 0.02050213\n",
      "Iteration 555, loss = 0.02038961\n",
      "Iteration 556, loss = 0.02037195\n",
      "Iteration 557, loss = 0.02048499\n",
      "Iteration 558, loss = 0.02048655\n",
      "Iteration 559, loss = 0.01998876\n",
      "Iteration 560, loss = 0.02086819\n",
      "Iteration 561, loss = 0.02067856\n",
      "Iteration 562, loss = 0.02015892\n",
      "Iteration 563, loss = 0.01991935\n",
      "Iteration 564, loss = 0.01969070\n",
      "Iteration 565, loss = 0.01992511\n",
      "Iteration 566, loss = 0.02060611\n",
      "Iteration 567, loss = 0.01945510\n",
      "Iteration 568, loss = 0.01964170\n",
      "Iteration 569, loss = 0.01981763\n",
      "Iteration 570, loss = 0.01939691\n",
      "Iteration 571, loss = 0.01967113\n",
      "Iteration 572, loss = 0.01970161\n",
      "Iteration 573, loss = 0.01993954\n",
      "Iteration 574, loss = 0.01882314\n",
      "Iteration 575, loss = 0.01893871\n",
      "Iteration 576, loss = 0.01911121\n",
      "Iteration 577, loss = 0.01870524\n",
      "Iteration 578, loss = 0.01902161\n",
      "Iteration 579, loss = 0.01908618\n",
      "Iteration 580, loss = 0.01878652\n",
      "Iteration 581, loss = 0.01847438\n",
      "Iteration 582, loss = 0.01873136\n",
      "Iteration 583, loss = 0.01844292\n",
      "Iteration 584, loss = 0.01854843\n",
      "Iteration 585, loss = 0.01850060\n",
      "Iteration 586, loss = 0.01832206\n",
      "Iteration 587, loss = 0.01809722\n",
      "Iteration 588, loss = 0.01863022\n",
      "Iteration 589, loss = 0.01879379\n",
      "Iteration 590, loss = 0.01883512\n",
      "Iteration 591, loss = 0.01863011\n",
      "Iteration 592, loss = 0.01778138\n",
      "Iteration 593, loss = 0.01783991\n",
      "Iteration 594, loss = 0.01775032\n",
      "Iteration 595, loss = 0.01750087\n",
      "Iteration 596, loss = 0.01778726\n",
      "Iteration 597, loss = 0.01761337\n",
      "Iteration 598, loss = 0.01740970\n",
      "Iteration 599, loss = 0.01740308\n",
      "Iteration 600, loss = 0.01736192\n",
      "Iteration 601, loss = 0.01747665\n",
      "Iteration 602, loss = 0.01735376\n",
      "Iteration 603, loss = 0.01713186\n",
      "Iteration 604, loss = 0.01747670\n",
      "Iteration 605, loss = 0.01726567\n",
      "Iteration 606, loss = 0.01745605\n",
      "Iteration 607, loss = 0.01713378\n",
      "Iteration 608, loss = 0.01726968\n",
      "Iteration 609, loss = 0.01685448\n",
      "Iteration 610, loss = 0.01676628\n",
      "Iteration 611, loss = 0.01687170\n",
      "Iteration 612, loss = 0.01674769\n",
      "Iteration 613, loss = 0.01658214\n",
      "Iteration 614, loss = 0.01684211\n",
      "Iteration 615, loss = 0.01660740\n",
      "Iteration 616, loss = 0.01636781\n",
      "Iteration 617, loss = 0.01624576\n",
      "Iteration 618, loss = 0.01649395\n",
      "Iteration 619, loss = 0.01637493\n",
      "Iteration 620, loss = 0.01666239\n",
      "Iteration 621, loss = 0.01629703\n",
      "Iteration 622, loss = 0.01688561\n",
      "Iteration 623, loss = 0.01654530\n",
      "Iteration 624, loss = 0.01627589\n",
      "Iteration 625, loss = 0.01609063\n",
      "Iteration 626, loss = 0.01607250\n",
      "Iteration 627, loss = 0.01575849\n",
      "Iteration 628, loss = 0.01624132\n",
      "Iteration 629, loss = 0.01578183\n",
      "Iteration 630, loss = 0.01554027\n",
      "Iteration 631, loss = 0.01602784\n",
      "Iteration 632, loss = 0.01585003\n",
      "Iteration 633, loss = 0.01568619\n",
      "Iteration 634, loss = 0.01609627\n",
      "Iteration 635, loss = 0.01525939\n",
      "Iteration 636, loss = 0.01554781\n",
      "Iteration 637, loss = 0.01524939\n",
      "Iteration 638, loss = 0.01552143\n",
      "Iteration 639, loss = 0.01478749\n",
      "Iteration 640, loss = 0.01614882\n",
      "Iteration 641, loss = 0.01527627\n",
      "Iteration 642, loss = 0.01542445\n",
      "Iteration 643, loss = 0.01511842\n",
      "Iteration 644, loss = 0.01600946\n",
      "Iteration 645, loss = 0.01522177\n",
      "Iteration 646, loss = 0.01506085\n",
      "Iteration 647, loss = 0.01495650\n",
      "Iteration 648, loss = 0.01660011\n",
      "Iteration 649, loss = 0.01524476\n",
      "Iteration 650, loss = 0.01544839\n",
      "Training loss did not improve more than tol=0.000001 for 10 consecutive epochs. Stopping.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.71      0.83        28\n",
      "           1       1.00      0.95      0.97        19\n",
      "           2       0.92      0.96      0.94        25\n",
      "           3       0.81      1.00      0.89        17\n",
      "           4       0.95      0.95      0.95        19\n",
      "           5       0.78      1.00      0.88        14\n",
      "           6       0.85      1.00      0.92        23\n",
      "           7       0.82      0.82      0.82        17\n",
      "           8       0.89      1.00      0.94        17\n",
      "           9       0.94      0.75      0.83        20\n",
      "          10       1.00      0.83      0.91        24\n",
      "          11       0.95      0.95      0.95        19\n",
      "          12       0.94      0.89      0.91        18\n",
      "          13       1.00      0.89      0.94        19\n",
      "          14       0.83      0.88      0.86        17\n",
      "          15       0.82      1.00      0.90         9\n",
      "          16       1.00      1.00      1.00        18\n",
      "          17       0.95      1.00      0.97        19\n",
      "          18       0.76      0.94      0.84        17\n",
      "          19       0.88      0.83      0.86        18\n",
      "          20       0.94      0.89      0.92        19\n",
      "          21       0.95      0.90      0.93        21\n",
      "          22       1.00      1.00      1.00        21\n",
      "          23       0.86      1.00      0.92        12\n",
      "          24       1.00      0.83      0.91        24\n",
      "          25       0.93      0.78      0.85        18\n",
      "          26       1.00      0.94      0.97        18\n",
      "          27       0.95      0.95      0.95        19\n",
      "          28       1.00      0.92      0.96        13\n",
      "          29       1.00      1.00      1.00        18\n",
      "          30       1.00      1.00      1.00        29\n",
      "          31       0.94      0.94      0.94        16\n",
      "          32       0.73      0.94      0.82        17\n",
      "          33       0.89      1.00      0.94        17\n",
      "          34       0.96      0.85      0.90        26\n",
      "          35       0.90      0.86      0.88        21\n",
      "          36       0.94      0.89      0.91        18\n",
      "          37       0.79      1.00      0.88        19\n",
      "\n",
      "    accuracy                           0.92       723\n",
      "   macro avg       0.92      0.92      0.92       723\n",
      "weighted avg       0.93      0.92      0.92       723\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "hidden_layers = (30,)\n",
    "activation = 'logistic'\n",
    "opts = dict(hidden_layer_sizes = hidden_layers,verbose = True,activation = activation,tol = 1e-6,max_iter = int(1e6))\n",
    "solver = 'adam'\n",
    "clf_MLP = MLPClassifier(solver = solver, **opts)\n",
    "clf_MLP.fit(Z_train,y_train)\n",
    "predictions = clf_MLP.predict(Z_test)\n",
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "訓練資料的classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        36\n",
      "           1       1.00      1.00      1.00        43\n",
      "           2       1.00      1.00      1.00        39\n",
      "           3       1.00      1.00      1.00        47\n",
      "           4       1.00      1.00      1.00        43\n",
      "           5       1.00      1.00      1.00        50\n",
      "           6       1.00      1.00      1.00        41\n",
      "           7       1.00      1.00      1.00        47\n",
      "           8       1.00      1.00      1.00        47\n",
      "           9       1.00      1.00      1.00        44\n",
      "          10       1.00      1.00      1.00        36\n",
      "          11       1.00      1.00      1.00        40\n",
      "          12       1.00      1.00      1.00        42\n",
      "          13       1.00      1.00      1.00        44\n",
      "          14       1.00      1.00      1.00        45\n",
      "          15       1.00      1.00      1.00        54\n",
      "          16       1.00      1.00      1.00        45\n",
      "          17       1.00      1.00      1.00        45\n",
      "          18       1.00      1.00      1.00        47\n",
      "          19       1.00      1.00      1.00        46\n",
      "          20       1.00      1.00      1.00        45\n",
      "          21       1.00      1.00      1.00        43\n",
      "          22       1.00      1.00      1.00        43\n",
      "          23       1.00      1.00      1.00        52\n",
      "          24       1.00      1.00      1.00        40\n",
      "          25       1.00      1.00      1.00        46\n",
      "          26       1.00      1.00      1.00        46\n",
      "          27       1.00      1.00      1.00        45\n",
      "          28       1.00      1.00      1.00        51\n",
      "          29       1.00      1.00      1.00        46\n",
      "          30       1.00      1.00      1.00        35\n",
      "          31       1.00      1.00      1.00        48\n",
      "          32       0.96      1.00      0.98        47\n",
      "          33       1.00      0.98      0.99        47\n",
      "          34       1.00      0.97      0.99        38\n",
      "          35       1.00      1.00      1.00        43\n",
      "          36       1.00      1.00      1.00        46\n",
      "          37       1.00      1.00      1.00        45\n",
      "\n",
      "    accuracy                           1.00      1687\n",
      "   macro avg       1.00      1.00      1.00      1687\n",
      "weighted avg       1.00      1.00      1.00      1687\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred = clf_MLP.predict(Z_train)\n",
    "print(classification_report(y_train,pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 結論:\n",
    "### 1. 當成分比例採0.95時，且只用 hidden layers = (30,) 準確率就能達到92%，因此不需要將hidden layers = (512,)，有可能會過度擬合，如果擴展到過多的神經元，其實資料無法提供足夠多的細節，也因此造成了許多無用、重複的數據。\n",
    "### 2. 訓練資料的準確率已達100%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
